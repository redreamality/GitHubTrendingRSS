<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="data:text/xsl;base64,<?xml version="1.0" encoding="utf-8"?><xsl:stylesheet version="3.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform" xmlns:atom="http://www.w3.org/2005/Atom"><xsl:output method="html" version="1.0" encoding="UTF-8" indent="yes"/><xsl:template match="/"><xsl:variable name="title"><xsl:value-of select="/rss/channel/title"/></xsl:variable><xsl:variable name="description"><xsl:value-of select="/rss/channel/description"/></xsl:variable><xsl:variable name="link"><xsl:value-of select="/rss/channel/link"/></xsl:variable><html class="dark scroll-smooth"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="referrer" content="unsafe-url"/><title><xsl:value-of select="$title"/></title><style>*,:after,:before{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-gradient-from-position: ;--tw-gradient-via-position: ;--tw-gradient-to-position: ;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgba(59,130,246,.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: ;--tw-contain-size: ;--tw-contain-layout: ;--tw-contain-paint: ;--tw-contain-style: }::backdrop{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-gradient-from-position: ;--tw-gradient-via-position: ;--tw-gradient-to-position: ;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgba(59,130,246,.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: ;--tw-contain-size: ;--tw-contain-layout: ;--tw-contain-paint: ;--tw-contain-style: }
        
        /*! tailwindcss v3.4.17 | MIT License | https://tailwindcss.com*/*,:after,:before{box-sizing:border-box;border:0 solid #e7e7f0}:after,:before{--tw-content:""}:host,html{line-height:1.5;-webkit-text-size-adjust:100%;-moz-tab-size:4;-o-tab-size:4;tab-size:4;font-family:ui-sans-serif,system-ui,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;font-feature-settings:normal;font-variation-settings:normal;-webkit-tap-highlight-color:transparent}body{margin:0;line-height:inherit}hr{height:0;color:inherit;border-top-width:1px}abbr:where([title]){-webkit-text-decoration:underline dotted;text-decoration:underline dotted}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}b,strong{font-weight:bolder}code,kbd,pre,samp{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-feature-settings:normal;font-variation-settings:normal;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}table{text-indent:0;border-color:inherit;border-collapse:collapse}button,input,optgroup,select,textarea{font-family:inherit;font-feature-settings:inherit;font-variation-settings:inherit;font-size:100%;font-weight:inherit;line-height:inherit;letter-spacing:inherit;color:inherit;margin:0;padding:0}button,select{text-transform:none}button,input:where([type=button]),input:where([type=reset]),input:where([type=submit]){-webkit-appearance:button;background-color:transparent;background-image:none}:-moz-focusring{outline:auto}:-moz-ui-invalid{box-shadow:none}progress{vertical-align:baseline}::-webkit-inner-spin-button,::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}summary{display:list-item}blockquote,dd,dl,figure,h1,h2,h3,h4,h5,h6,hr,p,pre{margin:0}fieldset{margin:0}fieldset,legend{padding:0}menu,ol,ul{list-style:none;margin:0;padding:0}dialog{padding:0}textarea{resize:vertical}input::-moz-placeholder,textarea::-moz-placeholder{opacity:1;color:#a8a8b8}input::placeholder,textarea::placeholder{opacity:1;color:#a8a8b8}[role=button],button{cursor:pointer}:disabled{cursor:default}audio,canvas,embed,iframe,img,object,svg,video{display:block;vertical-align:middle}img,video{max-width:100%;height:auto}[hidden]:where(:not([hidden=until-found])){display:none}:root{--card-radius:0.75rem;--btn-radius:var(--card-radius);--badge-radius:var(--btn-radius);--input-radius:var(--btn-radius);--avatar-radius:9999px;--annonce-radius:var(--avatar-radius);--ui-border-color:#1f1f31;--btn-border:#1f1f31;--badge-border:var(--btn-border);--input-border:var(--ui-border-color);--ui-disabled-border:#121220;--ui-error-border:#e11d48;--ui-success-border:#65a30d;--input-outline:#4f46e5;--ui-bg:rgb(18 18 32/var(--ui-bg-opacity));--ui-soft-bg:#1f1f31;--overlay-bg:rgba(2,2,13,.25);--input-bg:var(--ui-soft-bg);--ui-disabled-bg:#121220;--card-padding:1.5rem;--display-text-color:#fff;--title-text-color:var(--display-text-color);--body-text-color:#d6d6e1;--caption-text-color:#6e6e81;--placeholder-text-color:#4d4d5f;--ui-bg-opacity:1;color:var(--body-text-color)}*,.border{border-color:var(--ui-border-color)}button:disabled{border:none!important;background:var(--ui-disabled-bg)!important;background-image:none!important;box-shadow:none!important;color:var(--placeholder-text-color)!important;pointer-events:none!important}button:disabled:before{content:var(--tw-content);display:none}a:focus-visible,button:focus-visible{outline-width:2px;outline-offset:2px;outline-color:#4f46e5}a:focus-visible:focus-visible,button:focus-visible:focus-visible{outline-style:solid}input:user-invalid,select:user-invalid,textarea:user-invalid{--input-border:var(--ui-error-border);--ui-border-color:var(--ui-error-border);--input-outline:var(--ui-error-border);--title-text-color:#fb7185}[data-rounded=none]{--card-radius:0px;--avatar-radius:0px}[data-rounded=default]{--card-radius:0.25rem}[data-rounded=small]{--card-radius:0.125rem}[data-rounded=medium]{--card-radius:0.375rem}[data-rounded=large]{--card-radius:0.5rem}[data-rounded=xlarge]{--card-radius:0.75rem}[data-rounded="2xlarge"]{--card-radius:1rem;--input-radius:0.75rem}[data-rounded="3xlarge"]{--card-radius:1.5rem;--input-radius:0.75rem}[data-rounded=full]{--card-radius:1.5rem;--btn-radius:9999px;--input-radius:1rem}[data-shade=glassy]{--ui-bd-blur:40px;--ui-bg-opacity:0.75;--ui-bg:rgb(58 58 75/var(--ui-bg-opacity));--ui-border-color:rgba(250,250,254,.1);--ui-soft-bg:rgba(77,77,95,.5)}[data-shade="800"]{--ui-border-color:#3a3a4b;--ui-bg:#1f1f31;--ui-soft-bg:#121220}[data-shade="900"]{--ui-border-color:#1f1f31;--ui-bg:#121220;--ui-soft-bg:#1f1f31}[data-shade="950"]{--ui-border-color:#1f1f31;--ui-bg:#02020d;--ui-soft-bg:#1f1f31}.container{width:100%}@media (min-width:640px){.container{max-width:640px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:1024px){.container{max-width:1024px}}@media (min-width:1280px){.container{max-width:1280px}}@media (min-width:1536px){.container{max-width:1536px}}.icon-\[tabler--rss\]{display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24'%3E%3Cpath fill='none' stroke='%23000' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M4 19a1 1 0 1 0 2 0 1 1 0 1 0-2 0M4 4a16 16 0 0 1 16 16M4 11a9 9 0 0 1 9 9'/%3E%3C/svg%3E")}.link{--tw-text-opacity:1;color:rgb(129 140 248/var(--tw-text-opacity,1));transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,-webkit-backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter,-webkit-backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1);transition-duration:.15s}.link.variant-ghost:hover,.link.variant-underlined{text-decoration-line:underline}.link.variant-animated{position:relative}.link.variant-animated:before{position:absolute;left:0;right:0;bottom:0;height:1px;transform-origin:right;--tw-scale-x:0;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y));transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,-webkit-backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter,-webkit-backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1);content:var(--tw-content);transition-duration:.2s}.link.variant-animated:hover:before{transform-origin:left;content:var(--tw-content);--tw-scale-x:1;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.link.intent-info{--tw-text-opacity:1;color:rgb(96 165 250/var(--tw-text-opacity,1))}.link.intent-neutral{--tw-text-opacity:1;color:rgb(255 255 255/var(--tw-text-opacity,1))}.link.variant-animated.intent-neutral:before{content:var(--tw-content);background-color:hsla(0,0%,100%,.5)}.link.variant-animated.intent-info:before{content:var(--tw-content);--tw-bg-opacity:1;background-color:rgb(37 99 235/var(--tw-bg-opacity,1))}.link.variant-animated.intent-primary:before{content:var(--tw-content);--tw-bg-opacity:1;background-color:rgb(79 70 229/var(--tw-bg-opacity,1))}.link.variant-ghost.intent-neutral,.link.variant-underlined.intent-neutral{text-decoration-color:hsla(0,0%,100%,.5)}.mx-auto{margin-left:auto;margin-right:auto}.my-2{margin-top:.5rem;margin-bottom:.5rem}.my-6{margin-top:1.5rem;margin-bottom:1.5rem}.mb-2{margin-bottom:.5rem}.mb-6{margin-bottom:1.5rem}.mb-8{margin-bottom:2rem}.ml-1{margin-left:.25rem}.ml-4{margin-left:1rem}.mr-2{margin-right:.5rem}.mt-1{margin-top:.25rem}.mt-2{margin-top:.5rem}.mt-3{margin-top:.75rem}.block{display:block}.inline-block{display:inline-block}.inline{display:inline}.flex{display:flex}.grid{display:grid}.hidden{display:none}.h-4{height:1rem}.h-8{height:2rem}.min-h-screen{min-height:100vh}.min-h-svh{min-height:100svh}.w-4{width:1rem}.w-8{width:2rem}.max-w-full{max-width:100%}.max-w-screen-lg{max-width:1024px}.flex-1{flex:1 1 0%}.cursor-pointer{cursor:pointer}.list-disc{list-style-type:disc}.grid-cols-1{grid-template-columns:repeat(1,minmax(0,1fr))}.flex-col{flex-direction:column}.items-center{align-items:center}.justify-between{justify-content:space-between}.gap-4{gap:1rem}.gap-6{gap:1.5rem}.gap-8{gap:2rem}.space-y-2>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-top:calc(.5rem*(1 - var(--tw-space-y-reverse)));margin-bottom:calc(.5rem*var(--tw-space-y-reverse))}.space-y-3>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-top:calc(.75rem*(1 - var(--tw-space-y-reverse)));margin-bottom:calc(.75rem*var(--tw-space-y-reverse))}.space-y-4>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-top:calc(1rem*(1 - var(--tw-space-y-reverse)));margin-bottom:calc(1rem*var(--tw-space-y-reverse))}.space-y-6>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-top:calc(1.5rem*(1 - var(--tw-space-y-reverse)));margin-bottom:calc(1.5rem*var(--tw-space-y-reverse))}.scroll-smooth{scroll-behavior:smooth}.truncate{overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.bg-gray-925{--tw-bg-opacity:1;background-color:rgb(9 9 21/var(--tw-bg-opacity,1))}.bg-gradient-to-r{background-image:linear-gradient(to right,var(--tw-gradient-stops))}.from-primary-600{--tw-gradient-from:#4f46e5 var(--tw-gradient-from-position);--tw-gradient-to:rgba(79,70,229,0) var(--tw-gradient-to-position);--tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to)}.to-accent-400{--tw-gradient-to:#e879f9 var(--tw-gradient-to-position)}.bg-clip-text{-webkit-background-clip:text;background-clip:text}.p-1{padding:.25rem}.px-4{padding-left:1rem;padding-right:1rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.py-4{padding-top:1rem;padding-bottom:1rem}.py-6{padding-top:1.5rem;padding-bottom:1.5rem}.pl-5{padding-left:1.25rem}.pt-2{padding-top:.5rem}.text-center{text-align:center}.font-sans{font-family:ui-sans-serif,system-ui,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}.text-2xl{font-size:1.5rem;line-height:2rem}.text-lg{font-size:1.125rem;line-height:1.75rem}.text-sm{font-size:.875rem;line-height:1.25rem}.text-xs{font-size:.75rem;line-height:1rem}.font-bold{font-weight:700}.font-medium{font-weight:500}.font-semibold{font-weight:600}.leading-normal{line-height:1.5}.text-gray-400{--tw-text-opacity:1;color:rgb(168 168 184/var(--tw-text-opacity,1))}.text-gray-500{--tw-text-opacity:1;color:rgb(110 110 129/var(--tw-text-opacity,1))}.text-transparent{color:transparent}.antialiased{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}.transition{transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,-webkit-backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter,-webkit-backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1);transition-duration:.15s}.text-title{color:var(--title-text-color)}.text-body{color:var(--body-text-color)}.\!text-caption{color:var(--caption-text-color)!important}.text-caption{color:var(--caption-text-color)}.dark{--display-text-color:#fff;--title-text-color:var(--display-text-color);--caption-text-color:#6e6e81;--body-text-color:#d6d6e1;--placeholder-text-color:#4d4d5f;--ui-border-color:#232323}[data-shade="900"]:where(.dark,.dark *),[data-shade="925"]:where(.dark,.dark *),[data-shade="950"]:where(.dark,.dark *){--ui-border-color:#383838}.hover\:text-gray-300:hover{--tw-text-opacity:1;color:rgb(214 214 225/var(--tw-text-opacity,1))}.group[open] .group-open\:rotate-180{--tw-rotate:180deg;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}@media (min-width:768px){.md\:grid-cols-2{grid-template-columns:repeat(2,minmax(0,1fr))}.md\:p-4{padding:1rem}.md\:px-6{padding-left:1.5rem;padding-right:1.5rem}.md\:pt-6{padding-top:1.5rem}}@media (min-width:1024px){.lg\:grid-cols-3{grid-template-columns:repeat(3,minmax(0,1fr))}.lg\:dark\:bg-gray-900:is(.dark *){--tw-bg-opacity:1;background-color:rgb(18 18 32/var(--tw-bg-opacity,1))}}</style></head><body class="bg-gray-925 min-h-screen min-h-svh font-sans leading-normal antialiased lg:dark:bg-gray-900"><main class="min-w-screen container mx-auto flex min-h-screen max-w-screen-lg flex-col px-4 py-6 md:px-6"><header class="space-y-2 pt-2 md:pt-6"><a title="{$title}" href="{$link}" target="_blank" rel="noopener noreferrer"><h1 class="flex text-2xl"><span class="icon-[tabler--rss] mr-2 h-8 w-8"/><span class="lg2:text-3xl from-primary-600 to-accent-400 inline-block bg-gradient-to-r bg-clip-text font-bold text-transparent"><xsl:value-of select="$title" disable-output-escaping="yes"/></span></h1></a><p class="text-body pt-2 text-lg py-4"><xsl:value-of select="$description" disable-output-escaping="yes"/></p><p class="text-caption text-sm">
              This RSS feed for the
              <a class="link intent-neutral variant-animated !text-caption font-bold" title="{$title}" href="{$link}" target="_blank" rel="noopener noreferrer"><xsl:value-of select="$title"/></a>
              website.
            </p><p class="text-body text-sm hidden" id="subscribe-links">
              You can subscribe this RSS feed by
              <a class="link intent-neutral variant-animated font-bold" title="Feedly" data-href="https://feedly.com/i/subscription/feed/" target="_blank" rel="noopener noreferrer">Feedly</a>,
              <a class="link intent-neutral variant-animated font-bold" title="Inoreader" data-href="https://www.inoreader.com/feed/" target="_blank" rel="noopener noreferrer">Inoreader</a>,
              <a class="link intent-neutral variant-animated font-bold" title="Newsblur" data-href="https://www.newsblur.com/?url=" target="_blank" rel="noopener noreferrer">Newsblur</a>,
              <a class="link intent-neutral variant-animated font-bold" title="Follow" data-href="follow://add?url=" rel="noopener noreferrer">Follow</a>,
              <a class="link intent-neutral variant-animated font-bold" title="RSS Reader" data-href="feed:" data-raw="true" rel="noopener noreferrer">RSS Reader</a>
              or
              <a class="link intent-neutral variant-animated font-bold" title="{$title} 's feed source" data-href="" data-raw="true" rel="noopener noreferrer">View Source</a>.
            </p><script>
              document.addEventListener('DOMContentLoaded', function () {
                document.querySelectorAll('a[data-href]').forEach(function (a) {
                  const url = new URL(location.href)
                  const feed = url.searchParams.get('url') || location.href
                  const raw = a.getAttribute('data-raw')
                  if (raw) {
                    a.href = a.getAttribute('data-href') + feed
                  } else {
                    a.href = a.getAttribute('data-href') + encodeURIComponent(feed)
                  }
                })
                document.getElementById('subscribe-links').classList.remove('hidden')
              })
            </script></header><hr class="my-6"/><section class="flex-1 space-y-6 p-1 md:p-4"><xsl:choose><xsl:when test="/rss/channel/item"><xsl:for-each select="/rss/channel/item"><article class="space-y-2"><details><summary class="max-w-full truncate"><xsl:if test="title"><h2 class="text-title inline cursor-pointer text-lg font-semibold"><xsl:value-of select="title" disable-output-escaping="yes"/></h2></xsl:if><xsl:if test="pubDate"><time class="text-caption ml-4 mt-1 block text-sm"><xsl:value-of select="pubDate"/></time></xsl:if></summary><div class="text-body px-4 py-2"><p class="my-2"><xsl:choose><xsl:when test="description"><xsl:value-of select="description" disable-output-escaping="yes"/></xsl:when></xsl:choose></p><xsl:if test="link"><a class="link variant-animated intent-neutral font-bold" href="{link}" target="_blank" rel="noopener noreferrer">
                            Read More
                          </a></xsl:if></div></details></article></xsl:for-each></xsl:when><xsl:when test="/atom:feed/atom:entry"><xsl:for-each select="/atom:feed/atom:entry"><article class="space-y-2"><details><summary class="max-w-full truncate"><xsl:if test="atom:title"><h2 class="text-title inline cursor-pointer text-lg font-semibold"><xsl:value-of select="atom:title" disable-output-escaping="yes"/></h2></xsl:if><xsl:if test="atom:updated"><time class="text-caption ml-4 mt-1 block text-sm"><xsl:value-of select="atom:updated"/></time></xsl:if></summary><div class="text-body px-4 py-2"><p class="my-2"><xsl:choose><xsl:when test="atom:summary"><xsl:value-of select="atom:summary" disable-output-escaping="yes"/></xsl:when><xsl:when test="atom:content"><xsl:value-of select="atom:content" disable-output-escaping="yes"/></xsl:when></xsl:choose></p><xsl:if test="atom:link/@href"><a class="link variant-animated intent-neutral font-bold" href="{atom:link/@href}" target="_blank" rel="noopener noreferrer">
                            Read More
                          </a></xsl:if></div></details></article></xsl:for-each></xsl:when></xsl:choose></section><hr class="my-6"/><footer class="text-gray-400"><div class="container mx-auto px-4"><div class="mb-8"><h3 class="text-lg font-semibold text-title mb-6">Popular Feed Collections</h3><div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6"><div class="space-y-3"><a href="https://redreamality.com/GitHubTrendingRSS/weekly/python.xml" class="block hover:text-gray-300"><div class="font-medium">🐍 Python TrendWatch</div><div class="text-xs text-gray-500">AI, ML &amp; Data Science Innovation Feed</div></a><a href="https://redreamality.com/GitHubTrendingRSS/weekly/cuda.xml" class="block hover:text-gray-300"><div class="font-medium">⚡ CUDA Accelerator</div><div class="text-xs text-gray-500">GPU Computing &amp; Deep Learning Updates</div></a><a href="https://redreamality.com/GitHubTrendingRSS/weekly/matlab.xml" class="block hover:text-gray-300"><div class="font-medium">🧠 MATLAB TrendPulse</div><div class="text-xs text-gray-500">MEG, EEG and iEEG Research Feed</div></a></div><div class="space-y-3"><a href="https://redreamality.com/GitHubTrendingRSS/weekly/rust.xml" class="block hover:text-gray-300"><div class="font-medium">🦀 Rust Systems Feed</div><div class="text-xs text-gray-500">High-Performance &amp; Safe Systems Programming</div></a><a href="https://redreamality.com/GitHubTrendingRSS/weekly/go.xml" class="block hover:text-gray-300"><div class="font-medium">🚀 Go Infrastructure</div><div class="text-xs text-gray-500">Cloud Native &amp; DevOps Excellence</div></a><a href="https://redreamality.com/GitHubTrendingRSS/weekly/typescript.xml" class="block hover:text-gray-300"><div class="font-medium">📱 TypeScript Ecosystem</div><div class="text-xs text-gray-500">Modern Web &amp; App Development</div></a></div><div class="space-y-3"><a href="https://redreamality.com/GitHubTrendingRSS/daily/adblock-filter-list.xml" class="block hover:text-gray-300"><div class="font-medium">🛡️ Privacy Shield</div><div class="text-xs text-gray-500">AdBlock &amp; Security Updates</div></a><a href="https://redreamality.com/GitHubTrendingRSS/daily/all.xml" class="block hover:text-gray-300"><div class="font-medium">🌟 Global TechRadar</div><div class="text-xs text-gray-500">Cross-Language Innovation Pulse, add it to your RSS reader rsshub://github/trending/monthly/any/zh</div></a></div></div></div><div class="grid grid-cols-1 md:grid-cols-2 gap-8 mb-8"><div class="space-y-4"><h3 class="text-lg font-semibold text-title">Getting Started</h3><div class="grid grid-cols-1 gap-4"><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">📖 Feed Integration Guide</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">1. Choose Your RSS Reader:</p><ul class="list-disc pl-5 space-y-2"><li>Feedly: Professional choice for cross-platform sync</li><li>Inoreader: Advanced filtering capabilities</li><li>NetNewsWire: Perfect for macOS/iOS users</li><li>FreshRSS: Self-hosted option with full control</li></ul><p class="mt-3 mb-2">2. Add Our Feeds:</p><ul class="list-disc pl-5 space-y-2"><li>Copy the feed URL (e.g., rsshub://github/trending/monthly/any/zh)</li><li>In your RSS reader, look for "Add Feed" or "Subscribe"</li><li>Paste the URL and customize update frequency</li></ul></div></details><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">🎯 Custom Feed Creation</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Create Your Perfect Feed:</p><ul class="list-disc pl-5 space-y-2"><li>Language-specific: /GitHubTrendingRSS/[frequency]/[language].xml</li><li>Topic-focused: Combine multiple language feeds</li><li>Custom time ranges: daily, weekly, or monthly updates</li><li>Regional feeds: Focus on specific developer communities</li></ul><p class="mt-3">Pro tip: Use tags in your RSS reader to organize feeds by topic, language, or priority.</p></div></details><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">⚡ Feed Management Tips</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Optimize Your Feed Reading:</p><ul class="list-disc pl-5 space-y-2"><li>Set update frequencies based on feed importance</li><li>Use folders to group related feeds (e.g., AI/ML, Web Dev)</li><li>Enable notifications only for high-priority feeds</li><li>Archive valuable resources for future reference</li></ul><p class="mt-3">Advanced Features:</p><ul class="list-disc pl-5 space-y-2"><li>Filter feeds using keywords to focus on specific topics</li><li>Set up IFTTT integrations for automated workflows</li><li>Export/backup your feed collection regularly</li></ul></div></details></div></div><div class="space-y-4"><h3 class="text-lg font-semibold text-title">Common Questions</h3><div class="grid grid-cols-1 gap-4"><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">🤔 About Github Radar</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Github Radar is your intelligent curator for:</p><ul class="list-disc pl-5 space-y-2"><li>Trending repositories across all programming languages</li><li>Language-specific innovation and updates</li><li>Regional development trends and patterns</li><li>Open source community movements</li></ul><p class="mt-3">Our mission is to help developers stay updated with minimal effort through smart feed curation and organization.</p></div></details><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">📊 Feed Frequency Options</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Choose Your Update Rhythm:</p><ul class="list-disc pl-5 space-y-2"><li>Daily: Perfect for fast-moving technologies and security updates</li><li>Weekly: Ideal for maintaining awareness without overwhelm</li><li>Monthly: Best for long-term trend analysis and strategic planning</li></ul><p class="mt-3">Customize by combining different frequencies for different topics based on your needs.</p></div></details><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">🔧 Technical Support</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Supported RSS Readers:</p><ul class="list-disc pl-5 space-y-2"><li>Desktop: NetNewsWire, Reeder, FeedReader</li><li>Mobile: Feedly, Inoreader, NewsBlur</li><li>Self-hosted: FreshRSS, Tiny Tiny RSS</li><li>Browser-based: Feedbro, RSS Feed Reader</li></ul><p class="mt-3">Common Issues:</p><ul class="list-disc pl-5 space-y-2"><li>Feed not updating? Check your reader's refresh settings</li><li>Missing content? Verify your internet connection</li><li>Format issues? Try re-subscribing to the feed</li></ul></div></details></div></div></div><div class="text-center text-sm"><p class="mt-2">Acknowledgement: Page decorated by <a href="https://github.com/ccbikai/RSS.Beauty"><svg class="inline-block w-4 h-4 ml-1" viewBox="0 0 16 16" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg></a> RSS.Beauty</p></div></div></footer></main></body></html></xsl:template></xsl:stylesheet>"?>
<rss version="2.0">
  <channel>
    <title>GitHub C++ Weekly Trending</title>
    <description>Weekly Trending of C++ in GitHub</description>
    <pubDate>Sat, 29 Mar 2025 02:31:47 GMT</pubDate>
    <link>http://redreamality.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>opencv/opencv</title>
      <link>https://github.com/opencv/opencv</link>
      <description>&lt;p&gt;Open Source Computer Vision Library&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;OpenCV: Open Source Computer Vision Library&lt;/h2&gt; 
&lt;h3&gt;Resources&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Homepage: &lt;a href=&quot;https://opencv.org&quot;&gt;https://opencv.org&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Courses: &lt;a href=&quot;https://opencv.org/courses&quot;&gt;https://opencv.org/courses&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Docs: &lt;a href=&quot;https://docs.opencv.org/4.x/&quot;&gt;https://docs.opencv.org/4.x/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Q&amp;amp;A forum: &lt;a href=&quot;https://forum.opencv.org&quot;&gt;https://forum.opencv.org&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;previous forum (read only): &lt;a href=&quot;http://answers.opencv.org&quot;&gt;http://answers.opencv.org&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Issue tracking: &lt;a href=&quot;https://github.com/opencv/opencv/issues&quot;&gt;https://github.com/opencv/opencv/issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Additional OpenCV functionality: &lt;a href=&quot;https://github.com/opencv/opencv_contrib&quot;&gt;https://github.com/opencv/opencv_contrib&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Donate to OpenCV: &lt;a href=&quot;https://opencv.org/support/&quot;&gt;https://opencv.org/support/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;Please read the &lt;a href=&quot;https://github.com/opencv/opencv/wiki/How_to_contribute&quot;&gt;contribution guidelines&lt;/a&gt; before starting work on a pull request.&lt;/p&gt; 
&lt;h4&gt;Summary of the guidelines:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;One pull request per issue;&lt;/li&gt; 
 &lt;li&gt;Choose the right base branch;&lt;/li&gt; 
 &lt;li&gt;Include tests and documentation;&lt;/li&gt; 
 &lt;li&gt;Clean up &quot;oops&quot; commits before submitting;&lt;/li&gt; 
 &lt;li&gt;Follow the &lt;a href=&quot;https://github.com/opencv/opencv/wiki/Coding_Style_Guide&quot;&gt;coding style guide&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Additional Resources&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://form.jotform.com/233105358823151&quot;&gt;Submit your OpenCV-based project&lt;/a&gt; for inclusion in Community Friday on opencv.org&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://youtube.com/@opencvofficial&quot;&gt;Subscribe to the OpenCV YouTube Channel&lt;/a&gt; featuring OpenCV Live, an hour-long streaming show&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://linkedin.com/company/opencv/&quot;&gt;Follow OpenCV on LinkedIn&lt;/a&gt; for daily posts showing the state-of-the-art in computer vision &amp;amp; AI&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://form.jotform.com/232745316792159&quot;&gt;Apply to be an OpenCV Volunteer&lt;/a&gt; to help organize events and online campaigns as well as amplify them&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://mastodon.social/@opencv&quot;&gt;Follow OpenCV on Mastodon&lt;/a&gt; in the Fediverse&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://twitter.com/opencvlive&quot;&gt;Follow OpenCV on Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://opencv.ai&quot;&gt;OpenCV.ai&lt;/a&gt;: Computer Vision and AI development services from the OpenCV team.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>shadps4-emu/shadPS4</title>
      <link>https://github.com/shadps4-emu/shadPS4</link>
      <description>&lt;p&gt;PlayStation 4 emulator for Windows, Linux and macOS written in C++&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&quot;center&quot;&gt; &lt;br&gt; &lt;a href=&quot;https://shadps4.net/&quot;&gt;&lt;img src=&quot;https://github.com/shadps4-emu/shadPS4/raw/main/.github/shadps4.png&quot; width=&quot;220&quot;&gt;&lt;/a&gt; &lt;br&gt; &lt;b&gt;shadPS4&lt;/b&gt; &lt;br&gt; &lt;/h1&gt; 
&lt;h1 align=&quot;center&quot;&gt; &lt;a href=&quot;https://discord.gg/bFJxfftGW6&quot;&gt; &lt;img src=&quot;https://img.shields.io/discord/1080089157554155590?color=5865F2&amp;amp;label=shadPS4%20Discord&amp;amp;logo=Discord&amp;amp;logoColor=white&quot; width=&quot;240&quot;&gt; &lt;/a&gt;&lt;a href=&quot;https://github.com/shadps4-emu/shadPS4/releases/latest&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/downloads/shadps4-emu/shadPS4/total.svg?sanitize=true&quot; width=&quot;140&quot;&gt; &lt;/a&gt;&lt;a href=&quot;https://shadps4.net/&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/shadPS4-website-8A2BE2&quot; width=&quot;150&quot;&gt; &lt;/a&gt;&lt;a href=&quot;https://x.com/shadps4&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/-Join%20us-black?logo=X&amp;amp;logoColor=white&quot; width=&quot;100&quot;&gt; &lt;/a&gt;&lt;a href=&quot;https://github.com/shadps4-emu/shadPS4/stargazers&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/stars/shadps4-emu/shadPS4&quot; width=&quot;120&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;a href=&quot;https://github.com/shadps4-emu/shadPS4/stargazers&quot;&gt; &lt;/a&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/shadps4-emu/shadPS4/stargazers&quot;&gt; &lt;/a&gt;&lt;a href=&quot;https://shadps4.net/&quot;&gt; &lt;img src=&quot;https://github.com/shadps4-emu/shadPS4/raw/main/documents/Screenshots/1.png&quot; width=&quot;400&quot;&gt; &lt;img src=&quot;https://github.com/shadps4-emu/shadPS4/raw/main/documents/Screenshots/2.png&quot; width=&quot;400&quot;&gt; &lt;img src=&quot;https://github.com/shadps4-emu/shadPS4/raw/main/documents/Screenshots/3.png&quot; width=&quot;400&quot;&gt; &lt;img src=&quot;https://github.com/shadps4-emu/shadPS4/raw/main/documents/Screenshots/4.png&quot; width=&quot;400&quot;&gt; &lt;/a&gt;&lt;/p&gt;
&lt;a href=&quot;https://shadps4.net/&quot;&gt; &lt;h1&gt;General information&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;shadPS4&lt;/strong&gt; is an early &lt;strong&gt;PlayStation 4&lt;/strong&gt; emulator for &lt;strong&gt;Windows&lt;/strong&gt;, &lt;strong&gt;Linux&lt;/strong&gt; and &lt;strong&gt;macOS&lt;/strong&gt; written in C++.&lt;/p&gt; &lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://shadps4.net/&quot;&gt;If you encounter problems or have doubts, do not hesitate to look at the &lt;/a&gt;&lt;a href=&quot;https://github.com/shadps4-emu/shadPS4/raw/main/documents/Quickstart/Quickstart.md&quot;&gt;&lt;strong&gt;Quickstart&lt;/strong&gt;&lt;/a&gt;.&lt;br&gt; To verify that a game works, you can look at &lt;a href=&quot;https://github.com/shadps4-emu/shadps4-game-compatibility&quot;&gt;&lt;strong&gt;shadPS4 Game Compatibility&lt;/strong&gt;&lt;/a&gt;.&lt;br&gt; To discuss shadPS4 development, suggest ideas or to ask for help, join our &lt;a href=&quot;https://discord.gg/bFJxfftGW6&quot;&gt;&lt;strong&gt;Discord server&lt;/strong&gt;&lt;/a&gt;.&lt;br&gt; To get the latest news, go to our &lt;a href=&quot;https://x.com/shadps4&quot;&gt;&lt;strong&gt;X (Twitter)&lt;/strong&gt;&lt;/a&gt; or our &lt;a href=&quot;https://shadps4.net/&quot;&gt;&lt;strong&gt;website&lt;/strong&gt;&lt;/a&gt;.&lt;br&gt; For those who&#39;d like to donate to the project, we now have a &lt;a href=&quot;https://ko-fi.com/shadps4&quot;&gt;&lt;strong&gt;Kofi page&lt;/strong&gt;&lt;/a&gt;!&lt;/p&gt; 
&lt;h1&gt;Status&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] shadPS4 is early in development, don&#39;t expect a flawless experience.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Currently, the emulator can successfully run games like &lt;a href=&quot;https://www.youtube.com/watch?v=wC6s0avpQRE&quot;&gt;&lt;strong&gt;Bloodborne&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/watch?v=-3PA-Xwszts&quot;&gt;&lt;strong&gt;Dark Souls Remastered&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/watch?v=Al7yz_5nLag&quot;&gt;&lt;strong&gt;Red Dead Redemption&lt;/strong&gt;&lt;/a&gt; and many other games.&lt;/p&gt; 
&lt;h1&gt;Why&lt;/h1&gt; 
&lt;p&gt;This project began as a fun project. Given our limited free time, it may take some time before shadPS4 can run more complex games, but we&#39;re committed to making small, regular updates.&lt;/p&gt; 
&lt;h1&gt;Building&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] If you want to use shadPS4 to play your games, you don&#39;t have to follow the build instructions, you can simply download the emulator from either the &lt;a href=&quot;https://github.com/shadps4-emu/shadPS4/releases&quot;&gt;&lt;strong&gt;release tab&lt;/strong&gt;&lt;/a&gt; or the &lt;a href=&quot;https://github.com/shadps4-emu/shadPS4/actions&quot;&gt;&lt;strong&gt;action tab&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Windows&lt;/h2&gt; 
&lt;p&gt;Check the build instructions for &lt;a href=&quot;https://github.com/shadps4-emu/shadPS4/raw/main/documents/building-windows.md&quot;&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Linux&lt;/h2&gt; 
&lt;p&gt;Check the build instructions for &lt;a href=&quot;https://github.com/shadps4-emu/shadPS4/raw/main/documents/building-linux.md&quot;&gt;&lt;strong&gt;Linux&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;macOS&lt;/h2&gt; 
&lt;p&gt;Check the build instructions for &lt;a href=&quot;https://github.com/shadps4-emu/shadPS4/raw/main/documents/building-macos.md&quot;&gt;&lt;strong&gt;macOS&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] macOS users need at least macOS 15 on Apple Silicon-based Mac devices and at least macOS 14 on Intel-based Mac devices.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;Debugging and reporting issues&lt;/h1&gt; 
&lt;p&gt;For more information on how to test, debug and report issues with the emulator or games, read the &lt;a href=&quot;https://github.com/shadps4-emu/shadPS4/raw/main/documents/Debugging/Debugging.md&quot;&gt;&lt;strong&gt;Debugging documentation&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Keyboard and Mouse Mappings&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Some keyboards may also require you to hold the Fn key to use the F* keys. Mac users should use the Command key instead of Control, and need to use Command+F11 for full screen to avoid conflicting with system key bindings.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Button&lt;/th&gt; 
   &lt;th&gt;Function&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;F10&lt;/td&gt; 
   &lt;td&gt;FPS Counter&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ctrl+F10&lt;/td&gt; 
   &lt;td&gt;Video Debug Info&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;F11&lt;/td&gt; 
   &lt;td&gt;Fullscreen&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;F12&lt;/td&gt; 
   &lt;td&gt;Trigger RenderDoc Capture&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Xbox and DualShock controllers work out of the box.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Controller button&lt;/th&gt; 
   &lt;th&gt;Keyboard equivalent&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LEFT AXIS UP&lt;/td&gt; 
   &lt;td&gt;W&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LEFT AXIS DOWN&lt;/td&gt; 
   &lt;td&gt;S&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LEFT AXIS LEFT&lt;/td&gt; 
   &lt;td&gt;A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LEFT AXIS RIGHT&lt;/td&gt; 
   &lt;td&gt;D&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RIGHT AXIS UP&lt;/td&gt; 
   &lt;td&gt;I&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RIGHT AXIS DOWN&lt;/td&gt; 
   &lt;td&gt;K&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RIGHT AXIS LEFT&lt;/td&gt; 
   &lt;td&gt;J&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RIGHT AXIS RIGHT&lt;/td&gt; 
   &lt;td&gt;L&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TRIANGLE&lt;/td&gt; 
   &lt;td&gt;Numpad 8 or C&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CIRCLE&lt;/td&gt; 
   &lt;td&gt;Numpad 6 or B&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CROSS&lt;/td&gt; 
   &lt;td&gt;Numpad 2 or N&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SQUARE&lt;/td&gt; 
   &lt;td&gt;Numpad 4 or V&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PAD UP&lt;/td&gt; 
   &lt;td&gt;UP&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PAD DOWN&lt;/td&gt; 
   &lt;td&gt;DOWN&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PAD LEFT&lt;/td&gt; 
   &lt;td&gt;LEFT&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PAD RIGHT&lt;/td&gt; 
   &lt;td&gt;RIGHT&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OPTIONS&lt;/td&gt; 
   &lt;td&gt;RETURN&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BACK BUTTON / TOUCH PAD&lt;/td&gt; 
   &lt;td&gt;SPACE&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L1&lt;/td&gt; 
   &lt;td&gt;Q&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;R1&lt;/td&gt; 
   &lt;td&gt;U&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L2&lt;/td&gt; 
   &lt;td&gt;E&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;R2&lt;/td&gt; 
   &lt;td&gt;O&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;L3&lt;/td&gt; 
   &lt;td&gt;X&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;R3&lt;/td&gt; 
   &lt;td&gt;M&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Keyboard and mouse inputs can be customized in the settings menu by clicking the Controller button, and further details and help on controls are also found there. Custom bindings are saved per-game. Inputs support up to three keys per binding, mouse buttons, mouse movement mapped to joystick input, and more.&lt;/p&gt; 
&lt;h1&gt;Main team&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/georgemoralis&quot;&gt;&lt;strong&gt;georgemoralis&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/raphaelthegreat&quot;&gt;&lt;strong&gt;raphaelthegreat&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/psucien&quot;&gt;&lt;strong&gt;psucien&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/skmp&quot;&gt;&lt;strong&gt;skmp&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/wheremyfoodat&quot;&gt;&lt;strong&gt;wheremyfoodat&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/raziel1000&quot;&gt;&lt;strong&gt;raziel1000&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/viniciuslrangel&quot;&gt;&lt;strong&gt;viniciuslrangel&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/vladmikhalin&quot;&gt;&lt;strong&gt;roamic&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/polybiusproxy&quot;&gt;&lt;strong&gt;poly&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/squidbus&quot;&gt;&lt;strong&gt;squidbus&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/baggins183&quot;&gt;&lt;strong&gt;frodo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Logo is done by &lt;a href=&quot;https://github.com/Xphalnos&quot;&gt;&lt;strong&gt;Xphalnos&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;If you want to contribute, please look the &lt;a href=&quot;https://github.com/shadps4-emu/shadPS4/raw/main/CONTRIBUTING.md&quot;&gt;&lt;strong&gt;CONTRIBUTING.md&lt;/strong&gt;&lt;/a&gt; file.&lt;br&gt; Open a PR and we&#39;ll check it :)&lt;/p&gt; 
&lt;h1&gt;Translations&lt;/h1&gt; 
&lt;p&gt;If you want to translate shadPS4 to your language we use &lt;a href=&quot;https://crowdin.com/project/shadps4-emulator&quot;&gt;&lt;strong&gt;crowdin&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Contributors&lt;/h1&gt; 
&lt;a href=&quot;https://github.com/shadps4-emu/shadPS4/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=shadps4-emu/shadPS4&amp;amp;max=15&quot;&gt; &lt;/a&gt; 
&lt;h1&gt;Special Thanks&lt;/h1&gt; 
&lt;p&gt;A few noteworthy teams/projects who&#39;ve helped us along the way are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/wheremyfoodat/Panda3DS&quot;&gt;&lt;strong&gt;Panda3DS&lt;/strong&gt;&lt;/a&gt;: A multiplatform 3DS emulator from our co-author wheremyfoodat. They have been incredibly helpful in understanding and solving problems that came up from natively executing the x64 code of PS4 binaries&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/red-prig/fpPS4&quot;&gt;&lt;strong&gt;fpPS4&lt;/strong&gt;&lt;/a&gt;: The fpPS4 team has assisted massively with understanding some of the more complex parts of the PS4 operating system and libraries, by helping with reverse engineering work and research.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;yuzu&lt;/strong&gt;: Our shader compiler has been designed with yuzu&#39;s Hades compiler as a blueprint. This allowed us to focus on the challenges of emulating a modern AMD GPU while having a high-quality optimizing shader compiler implementation as a base.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/OFFTKP/felix86&quot;&gt;&lt;strong&gt;felix86&lt;/strong&gt;&lt;/a&gt;: A new x86-64  RISC-V Linux userspace emulator&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/shadps4-emu/shadPS4/raw/main/LICENSE&quot;&gt;&lt;strong&gt;GPL-2.0 license&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>k2-fsa/sherpa-onnx</title>
      <link>https://github.com/k2-fsa/sherpa-onnx</link>
      <description>&lt;p&gt;Speech-to-text, text-to-speech, speaker diarization, speech enhancement, and VAD using next-gen Kaldi with onnxruntime without Internet connection. Support embedded systems, Android, iOS, HarmonyOS, Raspberry Pi, RISC-V, x86_64 servers, websocket server/client, support 11 programming languages&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;Supported functions&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Speech recognition&lt;/th&gt; 
   &lt;th&gt;Speech synthesis&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Speaker identification&lt;/th&gt; 
   &lt;th&gt;Speaker diarization&lt;/th&gt; 
   &lt;th&gt;Speaker verification&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Spoken Language identification&lt;/th&gt; 
   &lt;th&gt;Audio tagging&lt;/th&gt; 
   &lt;th&gt;Voice activity detection&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Keyword spotting&lt;/th&gt; 
   &lt;th&gt;Add punctuation&lt;/th&gt; 
   &lt;th&gt;Speech enhancement&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Supported platforms&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Architecture&lt;/th&gt; 
   &lt;th&gt;Android&lt;/th&gt; 
   &lt;th&gt;iOS&lt;/th&gt; 
   &lt;th&gt;Windows&lt;/th&gt; 
   &lt;th&gt;macOS&lt;/th&gt; 
   &lt;th&gt;linux&lt;/th&gt; 
   &lt;th&gt;HarmonyOS&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;x64&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;arm64&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;arm32&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;riscv64&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Supported programming languages&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;1. C++&lt;/th&gt; 
   &lt;th&gt;2. C&lt;/th&gt; 
   &lt;th&gt;3. Python&lt;/th&gt; 
   &lt;th&gt;4. JavaScript&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;5. Java&lt;/th&gt; 
   &lt;th&gt;6. C#&lt;/th&gt; 
   &lt;th&gt;7. Kotlin&lt;/th&gt; 
   &lt;th&gt;8. Swift&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;9. Go&lt;/th&gt; 
   &lt;th&gt;10. Dart&lt;/th&gt; 
   &lt;th&gt;11. Rust&lt;/th&gt; 
   &lt;th&gt;12. Pascal&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For Rust support, please see &lt;a href=&quot;https://github.com/thewh1teagle/sherpa-rs&quot;&gt;sherpa-rs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;It also supports WebAssembly.&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;This repository supports running the following functions &lt;strong&gt;locally&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Speech-to-text (i.e., ASR); both streaming and non-streaming are supported&lt;/li&gt; 
 &lt;li&gt;Text-to-speech (i.e., TTS)&lt;/li&gt; 
 &lt;li&gt;Speaker diarization&lt;/li&gt; 
 &lt;li&gt;Speaker identification&lt;/li&gt; 
 &lt;li&gt;Speaker verification&lt;/li&gt; 
 &lt;li&gt;Spoken language identification&lt;/li&gt; 
 &lt;li&gt;Audio tagging&lt;/li&gt; 
 &lt;li&gt;VAD (e.g., &lt;a href=&quot;https://github.com/snakers4/silero-vad&quot;&gt;silero-vad&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Keyword spotting&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;on the following platforms and operating systems:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;x86, &lt;code&gt;x86_64&lt;/code&gt;, 32-bit ARM, 64-bit ARM (arm64, aarch64), RISC-V (riscv64)&lt;/li&gt; 
 &lt;li&gt;Linux, macOS, Windows, openKylin&lt;/li&gt; 
 &lt;li&gt;Android, WearOS&lt;/li&gt; 
 &lt;li&gt;iOS&lt;/li&gt; 
 &lt;li&gt;HarmonyOS&lt;/li&gt; 
 &lt;li&gt;NodeJS&lt;/li&gt; 
 &lt;li&gt;WebAssembly&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://developer.download.nvidia.com/assets/embedded/secure/jetson/orin_nx/docs/Jetson_Orin_NX_DS-10712-001_v0.5.pdf?RCPGu9Q6OVAOv7a7vgtwc9-BLScXRIWq6cSLuditMALECJ_dOj27DgnqAPGVnT2VpiNpQan9SyFy-9zRykR58CokzbXwjSA7Gj819e91AXPrWkGZR3oS1VLxiDEpJa_Y0lr7UT-N4GnXtb8NlUkP4GkCkkF_FQivGPrAucCUywL481GH_WpP_p7ziHU1Wg==&amp;amp;t=eyJscyI6ImdzZW8iLCJsc2QiOiJodHRwczovL3d3dy5nb29nbGUuY29tLmhrLyJ9&quot;&gt;NVIDIA Jetson Orin NX&lt;/a&gt; (Support running on both CPU and GPU)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.seeedstudio.com/blog/2020/01/16/new-revision-of-jetson-nano-dev-kit-now-supports-new-jetson-nano-module/&quot;&gt;NVIDIA Jetson Nano B01&lt;/a&gt; (Support running on both CPU and GPU)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.raspberrypi.com/&quot;&gt;Raspberry Pi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.rock-chips.com/uploads/pdf/2022.8.26/191/RV1126%20Brief%20Datasheet.pdf&quot;&gt;RV1126&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://sipeed.com/licheepi4a&quot;&gt;LicheePi4A&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.starfivetech.com/en/site/boards&quot;&gt;VisionFive 2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://developer.horizon.ai/api/v1/fileData/documents_pi/index.html&quot;&gt;X3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://wiki.sipeed.com/hardware/zh/maixIII/ax-pi/axpi.html&quot;&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;etc&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;with the following APIs&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;C++, C, Python, Go, &lt;code&gt;C#&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Java, Kotlin, JavaScript&lt;/li&gt; 
 &lt;li&gt;Swift, Rust&lt;/li&gt; 
 &lt;li&gt;Dart, Object Pascal&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Links for Huggingface Spaces&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;You can visit the following Huggingface spaces to try sherpa-onnx without installing anything. All you need is a browser.&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;URL&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speaker diarization&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/speaker-diarization&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speech recognition&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/automatic-speech-recognition&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speech recognition with &lt;a href=&quot;https://github.com/openai/whisper&quot;&gt;Whisper&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/automatic-speech-recognition-with-whisper&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speech synthesis&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/text-to-speech&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Generate subtitles&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/generate-subtitles-for-videos&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Audio tagging&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/audio-tagging&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Spoken language identification with &lt;a href=&quot;https://github.com/openai/whisper&quot;&gt;Whisper&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/spoken-language-identification&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;We also have spaces built using WebAssembly. They are listed below:&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Huggingface space&lt;/th&gt; 
    &lt;th&gt;ModelScope space&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Voice activity detection with &lt;a href=&quot;https://github.com/snakers4/silero-vad&quot;&gt;silero-vad&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-sherpa-onnx&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://modelscope.cn/studios/csukuangfj/web-assembly-vad-sherpa-onnx&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Real-time speech recognition (Chinese + English) with Zipformer&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-asr-sherpa-onnx-zh-en&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-asr-sherpa-onnx-zh-en&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Real-time speech recognition (Chinese + English) with Paraformer&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-asr-sherpa-onnx-zh-en-paraformer&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://modelscope.cn/studios/k2-fsa/web-assembly-asr-sherpa-onnx-zh-en-paraformer&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Real-time speech recognition (Chinese + English + Cantonese) with &lt;a href=&quot;https://www.modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary&quot;&gt;Paraformer-large&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-asr-sherpa-onnx-zh-cantonese-en-paraformer&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://modelscope.cn/studios/k2-fsa/web-assembly-asr-sherpa-onnx-zh-cantonese-en-paraformer&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Real-time speech recognition (English)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-asr-sherpa-onnx-en&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://modelscope.cn/studios/k2-fsa/web-assembly-asr-sherpa-onnx-en&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (Chinese + English + Korean + Japanese + Cantonese) with &lt;a href=&quot;https://github.com/FunAudioLLM/SenseVoice&quot;&gt;SenseVoice&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-en-ja-ko-cantonese-sense-voice&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/csukuangfj/web-assembly-vad-asr-sherpa-onnx-zh-en-jp-ko-cantonese-sense-voice&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (English) with &lt;a href=&quot;https://github.com/openai/whisper&quot;&gt;Whisper&lt;/a&gt; tiny.en&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-en-whisper-tiny&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/csukuangfj/web-assembly-vad-asr-sherpa-onnx-en-whisper-tiny&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (English) with &lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-moonshine-tiny-en-int8.tar.bz2&quot;&gt;Moonshine tiny&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-en-moonshine-tiny&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/csukuangfj/web-assembly-vad-asr-sherpa-onnx-en-moonshine-tiny&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (English) with Zipformer trained with &lt;a href=&quot;https://github.com/SpeechColab/GigaSpeech&quot;&gt;GigaSpeech&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-en-zipformer-gigaspeech&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/k2-fsa/web-assembly-vad-asr-sherpa-onnx-en-zipformer-gigaspeech&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (Chinese) with Zipformer trained with &lt;a href=&quot;https://github.com/wenet-e2e/WenetSpeech&quot;&gt;WenetSpeech&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-zipformer-wenetspeech&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-zipformer-wenetspeech&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (Japanese) with Zipformer trained with &lt;a href=&quot;https://research.reazon.jp/_static/reazonspeech_nlp2023.pdf&quot;&gt;ReazonSpeech&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-ja-zipformer&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/csukuangfj/web-assembly-vad-asr-sherpa-onnx-ja-zipformer&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (Thai) with Zipformer trained with &lt;a href=&quot;https://github.com/SpeechColab/GigaSpeech2&quot;&gt;GigaSpeech2&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-th-zipformer&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/csukuangfj/web-assembly-vad-asr-sherpa-onnx-th-zipformer&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (Chinese ) with a &lt;a href=&quot;https://github.com/Tele-AI/TeleSpeech-ASR&quot;&gt;TeleSpeech-ASR&lt;/a&gt; CTC model&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-telespeech&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-telespeech&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (English + Chinese, ) with Paraformer-large&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-en-paraformer&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-en-paraformer&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (English + Chinese, ) with Paraformer-small&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-en-paraformer-small&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-en-paraformer-small&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speech synthesis (English)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-tts-sherpa-onnx-en&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://modelscope.cn/studios/k2-fsa/web-assembly-tts-sherpa-onnx-en&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speech synthesis (German)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-tts-sherpa-onnx-de&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://modelscope.cn/studios/k2-fsa/web-assembly-tts-sherpa-onnx-de&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speaker diarization&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-speaker-diarization-sherpa-onnx&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/csukuangfj/web-assembly-speaker-diarization-sherpa-onnx&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Links for pre-built Android APKs&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;You can find pre-built Android APKs for this repository in the following table&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;URL&lt;/th&gt; 
    &lt;th&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speaker diarization&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/speaker-diarization/apk.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/speaker-diarization/apk-cn.html&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Streaming speech recognition&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/android/apk.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/android/apk-cn.html&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Text-to-speech&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/tts/apk-engine.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/tts/apk-engine-cn.html&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Voice activity detection (VAD)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/vad/apk.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/vad/apk-cn.html&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + non-streaming speech recognition&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/vad/apk-asr.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/vad/apk-asr-cn.html&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Two-pass speech recognition&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/android/apk-2pass.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/android/apk-2pass-cn.html&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Audio tagging&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/audio-tagging/apk.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/audio-tagging/apk-cn.html&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Audio tagging (WearOS)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/audio-tagging/apk-wearos.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/audio-tagging/apk-wearos-cn.html&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speaker identification&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/speaker-identification/apk.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/speaker-identification/apk-cn.html&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Spoken language identification&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/spoken-language-identification/apk.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/spoken-language-identification/apk-cn.html&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Keyword spotting&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/kws/apk.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/kws/apk-cn.html&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Links for pre-built Flutter APPs&lt;/h3&gt; 
&lt;details&gt; 
 &lt;h4&gt;Real-time speech recognition&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;URL&lt;/th&gt; 
    &lt;th&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Streaming speech recognition&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/asr/app.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/asr/app-cn.html&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h4&gt;Text-to-speech&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;URL&lt;/th&gt; 
    &lt;th&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Android (arm64-v8a, armeabi-v7a, x86_64)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-android.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-android-cn.html&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Linux (x64)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-linux.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-linux-cn.html&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;macOS (x64)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-macos-x64.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-macos-x64-cn.html&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;macOS (arm64)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-macos-arm64.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-macos-arm64-cn.html&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Windows (x64)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-win.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-win-cn.html&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Note: You need to build from source for iOS.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;h3&gt;Links for pre-built Lazarus APPs&lt;/h3&gt; 
&lt;details&gt; 
 &lt;h4&gt;Generating subtitles&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;URL&lt;/th&gt; 
    &lt;th&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Generate subtitles ()&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/lazarus/download-generated-subtitles.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/lazarus/download-generated-subtitles-cn.html&quot;&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Links for pre-trained models&lt;/h3&gt; 
&lt;details&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;URL&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speech recognition (speech to text, ASR)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Text-to-speech (TTS)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/tts-models&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/silero_vad.onnx&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Keyword spotting&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/kws-models&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Audio tagging&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/audio-tagging-models&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speaker identification (Speaker ID)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/speaker-recongition-models&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Spoken language identification (Language ID)&lt;/td&gt; 
    &lt;td&gt;See multi-lingual &lt;a href=&quot;https://github.com/openai/whisper&quot;&gt;Whisper&lt;/a&gt; ASR models from &lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models&quot;&gt;Speech recognition&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Punctuation&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/punctuation-models&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speaker segmentation&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/speaker-segmentation-models&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speech enhancement&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/speech-enhancement-models&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h4&gt;Some pre-trained ASR models (Streaming)&lt;/h4&gt; 
&lt;details&gt; 
 &lt;p&gt;Please see&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-transducer/index.html&quot;&gt;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-transducer/index.html&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-paraformer/index.html&quot;&gt;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-paraformer/index.html&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-ctc/index.html&quot;&gt;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-ctc/index.html&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;for more models. The following table lists only &lt;strong&gt;SOME&lt;/strong&gt; of them.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Name&lt;/th&gt; 
    &lt;th&gt;Supported Languages&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-streaming-zipformer-bilingual-zh-en-2023-02-20.tar.bz2&quot;&gt;sherpa-onnx-streaming-zipformer-bilingual-zh-en-2023-02-20&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Chinese, English&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-transducer/zipformer-transducer-models.html#csukuangfj-sherpa-onnx-streaming-zipformer-bilingual-zh-en-2023-02-20-bilingual-chinese-english&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-streaming-zipformer-small-bilingual-zh-en-2023-02-16.tar.bz2&quot;&gt;sherpa-onnx-streaming-zipformer-small-bilingual-zh-en-2023-02-16&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Chinese, English&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-transducer/zipformer-transducer-models.html#sherpa-onnx-streaming-zipformer-small-bilingual-zh-en-2023-02-16-bilingual-chinese-english&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23.tar.bz2&quot;&gt;sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Chinese&lt;/td&gt; 
    &lt;td&gt;Suitable for Cortex A7 CPU. See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-transducer/zipformer-transducer-models.html#sherpa-onnx-streaming-zipformer-zh-14m-2023-02-23&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-streaming-zipformer-en-20M-2023-02-17.tar.bz2&quot;&gt;sherpa-onnx-streaming-zipformer-en-20M-2023-02-17&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;English&lt;/td&gt; 
    &lt;td&gt;Suitable for Cortex A7 CPU. See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-transducer/zipformer-transducer-models.html#sherpa-onnx-streaming-zipformer-en-20m-2023-02-17&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-streaming-zipformer-korean-2024-06-16.tar.bz2&quot;&gt;sherpa-onnx-streaming-zipformer-korean-2024-06-16&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Korean&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-transducer/zipformer-transducer-models.html#sherpa-onnx-streaming-zipformer-korean-2024-06-16-korean&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-streaming-zipformer-fr-2023-04-14.tar.bz2&quot;&gt;sherpa-onnx-streaming-zipformer-fr-2023-04-14&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;French&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-transducer/zipformer-transducer-models.html#shaojieli-sherpa-onnx-streaming-zipformer-fr-2023-04-14-french&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h4&gt;Some pre-trained ASR models (Non-Streaming)&lt;/h4&gt; 
&lt;details&gt; 
 &lt;p&gt;Please see&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/index.html&quot;&gt;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/index.html&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-paraformer/index.html&quot;&gt;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-paraformer/index.html&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-ctc/index.html&quot;&gt;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-ctc/index.html&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/telespeech/index.html&quot;&gt;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/telespeech/index.html&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/whisper/index.html&quot;&gt;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/whisper/index.html&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;for more models. The following table lists only &lt;strong&gt;SOME&lt;/strong&gt; of them.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Name&lt;/th&gt; 
    &lt;th&gt;Supported Languages&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-whisper-tiny.en.tar.bz2&quot;&gt;Whisper tiny.en&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;English&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/whisper/tiny.en.html&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-moonshine-tiny-en-int8.tar.bz2&quot;&gt;Moonshine tiny&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;English&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://github.com/usefulsensors/moonshine&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2&quot;&gt;sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Chinese, Cantonese, English, Korean, Japanese&lt;/td&gt; 
    &lt;td&gt;. See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/sense-voice/index.html&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-paraformer-zh-2024-03-09.tar.bz2&quot;&gt;sherpa-onnx-paraformer-zh-2024-03-09&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Chinese, English&lt;/td&gt; 
    &lt;td&gt;. See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-paraformer/paraformer-models.html#csukuangfj-sherpa-onnx-paraformer-zh-2024-03-09-chinese-english&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-zipformer-ja-reazonspeech-2024-08-01.tar.bz2&quot;&gt;sherpa-onnx-zipformer-ja-reazonspeech-2024-08-01&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Japanese&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/zipformer-transducer-models.html#sherpa-onnx-zipformer-ja-reazonspeech-2024-08-01-japanese&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-nemo-transducer-giga-am-russian-2024-10-24.tar.bz2&quot;&gt;sherpa-onnx-nemo-transducer-giga-am-russian-2024-10-24&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Russian&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/nemo-transducer-models.html#sherpa-onnx-nemo-transducer-giga-am-russian-2024-10-24-russian&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-nemo-ctc-giga-am-russian-2024-10-24.tar.bz2&quot;&gt;sherpa-onnx-nemo-ctc-giga-am-russian-2024-10-24&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Russian&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-ctc/nemo/russian.html#sherpa-onnx-nemo-ctc-giga-am-russian-2024-10-24&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-zipformer-ru-2024-09-18.tar.bz2&quot;&gt;sherpa-onnx-zipformer-ru-2024-09-18&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Russian&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/zipformer-transducer-models.html#sherpa-onnx-zipformer-ru-2024-09-18-russian&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-zipformer-korean-2024-06-24.tar.bz2&quot;&gt;sherpa-onnx-zipformer-korean-2024-06-24&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Korean&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/zipformer-transducer-models.html#sherpa-onnx-zipformer-korean-2024-06-24-korean&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-zipformer-thai-2024-06-20.tar.bz2&quot;&gt;sherpa-onnx-zipformer-thai-2024-06-20&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Thai&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/zipformer-transducer-models.html#sherpa-onnx-zipformer-thai-2024-06-20-thai&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-telespeech-ctc-int8-zh-2024-06-04.tar.bz2&quot;&gt;sherpa-onnx-telespeech-ctc-int8-zh-2024-06-04&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Chinese&lt;/td&gt; 
    &lt;td&gt;. See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/telespeech/models.html#sherpa-onnx-telespeech-ctc-int8-zh-2024-06-04&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Useful links&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Documentation: &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/&quot;&gt;https://k2-fsa.github.io/sherpa/onnx/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Bilibili : &lt;a href=&quot;https://search.bilibili.com/all?keyword=%E6%96%B0%E4%B8%80%E4%BB%A3Kaldi&quot;&gt;https://search.bilibili.com/all?keyword=%E6%96%B0%E4%B8%80%E4%BB%A3Kaldi&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;How to reach us&lt;/h3&gt; 
&lt;p&gt;Please see &lt;a href=&quot;https://k2-fsa.github.io/sherpa/social-groups.html&quot;&gt;https://k2-fsa.github.io/sherpa/social-groups.html&lt;/a&gt; for  Kaldi &lt;strong&gt;&lt;/strong&gt; and &lt;strong&gt;QQ &lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;Projects using sherpa-onnx&lt;/h2&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/t41372/Open-LLM-VTuber&quot;&gt;Open-LLM-VTuber&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Talk to any LLM with hands-free voice interaction, voice interruption, and Live2D taking face running locally across platforms&lt;/p&gt; 
&lt;p&gt;See also &lt;a href=&quot;https://github.com/t41372/Open-LLM-VTuber/pull/50&quot;&gt;https://github.com/t41372/Open-LLM-VTuber/pull/50&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/ruzhila/voiceapi&quot;&gt;voiceapi&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Streaming ASR and TTS based on FastAPI&lt;/summary&gt; 
 &lt;p&gt;It shows how to use the ASR and TTS Python APIs with FastAPI.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/jxlpzqc/TMSpeech&quot;&gt; TMSpeech&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Uses streaming ASR in C# with graphical user interface.&lt;/p&gt; 
&lt;p&gt;Video demo in Chinese: &lt;a href=&quot;https://www.bilibili.com/video/BV1rX4y1p7Nx&quot;&gt;Windows/&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/l1veIn/lol-wom-electron&quot;&gt;lol&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;It uses the JavaScript API of sherpa-onnx along with &lt;a href=&quot;https://electronjs.org/&quot;&gt;Electron&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Video demo in Chinese: &lt;a href=&quot;https://www.bilibili.com/video/BV142tje9E74&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/hfyydd/sherpa-onnx-server&quot;&gt;Sherpa-ONNX &lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;A server based on nodejs providing Restful API for speech recognition.&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/xinhecuican/QSmartAssistant&quot;&gt;QSmartAssistant&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;/&lt;/p&gt; 
&lt;p&gt;It uses QT. Both &lt;a href=&quot;https://github.com/xinhecuican/QSmartAssistant/raw/master/doc/%E5%AE%89%E8%A3%85.md#asr&quot;&gt;ASR&lt;/a&gt; and &lt;a href=&quot;https://github.com/xinhecuican/QSmartAssistant/raw/master/doc/%E5%AE%89%E8%A3%85.md#tts&quot;&gt;TTS&lt;/a&gt; are used.&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/Jason-chen-coder/Flutter-EasySpeechRecognition&quot;&gt;Flutter-EasySpeechRecognition&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;It extends &lt;a href=&quot;https://raw.githubusercontent.com/k2-fsa/sherpa-onnx/master/flutter-examples/streaming_asr&quot;&gt;./flutter-examples/streaming_asr&lt;/a&gt; by downloading models inside the app to reduce the size of the app.&lt;/p&gt; 
&lt;p&gt;Note: &lt;a href=&quot;https://github.com/umgc/spring2025/pull/82&quot;&gt;[Team B] Sherpa AI backend&lt;/a&gt; also uses sherpa-onnx in a Flutter APP.&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/xue-fei/sherpa-onnx-unity&quot;&gt;sherpa-onnx-unity&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;sherpa-onnx in Unity. See also &lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/issues/1695&quot;&gt;#1695&lt;/a&gt;, &lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/issues/1892&quot;&gt;#1892&lt;/a&gt;, and &lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/issues/1859&quot;&gt;#1859&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/xinnan-tech/xiaozhi-esp32-server&quot;&gt;xiaozhi-esp32-server&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;xiaozhi-esp32ESP32 Backend service for xiaozhi-esp32, helps you quickly build an ESP32 device control server.&lt;/p&gt; 
&lt;p&gt;See also&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xinnan-tech/xiaozhi-esp32-server/issues/315&quot;&gt;ASRsherpa-onnx-asr&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xinnan-tech/xiaozhi-esp32-server/pull/379&quot;&gt;feat: ASRsherpa-onnx&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/EternityForest/KaithemAutomation&quot;&gt;KaithemAutomation&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Pure Python, GUI-focused home automation/consumer grade SCADA.&lt;/p&gt; 
&lt;p&gt;It uses TTS from sherpa-onnx. See also &lt;a href=&quot;https://github.com/EternityForest/KaithemAutomation/commit/8e64d2b138725e426532f7d66bb69dd0b4f53693&quot;&gt; Speak command that uses the new globally configured TTS model.&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>electron/electron</title>
      <link>https://github.com/electron/electron</link>
      <description>&lt;p&gt;Build cross-platform desktop apps with JavaScript, HTML, and CSS&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://electronjs.org&quot;&gt;&lt;img src=&quot;https://electronjs.org/images/electron-logo.svg?sanitize=true&quot; alt=&quot;Electron Logo&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/electron/electron/actions/workflows/build.yml&quot;&gt;&lt;img src=&quot;https://github.com/electron/electron/actions/workflows/build.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/electronjs&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/745037351163527189?color=%237289DA&amp;amp;label=chat&amp;amp;logo=discord&amp;amp;logoColor=white&quot; alt=&quot;Electron Discord Invite&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;&lt;/span&gt; Available Translations:        . View these docs in other languages on our &lt;a href=&quot;https://crowdin.com/project/electron&quot;&gt;Crowdin&lt;/a&gt; project.&lt;/p&gt; 
&lt;p&gt;The Electron framework lets you write cross-platform desktop applications using JavaScript, HTML and CSS. It is based on &lt;a href=&quot;https://nodejs.org/&quot;&gt;Node.js&lt;/a&gt; and &lt;a href=&quot;https://www.chromium.org&quot;&gt;Chromium&lt;/a&gt; and is used by the &lt;a href=&quot;https://github.com/Microsoft/vscode/&quot;&gt;Visual Studio Code&lt;/a&gt; and many other &lt;a href=&quot;https://electronjs.org/apps&quot;&gt;apps&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Follow &lt;a href=&quot;https://twitter.com/electronjs&quot;&gt;@electronjs&lt;/a&gt; on Twitter for important announcements.&lt;/p&gt; 
&lt;p&gt;This project adheres to the Contributor Covenant &lt;a href=&quot;https://github.com/electron/electron/tree/main/CODE_OF_CONDUCT.md&quot;&gt;code of conduct&lt;/a&gt;. By participating, you are expected to uphold this code. Please report unacceptable behavior to &lt;a href=&quot;mailto:coc@electronjs.org&quot;&gt;coc@electronjs.org&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install prebuilt Electron binaries, use &lt;a href=&quot;https://docs.npmjs.com/&quot;&gt;&lt;code&gt;npm&lt;/code&gt;&lt;/a&gt;. The preferred method is to install Electron as a development dependency in your app:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;npm install electron --save-dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more installation options and troubleshooting tips, see &lt;a href=&quot;https://raw.githubusercontent.com/electron/electron/main/docs/tutorial/installation.md&quot;&gt;installation&lt;/a&gt;. For info on how to manage Electron versions in your apps, see &lt;a href=&quot;https://raw.githubusercontent.com/electron/electron/main/docs/tutorial/electron-versioning.md&quot;&gt;Electron versioning&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Platform support&lt;/h2&gt; 
&lt;p&gt;Each Electron release provides binaries for macOS, Windows, and Linux.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;macOS (Big Sur and up): Electron provides 64-bit Intel and Apple Silicon / ARM binaries for macOS.&lt;/li&gt; 
 &lt;li&gt;Windows (Windows 10 and up): Electron provides &lt;code&gt;ia32&lt;/code&gt; (&lt;code&gt;x86&lt;/code&gt;), &lt;code&gt;x64&lt;/code&gt; (&lt;code&gt;amd64&lt;/code&gt;), and &lt;code&gt;arm64&lt;/code&gt; binaries for Windows. Windows on ARM support was added in Electron 5.0.8. Support for Windows 7, 8 and 8.1 was &lt;a href=&quot;https://www.electronjs.org/blog/windows-7-to-8-1-deprecation-notice&quot;&gt;removed in Electron 23, in line with Chromium&#39;s Windows deprecation policy&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Linux: The prebuilt binaries of Electron are built on Ubuntu 20.04. They have also been verified to work on: 
  &lt;ul&gt; 
   &lt;li&gt;Ubuntu 18.04 and newer&lt;/li&gt; 
   &lt;li&gt;Fedora 32 and newer&lt;/li&gt; 
   &lt;li&gt;Debian 10 and newer&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick start &amp;amp; Electron Fiddle&lt;/h2&gt; 
&lt;p&gt;Use &lt;a href=&quot;https://github.com/electron/fiddle&quot;&gt;&lt;code&gt;Electron Fiddle&lt;/code&gt;&lt;/a&gt; to build, run, and package small Electron experiments, to see code examples for all of Electron&#39;s APIs, and to try out different versions of Electron. It&#39;s designed to make the start of your journey with Electron easier.&lt;/p&gt; 
&lt;p&gt;Alternatively, clone and run the &lt;a href=&quot;https://github.com/electron/electron-quick-start&quot;&gt;electron/electron-quick-start&lt;/a&gt; repository to see a minimal Electron app in action:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;git clone https://github.com/electron/electron-quick-start
cd electron-quick-start
npm install
npm start
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Resources for learning Electron&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://electronjs.org/docs&quot;&gt;electronjs.org/docs&lt;/a&gt; - All of Electron&#39;s documentation&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/electron/fiddle&quot;&gt;electron/fiddle&lt;/a&gt; - A tool to build, run, and package small Electron experiments&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/electron/electron-quick-start&quot;&gt;electron/electron-quick-start&lt;/a&gt; - A very basic starter Electron app&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://electronjs.org/community#boilerplates&quot;&gt;electronjs.org/community#boilerplates&lt;/a&gt; - Sample starter apps created by the community&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Programmatic usage&lt;/h2&gt; 
&lt;p&gt;Most people use Electron from the command line, but if you require &lt;code&gt;electron&lt;/code&gt; inside your &lt;strong&gt;Node app&lt;/strong&gt; (not your Electron app) it will return the file path to the binary. Use this to spawn Electron from Node scripts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;const electron = require(&#39;electron&#39;)
const proc = require(&#39;node:child_process&#39;)

// will print something similar to /Users/maf/.../Electron
console.log(electron)

// spawn Electron
const child = proc.spawn(electron)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Mirrors&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://npmmirror.com/mirrors/electron/&quot;&gt;China&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://www.electronjs.org/docs/latest/tutorial/installation#mirror&quot;&gt;Advanced Installation Instructions&lt;/a&gt; to learn how to use a custom mirror.&lt;/p&gt; 
&lt;h2&gt;Documentation translations&lt;/h2&gt; 
&lt;p&gt;We crowdsource translations for our documentation via &lt;a href=&quot;https://crowdin.com/project/electron&quot;&gt;Crowdin&lt;/a&gt;. We currently accept translations for Chinese (Simplified), French, German, Japanese, Portuguese, Russian, and Spanish.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;If you are interested in reporting/fixing issues and contributing directly to the code base, please see &lt;a href=&quot;https://raw.githubusercontent.com/electron/electron/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for more information on what we&#39;re looking for and how to get started.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Info on reporting bugs, getting help, finding third-party tools and sample apps, and more can be found on the &lt;a href=&quot;https://www.electronjs.org/community&quot;&gt;Community page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/electron/electron/raw/main/LICENSE&quot;&gt;MIT&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;When using Electron logos, make sure to follow &lt;a href=&quot;https://trademark-policy.openjsf.org/&quot;&gt;OpenJS Foundation Trademark Policy&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ml-explore/mlx</title>
      <link>https://github.com/ml-explore/mlx</link>
      <description>&lt;p&gt;MLX: An array framework for Apple silicon&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLX&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ml-explore/mlx/main/#quickstart&quot;&gt;&lt;strong&gt;Quickstart&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/ml-explore/mlx/main/#installation&quot;&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://ml-explore.github.io/mlx/build/html/index.html&quot;&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/ml-explore/mlx/main/#examples&quot;&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://circleci.com/gh/ml-explore/mlx&quot;&gt;&lt;img src=&quot;https://circleci.com/gh/ml-explore/mlx.svg?style=svg&quot; alt=&quot;CircleCI&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;MLX is an array framework for machine learning on Apple silicon, brought to you by Apple machine learning research.&lt;/p&gt; 
&lt;p&gt;Some key features of MLX include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Familiar APIs&lt;/strong&gt;: MLX has a Python API that closely follows NumPy. MLX also has fully featured C++, &lt;a href=&quot;https://github.com/ml-explore/mlx-c&quot;&gt;C&lt;/a&gt;, and &lt;a href=&quot;https://github.com/ml-explore/mlx-swift/&quot;&gt;Swift&lt;/a&gt; APIs, which closely mirror the Python API. MLX has higher-level packages like &lt;code&gt;mlx.nn&lt;/code&gt; and &lt;code&gt;mlx.optimizers&lt;/code&gt; with APIs that closely follow PyTorch to simplify building more complex models.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Composable function transformations&lt;/strong&gt;: MLX supports composable function transformations for automatic differentiation, automatic vectorization, and computation graph optimization.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Lazy computation&lt;/strong&gt;: Computations in MLX are lazy. Arrays are only materialized when needed.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dynamic graph construction&lt;/strong&gt;: Computation graphs in MLX are constructed dynamically. Changing the shapes of function arguments does not trigger slow compilations, and debugging is simple and intuitive.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-device&lt;/strong&gt;: Operations can run on any of the supported devices (currently the CPU and the GPU).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified memory&lt;/strong&gt;: A notable difference from MLX and other frameworks is the &lt;em&gt;unified memory model&lt;/em&gt;. Arrays in MLX live in shared memory. Operations on MLX arrays can be performed on any of the supported device types without transferring data.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;MLX is designed by machine learning researchers for machine learning researchers. The framework is intended to be user-friendly, but still efficient to train and deploy models. The design of the framework itself is also conceptually simple. We intend to make it easy for researchers to extend and improve MLX with the goal of quickly exploring new ideas.&lt;/p&gt; 
&lt;p&gt;The design of MLX is inspired by frameworks like &lt;a href=&quot;https://numpy.org/doc/stable/index.html&quot;&gt;NumPy&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt;, &lt;a href=&quot;https://github.com/google/jax&quot;&gt;Jax&lt;/a&gt;, and &lt;a href=&quot;https://arrayfire.org/&quot;&gt;ArrayFire&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;The &lt;a href=&quot;https://github.com/ml-explore/mlx-examples&quot;&gt;MLX examples repo&lt;/a&gt; has a variety of examples, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ml-explore/mlx-examples/tree/main/transformer_lm&quot;&gt;Transformer language model&lt;/a&gt; training.&lt;/li&gt; 
 &lt;li&gt;Large-scale text generation with &lt;a href=&quot;https://github.com/ml-explore/mlx-examples/tree/main/llms/llama&quot;&gt;LLaMA&lt;/a&gt; and finetuning with &lt;a href=&quot;https://github.com/ml-explore/mlx-examples/tree/main/lora&quot;&gt;LoRA&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Generating images with &lt;a href=&quot;https://github.com/ml-explore/mlx-examples/tree/main/stable_diffusion&quot;&gt;Stable Diffusion&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Speech recognition with &lt;a href=&quot;https://github.com/ml-explore/mlx-examples/tree/main/whisper&quot;&gt;OpenAI&#39;s Whisper&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://ml-explore.github.io/mlx/build/html/usage/quick_start.html&quot;&gt;quick start guide&lt;/a&gt; in the documentation.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;MLX is available on &lt;a href=&quot;https://pypi.org/project/mlx/&quot;&gt;PyPI&lt;/a&gt;. To install the Python API, run:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;With &lt;code&gt;pip&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install mlx
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;With &lt;code&gt;conda&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;conda install -c conda-forge mlx
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Checkout the &lt;a href=&quot;https://ml-explore.github.io/mlx/build/html/install.html#&quot;&gt;documentation&lt;/a&gt; for more information on building the C++ and Python APIs from source.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Check out the &lt;a href=&quot;https://github.com/ml-explore/mlx/tree/main/CONTRIBUTING.md&quot;&gt;contribution guidelines&lt;/a&gt; for more information on contributing to MLX. See the &lt;a href=&quot;https://ml-explore.github.io/mlx/build/html/install.html&quot;&gt;docs&lt;/a&gt; for more information on building from source, and running tests.&lt;/p&gt; 
&lt;p&gt;We are grateful for all of &lt;a href=&quot;https://github.com/ml-explore/mlx/tree/main/ACKNOWLEDGMENTS.md#Individual-Contributors&quot;&gt;our contributors&lt;/a&gt;. If you contribute to MLX and wish to be acknowledged, please add your name to the list in your pull request.&lt;/p&gt; 
&lt;h2&gt;Citing MLX&lt;/h2&gt; 
&lt;p&gt;The MLX software suite was initially developed with equal contribution by Awni Hannun, Jagrit Digani, Angelos Katharopoulos, and Ronan Collobert. If you find MLX useful in your research and wish to cite it, please use the following BibTex entry:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@software{mlx2023,
  author = {Awni Hannun and Jagrit Digani and Angelos Katharopoulos and Ronan Collobert},
  title = {{MLX}: Efficient and flexible machine learning on Apple silicon},
  url = {https://github.com/ml-explore},
  version = {0.0},
  year = {2023},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/TensorRT-LLM</title>
      <link>https://github.com/NVIDIA/TensorRT-LLM</link>
      <description>&lt;p&gt;TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines.&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;h1&gt;TensorRT-LLM&lt;/h1&gt; 
 &lt;h4&gt; A TensorRT Toolbox for Optimized Large Language Model Inference&lt;/h4&gt; 
 &lt;p&gt;&lt;a href=&quot;https://nvidia.github.io/TensorRT-LLM/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat&quot; alt=&quot;Documentation&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.python.org/downloads/release/python-3123/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.12-green&quot; alt=&quot;python&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.python.org/downloads/release/python-31012/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.10-green&quot; alt=&quot;python&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://developer.nvidia.com/cuda-downloads&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/cuda-12.8.0-green&quot; alt=&quot;cuda&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://developer.nvidia.com/tensorrt&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/TRT-10.8.0-green&quot; alt=&quot;trt&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/tensorrt_llm/version.py&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/release-0.19.0.dev-green&quot; alt=&quot;version&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-Apache%202-blue&quot; alt=&quot;license&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/architecture/overview.md&quot;&gt;Architecture&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/performance/perf-overview.md&quot;&gt;Performance&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/examples/&quot;&gt;Examples&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/&quot;&gt;Documentation&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://docs.google.com/presentation/d/1gycPmtdh7uUcH6laOvW65Dbp9F1McUkGDIcAyjicBZs/edit?usp=sharing&quot;&gt;Roadmap&lt;/a&gt;&lt;/p&gt; 
 &lt;hr&gt; 
 &lt;div align=&quot;left&quot;&gt; 
  &lt;h2&gt;Latest News&lt;/h2&gt; 
  &lt;ul&gt; 
   &lt;li&gt;[2025/02/28] Spotlight NAVER Place Optimizes SLM-Based Vertical Services with NVIDIA TensorRT-LLM &lt;a href=&quot;https://developer.nvidia.com/blog/spotlight-naver-place-optimizes-slm-based-vertical-services-with-nvidia-tensorrt-llm/&quot;&gt; link&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;div align=&quot;center&quot;&gt; 
   &lt;img src=&quot;https://developer-blogs.nvidia.com/wp-content/uploads/2025/02/naver-place-app-graphic.jpg&quot; width=&quot;50%&quot;&gt; 
   &lt;div align=&quot;left&quot;&gt; 
    &lt;ul&gt; 
     &lt;li&gt; &lt;p&gt;[2025/02/25]  DeepSeek-R1 performance now optimized for Blackwell &lt;a href=&quot;https://huggingface.co/nvidia/DeepSeek-R1-FP4&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;[2025/02/20] Stay ahead of the curve  Leverage the most performant &amp;amp; capable platform for inference. Explore the complete guide to achieve great accuracy, high throughput, and low latency at the lowest cost for your business  &lt;a href=&quot;https://www.nvidia.com/en-us/solutions/ai/inference/balancing-cost-latency-and-performance-ebook/?ncid=so-twit-348956&amp;amp;linkId=100000341423615&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;[2025/02/18] Unlock #LLM inference with auto-scaling on @AWS EKS   Set up EFA networking &amp;amp; EFS storage with NVLink-connected H100 GPUs on P5.48xlarge  Deploy multi-node Triton Inference Server + TRT-LLM to scale LLaMa 3.1 405B models  Use LeaderWorkerSet &amp;amp; Prometheus metrics for autoscaling and orchestration &lt;a href=&quot;https://aws.amazon.com/blogs/hpc/scaling-your-llm-inference-workloads-multi-node-deployment-with-tensorrt-llm-and-triton-on-amazon-eks/&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;[2025/02/12]  Automating GPU Kernel Generation with DeepSeek-R1 and Inference Time Scaling &lt;a href=&quot;https://developer.nvidia.com/blog/automating-gpu-kernel-generation-with-deepseek-r1-and-inference-time-scaling/?ncid=so-twit-997075&amp;amp;linkId=100000338909937&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;[2025/02/12]  How Scaling Laws Drive Smarter, More Powerful AI &lt;a href=&quot;https://blogs.nvidia.com/blog/ai-scaling-laws/?ncid=so-link-889273&amp;amp;linkId=100000338837832&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;[2025/01/25]  Nvidia moves AI focus to inference cost, efficiency  &lt;a href=&quot;https://www.fierceelectronics.com/ai/nvidia-moves-ai-focus-inference-cost-efficiency?linkId=100000332985606&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;[2025/01/24]  Optimize AI Inference Performance with NVIDIA Full-Stack Solutions &lt;a href=&quot;https://developer.nvidia.com/blog/optimize-ai-inference-performance-with-nvidia-full-stack-solutions/?ncid=so-twit-400810&amp;amp;linkId=100000332621049&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;[2025/01/23]  Fast, Low-Cost Inference Offers Key to Profitable AI &lt;a href=&quot;https://blogs.nvidia.com/blog/ai-inference-platform/?ncid=so-twit-693236-vt04&amp;amp;linkId=100000332307804&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;[2025/01/16]  Introducing New KV Cache Reuse Optimizations in NVIDIA TensorRT-LLM &lt;a href=&quot;https://developer.nvidia.com/blog/introducing-new-kv-cache-reuse-optimizations-in-nvidia-tensorrt-llm/?ncid=so-twit-363876&amp;amp;linkId=100000330323229&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;[2025/01/14]  Bing&#39;s Transition to LLM/SLM Models: Optimizing Search with TensorRT-LLM &lt;a href=&quot;https://blogs.bing.com/search-quality-insights/December-2024/Bing-s-Transition-to-LLM-SLM-Models-Optimizing-Search-with-TensorRT-LLM&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;[2025/01/04] Boost Llama 3.3 70B Inference Throughput 3x with NVIDIA TensorRT-LLM Speculative Decoding &lt;a href=&quot;https://developer.nvidia.com/blog/boost-llama-3-3-70b-inference-throughput-3x-with-nvidia-tensorrt-llm-speculative-decoding/&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;/ul&gt; 
    &lt;details close&gt; 
     &lt;summary&gt;Previous News&lt;/summary&gt; 
     &lt;ul&gt; 
      &lt;li&gt; &lt;p&gt;[2024/12/10]  Llama 3.3 70B from AI at Meta is accelerated by TensorRT-LLM.  State-of-the-art model on par with Llama 3.1 405B for reasoning, math, instruction following and tool use. Explore the preview &lt;a href=&quot;https://build.nvidia.com/meta/llama-3_3-70b-instruct&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/12/03]  Boost your AI inference throughput by up to 3.6x. We now support speculative decoding and tripling token throughput with our NVIDIA TensorRT-LLM. Perfect for your generative AI apps. Learn how in this technical deep dive &lt;a href=&quot;https://nvda.ws/3ZCZTzD&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/12/02] Working on deploying ONNX models for performance-critical applications? Try our NVIDIA Nsight Deep Learning Designer  A user-friendly GUI and tight integration with NVIDIA TensorRT that offers:  Intuitive visualization of ONNX model graphs  Quick tweaking of model architecture and parameters  Detailed performance profiling with either ORT or TensorRT  Easy building of TensorRT engines &lt;a href=&quot;https://developer.nvidia.com/nsight-dl-designer?ncid=so-link-485689&amp;amp;linkId=100000315016072&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/11/26]  Introducing TensorRT-LLM for Jetson AGX Orin, making it even easier to deploy on Jetson AGX Orin with initial support in JetPack 6.1 via the v0.12.0-jetson branch of the TensorRT-LLM repo.  Pre-compiled TensorRT-LLM wheels &amp;amp; containers for easy integration  Comprehensive guides &amp;amp; docs to get you started &lt;a href=&quot;https://forums.developer.nvidia.com/t/tensorrt-llm-for-jetson/313227?linkId=100000312718869&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/11/21] NVIDIA TensorRT-LLM Multiblock Attention Boosts Throughput by More Than 3x for Long Sequence Lengths on NVIDIA HGX H200 &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-tensorrt-llm-multiblock-attention-boosts-throughput-by-more-than-3x-for-long-sequence-lengths-on-nvidia-hgx-h200/&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/11/19] Llama 3.2 Full-Stack Optimizations Unlock High Performance on NVIDIA GPUs &lt;a href=&quot;https://developer.nvidia.com/blog/llama-3-2-full-stack-optimizations-unlock-high-performance-on-nvidia-gpus/?ncid=so-link-721194&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/11/09]  3x Faster AllReduce with NVSwitch and TensorRT-LLM MultiShot &lt;a href=&quot;https://developer.nvidia.com/blog/3x-faster-allreduce-with-nvswitch-and-tensorrt-llm-multishot/&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/11/09]  NVIDIA advances the AI ecosystem with the AI model of LG AI Research  &lt;a href=&quot;https://blogs.nvidia.co.kr/blog/nvidia-lg-ai-research/&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/11/02]  NVIDIA and LlamaIndex Developer Contest  Enter for a chance to win prizes including an NVIDIA GeForce RTX 4080 SUPER GPU, DLI credits, and more &lt;a href=&quot;https://developer.nvidia.com/llamaindex-developer-contest&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/10/28]  NVIDIA GH200 Superchip Accelerates Inference by 2x in Multiturn Interactions with Llama Models &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-gh200-superchip-accelerates-inference-by-2x-in-multiturn-interactions-with-llama-models/&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/10/22] New  Step-by-step instructions on how to  Optimize LLMs with NVIDIA TensorRT-LLM,  Deploy the optimized models with Triton Inference Server,  Autoscale LLMs deployment in a Kubernetes environment.  Technical Deep Dive: &lt;a href=&quot;https://nvda.ws/3YgI8UT&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/10/07] Optimizing Microsoft Bing Visual Search with NVIDIA Accelerated Libraries &lt;a href=&quot;https://developer.nvidia.com/blog/optimizing-microsoft-bing-visual-search-with-nvidia-accelerated-libraries/&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/09/29]  AI at Meta PyTorch + TensorRT v2.4  TensorRT 10.1 PyTorch 2.4 CUDA 12.4 Python 3.12 &lt;a href=&quot;https://github.com/pytorch/TensorRT/releases/tag/v2.4.0&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/09/17]  NVIDIA TensorRT-LLM Meetup &lt;a href=&quot;https://drive.google.com/file/d/1RR8GqC-QbuaKuHj82rZcXb3MS20SWo6F/view?usp=share_link&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/09/17]  Accelerating LLM Inference at Databricks with TensorRT-LLM &lt;a href=&quot;https://drive.google.com/file/d/1NeSmrLaWRJAY1rxD9lJmzpB9rzr38j8j/view?usp=sharing&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/09/17]  TensorRT-LLM @ Baseten &lt;a href=&quot;https://drive.google.com/file/d/1Y7L2jqW-aRmt31mCdqhwvGMmCSOzBUjG/view?usp=share_link&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/09/04]  Best Practices for Tuning TensorRT-LLM for Optimal Serving with BentoML &lt;a href=&quot;https://www.bentoml.com/blog/tuning-tensor-rt-llm-for-optimal-serving-with-bentoml&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/08/20] SDXL with #TensorRT Model Optimizer   cache diffusion  quantization aware training  QLoRA  #Python 3.12 &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-tensorrt-model-optimizer-v0-15-boosts-inference-performance-and-expands-model-support/&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/08/13]  DIY Code Completion with #Mamba  #TensorRT #LLM for speed  NIM for ease  deploy anywhere &lt;a href=&quot;https://developer.nvidia.com/blog/revolutionizing-code-completion-with-codestral-mamba-the-next-gen-coding-llm/&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/08/06]  Multilingual Challenge Accepted   #TensorRT #LLM boosts low-resource languages like Hebrew, Indonesian and Vietnamese &lt;a href=&quot;https://developer.nvidia.com/blog/accelerating-hebrew-llm-performance-with-nvidia-tensorrt-llm/?linkId=100000278659647&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/07/30] Introducing @SliceXAI ELM Turbo  train ELM once  #TensorRT #LLM optimize  deploy anywhere &lt;a href=&quot;https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/07/23]  @AIatMeta Llama 3.1 405B trained on 16K NVIDIA H100s - inference is #TensorRT #LLM optimized   400 tok/s - per node  37 tok/s - per user  1 node inference &lt;a href=&quot;https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/07/09] Checklist to maximize multi-language performance of @meta #Llama3 with #TensorRT #LLM inference:  MultiLingual  NIM  LoRA tuned adaptors&lt;a href=&quot;https://developer.nvidia.com/blog/deploy-multilingual-llms-with-nvidia-nim/&quot;&gt; Tech blog&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/07/02] Let the @MistralAI MoE tokens fly   #Mixtral 8x7B with NVIDIA #TensorRT #LLM on #H100. &lt;a href=&quot;https://developer.nvidia.com/blog/achieving-high-mixtral-8x7b-performance-with-nvidia-h100-tensor-core-gpus-and-tensorrt-llm?ncid=so-twit-928467&quot;&gt; Tech blog&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/06/24] Enhanced with NVIDIA #TensorRT #LLM, @upstage.ais solar-10.7B-instruct is ready to power your developer projects through our API catalog . &lt;a href=&quot;https://build.nvidia.com/upstage/solar-10_7b-instruct?snippet_tab=Try&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/06/18] CYMI:  Stable Diffusion 3 dropped last week   Speed up your SD3 with #TensorRT INT8 Quantization&lt;a href=&quot;https://build.nvidia.com/upstage/solar-10_7b-instruct?snippet_tab=Try&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/06/18] Deploying ComfyUI with TensorRT? Heres your setup guide &lt;a href=&quot;https://github.com/comfyanonymous/ComfyUI_TensorRT&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/06/11] #TensorRT Weight-Stripped Engines  Technical Deep Dive for serious coders +99% compression 1 set of weights  ** GPUs 0 performance loss ** modelsLLM, CNN, etc.&lt;a href=&quot;https://developer.nvidia.com/blog/maximum-performance-and-minimum-footprint-for-ai-apps-with-nvidia-tensorrt-weight-stripped-engines/&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/06/04]  #TensorRT and GeForce #RTX unlock ComfyUI SD superhero powers   Demo: &lt;a href=&quot;https://youtu.be/64QEVfbPHyg&quot;&gt; link&lt;/a&gt;  DIY notebook: &lt;a href=&quot;https://console.brev.dev/launchable/deploy?userID=2x2sil999&amp;amp;orgID=ktj33l4xj&amp;amp;name=ComfyUI_TensorRT&amp;amp;instance=L4%40g2-standard-4%3Anvidia-l4%3A1&amp;amp;diskStorage=500&amp;amp;cloudID=GCP&amp;amp;baseImage=docker.io%2Fpytorch%2Fpytorch%3A2.2.0-cuda12.1-cudnn8-runtime&amp;amp;ports=ComfUI%3A8188&amp;amp;file=https%3A%2F%2Fgithub.com%2Fbrevdev%2Fnotebooks%2Fblob%2Fmain%2Ftensorrt-comfyui.ipynb&amp;amp;launchableID=env-2hQX3n7ae5mq3NjNZ32DfAG0tJf&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/05/28] #TensorRT weight stripping for ResNet-50  +99% compression 1 set of weights  ** GPUs\ 0 performance loss ** modelsLLM, CNN, etc   DIY &lt;a href=&quot;https://console.brev.dev/launchable/deploy?userID=2x2sil999&amp;amp;orgID=ktj33l4xj&amp;amp;launchableID=env-2h6bym7h5GFNho3vpWQQeUYMwTM&amp;amp;instance=L4%40g6.xlarge&amp;amp;diskStorage=500&amp;amp;cloudID=devplane-brev-1&amp;amp;baseImage=nvcr.io%2Fnvidia%2Ftensorrt%3A24.05-py3&amp;amp;file=https%3A%2F%2Fgithub.com%2FNVIDIA%2FTensorRT%2Fblob%2Frelease%2F10.0%2Fsamples%2Fpython%2Fsample_weight_stripping%2Fnotebooks%2Fweight_stripping.ipynb&amp;amp;name=tensorrt_weight_stripping_resnet50&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/05/21] @modal_labs has the codes for serverless @AIatMeta Llama 3 on #TensorRT #LLM   Marvelous Modal Manual: Serverless TensorRT-LLM (LLaMA 3 8B) | Modal Docs &lt;a href=&quot;https://modal.com/docs/examples/trtllm_llama&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/05/08] NVIDIA TensorRT Model Optimizer -- the newest member of the #TensorRT ecosystem is a library of post-training and training-in-the-loop model optimization techniques quantization sparsity QAT &lt;a href=&quot;https://developer.nvidia.com/blog/accelerate-generative-ai-inference-performance-with-nvidia-tensorrt-model-optimizer-now-publicly-available/&quot;&gt; blog&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/05/07]  24,000 tokens per second Meta Llama 3 takes off with #TensorRT #LLM &lt;a href=&quot;https://blogs.nvidia.com/blog/meta-llama3-inference-acceleration/&quot;&gt; link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/02/06] &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/quantization-in-TRT-LLM.md&quot;&gt; Speed up inference with SOTA quantization techniques in TRT-LLM&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2024/01/30] &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/XQA-kernel.md&quot;&gt; New XQA-kernel provides 2.4x more Llama-70B throughput within the same latency budget&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2023/12/04] &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/Falcon180B-H200.md&quot;&gt;Falcon-180B on a single H200 GPU with INT4 AWQ, and 6.7x faster Llama-70B over A100&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2023/11/27] &lt;a href=&quot;https://aws.amazon.com/blogs/machine-learning/boost-inference-performance-for-llms-with-new-amazon-sagemaker-containers/&quot;&gt;SageMaker LMI now supports TensorRT-LLM - improves throughput by 60%, compared to previous version&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2023/11/13] &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/H200launch.md&quot;&gt;H200 achieves nearly 12,000 tok/sec on Llama2-13B&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2023/10/22] &lt;a href=&quot;https://github.com/NVIDIA/trt-llm-rag-windows#readme&quot;&gt; RAG on Windows using TensorRT-LLM and LlamaIndex &lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2023/10/19] Getting Started Guide - &lt;a href=&quot;https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/&quot;&gt;Optimizing Inference on Large Language Models with NVIDIA TensorRT-LLM, Now Publicly Available &lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;[2023/10/17] &lt;a href=&quot;https://blogs.nvidia.com/blog/2023/10/17/tensorrt-llm-windows-stable-diffusion-rtx/&quot;&gt;Large Language Models up to 4x Faster on RTX With TensorRT-LLM for Windows &lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/details&gt; 
    &lt;h2&gt;TensorRT-LLM Overview&lt;/h2&gt; 
    &lt;p&gt;TensorRT-LLM is a library for optimizing Large Language Model (LLM) inference. It provides state-of-the-art optimizations, including custom attention kernels, inflight batching, paged KV caching, quantization (FP8, INT4 &lt;a href=&quot;https://arxiv.org/abs/2306.00978&quot;&gt;AWQ&lt;/a&gt;, INT8 &lt;a href=&quot;https://arxiv.org/abs/2211.10438&quot;&gt;SmoothQuant&lt;/a&gt;, ++) and much more, to perform inference efficiently on NVIDIA GPUs&lt;/p&gt; 
    &lt;p&gt;TensorRT-LLM provides a Python API to build LLMs into optimized &lt;a href=&quot;https://developer.nvidia.com/tensorrt&quot;&gt;TensorRT&lt;/a&gt; engines. It contains runtimes in Python (bindings) and C++ to execute those TensorRT engines. It also includes a &lt;a href=&quot;https://github.com/triton-inference-server/tensorrtllm_backend&quot;&gt;backend&lt;/a&gt; for integration with the &lt;a href=&quot;https://developer.nvidia.com/nvidia-triton-inference-server&quot;&gt;NVIDIA Triton Inference Server&lt;/a&gt;. Models built with TensorRT-LLM can be executed on a wide range of configurations from a single GPU to multiple nodes with multiple GPUs (using &lt;a href=&quot;https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/features/parallelisms.html#tensor-parallelism&quot;&gt;Tensor Parallelism&lt;/a&gt; and/or &lt;a href=&quot;https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/features/parallelisms.html#pipeline-parallelism&quot;&gt;Pipeline Parallelism&lt;/a&gt;).&lt;/p&gt; 
    &lt;p&gt;TensorRT-LLM comes with several popular models pre-defined. They can easily be modified and extended to fit custom needs via a PyTorch-like Python API. Refer to the &lt;a href=&quot;https://nvidia.github.io/TensorRT-LLM/reference/support-matrix.html&quot;&gt;Support Matrix&lt;/a&gt; for a list of supported models.&lt;/p&gt; 
    &lt;p&gt;TensorRT-LLM is built on top of the &lt;a href=&quot;https://developer.nvidia.com/tensorrt&quot;&gt;TensorRT&lt;/a&gt; Deep Learning Inference library. It leverages much of TensorRT&#39;s deep learning optimizations and adds LLM-specific optimizations on top, as described above. TensorRT is an ahead-of-time compiler; it builds &quot;&lt;a href=&quot;https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html#ecosystem&quot;&gt;Engines&lt;/a&gt;&quot; which are optimized representations of the compiled model containing the entire execution graph. These engines are optimized for a specific GPU architecture, and can be validated, benchmarked, and serialized for later deployment in a production environment.&lt;/p&gt; 
    &lt;h2&gt;Getting Started&lt;/h2&gt; 
    &lt;p&gt;To get started with TensorRT-LLM, visit our documentation:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://nvidia.github.io/TensorRT-LLM/quick-start-guide.html&quot;&gt;Quick Start Guide&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://nvidia.github.io/TensorRT-LLM/release-notes.html&quot;&gt;Release Notes&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://nvidia.github.io/TensorRT-LLM/installation/linux.html&quot;&gt;Installation Guide for Linux&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://nvidia.github.io/TensorRT-LLM/installation/grace-hopper.html&quot;&gt;Installation Guide for Grace Hopper&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://nvidia.github.io/TensorRT-LLM/reference/support-matrix.html&quot;&gt;Supported Hardware, Models, and other Software&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; 
    &lt;h2&gt;Community&lt;/h2&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://huggingface.co/TheFloat16&quot;&gt;Model zoo&lt;/a&gt; (generated by TRT-LLM rel 0.9 a9356d4b7610330e89c1010f342a9ac644215c52)&lt;/li&gt; 
    &lt;/ul&gt; 
   &lt;/div&gt;
  &lt;/div&gt;
 &lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>YimMenu/YimMenuV2</title>
      <link>https://github.com/YimMenu/YimMenuV2</link>
      <description>&lt;p&gt;Unfinished WIP&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;YimMenuV2&lt;/h1&gt; 
&lt;p&gt;Experimental menu for GTA 5: Enhanced&lt;/p&gt; 
&lt;h2&gt;Structure&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;core/&lt;/code&gt; : Essential general features for the base&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;game/&lt;/code&gt; : Game specific implemented things&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;util/&lt;/code&gt; : Loose functions that aren&#39;t game specific&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA-Omniverse/PhysX</title>
      <link>https://github.com/NVIDIA-Omniverse/PhysX</link>
      <description>&lt;p&gt;NVIDIA PhysX SDK&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NVIDIA PhysX&lt;/h1&gt; 
&lt;p&gt;Copyright (c) 2008-2025 NVIDIA Corporation. All rights reserved.&lt;/p&gt; 
&lt;p&gt;Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.&lt;/li&gt; 
 &lt;li&gt;Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.&lt;/li&gt; 
 &lt;li&gt;Neither the name of NVIDIA CORPORATION nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS &quot;AS IS&quot; AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Welcome to the NVIDIA PhysX source code repository.&lt;/p&gt; 
&lt;p&gt;This repository contains source releases of the PhysX, Flow, and Blast SDKs used in NVIDIA Omniverse.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The user guide and API documentation are available on &lt;a href=&quot;https://nvidia-omniverse.github.io/PhysX&quot;&gt;GitHub Pages&lt;/a&gt;. Please create an &lt;a href=&quot;https://github.com/NVIDIA-Omniverse/PhysX/issues/&quot;&gt;Issue&lt;/a&gt; if you find a documentation issue.&lt;/p&gt; 
&lt;h2&gt;Instructions&lt;/h2&gt; 
&lt;p&gt;Please see instructions specific to each of the libraries in the respective subfolder.&lt;/p&gt; 
&lt;h2&gt;Community-Maintained Build Configuration Fork&lt;/h2&gt; 
&lt;p&gt;Please see &lt;a href=&quot;https://github.com/o3de/PhysX&quot;&gt;the O3DE Fork&lt;/a&gt; for community-maintained additional build configurations.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Please use GitHub &lt;a href=&quot;https://github.com/NVIDIA-Omniverse/PhysX/discussions/&quot;&gt;Discussions&lt;/a&gt; for questions and comments.&lt;/li&gt; 
 &lt;li&gt;GitHub &lt;a href=&quot;https://github.com/NVIDIA-Omniverse/PhysX/issues&quot;&gt;Issues&lt;/a&gt; should only be used for bug reports or documentation issues.&lt;/li&gt; 
 &lt;li&gt;You can also ask questions in the NVIDIA Omniverse #physics &lt;a href=&quot;https://discord.com/invite/XWQNJDNuaC&quot;&gt;Discord Channel&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>google/googletest</title>
      <link>https://github.com/google/googletest</link>
      <description>&lt;p&gt;GoogleTest - Google Testing and Mocking Framework&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GoogleTest&lt;/h1&gt; 
&lt;h3&gt;Announcements&lt;/h3&gt; 
&lt;h4&gt;Live at Head&lt;/h4&gt; 
&lt;p&gt;GoogleTest now follows the &lt;a href=&quot;https://abseil.io/about/philosophy#upgrade-support&quot;&gt;Abseil Live at Head philosophy&lt;/a&gt;. We recommend &lt;a href=&quot;https://github.com/abseil/abseil-cpp/raw/master/FAQ.md#what-is-live-at-head-and-how-do-i-do-it&quot;&gt;updating to the latest commit in the &lt;code&gt;main&lt;/code&gt; branch as often as possible&lt;/a&gt;. We do publish occasional semantic versions, tagged with &lt;code&gt;v${major}.${minor}.${patch}&lt;/code&gt; (e.g. &lt;code&gt;v1.16.0&lt;/code&gt;).&lt;/p&gt; 
&lt;h4&gt;Documentation Updates&lt;/h4&gt; 
&lt;p&gt;Our documentation is now live on GitHub Pages at &lt;a href=&quot;https://google.github.io/googletest/&quot;&gt;https://google.github.io/googletest/&lt;/a&gt;. We recommend browsing the documentation on GitHub Pages rather than directly in the repository.&lt;/p&gt; 
&lt;h4&gt;Release 1.16.0&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/google/googletest/releases/tag/v1.16.0&quot;&gt;Release 1.16.0&lt;/a&gt; is now available.&lt;/p&gt; 
&lt;p&gt;The 1.16.x branch requires at least C++14.&lt;/p&gt; 
&lt;p&gt;The 1.16.x branch will be the last to support C++14. Future development will &lt;a href=&quot;https://opensource.google/documentation/policies/cplusplus-support#c_language_standard&quot;&gt;require at least C++17&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Continuous Integration&lt;/h4&gt; 
&lt;p&gt;We use Google&#39;s internal systems for continuous integration.&lt;/p&gt; 
&lt;h4&gt;Coming Soon&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;We are planning to take a dependency on &lt;a href=&quot;https://github.com/abseil/abseil-cpp&quot;&gt;Abseil&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Welcome to &lt;strong&gt;GoogleTest&lt;/strong&gt;, Google&#39;s C++ test framework!&lt;/h2&gt; 
&lt;p&gt;This repository is a merger of the formerly separate GoogleTest and GoogleMock projects. These were so closely related that it makes sense to maintain and release them together.&lt;/p&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://google.github.io/googletest/&quot;&gt;GoogleTest User&#39;s Guide&lt;/a&gt; for documentation. We recommend starting with the &lt;a href=&quot;https://google.github.io/googletest/primer.html&quot;&gt;GoogleTest Primer&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;More information about building GoogleTest can be found at &lt;a href=&quot;https://raw.githubusercontent.com/google/googletest/main/googletest/README.md&quot;&gt;googletest/README.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;xUnit test framework: &lt;br&gt; Googletest is based on the &lt;a href=&quot;https://en.wikipedia.org/wiki/XUnit&quot;&gt;xUnit&lt;/a&gt; testing framework, a popular architecture for unit testing&lt;/li&gt; 
 &lt;li&gt;Test discovery: &lt;br&gt; Googletest automatically discovers and runs your tests, eliminating the need to manually register your tests&lt;/li&gt; 
 &lt;li&gt;Rich set of assertions: &lt;br&gt; Googletest provides a variety of assertions, such as equality, inequality, exceptions, and more, making it easy to test your code&lt;/li&gt; 
 &lt;li&gt;User-defined assertions: &lt;br&gt; You can define your own assertions with Googletest, making it simple to write tests that are specific to your code&lt;/li&gt; 
 &lt;li&gt;Death tests: &lt;br&gt; Googletest supports death tests, which verify that your code exits in a certain way, making it useful for testing error-handling code&lt;/li&gt; 
 &lt;li&gt;Fatal and non-fatal failures: &lt;br&gt; You can specify whether a test failure should be treated as fatal or non-fatal with Googletest, allowing tests to continue running even if a failure occurs&lt;/li&gt; 
 &lt;li&gt;Value-parameterized tests: &lt;br&gt; Googletest supports value-parameterized tests, which run multiple times with different input values, making it useful for testing functions that take different inputs&lt;/li&gt; 
 &lt;li&gt;Type-parameterized tests: &lt;br&gt; Googletest also supports type-parameterized tests, which run with different data types, making it useful for testing functions that work with different data types&lt;/li&gt; 
 &lt;li&gt;Various options for running tests: &lt;br&gt; Googletest provides many options for running tests including running individual tests, running tests in a specific order and running tests in parallel&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Platforms&lt;/h2&gt; 
&lt;p&gt;GoogleTest follows Google&#39;s &lt;a href=&quot;https://opensource.google/documentation/policies/cplusplus-support&quot;&gt;Foundational C++ Support Policy&lt;/a&gt;. See &lt;a href=&quot;https://github.com/google/oss-policies-info/raw/main/foundational-cxx-support-matrix.md&quot;&gt;this table&lt;/a&gt; for a list of currently supported versions of compilers, platforms, and build tools.&lt;/p&gt; 
&lt;h2&gt;Who Is Using GoogleTest?&lt;/h2&gt; 
&lt;p&gt;In addition to many internal projects at Google, GoogleTest is also used by the following notable projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;a href=&quot;https://www.chromium.org/&quot;&gt;Chromium projects&lt;/a&gt; (behind the Chrome browser and Chrome OS).&lt;/li&gt; 
 &lt;li&gt;The &lt;a href=&quot;https://llvm.org/&quot;&gt;LLVM&lt;/a&gt; compiler.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/protobuf&quot;&gt;Protocol Buffers&lt;/a&gt;, Google&#39;s data interchange format.&lt;/li&gt; 
 &lt;li&gt;The &lt;a href=&quot;https://opencv.org/&quot;&gt;OpenCV&lt;/a&gt; computer vision library.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Related Open Source Projects&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/nholthaus/gtest-runner&quot;&gt;GTest Runner&lt;/a&gt; is a Qt5 based automated test-runner and Graphical User Interface with powerful features for Windows and Linux platforms.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/ospector/gtest-gbar&quot;&gt;GoogleTest UI&lt;/a&gt; is a test runner that runs your test binary, allows you to track its progress via a progress bar, and displays a list of test failures. Clicking on one shows failure text. GoogleTest UI is written in C#.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/kinow/gtest-tap-listener&quot;&gt;GTest TAP Listener&lt;/a&gt; is an event listener for GoogleTest that implements the &lt;a href=&quot;https://en.wikipedia.org/wiki/Test_Anything_Protocol&quot;&gt;TAP protocol&lt;/a&gt; for test result output. If your test runner understands TAP, you may find it useful.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/google/gtest-parallel&quot;&gt;gtest-parallel&lt;/a&gt; is a test runner that runs tests from your binary in parallel to provide significant speed-up.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=DavidSchuldenfrei.gtest-adapter&quot;&gt;GoogleTest Adapter&lt;/a&gt; is a VS Code extension allowing to view GoogleTest in a tree view and run/debug your tests.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/matepek/vscode-catch2-test-adapter&quot;&gt;C++ TestMate&lt;/a&gt; is a VS Code extension allowing to view GoogleTest in a tree view and run/debug your tests.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://pypi.org/project/cornichon/&quot;&gt;Cornichon&lt;/a&gt; is a small Gherkin DSL parser that generates stub code for GoogleTest.&lt;/p&gt; 
&lt;h2&gt;Contributing Changes&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href=&quot;https://github.com/google/googletest/raw/main/CONTRIBUTING.md&quot;&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt; for details on how to contribute to this project.&lt;/p&gt; 
&lt;p&gt;Happy testing!&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
