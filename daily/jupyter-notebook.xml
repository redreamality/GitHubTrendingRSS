<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="data:text/xsl;base64,<?xml version="1.0" encoding="utf-8"?><xsl:stylesheet version="3.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform" xmlns:atom="http://www.w3.org/2005/Atom"><xsl:output method="html" version="1.0" encoding="UTF-8" indent="yes"/><xsl:template match="/"><xsl:variable name="title"><xsl:value-of select="/rss/channel/title"/></xsl:variable><xsl:variable name="description"><xsl:value-of select="/rss/channel/description"/></xsl:variable><xsl:variable name="link"><xsl:value-of select="/rss/channel/link"/></xsl:variable><html class="dark scroll-smooth"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="referrer" content="unsafe-url"/><title><xsl:value-of select="$title"/></title><style>*,:after,:before{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-gradient-from-position: ;--tw-gradient-via-position: ;--tw-gradient-to-position: ;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgba(59,130,246,.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: ;--tw-contain-size: ;--tw-contain-layout: ;--tw-contain-paint: ;--tw-contain-style: }::backdrop{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-gradient-from-position: ;--tw-gradient-via-position: ;--tw-gradient-to-position: ;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgba(59,130,246,.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: ;--tw-contain-size: ;--tw-contain-layout: ;--tw-contain-paint: ;--tw-contain-style: }
        
        /*! tailwindcss v3.4.17 | MIT License | https://tailwindcss.com*/*,:after,:before{box-sizing:border-box;border:0 solid #e7e7f0}:after,:before{--tw-content:""}:host,html{line-height:1.5;-webkit-text-size-adjust:100%;-moz-tab-size:4;-o-tab-size:4;tab-size:4;font-family:ui-sans-serif,system-ui,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;font-feature-settings:normal;font-variation-settings:normal;-webkit-tap-highlight-color:transparent}body{margin:0;line-height:inherit}hr{height:0;color:inherit;border-top-width:1px}abbr:where([title]){-webkit-text-decoration:underline dotted;text-decoration:underline dotted}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}b,strong{font-weight:bolder}code,kbd,pre,samp{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-feature-settings:normal;font-variation-settings:normal;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}table{text-indent:0;border-color:inherit;border-collapse:collapse}button,input,optgroup,select,textarea{font-family:inherit;font-feature-settings:inherit;font-variation-settings:inherit;font-size:100%;font-weight:inherit;line-height:inherit;letter-spacing:inherit;color:inherit;margin:0;padding:0}button,select{text-transform:none}button,input:where([type=button]),input:where([type=reset]),input:where([type=submit]){-webkit-appearance:button;background-color:transparent;background-image:none}:-moz-focusring{outline:auto}:-moz-ui-invalid{box-shadow:none}progress{vertical-align:baseline}::-webkit-inner-spin-button,::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}summary{display:list-item}blockquote,dd,dl,figure,h1,h2,h3,h4,h5,h6,hr,p,pre{margin:0}fieldset{margin:0}fieldset,legend{padding:0}menu,ol,ul{list-style:none;margin:0;padding:0}dialog{padding:0}textarea{resize:vertical}input::-moz-placeholder,textarea::-moz-placeholder{opacity:1;color:#a8a8b8}input::placeholder,textarea::placeholder{opacity:1;color:#a8a8b8}[role=button],button{cursor:pointer}:disabled{cursor:default}audio,canvas,embed,iframe,img,object,svg,video{display:block;vertical-align:middle}img,video{max-width:100%;height:auto}[hidden]:where(:not([hidden=until-found])){display:none}:root{--card-radius:0.75rem;--btn-radius:var(--card-radius);--badge-radius:var(--btn-radius);--input-radius:var(--btn-radius);--avatar-radius:9999px;--annonce-radius:var(--avatar-radius);--ui-border-color:#1f1f31;--btn-border:#1f1f31;--badge-border:var(--btn-border);--input-border:var(--ui-border-color);--ui-disabled-border:#121220;--ui-error-border:#e11d48;--ui-success-border:#65a30d;--input-outline:#4f46e5;--ui-bg:rgb(18 18 32/var(--ui-bg-opacity));--ui-soft-bg:#1f1f31;--overlay-bg:rgba(2,2,13,.25);--input-bg:var(--ui-soft-bg);--ui-disabled-bg:#121220;--card-padding:1.5rem;--display-text-color:#fff;--title-text-color:var(--display-text-color);--body-text-color:#d6d6e1;--caption-text-color:#6e6e81;--placeholder-text-color:#4d4d5f;--ui-bg-opacity:1;color:var(--body-text-color)}*,.border{border-color:var(--ui-border-color)}button:disabled{border:none!important;background:var(--ui-disabled-bg)!important;background-image:none!important;box-shadow:none!important;color:var(--placeholder-text-color)!important;pointer-events:none!important}button:disabled:before{content:var(--tw-content);display:none}a:focus-visible,button:focus-visible{outline-width:2px;outline-offset:2px;outline-color:#4f46e5}a:focus-visible:focus-visible,button:focus-visible:focus-visible{outline-style:solid}input:user-invalid,select:user-invalid,textarea:user-invalid{--input-border:var(--ui-error-border);--ui-border-color:var(--ui-error-border);--input-outline:var(--ui-error-border);--title-text-color:#fb7185}[data-rounded=none]{--card-radius:0px;--avatar-radius:0px}[data-rounded=default]{--card-radius:0.25rem}[data-rounded=small]{--card-radius:0.125rem}[data-rounded=medium]{--card-radius:0.375rem}[data-rounded=large]{--card-radius:0.5rem}[data-rounded=xlarge]{--card-radius:0.75rem}[data-rounded="2xlarge"]{--card-radius:1rem;--input-radius:0.75rem}[data-rounded="3xlarge"]{--card-radius:1.5rem;--input-radius:0.75rem}[data-rounded=full]{--card-radius:1.5rem;--btn-radius:9999px;--input-radius:1rem}[data-shade=glassy]{--ui-bd-blur:40px;--ui-bg-opacity:0.75;--ui-bg:rgb(58 58 75/var(--ui-bg-opacity));--ui-border-color:rgba(250,250,254,.1);--ui-soft-bg:rgba(77,77,95,.5)}[data-shade="800"]{--ui-border-color:#3a3a4b;--ui-bg:#1f1f31;--ui-soft-bg:#121220}[data-shade="900"]{--ui-border-color:#1f1f31;--ui-bg:#121220;--ui-soft-bg:#1f1f31}[data-shade="950"]{--ui-border-color:#1f1f31;--ui-bg:#02020d;--ui-soft-bg:#1f1f31}.container{width:100%}@media (min-width:640px){.container{max-width:640px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:1024px){.container{max-width:1024px}}@media (min-width:1280px){.container{max-width:1280px}}@media (min-width:1536px){.container{max-width:1536px}}.icon-\[tabler--rss\]{display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24'%3E%3Cpath fill='none' stroke='%23000' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M4 19a1 1 0 1 0 2 0 1 1 0 1 0-2 0M4 4a16 16 0 0 1 16 16M4 11a9 9 0 0 1 9 9'/%3E%3C/svg%3E")}.link{--tw-text-opacity:1;color:rgb(129 140 248/var(--tw-text-opacity,1));transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,-webkit-backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter,-webkit-backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1);transition-duration:.15s}.link.variant-ghost:hover,.link.variant-underlined{text-decoration-line:underline}.link.variant-animated{position:relative}.link.variant-animated:before{position:absolute;left:0;right:0;bottom:0;height:1px;transform-origin:right;--tw-scale-x:0;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y));transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,-webkit-backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter,-webkit-backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1);content:var(--tw-content);transition-duration:.2s}.link.variant-animated:hover:before{transform-origin:left;content:var(--tw-content);--tw-scale-x:1;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.link.intent-info{--tw-text-opacity:1;color:rgb(96 165 250/var(--tw-text-opacity,1))}.link.intent-neutral{--tw-text-opacity:1;color:rgb(255 255 255/var(--tw-text-opacity,1))}.link.variant-animated.intent-neutral:before{content:var(--tw-content);background-color:hsla(0,0%,100%,.5)}.link.variant-animated.intent-info:before{content:var(--tw-content);--tw-bg-opacity:1;background-color:rgb(37 99 235/var(--tw-bg-opacity,1))}.link.variant-animated.intent-primary:before{content:var(--tw-content);--tw-bg-opacity:1;background-color:rgb(79 70 229/var(--tw-bg-opacity,1))}.link.variant-ghost.intent-neutral,.link.variant-underlined.intent-neutral{text-decoration-color:hsla(0,0%,100%,.5)}.mx-auto{margin-left:auto;margin-right:auto}.my-2{margin-top:.5rem;margin-bottom:.5rem}.my-6{margin-top:1.5rem;margin-bottom:1.5rem}.mb-2{margin-bottom:.5rem}.mb-6{margin-bottom:1.5rem}.mb-8{margin-bottom:2rem}.ml-1{margin-left:.25rem}.ml-4{margin-left:1rem}.mr-2{margin-right:.5rem}.mt-1{margin-top:.25rem}.mt-2{margin-top:.5rem}.mt-3{margin-top:.75rem}.block{display:block}.inline-block{display:inline-block}.inline{display:inline}.flex{display:flex}.grid{display:grid}.hidden{display:none}.h-4{height:1rem}.h-8{height:2rem}.min-h-screen{min-height:100vh}.min-h-svh{min-height:100svh}.w-4{width:1rem}.w-8{width:2rem}.max-w-full{max-width:100%}.max-w-screen-lg{max-width:1024px}.flex-1{flex:1 1 0%}.cursor-pointer{cursor:pointer}.list-disc{list-style-type:disc}.grid-cols-1{grid-template-columns:repeat(1,minmax(0,1fr))}.flex-col{flex-direction:column}.items-center{align-items:center}.justify-between{justify-content:space-between}.gap-4{gap:1rem}.gap-6{gap:1.5rem}.gap-8{gap:2rem}.space-y-2>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-top:calc(.5rem*(1 - var(--tw-space-y-reverse)));margin-bottom:calc(.5rem*var(--tw-space-y-reverse))}.space-y-3>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-top:calc(.75rem*(1 - var(--tw-space-y-reverse)));margin-bottom:calc(.75rem*var(--tw-space-y-reverse))}.space-y-4>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-top:calc(1rem*(1 - var(--tw-space-y-reverse)));margin-bottom:calc(1rem*var(--tw-space-y-reverse))}.space-y-6>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-top:calc(1.5rem*(1 - var(--tw-space-y-reverse)));margin-bottom:calc(1.5rem*var(--tw-space-y-reverse))}.scroll-smooth{scroll-behavior:smooth}.truncate{overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.bg-gray-925{--tw-bg-opacity:1;background-color:rgb(9 9 21/var(--tw-bg-opacity,1))}.bg-gradient-to-r{background-image:linear-gradient(to right,var(--tw-gradient-stops))}.from-primary-600{--tw-gradient-from:#4f46e5 var(--tw-gradient-from-position);--tw-gradient-to:rgba(79,70,229,0) var(--tw-gradient-to-position);--tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to)}.to-accent-400{--tw-gradient-to:#e879f9 var(--tw-gradient-to-position)}.bg-clip-text{-webkit-background-clip:text;background-clip:text}.p-1{padding:.25rem}.px-4{padding-left:1rem;padding-right:1rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.py-4{padding-top:1rem;padding-bottom:1rem}.py-6{padding-top:1.5rem;padding-bottom:1.5rem}.pl-5{padding-left:1.25rem}.pt-2{padding-top:.5rem}.text-center{text-align:center}.font-sans{font-family:ui-sans-serif,system-ui,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}.text-2xl{font-size:1.5rem;line-height:2rem}.text-lg{font-size:1.125rem;line-height:1.75rem}.text-sm{font-size:.875rem;line-height:1.25rem}.text-xs{font-size:.75rem;line-height:1rem}.font-bold{font-weight:700}.font-medium{font-weight:500}.font-semibold{font-weight:600}.leading-normal{line-height:1.5}.text-gray-400{--tw-text-opacity:1;color:rgb(168 168 184/var(--tw-text-opacity,1))}.text-gray-500{--tw-text-opacity:1;color:rgb(110 110 129/var(--tw-text-opacity,1))}.text-transparent{color:transparent}.antialiased{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}.transition{transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,-webkit-backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter,-webkit-backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1);transition-duration:.15s}.text-title{color:var(--title-text-color)}.text-body{color:var(--body-text-color)}.\!text-caption{color:var(--caption-text-color)!important}.text-caption{color:var(--caption-text-color)}.dark{--display-text-color:#fff;--title-text-color:var(--display-text-color);--caption-text-color:#6e6e81;--body-text-color:#d6d6e1;--placeholder-text-color:#4d4d5f;--ui-border-color:#232323}[data-shade="900"]:where(.dark,.dark *),[data-shade="925"]:where(.dark,.dark *),[data-shade="950"]:where(.dark,.dark *){--ui-border-color:#383838}.hover\:text-gray-300:hover{--tw-text-opacity:1;color:rgb(214 214 225/var(--tw-text-opacity,1))}.group[open] .group-open\:rotate-180{--tw-rotate:180deg;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}@media (min-width:768px){.md\:grid-cols-2{grid-template-columns:repeat(2,minmax(0,1fr))}.md\:p-4{padding:1rem}.md\:px-6{padding-left:1.5rem;padding-right:1.5rem}.md\:pt-6{padding-top:1.5rem}}@media (min-width:1024px){.lg\:grid-cols-3{grid-template-columns:repeat(3,minmax(0,1fr))}.lg\:dark\:bg-gray-900:is(.dark *){--tw-bg-opacity:1;background-color:rgb(18 18 32/var(--tw-bg-opacity,1))}}</style></head><body class="bg-gray-925 min-h-screen min-h-svh font-sans leading-normal antialiased lg:dark:bg-gray-900"><main class="min-w-screen container mx-auto flex min-h-screen max-w-screen-lg flex-col px-4 py-6 md:px-6"><header class="space-y-2 pt-2 md:pt-6"><a title="{$title}" href="{$link}" target="_blank" rel="noopener noreferrer"><h1 class="flex text-2xl"><span class="icon-[tabler--rss] mr-2 h-8 w-8"/><span class="lg2:text-3xl from-primary-600 to-accent-400 inline-block bg-gradient-to-r bg-clip-text font-bold text-transparent"><xsl:value-of select="$title" disable-output-escaping="yes"/></span></h1></a><p class="text-body pt-2 text-lg py-4"><xsl:value-of select="$description" disable-output-escaping="yes"/></p><p class="text-caption text-sm">
              This RSS feed for the
              <a class="link intent-neutral variant-animated !text-caption font-bold" title="{$title}" href="{$link}" target="_blank" rel="noopener noreferrer"><xsl:value-of select="$title"/></a>
              website.
            </p><p class="text-body text-sm hidden" id="subscribe-links">
              You can subscribe this RSS feed by
              <a class="link intent-neutral variant-animated font-bold" title="Feedly" data-href="https://feedly.com/i/subscription/feed/" target="_blank" rel="noopener noreferrer">Feedly</a>,
              <a class="link intent-neutral variant-animated font-bold" title="Inoreader" data-href="https://www.inoreader.com/feed/" target="_blank" rel="noopener noreferrer">Inoreader</a>,
              <a class="link intent-neutral variant-animated font-bold" title="Newsblur" data-href="https://www.newsblur.com/?url=" target="_blank" rel="noopener noreferrer">Newsblur</a>,
              <a class="link intent-neutral variant-animated font-bold" title="Follow" data-href="follow://add?url=" rel="noopener noreferrer">Follow</a>,
              <a class="link intent-neutral variant-animated font-bold" title="RSS Reader" data-href="feed:" data-raw="true" rel="noopener noreferrer">RSS Reader</a>
              or
              <a class="link intent-neutral variant-animated font-bold" title="{$title} 's feed source" data-href="" data-raw="true" rel="noopener noreferrer">View Source</a>.
            </p><script>
              document.addEventListener('DOMContentLoaded', function () {
                document.querySelectorAll('a[data-href]').forEach(function (a) {
                  const url = new URL(location.href)
                  const feed = url.searchParams.get('url') || location.href
                  const raw = a.getAttribute('data-raw')
                  if (raw) {
                    a.href = a.getAttribute('data-href') + feed
                  } else {
                    a.href = a.getAttribute('data-href') + encodeURIComponent(feed)
                  }
                })
                document.getElementById('subscribe-links').classList.remove('hidden')
              })
            </script></header><hr class="my-6"/><section class="flex-1 space-y-6 p-1 md:p-4"><xsl:choose><xsl:when test="/rss/channel/item"><xsl:for-each select="/rss/channel/item"><article class="space-y-2"><details><summary class="max-w-full truncate"><xsl:if test="title"><h2 class="text-title inline cursor-pointer text-lg font-semibold"><xsl:value-of select="title" disable-output-escaping="yes"/></h2></xsl:if><xsl:if test="pubDate"><time class="text-caption ml-4 mt-1 block text-sm"><xsl:value-of select="pubDate"/></time></xsl:if></summary><div class="text-body px-4 py-2"><p class="my-2"><xsl:choose><xsl:when test="description"><xsl:value-of select="description" disable-output-escaping="yes"/></xsl:when></xsl:choose></p><xsl:if test="link"><a class="link variant-animated intent-neutral font-bold" href="{link}" target="_blank" rel="noopener noreferrer">
                            Read More
                          </a></xsl:if></div></details></article></xsl:for-each></xsl:when><xsl:when test="/atom:feed/atom:entry"><xsl:for-each select="/atom:feed/atom:entry"><article class="space-y-2"><details><summary class="max-w-full truncate"><xsl:if test="atom:title"><h2 class="text-title inline cursor-pointer text-lg font-semibold"><xsl:value-of select="atom:title" disable-output-escaping="yes"/></h2></xsl:if><xsl:if test="atom:updated"><time class="text-caption ml-4 mt-1 block text-sm"><xsl:value-of select="atom:updated"/></time></xsl:if></summary><div class="text-body px-4 py-2"><p class="my-2"><xsl:choose><xsl:when test="atom:summary"><xsl:value-of select="atom:summary" disable-output-escaping="yes"/></xsl:when><xsl:when test="atom:content"><xsl:value-of select="atom:content" disable-output-escaping="yes"/></xsl:when></xsl:choose></p><xsl:if test="atom:link/@href"><a class="link variant-animated intent-neutral font-bold" href="{atom:link/@href}" target="_blank" rel="noopener noreferrer">
                            Read More
                          </a></xsl:if></div></details></article></xsl:for-each></xsl:when></xsl:choose></section><hr class="my-6"/><footer class="text-gray-400"><div class="container mx-auto px-4"><div class="mb-8"><h3 class="text-lg font-semibold text-title mb-6">Popular Feed Collections</h3><div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6"><div class="space-y-3"><a href="https://redreamality.com/GitHubTrendingRSS/weekly/python.xml" class="block hover:text-gray-300"><div class="font-medium">🐍 Python TrendWatch</div><div class="text-xs text-gray-500">AI, ML &amp; Data Science Innovation Feed</div></a><a href="https://redreamality.com/GitHubTrendingRSS/weekly/cuda.xml" class="block hover:text-gray-300"><div class="font-medium">⚡ CUDA Accelerator</div><div class="text-xs text-gray-500">GPU Computing &amp; Deep Learning Updates</div></a><a href="https://redreamality.com/GitHubTrendingRSS/weekly/matlab.xml" class="block hover:text-gray-300"><div class="font-medium">🧠 MATLAB TrendPulse</div><div class="text-xs text-gray-500">MEG, EEG and iEEG Research Feed</div></a></div><div class="space-y-3"><a href="https://redreamality.com/GitHubTrendingRSS/weekly/rust.xml" class="block hover:text-gray-300"><div class="font-medium">🦀 Rust Systems Feed</div><div class="text-xs text-gray-500">High-Performance &amp; Safe Systems Programming</div></a><a href="https://redreamality.com/GitHubTrendingRSS/weekly/go.xml" class="block hover:text-gray-300"><div class="font-medium">🚀 Go Infrastructure</div><div class="text-xs text-gray-500">Cloud Native &amp; DevOps Excellence</div></a><a href="https://redreamality.com/GitHubTrendingRSS/weekly/typescript.xml" class="block hover:text-gray-300"><div class="font-medium">📱 TypeScript Ecosystem</div><div class="text-xs text-gray-500">Modern Web &amp; App Development</div></a></div><div class="space-y-3"><a href="https://redreamality.com/GitHubTrendingRSS/daily/adblock-filter-list.xml" class="block hover:text-gray-300"><div class="font-medium">🛡️ Privacy Shield</div><div class="text-xs text-gray-500">AdBlock &amp; Security Updates</div></a><a href="https://redreamality.com/GitHubTrendingRSS/daily/all.xml" class="block hover:text-gray-300"><div class="font-medium">🌟 Global TechRadar</div><div class="text-xs text-gray-500">Cross-Language Innovation Pulse, add it to your RSS reader rsshub://github/trending/monthly/any/zh</div></a></div></div></div><div class="grid grid-cols-1 md:grid-cols-2 gap-8 mb-8"><div class="space-y-4"><h3 class="text-lg font-semibold text-title">Getting Started</h3><div class="grid grid-cols-1 gap-4"><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">📖 Feed Integration Guide</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">1. Choose Your RSS Reader:</p><ul class="list-disc pl-5 space-y-2"><li>Feedly: Professional choice for cross-platform sync</li><li>Inoreader: Advanced filtering capabilities</li><li>NetNewsWire: Perfect for macOS/iOS users</li><li>FreshRSS: Self-hosted option with full control</li></ul><p class="mt-3 mb-2">2. Add Our Feeds:</p><ul class="list-disc pl-5 space-y-2"><li>Copy the feed URL (e.g., rsshub://github/trending/monthly/any/zh)</li><li>In your RSS reader, look for "Add Feed" or "Subscribe"</li><li>Paste the URL and customize update frequency</li></ul></div></details><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">🎯 Custom Feed Creation</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Create Your Perfect Feed:</p><ul class="list-disc pl-5 space-y-2"><li>Language-specific: /GitHubTrendingRSS/[frequency]/[language].xml</li><li>Topic-focused: Combine multiple language feeds</li><li>Custom time ranges: daily, weekly, or monthly updates</li><li>Regional feeds: Focus on specific developer communities</li></ul><p class="mt-3">Pro tip: Use tags in your RSS reader to organize feeds by topic, language, or priority.</p></div></details><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">⚡ Feed Management Tips</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Optimize Your Feed Reading:</p><ul class="list-disc pl-5 space-y-2"><li>Set update frequencies based on feed importance</li><li>Use folders to group related feeds (e.g., AI/ML, Web Dev)</li><li>Enable notifications only for high-priority feeds</li><li>Archive valuable resources for future reference</li></ul><p class="mt-3">Advanced Features:</p><ul class="list-disc pl-5 space-y-2"><li>Filter feeds using keywords to focus on specific topics</li><li>Set up IFTTT integrations for automated workflows</li><li>Export/backup your feed collection regularly</li></ul></div></details></div></div><div class="space-y-4"><h3 class="text-lg font-semibold text-title">Common Questions</h3><div class="grid grid-cols-1 gap-4"><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">🤔 About Github Radar</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Github Radar is your intelligent curator for:</p><ul class="list-disc pl-5 space-y-2"><li>Trending repositories across all programming languages</li><li>Language-specific innovation and updates</li><li>Regional development trends and patterns</li><li>Open source community movements</li></ul><p class="mt-3">Our mission is to help developers stay updated with minimal effort through smart feed curation and organization.</p></div></details><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">📊 Feed Frequency Options</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Choose Your Update Rhythm:</p><ul class="list-disc pl-5 space-y-2"><li>Daily: Perfect for fast-moving technologies and security updates</li><li>Weekly: Ideal for maintaining awareness without overwhelm</li><li>Monthly: Best for long-term trend analysis and strategic planning</li></ul><p class="mt-3">Customize by combining different frequencies for different topics based on your needs.</p></div></details><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">🔧 Technical Support</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Supported RSS Readers:</p><ul class="list-disc pl-5 space-y-2"><li>Desktop: NetNewsWire, Reeder, FeedReader</li><li>Mobile: Feedly, Inoreader, NewsBlur</li><li>Self-hosted: FreshRSS, Tiny Tiny RSS</li><li>Browser-based: Feedbro, RSS Feed Reader</li></ul><p class="mt-3">Common Issues:</p><ul class="list-disc pl-5 space-y-2"><li>Feed not updating? Check your reader's refresh settings</li><li>Missing content? Verify your internet connection</li><li>Format issues? Try re-subscribing to the feed</li></ul></div></details></div></div></div><div class="text-center text-sm"><p class="mt-2">Acknowledgement: Page decorated by <a href="https://github.com/ccbikai/RSS.Beauty"><svg class="inline-block w-4 h-4 ml-1" viewBox="0 0 16 16" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg></a> RSS.Beauty</p></div></div></footer></main></body></html></xsl:template></xsl:stylesheet>"?>
<rss version="2.0">
  <channel>
    <title>GitHub Jupyter Notebook Daily Trending</title>
    <description>Daily Trending of Jupyter Notebook in GitHub</description>
    <pubDate>Sat, 29 Mar 2025 02:26:11 GMT</pubDate>
    <link>http://redreamality.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>langchain-ai/langchain</title>
      <link>https://github.com/langchain-ai/langchain</link>
      <description>&lt;p&gt;🦜🔗 Build context-aware reasoning applications&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;docs/static/img/logo-dark.svg&quot;&gt; 
 &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;docs/static/img/logo-light.svg&quot;&gt; 
 &lt;img alt=&quot;LangChain Logo&quot; src=&quot;https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/static/img/logo-dark.svg?sanitize=true&quot; width=&quot;80%&quot;&gt; 
&lt;/picture&gt; 
&lt;div&gt; 
 &lt;br&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/langchain-ai/langchain/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/release/langchain-ai/langchain?style=flat-square&quot; alt=&quot;Release Notes&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/langchain-ai/langchain/actions/workflows/check_diffs.yml&quot;&gt;&lt;img src=&quot;https://github.com/langchain-ai/langchain/actions/workflows/check_diffs.yml/badge.svg?sanitize=true&quot; alt=&quot;CI&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/l/langchain-core?style=flat-square&quot; alt=&quot;PyPI - License&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypistats.org/packages/langchain-core&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/dm/langchain-core?style=flat-square&quot; alt=&quot;PyPI - Downloads&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://star-history.com/#langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/langchain-ai/langchain?style=flat-square&quot; alt=&quot;GitHub star chart&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/langchain-ai/langchain/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-raw/langchain-ai/langchain?style=flat-square&quot; alt=&quot;Open Issues&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode&amp;amp;style=flat-square&quot; alt=&quot;Open in Dev Containers&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://codespaces.new/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://github.com/codespaces/badge.svg?sanitize=true&quot; alt=&quot;Open in GitHub Codespaces&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/langchainai&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&amp;amp;label=Follow%20%40LangChainAI&quot; alt=&quot;Twitter&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Looking for the JS/TS library? Check out &lt;a href=&quot;https://github.com/langchain-ai/langchainjs&quot;&gt;LangChain.js&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;LangChain is a framework for building LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development — all while future-proofing decisions as the underlying technology evolves.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U langchain
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To learn more about LangChain, check out &lt;a href=&quot;https://python.langchain.com/docs/introduction/&quot;&gt;the docs&lt;/a&gt;. If you’re looking for more advanced customization or agent orchestration, check out &lt;a href=&quot;https://langchain-ai.github.io/langgraph/&quot;&gt;LangGraph&lt;/a&gt;, our framework for building controllable agent workflows.&lt;/p&gt; 
&lt;h2&gt;Why use LangChain?&lt;/h2&gt; 
&lt;p&gt;LangChain helps developers build applications powered by LLMs through a standard interface for models, embeddings, vector stores, and more.&lt;/p&gt; 
&lt;p&gt;Use LangChain for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time data augmentation&lt;/strong&gt;. Easily connect LLMs to diverse data sources and external / internal systems, drawing from LangChain’s vast library of integrations with model providers, tools, vector stores, retrievers, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model interoperability&lt;/strong&gt;. Swap models in and out as your engineering team experiments to find the best choice for your application’s needs. As the industry frontier evolves, adapt quickly — LangChain’s abstractions keep you moving without losing momentum.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;LangChain’s ecosystem&lt;/h2&gt; 
&lt;p&gt;While the LangChain framework can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools when building LLM applications.&lt;/p&gt; 
&lt;p&gt;To improve your LLM application development, pair LangChain with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;http://www.langchain.com/langsmith&quot;&gt;LangSmith&lt;/a&gt; - Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://langchain-ai.github.io/langgraph/&quot;&gt;LangGraph&lt;/a&gt; - Build agents that can reliably handle complex tasks with LangGraph, our low-level agent orchestration framework. LangGraph offers customizable architecture, long-term memory, and human-in-the-loop workflows — and is trusted in production by companies like LinkedIn, Uber, Klarna, and GitLab.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/#langgraph-platform&quot;&gt;LangGraph Platform&lt;/a&gt; - Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in &lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/&quot;&gt;LangGraph Studio&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Additional resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/tutorials/&quot;&gt;Tutorials&lt;/a&gt;: Simple walkthroughs with guided examples on getting started with LangChain.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/how_to/&quot;&gt;How-to Guides&lt;/a&gt;: Quick, actionable code snippets for topics such as tool calling, RAG use cases, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/concepts/&quot;&gt;Conceptual Guides&lt;/a&gt;: Explanations of key concepts behind the LangChain framework.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/api_reference/&quot;&gt;API Reference&lt;/a&gt;: Detailed reference on navigating base packages and integrations for LangChain.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>topoteretes/cognee</title>
      <link>https://github.com/topoteretes/cognee</link>
      <description>&lt;p&gt;Reliable LLM Memory for AI Applications and AI Agents&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://github.com/topoteretes/cognee&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png&quot; alt=&quot;Cognee Logo&quot; height=&quot;60&quot;&gt; &lt;/a&gt; 
 &lt;br&gt; 
 &lt;p&gt;cognee - memory layer for AI apps and Agents&lt;/p&gt; 
 &lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=1bezuvLwJmw&amp;amp;t=2s&quot;&gt;Demo&lt;/a&gt; . &lt;a href=&quot;https://cognee.ai&quot;&gt;Learn more&lt;/a&gt; · &lt;a href=&quot;https://discord.gg/NQPKmU5CCg&quot;&gt;Join Discord&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://GitHub.com/topoteretes/cognee/network/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&amp;amp;label=Fork&amp;amp;maxAge=2592000&quot; alt=&quot;GitHub forks&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/topoteretes/cognee/stargazers/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&amp;amp;label=Star&amp;amp;maxAge=2592000&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/topoteretes/cognee/commit/&quot;&gt;&lt;img src=&quot;https://badgen.net/github/commits/topoteretes/cognee&quot; alt=&quot;GitHub commits&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/topoteretes/cognee/tags/&quot;&gt;&lt;img src=&quot;https://badgen.net/github/tag/topoteretes/cognee&quot; alt=&quot;Github tag&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pepy.tech/project/cognee&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/cognee&quot; alt=&quot;Downloads&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/topoteretes/cognee/raw/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&amp;amp;colorB=000000&quot; alt=&quot;License&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/topoteretes/cognee/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&amp;amp;colorB=000000&quot; alt=&quot;Contributors&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;AI Agent responses you can rely on.&lt;/p&gt; 
 &lt;p&gt;Build dynamic Agent memory using scalable, modular ECL (Extract, Cognify, Load) pipelines.&lt;/p&gt; 
 &lt;p&gt;More on &lt;a href=&quot;https://docs.cognee.ai/use_cases&quot;&gt;use-cases&lt;/a&gt;.&lt;/p&gt; 
 &lt;div style=&quot;text-align: center&quot;&gt; 
  &lt;img src=&quot;https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee_benefits.png&quot; alt=&quot;Why cognee?&quot; width=&quot;100%&quot;&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Interconnect and retrieve your past conversations, documents, images and audio transcriptions&lt;/li&gt; 
 &lt;li&gt;Reduce hallucinations, developer effort, and cost.&lt;/li&gt; 
 &lt;li&gt;Load data to graph and vector databases using only Pydantic&lt;/li&gt; 
 &lt;li&gt;Manipulate your data while ingesting from 30+ data sources&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;p&gt;Get started quickly with a Google Colab &lt;a href=&quot;https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing&quot;&gt;notebook&lt;/a&gt; or &lt;a href=&quot;https://github.com/topoteretes/cognee-starter&quot;&gt;starter repo&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Your contributions are at the core of making this a true open source project. Any contributions you make are &lt;strong&gt;greatly appreciated&lt;/strong&gt;. See &lt;a href=&quot;https://raw.githubusercontent.com/topoteretes/cognee/dev/CONTRIBUTING.md&quot;&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;📦 Installation&lt;/h2&gt; 
&lt;p&gt;You can install Cognee using either &lt;strong&gt;pip&lt;/strong&gt;, &lt;strong&gt;poetry&lt;/strong&gt;, &lt;strong&gt;uv&lt;/strong&gt; or any other python package manager.&lt;/p&gt; 
&lt;h3&gt;With pip&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install cognee
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;💻 Basic Usage&lt;/h2&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;import os
os.environ[&quot;LLM_API_KEY&quot;] = &quot;YOUR OPENAI_API_KEY&quot;

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also set the variables by creating .env file, using our &lt;a href=&quot;https://github.com/topoteretes/cognee/raw/main/.env.template&quot;&gt;template.&lt;/a&gt; To use different LLM providers, for more info check out our &lt;a href=&quot;https://docs.cognee.ai&quot;&gt;documentation&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Simple example&lt;/h3&gt; 
&lt;p&gt;This script will run the default pipeline:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import cognee
import asyncio


async def main():
    # Add text to cognee
    await cognee.add(&quot;Natural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval.&quot;)

    # Generate the knowledge graph
    await cognee.cognify()

    # Query the knowledge graph
    results = await cognee.search(&quot;Tell me about NLP&quot;)

    # Display the results
    for result in results:
        print(result)


if __name__ == &#39;__main__&#39;:
    asyncio.run(main())

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example output:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;       # ({&#39;id&#39;: UUID(&#39;bc338a39-64d6-549a-acec-da60846dd90d&#39;), &#39;updated_at&#39;: datetime.datetime(2024, 11, 21, 12, 23, 1, 211808, tzinfo=datetime.timezone.utc), &#39;name&#39;: &#39;natural language processing&#39;, &#39;description&#39;: &#39;An interdisciplinary subfield of computer science and information retrieval.&#39;}, {&#39;relationship_name&#39;: &#39;is_a_subfield_of&#39;, &#39;source_node_id&#39;: UUID(&#39;bc338a39-64d6-549a-acec-da60846dd90d&#39;), &#39;target_node_id&#39;: UUID(&#39;6218dbab-eb6a-5759-a864-b3419755ffe0&#39;), &#39;updated_at&#39;: datetime.datetime(2024, 11, 21, 12, 23, 15, 473137, tzinfo=datetime.timezone.utc)}, {&#39;id&#39;: UUID(&#39;6218dbab-eb6a-5759-a864-b3419755ffe0&#39;), &#39;updated_at&#39;: datetime.datetime(2024, 11, 21, 12, 23, 1, 211808, tzinfo=datetime.timezone.utc), &#39;name&#39;: &#39;computer science&#39;, &#39;description&#39;: &#39;The study of computation and information processing.&#39;})
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Graph visualization: &lt;a href=&quot;https://rawcdn.githack.com/topoteretes/cognee/refs/heads/add-visualization-readme/assets/graph_visualization.html&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/topoteretes/cognee/dev/assets/graph_visualization.png&quot; width=&quot;100%&quot; alt=&quot;Graph Visualization&quot;&gt;&lt;/a&gt; Open in &lt;a href=&quot;https://rawcdn.githack.com/topoteretes/cognee/refs/heads/add-visualization-readme/assets/graph_visualization.html&quot;&gt;browser&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more advanced usage, have a look at our &lt;a href=&quot;https://docs.cognee.ai&quot;&gt; documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Understand our architecture&lt;/h2&gt; 
&lt;div style=&quot;text-align: center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/topoteretes/cognee/dev/assets/cognee_diagram.png&quot; alt=&quot;cognee concept diagram&quot; width=&quot;100%&quot;&gt; 
&lt;/div&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;p&gt;What is AI memory:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/8b2a0050-5ec4-424c-b417-8269971503f0&quot;&gt;Learn about cognee&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Code of Conduct&lt;/h2&gt; 
&lt;p&gt;We are committed to making open source an enjoyable and respectful experience for our community. See &lt;a href=&quot;https://github.com/topoteretes/cognee/raw/main/CODE_OF_CONDUCT.md&quot;&gt;&lt;code&gt;CODE_OF_CONDUCT&lt;/code&gt;&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;💫 Contributors&lt;/h2&gt; 
&lt;a href=&quot;https://github.com/topoteretes/cognee/graphs/contributors&quot;&gt; &lt;img alt=&quot;contributors&quot; src=&quot;https://contrib.rocks/image?repo=topoteretes/cognee&quot;&gt; &lt;/a&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://star-history.com/#topoteretes/cognee&amp;amp;Date&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=topoteretes/cognee&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>shivammehta25/Matcha-TTS</title>
      <link>https://github.com/shivammehta25/Matcha-TTS</link>
      <description>&lt;p&gt;[ICASSP 2024] 🍵 Matcha-TTS: A fast TTS architecture with conditional flow matching&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;h1&gt;🍵 Matcha-TTS: A fast TTS architecture with conditional flow matching&lt;/h1&gt; 
 &lt;h3&gt;&lt;a href=&quot;https://www.kth.se/profile/smehta&quot;&gt;Shivam Mehta&lt;/a&gt;, &lt;a href=&quot;https://www.kth.se/profile/ruibo&quot;&gt;Ruibo Tu&lt;/a&gt;, &lt;a href=&quot;https://www.kth.se/profile/beskow&quot;&gt;Jonas Beskow&lt;/a&gt;, &lt;a href=&quot;https://www.kth.se/profile/szekely&quot;&gt;Éva Székely&lt;/a&gt;, and &lt;a href=&quot;https://people.kth.se/~ghe/&quot;&gt;Gustav Eje Henter&lt;/a&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;a href=&quot;https://www.python.org/downloads/release/python-3100/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/-Python_3.10-blue?logo=python&amp;amp;logoColor=white&quot; alt=&quot;python&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PyTorch_2.0+-ee4c2c?logo=pytorch&amp;amp;logoColor=white&quot; alt=&quot;pytorch&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pytorchlightning.ai/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/-Lightning_2.0+-792ee5?logo=pytorchlightning&amp;amp;logoColor=white&quot; alt=&quot;lightning&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://hydra.cc/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Config-Hydra_1.3-89b8cd&quot; alt=&quot;hydra&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://black.readthedocs.io/en/stable/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Code%20Style-Black-black.svg?labelColor=gray&quot; alt=&quot;black&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pycqa.github.io/isort/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&amp;amp;labelColor=ef8336&quot; alt=&quot;isort&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p style=&quot;text-align: center;&quot;&gt; &lt;img src=&quot;https://shivammehta25.github.io/Matcha-TTS/images/logo.png&quot; height=&quot;128&quot;&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This is the official code implementation of 🍵 Matcha-TTS [ICASSP 2024].&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;We propose 🍵 Matcha-TTS, a new approach to non-autoregressive neural TTS, that uses &lt;a href=&quot;https://arxiv.org/abs/2210.02747&quot;&gt;conditional flow matching&lt;/a&gt; (similar to &lt;a href=&quot;https://arxiv.org/abs/2209.03003&quot;&gt;rectified flows&lt;/a&gt;) to speed up ODE-based speech synthesis. Our method:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Is probabilistic&lt;/li&gt; 
 &lt;li&gt;Has compact memory footprint&lt;/li&gt; 
 &lt;li&gt;Sounds highly natural&lt;/li&gt; 
 &lt;li&gt;Is very fast to synthesise from&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Check out our &lt;a href=&quot;https://shivammehta25.github.io/Matcha-TTS&quot;&gt;demo page&lt;/a&gt; and read &lt;a href=&quot;https://arxiv.org/abs/2309.03199&quot;&gt;our ICASSP 2024 paper&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://drive.google.com/drive/folders/17C_gYgEHOxI5ZypcfE_k1piKCtyR0isJ?usp=sharing&quot;&gt;Pre-trained models&lt;/a&gt; will be automatically downloaded with the CLI or gradio interface.&lt;/p&gt; 
&lt;p&gt;You can also &lt;a href=&quot;https://huggingface.co/spaces/shivammehta25/Matcha-TTS&quot;&gt;try 🍵 Matcha-TTS in your browser on HuggingFace 🤗 spaces&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Teaser video&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://youtu.be/xmvJkz3bqw0&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/xmvJkz3bqw0/hqdefault.jpg&quot; alt=&quot;Watch the video&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create an environment (suggested but optional)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;conda create -n matcha-tts python=3.10 -y
conda activate matcha-tts
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Install Matcha TTS using pip or from source&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install matcha-tts
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;from source&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install git+https://github.com/shivammehta25/Matcha-TTS.git
cd Matcha-TTS
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Run CLI / gradio app / jupyter notebook&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# This will download the required models
matcha-tts --text &quot;&amp;lt;INPUT TEXT&amp;gt;&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-tts-app
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or open &lt;code&gt;synthesis.ipynb&lt;/code&gt; on jupyter notebook&lt;/p&gt; 
&lt;h3&gt;CLI Arguments&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;To synthesise from given text, run:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-tts --text &quot;&amp;lt;INPUT TEXT&amp;gt;&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;To synthesise from a file, run:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-tts --file &amp;lt;PATH TO FILE&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;To batch synthesise from a file, run:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-tts --file &amp;lt;PATH TO FILE&amp;gt; --batched
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Additional arguments&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Speaking rate&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-tts --text &quot;&amp;lt;INPUT TEXT&amp;gt;&quot; --speaking_rate 1.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Sampling temperature&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-tts --text &quot;&amp;lt;INPUT TEXT&amp;gt;&quot; --temperature 0.667
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Euler ODE solver steps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-tts --text &quot;&amp;lt;INPUT TEXT&amp;gt;&quot; --steps 10
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Train with your own dataset&lt;/h2&gt; 
&lt;p&gt;Let&#39;s assume we are training with LJ Speech&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Download the dataset from &lt;a href=&quot;https://keithito.com/LJ-Speech-Dataset/&quot;&gt;here&lt;/a&gt;, extract it to &lt;code&gt;data/LJSpeech-1.1&lt;/code&gt;, and prepare the file lists to point to the extracted data like for &lt;a href=&quot;https://github.com/NVIDIA/tacotron2#setup&quot;&gt;item 5 in the setup of the NVIDIA Tacotron 2 repo&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone and enter the Matcha-TTS repository&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/shivammehta25/Matcha-TTS.git
cd Matcha-TTS
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Install the package from source&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt;Go to &lt;code&gt;configs/data/ljspeech.yaml&lt;/code&gt; and change&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;train_filelist_path: data/filelists/ljs_audio_text_train_filelist.txt
valid_filelist_path: data/filelists/ljs_audio_text_val_filelist.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;5&quot;&gt; 
 &lt;li&gt;Generate normalisation statistics with the yaml file of dataset configuration&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-data-stats -i ljspeech.yaml
# Output:
#{&#39;mel_mean&#39;: -5.53662231756592, &#39;mel_std&#39;: 2.1161014277038574}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Update these values in &lt;code&gt;configs/data/ljspeech.yaml&lt;/code&gt; under &lt;code&gt;data_statistics&lt;/code&gt; key.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;data_statistics:  # Computed for ljspeech dataset
  mel_mean: -5.536622
  mel_std: 2.116101
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;to the paths of your train and validation filelists.&lt;/p&gt; 
&lt;ol start=&quot;6&quot;&gt; 
 &lt;li&gt;Run the training script&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;make train-ljspeech
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python matcha/train.py experiment=ljspeech
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;for a minimum memory run&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python matcha/train.py experiment=ljspeech_min_memory
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;for multi-gpu training, run&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python matcha/train.py experiment=ljspeech trainer.devices=[0,1]
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;7&quot;&gt; 
 &lt;li&gt;Synthesise from the custom trained model&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-tts --text &quot;&amp;lt;INPUT TEXT&amp;gt;&quot; --checkpoint_path &amp;lt;PATH TO CHECKPOINT&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ONNX support&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Special thanks to &lt;a href=&quot;https://github.com/mush42&quot;&gt;@mush42&lt;/a&gt; for implementing ONNX export and inference support.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;It is possible to export Matcha checkpoints to &lt;a href=&quot;https://onnx.ai/&quot;&gt;ONNX&lt;/a&gt;, and run inference on the exported ONNX graph.&lt;/p&gt; 
&lt;h3&gt;ONNX export&lt;/h3&gt; 
&lt;p&gt;To export a checkpoint to ONNX, first install ONNX with&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install onnx
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;then run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python3 -m matcha.onnx.export matcha.ckpt model.onnx --n-timesteps 5
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optionally, the ONNX exporter accepts &lt;strong&gt;vocoder-name&lt;/strong&gt; and &lt;strong&gt;vocoder-checkpoint&lt;/strong&gt; arguments. This enables you to embed the vocoder in the exported graph and generate waveforms in a single run (similar to end-to-end TTS systems).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that &lt;code&gt;n_timesteps&lt;/code&gt; is treated as a hyper-parameter rather than a model input. This means you should specify it during export (not during inference). If not specified, &lt;code&gt;n_timesteps&lt;/code&gt; is set to &lt;strong&gt;5&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: for now, torch&amp;gt;=2.1.0 is needed for export since the &lt;code&gt;scaled_product_attention&lt;/code&gt; operator is not exportable in older versions. Until the final version is released, those who want to export their models must install torch&amp;gt;=2.1.0 manually as a pre-release.&lt;/p&gt; 
&lt;h3&gt;ONNX Inference&lt;/h3&gt; 
&lt;p&gt;To run inference on the exported model, first install &lt;code&gt;onnxruntime&lt;/code&gt; using&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install onnxruntime
pip install onnxruntime-gpu  # for GPU inference
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;then use the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python3 -m matcha.onnx.infer model.onnx --text &quot;hey&quot; --output-dir ./outputs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also control synthesis parameters:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python3 -m matcha.onnx.infer model.onnx --text &quot;hey&quot; --output-dir ./outputs --temperature 0.4 --speaking_rate 0.9 --spk 0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run inference on &lt;strong&gt;GPU&lt;/strong&gt;, make sure to install &lt;strong&gt;onnxruntime-gpu&lt;/strong&gt; package, and then pass &lt;code&gt;--gpu&lt;/code&gt; to the inference command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python3 -m matcha.onnx.infer model.onnx --text &quot;hey&quot; --output-dir ./outputs --gpu
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you exported only Matcha to ONNX, this will write mel-spectrogram as graphs and &lt;code&gt;numpy&lt;/code&gt; arrays to the output directory. If you embedded the vocoder in the exported graph, this will write &lt;code&gt;.wav&lt;/code&gt; audio files to the output directory.&lt;/p&gt; 
&lt;p&gt;If you exported only Matcha to ONNX, and you want to run a full TTS pipeline, you can pass a path to a vocoder model in &lt;code&gt;ONNX&lt;/code&gt; format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python3 -m matcha.onnx.infer model.onnx --text &quot;hey&quot; --output-dir ./outputs --vocoder hifigan.small.onnx
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will write &lt;code&gt;.wav&lt;/code&gt; audio files to the output directory.&lt;/p&gt; 
&lt;h2&gt;Extract phoneme alignments from Matcha-TTS&lt;/h2&gt; 
&lt;p&gt;If the dataset is structured as&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;data/
└── LJSpeech-1.1
    ├── metadata.csv
    ├── README
    ├── test.txt
    ├── train.txt
    ├── val.txt
    └── wavs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can extract the phoneme level alignments from a Trained Matcha-TTS model using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python  matcha/utils/get_durations_from_trained_model.py -i dataset_yaml -c &amp;lt;checkpoint&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python  matcha/utils/get_durations_from_trained_model.py -i ljspeech.yaml -c matcha_ljspeech.ckpt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or simply:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-tts-get-durations -i ljspeech.yaml -c matcha_ljspeech.ckpt
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h2&gt;Train using extracted alignments&lt;/h2&gt; 
&lt;p&gt;In the datasetconfig turn on load duration. Example: &lt;code&gt;ljspeech.yaml&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;load_durations: True
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or see an examples in configs/experiment/ljspeech_from_durations.yaml&lt;/p&gt; 
&lt;h2&gt;Citation information&lt;/h2&gt; 
&lt;p&gt;If you use our code or otherwise find this work useful, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;@inproceedings{mehta2024matcha,
  title={Matcha-{TTS}: A fast {TTS} architecture with conditional flow matching},
  author={Mehta, Shivam and Tu, Ruibo and Beskow, Jonas and Sz{\&#39;e}kely, {\&#39;E}va and Henter, Gustav Eje},
  booktitle={Proc. ICASSP},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;Since this code uses &lt;a href=&quot;https://github.com/ashleve/lightning-hydra-template&quot;&gt;Lightning-Hydra-Template&lt;/a&gt;, you have all the powers that come with it.&lt;/p&gt; 
&lt;p&gt;Other source code we would like to acknowledge:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/coqui-ai/TTS/tree/dev&quot;&gt;Coqui-TTS&lt;/a&gt;: For helping me figure out how to make cython binaries pip installable and encouragement&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face Diffusers&lt;/a&gt;: For their awesome diffusers library and its components&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/huawei-noah/Speech-Backbones/tree/main/Grad-TTS&quot;&gt;Grad-TTS&lt;/a&gt;: For the monotonic alignment search source code&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/DiffEqML/torchdyn&quot;&gt;torchdyn&lt;/a&gt;: Useful for trying other ODE solvers during research and development&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://nn.labml.ai/transformers/rope/index.html&quot;&gt;labml.ai&lt;/a&gt;: For the RoPE implementation&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>mrdbourke/pytorch-deep-learning</title>
      <link>https://github.com/mrdbourke/pytorch-deep-learning</link>
      <description>&lt;p&gt;Materials for the Learn PyTorch for Deep Learning: Zero to Mastery course.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Learn PyTorch for Deep Learning&lt;/h1&gt; 
&lt;p&gt;Welcome to the &lt;a href=&quot;https://dbourke.link/ZTMPyTorch&quot;&gt;Zero to Mastery Learn PyTorch for Deep Learning course&lt;/a&gt;, the second best place to learn PyTorch on the internet (the first being the &lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;PyTorch documentation&lt;/a&gt;).&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Update April 2023:&lt;/strong&gt; New &lt;a href=&quot;https://www.learnpytorch.io/pytorch_2_intro/&quot;&gt;tutorial for PyTorch 2.0&lt;/a&gt; is live! And because PyTorch 2.0 is an additive (new features) and backward-compatible release, all previous course materials will &lt;em&gt;still&lt;/em&gt; work with PyTorch 2.0.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://learnpytorch.io&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/misc-pytorch-course-launch-cover-white-text-black-background.jpg&quot; width=&quot;750&quot; alt=&quot;pytorch deep learning by zero to mastery cover photo with different sections of the course&quot;&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Contents of this page&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning#course-materialsoutline&quot;&gt;Course materials/outline&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning#about-this-course&quot;&gt;About this course&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning#status&quot;&gt;Status&lt;/a&gt; (the progress of the course creation)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning#log&quot;&gt;Log&lt;/a&gt; (a log of the course material creation process)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Course materials/outline&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;📖 &lt;strong&gt;Online book version:&lt;/strong&gt; All of course materials are available in a readable online book at &lt;a href=&quot;https://learnpytorch.io&quot;&gt;learnpytorch.io&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;🎥 &lt;strong&gt;First five sections on YouTube:&lt;/strong&gt; Learn Pytorch in a day by watching the &lt;a href=&quot;https://youtu.be/Z_ikDlimN6A&quot;&gt;first 25-hours of material&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;🔬 &lt;strong&gt;Course focus:&lt;/strong&gt; code, code, code, experiment, experiment, experiment.&lt;/li&gt; 
 &lt;li&gt;🏃‍♂️ &lt;strong&gt;Teaching style:&lt;/strong&gt; &lt;a href=&quot;https://sive.rs/kimo&quot;&gt;https://sive.rs/kimo&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;🤔 &lt;strong&gt;Ask a question:&lt;/strong&gt; See the &lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/discussions&quot;&gt;GitHub Discussions page&lt;/a&gt; for existing questions/ask your own.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Section&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;What does it cover?&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Exercises &amp;amp; Extra-curriculum&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Slides&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/00_pytorch_fundamentals/&quot;&gt;00 - PyTorch Fundamentals&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Many fundamental PyTorch operations used for deep learning and neural networks.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/00_pytorch_fundamentals/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/00_pytorch_and_deep_learning_fundamentals.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/01_pytorch_workflow/&quot;&gt;01 - PyTorch Workflow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Provides an outline for approaching deep learning problems and building neural networks with PyTorch.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/01_pytorch_workflow/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/01_pytorch_workflow.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/02_pytorch_classification/&quot;&gt;02 - PyTorch Neural Network Classification&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Uses the PyTorch workflow from 01 to go through a neural network classification problem.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/02_pytorch_classification/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/02_pytorch_classification.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/03_pytorch_computer_vision/&quot;&gt;03 - PyTorch Computer Vision&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Let&#39;s see how PyTorch can be used for computer vision problems using the same workflow from 01 &amp;amp; 02.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/03_pytorch_computer_vision/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/03_pytorch_computer_vision.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/04_pytorch_custom_datasets/&quot;&gt;04 - PyTorch Custom Datasets&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;How do you load a custom dataset into PyTorch? Also we&#39;ll be laying the foundations in this notebook for our modular code (covered in 05).&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/04_pytorch_custom_datasets/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/04_pytorch_custom_datasets.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/05_pytorch_going_modular/&quot;&gt;05 - PyTorch Going Modular&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;PyTorch is designed to be modular, let&#39;s turn what we&#39;ve created into a series of Python scripts (this is how you&#39;ll often find PyTorch code in the wild).&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/05_pytorch_going_modular/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/05_pytorch_going_modular.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/06_pytorch_transfer_learning/&quot;&gt;06 - PyTorch Transfer Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Let&#39;s take a well performing pre-trained model and adjust it to one of our own problems.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/06_pytorch_transfer_learning/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/06_pytorch_transfer_learning.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/07_pytorch_experiment_tracking/&quot;&gt;07 - Milestone Project 1: PyTorch Experiment Tracking&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;We&#39;ve built a bunch of models... wouldn&#39;t it be good to track how they&#39;re all going?&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/07_pytorch_experiment_tracking/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/07_pytorch_experiment_tracking.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/08_pytorch_paper_replicating/&quot;&gt;08 - Milestone Project 2: PyTorch Paper Replicating&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;PyTorch is the most popular deep learning framework for machine learning research, let&#39;s see why by replicating a machine learning paper.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/08_pytorch_paper_replicating/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/08_pytorch_paper_replicating.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/09_pytorch_model_deployment/&quot;&gt;09 - Milestone Project 3: Model Deployment&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;So we&#39;ve built a working PyTorch model... how do we get it in the hands of others? Hint: deploy it to the internet.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/09_pytorch_model_deployment/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/09_pytorch_model_deployment.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/pytorch_extra_resources/&quot;&gt;PyTorch Extra Resources&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;This course covers a large amount of PyTorch and deep learning but the field of machine learning is vast, inside here you&#39;ll find recommended books and resources for: PyTorch and deep learning, ML engineering, NLP (natural language processing), time series data, where to find datasets and more.&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/pytorch_cheatsheet/&quot;&gt;PyTorch Cheatsheet&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A very quick overview of some of the main features of PyTorch plus links to various resources where more can be found in the course and in the PyTorch documentation.&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/pytorch_2_intro/&quot;&gt;A Quick PyTorch 2.0 Tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A fasssssst introduction to PyTorch 2.0, what&#39;s new and how to get started along with resources to learn more.&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Status&lt;/h2&gt; 
&lt;p&gt;All materials completed and videos published on Zero to Mastery!&lt;/p&gt; 
&lt;p&gt;See the project page for work-in-progress board - &lt;a href=&quot;https://github.com/users/mrdbourke/projects/1&quot;&gt;https://github.com/users/mrdbourke/projects/1&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Total video count:&lt;/strong&gt; 321&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Done skeleton code for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Done annotations (text) for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Done images for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Done keynotes for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Done exercises and solutions for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning#log&quot;&gt;log&lt;/a&gt; for almost daily updates.&lt;/p&gt; 
&lt;h2&gt;About this course&lt;/h2&gt; 
&lt;h3&gt;Who is this course for?&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;You:&lt;/strong&gt; Are a beginner in the field of machine learning or deep learning and would like to learn PyTorch.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;This course:&lt;/strong&gt; Teaches you PyTorch and many machine learning concepts in a hands-on, code-first way.&lt;/p&gt; 
&lt;p&gt;If you already have 1-year+ experience in machine learning, this course may help but it is specifically designed to be beginner-friendly.&lt;/p&gt; 
&lt;h3&gt;What are the prerequisites?&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;3-6 months coding Python.&lt;/li&gt; 
 &lt;li&gt;At least one beginner machine learning course (however this might be able to be skipped, resources are linked for many different topics).&lt;/li&gt; 
 &lt;li&gt;Experience using Jupyter Notebooks or Google Colab (though you can pick this up as we go along).&lt;/li&gt; 
 &lt;li&gt;A willingness to learn (most important).&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For 1 &amp;amp; 2, I&#39;d recommend the &lt;a href=&quot;https://dbourke.link/ZTMMLcourse&quot;&gt;Zero to Mastery Data Science and Machine Learning Bootcamp&lt;/a&gt;, it&#39;ll teach you the fundamentals of machine learning and Python (I&#39;m biased though, I also teach that course).&lt;/p&gt; 
&lt;h3&gt;How is the course taught?&lt;/h3&gt; 
&lt;p&gt;All of the course materials are available for free in an online book at &lt;a href=&quot;https://learnpytorch.io&quot;&gt;learnpytorch.io&lt;/a&gt;. If you like to read, I&#39;d recommend going through the resources there.&lt;/p&gt; 
&lt;p&gt;If you prefer to learn via video, the course is also taught in apprenticeship-style format, meaning I write PyTorch code, you write PyTorch code.&lt;/p&gt; 
&lt;p&gt;There&#39;s a reason the course motto&#39;s include &lt;em&gt;if in doubt, run the code&lt;/em&gt; and &lt;em&gt;experiment, experiment, experiment!&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;My whole goal is to help you to do one thing: learn machine learning by writing PyTorch code.&lt;/p&gt; 
&lt;p&gt;The code is all written via &lt;a href=&quot;https://colab.research.google.com&quot;&gt;Google Colab Notebooks&lt;/a&gt; (you could also use Jupyter Notebooks), an incredible free resource to experiment with machine learning.&lt;/p&gt; 
&lt;h3&gt;What will I get if I finish the course?&lt;/h3&gt; 
&lt;p&gt;There&#39;s certificates and all that jazz if you go through the videos.&lt;/p&gt; 
&lt;p&gt;But certificates are meh.&lt;/p&gt; 
&lt;p&gt;You can consider this course a machine learning momentum builder.&lt;/p&gt; 
&lt;p&gt;By the end, you&#39;ll have written hundreds of lines of PyTorch code.&lt;/p&gt; 
&lt;p&gt;And will have been exposed to many of the most important concepts in machine learning.&lt;/p&gt; 
&lt;p&gt;So when you go to build your own machine learning projects or inspect a public machine learning project made with PyTorch, it&#39;ll feel familiar and if it doesn&#39;t, at least you&#39;ll know where to look.&lt;/p&gt; 
&lt;h3&gt;What will I build in the course?&lt;/h3&gt; 
&lt;p&gt;We start with the barebone fundamentals of PyTorch and machine learning, so even if you&#39;re new to machine learning you&#39;ll be caught up to speed.&lt;/p&gt; 
&lt;p&gt;Then we’ll explore more advanced areas including PyTorch neural network classification, PyTorch workflows, computer vision, custom datasets, experiment tracking, model deployment, and my personal favourite: transfer learning, a powerful technique for taking what one machine learning model has learned on another problem and applying it to your own!&lt;/p&gt; 
&lt;p&gt;Along the way, you’ll build three milestone projects surrounding an overarching project called FoodVision, a neural network computer vision model to classify images of food.&lt;/p&gt; 
&lt;p&gt;These milestone projects will help you practice using PyTorch to cover important machine learning concepts and create a portfolio you can show employers and say &quot;here&#39;s what I&#39;ve done&quot;.&lt;/p&gt; 
&lt;h3&gt;How do I get started?&lt;/h3&gt; 
&lt;p&gt;You can read the materials on any device but this course is best viewed and coded along within a desktop browser.&lt;/p&gt; 
&lt;p&gt;The course uses a free tool called Google Colab. If you&#39;ve got no experience with it, I&#39;d go through the free &lt;a href=&quot;https://colab.research.google.com/notebooks/basic_features_overview.ipynb&quot;&gt;Introduction to Google Colab tutorial&lt;/a&gt; and then come back here.&lt;/p&gt; 
&lt;p&gt;To start:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Click on one of the notebook or section links above like &quot;&lt;a href=&quot;https://www.learnpytorch.io/00_pytorch_fundamentals/&quot;&gt;00. PyTorch Fundamentals&lt;/a&gt;&quot;.&lt;/li&gt; 
 &lt;li&gt;Click the &quot;Open in Colab&quot; button up the top.&lt;/li&gt; 
 &lt;li&gt;Press SHIFT+Enter a few times and see what happens.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;My question isn&#39;t answered&lt;/h3&gt; 
&lt;p&gt;Please leave a &lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/discussions&quot;&gt;discussion&lt;/a&gt; or send me an email directly: daniel (at) mrdbourke (dot) com.&lt;/p&gt; 
&lt;h2&gt;Log&lt;/h2&gt; 
&lt;p&gt;Almost daily updates of what&#39;s happening.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;15 May 2023 - PyTorch 2.0 tutorial finished + videos added to ZTM/Udemy, see code: &lt;a href=&quot;https://www.learnpytorch.io/pytorch_2_intro/&quot;&gt;https://www.learnpytorch.io/pytorch_2_intro/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;13 Apr 2023 - update PyTorch 2.0 notebook&lt;/li&gt; 
 &lt;li&gt;30 Mar 2023 - update PyTorch 2.0 notebook with more info/clean code&lt;/li&gt; 
 &lt;li&gt;23 Mar 2023 - upgrade PyTorch 2.0 tutorial with annotations and images&lt;/li&gt; 
 &lt;li&gt;13 Mar 2023 - add starter code for PyTorch 2.0 tutorial&lt;/li&gt; 
 &lt;li&gt;18 Nov 2022 - add a reference for 3 most common errors in PyTorch + links to course sections for more: &lt;a href=&quot;https://www.learnpytorch.io/pytorch_most_common_errors/&quot;&gt;https://www.learnpytorch.io/pytorch_most_common_errors/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;9 Nov 2022 - add PyTorch cheatsheet for a very quick overview of the main features of PyTorch + links to course sections: &lt;a href=&quot;https://www.learnpytorch.io/pytorch_cheatsheet/&quot;&gt;https://www.learnpytorch.io/pytorch_cheatsheet/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;9 Nov 2022 - full course materials (300+ videos) are now live on Udemy! You can sign up here: &lt;a href=&quot;https://www.udemy.com/course/pytorch-for-deep-learning/?couponCode=ZTMGOODIES7&quot;&gt;https://www.udemy.com/course/pytorch-for-deep-learning/?couponCode=ZTMGOODIES7&lt;/a&gt; (launch deal code valid for 3-4 days from this line)&lt;/li&gt; 
 &lt;li&gt;4 Nov 2022 - add a notebook for PyTorch Cheatsheet in &lt;code&gt;extras/&lt;/code&gt; (a simple overview of many of the most important functionality of PyTorch)&lt;/li&gt; 
 &lt;li&gt;2 Oct 2022 - all videos for section 08 and 09 published (100+ videos for the last two sections)!&lt;/li&gt; 
 &lt;li&gt;30 Aug 2022 - recorded 15 videos for 09, total videos: 321, finished section 09 videos!!!! ... even bigger than 08!!&lt;/li&gt; 
 &lt;li&gt;29 Aug 2022 - recorded 16 videos for 09, total videos: 306&lt;/li&gt; 
 &lt;li&gt;28 Aug 2022 - recorded 11 videos for 09, total videos: 290&lt;/li&gt; 
 &lt;li&gt;27 Aug 2022 - recorded 16 videos for 09, total videos: 279&lt;/li&gt; 
 &lt;li&gt;26 Aug 2022 - add finishing touchs to notebook 09, add slides for 09, create solutions and exercises for 09&lt;/li&gt; 
 &lt;li&gt;25 Aug 2022 - add annotations and cleanup 09, remove TK&#39;s, cleanup images, make slides for 09&lt;/li&gt; 
 &lt;li&gt;24 Aug 2022 - add annotations to 09, main takeaways, exercises and extra-curriculum done&lt;/li&gt; 
 &lt;li&gt;23 Aug 2022 - add annotations to 09, add plenty of images/slides&lt;/li&gt; 
 &lt;li&gt;22 Aug 2022 - add annotations to 09, start working on slides/images&lt;/li&gt; 
 &lt;li&gt;20 Aug 2022 - add annotations to 09&lt;/li&gt; 
 &lt;li&gt;19 Aug 2022 - add annotations to 09, check out the awesome demos!&lt;/li&gt; 
 &lt;li&gt;18 Aug 2022 - add annotations to 09&lt;/li&gt; 
 &lt;li&gt;17 Aug 2022 - add annotations to 09&lt;/li&gt; 
 &lt;li&gt;16 Aug 2022 - add annotations to 09&lt;/li&gt; 
 &lt;li&gt;15 Aug 2022 - add annotations to 09&lt;/li&gt; 
 &lt;li&gt;13 Aug 2022 - add annotations to 09&lt;/li&gt; 
 &lt;li&gt;12 Aug 2022 - add demo files for notebook 09 to &lt;code&gt;demos/&lt;/code&gt;, start annotating notebook 09 with explainer text&lt;/li&gt; 
 &lt;li&gt;11 Aug 2022 - finish skeleton code for notebook 09, course finishes deploying 2x models, one for FoodVision Mini &amp;amp; one for (secret)&lt;/li&gt; 
 &lt;li&gt;10 Aug 2022 - add section for PyTorch Extra Resources (places to learn more about PyTorch/deep learning): &lt;a href=&quot;https://www.learnpytorch.io/pytorch_extra_resources/&quot;&gt;https://www.learnpytorch.io/pytorch_extra_resources/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;09 Aug 2022 - add more skeleton code to notebook 09&lt;/li&gt; 
 &lt;li&gt;08 Aug 2022 - create draft notebook for 09, end goal to deploy FoodVision Mini model and make it publically accessible&lt;/li&gt; 
 &lt;li&gt;05 Aug 2022 - recorded 11 videos for 08, total videos: 263, section 08 videos finished!... the biggest section so far&lt;/li&gt; 
 &lt;li&gt;04 Aug 2022 - recorded 13 videos for 08, total videos: 252&lt;/li&gt; 
 &lt;li&gt;03 Aug 2022 - recorded 3 videos for 08, total videos: 239&lt;/li&gt; 
 &lt;li&gt;02 Aug 2022 - recorded 12 videos for 08, total videos: 236&lt;/li&gt; 
 &lt;li&gt;30 July 2022 - recorded 11 videos for 08, total videos: 224&lt;/li&gt; 
 &lt;li&gt;29 July 2022 - add exercises + solutions for 08, see live walkthrough on YouTube: &lt;a href=&quot;https://youtu.be/tjpW_BY8y3g&quot;&gt;https://youtu.be/tjpW_BY8y3g&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;28 July 2022 - add slides for 08&lt;/li&gt; 
 &lt;li&gt;27 July 2022 - cleanup much of 08, start on slides for 08, exercises and extra-curriculum next&lt;/li&gt; 
 &lt;li&gt;26 July 2022 - add annotations and images for 08&lt;/li&gt; 
 &lt;li&gt;25 July 2022 - add annotations for 08&lt;/li&gt; 
 &lt;li&gt;24 July 2022 - launched first half of course (notebooks 00-04) in a single video (25+ hours!!!) on YouTube: &lt;a href=&quot;https://youtu.be/Z_ikDlimN6A&quot;&gt;https://youtu.be/Z_ikDlimN6A&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;21 July 2022 - add annotations and images for 08&lt;/li&gt; 
 &lt;li&gt;20 July 2022 - add annotations and images for 08, getting so close! this is an epic section&lt;/li&gt; 
 &lt;li&gt;19 July 2022 - add annotations and images for 08&lt;/li&gt; 
 &lt;li&gt;15 July 2022 - add annotations and images for 08&lt;/li&gt; 
 &lt;li&gt;14 July 2022 - add annotations for 08&lt;/li&gt; 
 &lt;li&gt;12 July 2022 - add annotations for 08, woo woo this is bigggg section!&lt;/li&gt; 
 &lt;li&gt;11 July 2022 - add annotations for 08&lt;/li&gt; 
 &lt;li&gt;9 July 2022 - add annotations for 08&lt;/li&gt; 
 &lt;li&gt;8 July 2022 - add a bunch of annotations to 08&lt;/li&gt; 
 &lt;li&gt;6 July 2022 - course launched on ZTM Academy with videos for sections 00-07! 🚀 - &lt;a href=&quot;https://dbourke.link/ZTMPyTorch&quot;&gt;https://dbourke.link/ZTMPyTorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;1 July 2022 - add annotations and images for 08&lt;/li&gt; 
 &lt;li&gt;30 June 2022 - add annotations for 08&lt;/li&gt; 
 &lt;li&gt;28 June 2022 - recorded 11 videos for section 07, total video count 213, all videos for section 07 complete!&lt;/li&gt; 
 &lt;li&gt;27 June 2022 - recorded 11 videos for section 07, total video count 202&lt;/li&gt; 
 &lt;li&gt;25 June 2022 - recreated 7 videos for section 06 to include updated APIs, total video count 191&lt;/li&gt; 
 &lt;li&gt;24 June 2022 - recreated 12 videos for section 06 to include updated APIs&lt;/li&gt; 
 &lt;li&gt;23 June 2022 - finish annotations for 07, add exercise template and solutions for 07 + video walkthrough on YouTube: &lt;a href=&quot;https://youtu.be/cO_r2FYcAjU&quot;&gt;https://youtu.be/cO_r2FYcAjU&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;21 June 2022 - make 08 runnable end-to-end, add images and annotations for 07&lt;/li&gt; 
 &lt;li&gt;17 June 2022 - fix up 06, 07 v2 for upcoming torchvision version upgrade, add plenty of annotations to 08&lt;/li&gt; 
 &lt;li&gt;13 June 2022 - add notebook 08 first version, starting to replicate the Vision Transformer paper&lt;/li&gt; 
 &lt;li&gt;10 June 2022 - add annotations for 07 v2&lt;/li&gt; 
 &lt;li&gt;09 June 2022 - create 07 v2 for &lt;code&gt;torchvision&lt;/code&gt; v0.13 (this will replace 07 v1 when &lt;code&gt;torchvision=0.13&lt;/code&gt; is released)&lt;/li&gt; 
 &lt;li&gt;08 June 2022 - adapt 06 v2 for &lt;code&gt;torchvision&lt;/code&gt; v0.13 (this will replace 06 v1 when &lt;code&gt;torchvision=0.13&lt;/code&gt; is released)&lt;/li&gt; 
 &lt;li&gt;07 June 2022 - create notebook 06 v2 for upcoming &lt;code&gt;torchvision&lt;/code&gt; v0.13 update (new transfer learning methods)&lt;/li&gt; 
 &lt;li&gt;04 June 2022 - add annotations for 07&lt;/li&gt; 
 &lt;li&gt;03 June 2022 - huuuuuuge amount of annotations added to 07&lt;/li&gt; 
 &lt;li&gt;31 May 2022 - add a bunch of annotations for 07, make code runnable end-to-end&lt;/li&gt; 
 &lt;li&gt;30 May 2022 - record 4 videos for 06, finished section 06, onto section 07, total videos 186&lt;/li&gt; 
 &lt;li&gt;28 May 2022 - record 10 videos for 06, total videos 182&lt;/li&gt; 
 &lt;li&gt;24 May 2022 - add solutions and exercises for 06&lt;/li&gt; 
 &lt;li&gt;23 May 2022 - finished annotations and images for 06, time to do exercises and solutions&lt;/li&gt; 
 &lt;li&gt;22 May 2202 - add plenty of images to 06&lt;/li&gt; 
 &lt;li&gt;18 May 2022 - add plenty of annotations to 06&lt;/li&gt; 
 &lt;li&gt;17 May 2022 - added a bunch of annotations for section 06&lt;/li&gt; 
 &lt;li&gt;16 May 2022 - recorded 10 videos for section 05, finish videos for section 05 ✅&lt;/li&gt; 
 &lt;li&gt;12 May 2022 - added exercises and solutions for 05&lt;/li&gt; 
 &lt;li&gt;11 May 2022 - clean up part 1 and part 2 notebooks for 05, make slides for 05, start on exercises and solutions for 05&lt;/li&gt; 
 &lt;li&gt;10 May 2022 - huuuuge updates to the 05 section, see the website, it looks pretty: &lt;a href=&quot;https://www.learnpytorch.io/05_pytorch_going_modular/&quot;&gt;https://www.learnpytorch.io/05_pytorch_going_modular/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;09 May 2022 - add a bunch of materials for 05, cleanup docs&lt;/li&gt; 
 &lt;li&gt;08 May 2022 - add a bunch of materials for 05&lt;/li&gt; 
 &lt;li&gt;06 May 2022 - continue making materials for 05&lt;/li&gt; 
 &lt;li&gt;05 May 2022 - update section 05 with headings/outline&lt;/li&gt; 
 &lt;li&gt;28 Apr 2022 - recorded 13 videos for 04, finished videos for 04, now to make materials for 05&lt;/li&gt; 
 &lt;li&gt;27 Apr 2022 - recorded 3 videos for 04&lt;/li&gt; 
 &lt;li&gt;26 Apr 2022 - recorded 10 videos for 04&lt;/li&gt; 
 &lt;li&gt;25 Apr 2022 - recorded 11 videos for 04&lt;/li&gt; 
 &lt;li&gt;24 Apr 2022 - prepared slides for 04&lt;/li&gt; 
 &lt;li&gt;23 Apr 2022 - recorded 6 videos for 03, finished videos for 03, now to 04&lt;/li&gt; 
 &lt;li&gt;22 Apr 2022 - recorded 5 videos for 03&lt;/li&gt; 
 &lt;li&gt;21 Apr 2022 - recorded 9 videos for 03&lt;/li&gt; 
 &lt;li&gt;20 Apr 2022 - recorded 3 videos for 03&lt;/li&gt; 
 &lt;li&gt;19 Apr 2022 - recorded 11 videos for 03&lt;/li&gt; 
 &lt;li&gt;18 Apr 2022 - finish exercises/solutions for 04, added live-coding walkthrough of 04 exercises/solutions on YouTube: &lt;a href=&quot;https://youtu.be/vsFMF9wqWx0&quot;&gt;https://youtu.be/vsFMF9wqWx0&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;16 Apr 2022 - finish exercises/solutions for 03, added live-coding walkthrough of 03 exercises/solutions on YouTube: &lt;a href=&quot;https://youtu.be/_PibmqpEyhA&quot;&gt;https://youtu.be/_PibmqpEyhA&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;14 Apr 2022 - add final images/annotations for 04, begin on exercises/solutions for 03 &amp;amp; 04&lt;/li&gt; 
 &lt;li&gt;13 Apr 2022 - add more images/annotations for 04&lt;/li&gt; 
 &lt;li&gt;3 Apr 2022 - add more annotations for 04&lt;/li&gt; 
 &lt;li&gt;2 Apr 2022 - add more annotations for 04&lt;/li&gt; 
 &lt;li&gt;1 Apr 2022 - add more annotations for 04&lt;/li&gt; 
 &lt;li&gt;31 Mar 2022 - add more annotations for 04&lt;/li&gt; 
 &lt;li&gt;29 Mar 2022 - add more annotations for 04&lt;/li&gt; 
 &lt;li&gt;27 Mar 2022 - starting to add annotations for 04&lt;/li&gt; 
 &lt;li&gt;26 Mar 2022 - making dataset for 04&lt;/li&gt; 
 &lt;li&gt;25 Mar 2022 - make slides for 03&lt;/li&gt; 
 &lt;li&gt;24 Mar 2022 - fix error for 03 not working in docs (finally)&lt;/li&gt; 
 &lt;li&gt;23 Mar 2022 - add more images for 03&lt;/li&gt; 
 &lt;li&gt;22 Mar 2022 - add images for 03&lt;/li&gt; 
 &lt;li&gt;20 Mar 2022 - add more annotations for 03&lt;/li&gt; 
 &lt;li&gt;18 Mar 2022 - add more annotations for 03&lt;/li&gt; 
 &lt;li&gt;17 Mar 2022 - add more annotations for 03&lt;/li&gt; 
 &lt;li&gt;16 Mar 2022 - add more annotations for 03&lt;/li&gt; 
 &lt;li&gt;15 Mar 2022 - add more annotations for 03&lt;/li&gt; 
 &lt;li&gt;14 Mar 2022 - start adding annotations for notebook 03, see the work in progress here: &lt;a href=&quot;https://www.learnpytorch.io/03_pytorch_computer_vision/&quot;&gt;https://www.learnpytorch.io/03_pytorch_computer_vision/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;12 Mar 2022 - recorded 12 videos for 02, finished section 02, now onto making materials for 03, 04, 05&lt;/li&gt; 
 &lt;li&gt;11 Mar 2022 - recorded 9 videos for 02&lt;/li&gt; 
 &lt;li&gt;10 Mar 2022 - recorded 10 videos for 02&lt;/li&gt; 
 &lt;li&gt;9 Mar 2022 - cleaning up slides/code for 02, getting ready for recording&lt;/li&gt; 
 &lt;li&gt;8 Mar 2022 - recorded 9 videos for section 01, finished section 01, now onto 02&lt;/li&gt; 
 &lt;li&gt;7 Mar 2022 - recorded 4 videos for section 01&lt;/li&gt; 
 &lt;li&gt;6 Mar 2022 - recorded 4 videos for section 01&lt;/li&gt; 
 &lt;li&gt;4 Mar 2022 - recorded 10 videos for section 01&lt;/li&gt; 
 &lt;li&gt;20 Feb 2022 - recorded 8 videos for section 00, finished section, now onto 01&lt;/li&gt; 
 &lt;li&gt;18 Feb 2022 - recorded 13 videos for section 00&lt;/li&gt; 
 &lt;li&gt;17 Feb 2022 - recorded 11 videos for section 00&lt;/li&gt; 
 &lt;li&gt;16 Feb 2022 - added setup guide&lt;/li&gt; 
 &lt;li&gt;12 Feb 2022 - tidy up README with table of course materials, finish images and slides for 01&lt;/li&gt; 
 &lt;li&gt;10 Feb 2022 - finished slides and images for 00, notebook is ready for publishing: &lt;a href=&quot;https://www.learnpytorch.io/00_pytorch_fundamentals/&quot;&gt;https://www.learnpytorch.io/00_pytorch_fundamentals/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;01-07 Feb 2022 - add annotations for 02, finished, still need images, going to work on exercises/solutions today&lt;/li&gt; 
 &lt;li&gt;31 Jan 2022 - start adding annotations for 02&lt;/li&gt; 
 &lt;li&gt;28 Jan 2022 - add exercies and solutions for 01&lt;/li&gt; 
 &lt;li&gt;26 Jan 2022 - lots more annotations to 01, should be finished tomorrow, will do exercises + solutions then too&lt;/li&gt; 
 &lt;li&gt;24 Jan 2022 - add a bunch of annotations to 01&lt;/li&gt; 
 &lt;li&gt;21 Jan 2022 - start adding annotations for 01&lt;/li&gt; 
 &lt;li&gt;20 Jan 2022 - finish annotations for 00 (still need to add images), add exercises and solutions for 00&lt;/li&gt; 
 &lt;li&gt;19 Jan 2022 - add more annotations for 00&lt;/li&gt; 
 &lt;li&gt;18 Jan 2022 - add more annotations for 00&lt;/li&gt; 
 &lt;li&gt;17 Jan 2022 - back from holidays, adding more annotations to 00&lt;/li&gt; 
 &lt;li&gt;10 Dec 2021 - start adding annotations for 00&lt;/li&gt; 
 &lt;li&gt;9 Dec 2021 - Created a website for the course (&lt;a href=&quot;https://learnpytorch.io&quot;&gt;learnpytorch.io&lt;/a&gt;) you&#39;ll see updates posted there as development continues&lt;/li&gt; 
 &lt;li&gt;8 Dec 2021 - Clean up notebook 07, starting to go back through code and add annotations&lt;/li&gt; 
 &lt;li&gt;26 Nov 2021 - Finish skeleton code for 07, added four different experiments, need to clean up and make more straightforward&lt;/li&gt; 
 &lt;li&gt;25 Nov 2021 - clean code for 06, add skeleton code for 07 (experiment tracking)&lt;/li&gt; 
 &lt;li&gt;24 Nov 2021 - Update 04, 05, 06 notebooks for easier digestion and learning, each section should cover a max of 3 big ideas, 05 is now dedicated to turning notebook code into modular code&lt;/li&gt; 
 &lt;li&gt;22 Nov 2021 - Update 04 train and test functions to make more straightforward&lt;/li&gt; 
 &lt;li&gt;19 Nov 2021 - Added 05 (transfer learning) notebook, update custom data loading code in 04&lt;/li&gt; 
 &lt;li&gt;18 Nov 2021 - Updated vision code for 03 and added custom dataset loading code in 04&lt;/li&gt; 
 &lt;li&gt;12 Nov 2021 - Added a bunch of skeleton code to notebook 04 for custom dataset loading, next is modelling with custom data&lt;/li&gt; 
 &lt;li&gt;10 Nov 2021 - researching best practice for custom datasets for 04&lt;/li&gt; 
 &lt;li&gt;9 Nov 2021 - Update 03 skeleton code to finish off building CNN model, onto 04 for loading custom datasets&lt;/li&gt; 
 &lt;li&gt;4 Nov 2021 - Add GPU code to 03 + train/test loops + &lt;code&gt;helper_functions.py&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;3 Nov 2021 - Add basic start for 03, going to finish by end of week&lt;/li&gt; 
 &lt;li&gt;29 Oct 2021 - Tidied up skeleton code for 02, still a few more things to clean/tidy, created 03&lt;/li&gt; 
 &lt;li&gt;28 Oct 2021 - Finished skeleton code for 02, going to clean/tidy tomorrow, 03 next week&lt;/li&gt; 
 &lt;li&gt;27 Oct 2021 - add a bunch of code for 02, going to finish tomorrow/by end of week&lt;/li&gt; 
 &lt;li&gt;26 Oct 2021 - update 00, 01, 02 with outline/code, skeleton code for 00 &amp;amp; 01 done, 02 next&lt;/li&gt; 
 &lt;li&gt;23, 24 Oct 2021 - update 00 and 01 notebooks with more outline/code&lt;/li&gt; 
 &lt;li&gt;20 Oct 2021 - add v0 outlines for 01 and 02, add rough outline of course to README, this course will focus on less but better&lt;/li&gt; 
 &lt;li&gt;19 Oct 2021 - Start repo 🔥, add fundamentals notebook draft v0&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>AI4Finance-Foundation/FinRobot</title>
      <link>https://github.com/AI4Finance-Foundation/FinRobot</link>
      <description>&lt;p&gt;FinRobot: An Open-Source AI Agent Platform for Financial Analysis using LLMs 🚀 🚀 🚀&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; width=&quot;30%&quot; alt=&quot;image&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinGPT/assets/31713746/e0371951-1ce1-488e-aa25-0992dafcc139&quot;&gt; 
&lt;/div&gt; 
&lt;h1&gt;FinRobot: An Open-Source AI Agent Platform for Financial Analysis using Large Language Models&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;%5Bhttps://pepy.tech/project/finrobot%5D(https://pepy.tech/project/finrobot)&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/finrobot&quot; alt=&quot;Downloads&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pepy.tech/project/finrobot&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/finrobot/week&quot; alt=&quot;Downloads&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.python.org/downloads/release/python-360/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.6-blue.svg?sanitize=true&quot; alt=&quot;Python 3.8&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/finrobot/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/finrobot.svg?sanitize=true&quot; alt=&quot;PyPI&quot;&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/github/license/AI4Finance-Foundation/finrobot.svg?color=brightgreen&quot; alt=&quot;License&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/issues-raw/AI4Finance-Foundation/finrobot?label=Issues&quot; alt=&quot;&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/issues-closed-raw/AI4Finance-Foundation/finrobot?label=Closed+Issues&quot; alt=&quot;&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/issues-pr-raw/AI4Finance-Foundation/finrobot?label=Open+PRs&quot; alt=&quot;&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/issues-pr-closed-raw/AI4Finance-Foundation/finrobot?label=Closed+PRs&quot; alt=&quot;&quot;&gt;&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRobot/master/figs/logo_white_background.jpg&quot; width=&quot;40%&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;FinRobot&lt;/strong&gt; is an AI Agent Platform that transcends the scope of FinGPT, representing a comprehensive solution meticulously designed for financial applications. It integrates &lt;strong&gt;a diverse array of AI technologies&lt;/strong&gt;, extending beyond mere language models. This expansive vision highlights the platform&#39;s versatility and adaptability, addressing the multifaceted needs of the financial industry.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Concept of AI Agent&lt;/strong&gt;: an AI Agent is an intelligent entity that uses large language models as its brain to perceive its environment, make decisions, and execute actions. Unlike traditional artificial intelligence, AI Agents possess the ability to independently think and utilize tools to progressively achieve given objectives.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14767&quot;&gt;Whitepaper of FinRobot&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/trsr8SXpW5&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/trsr8SXpW5&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://api.visitorbadge.io/api/VisitorHit?user=AI4Finance-Foundation&amp;amp;repo=FinRobot&amp;amp;countColor=%23B17A&quot; alt=&quot;Visitors&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;FinRobot Ecosystem&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/6b30d9c1-35e5-4d36-a138-7e2769718f62&quot; width=&quot;90%&quot;&gt; 
&lt;/div&gt; 
&lt;h3&gt;The overall framework of FinRobot is organized into four distinct layers, each designed to address specific aspects of financial AI processing and application:&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Financial AI Agents Layer&lt;/strong&gt;: The Financial AI Agents Layer now includes Financial Chain-of-Thought (CoT) prompting, enhancing complex analysis and decision-making capacity. Market Forecasting Agents, Document Analysis Agents, and Trading Strategies Agents utilize CoT to dissect financial challenges into logical steps, aligning their advanced algorithms and domain expertise with the evolving dynamics of financial markets for precise, actionable insights.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Financial LLMs Algorithms Layer&lt;/strong&gt;: The Financial LLMs Algorithms Layer configures and utilizes specially tuned models tailored to specific domains and global market analysis.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLMOps and DataOps Layers&lt;/strong&gt;: The LLMOps layer implements a multi-source integration strategy that selects the most suitable LLMs for specific financial tasks, utilizing a range of state-of-the-art models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-source LLM Foundation Models Layer&lt;/strong&gt;: This foundational layer supports the plug-and-play functionality of various general and specialized LLMs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;FinRobot: Agent Workflow&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/ff8033be-2326-424a-ac11-17e2c9c4983d&quot; width=&quot;60%&quot;&gt; 
&lt;/div&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Perception&lt;/strong&gt;: This module captures and interprets multimodal financial data from market feeds, news, and economic indicators, using sophisticated techniques to structure the data for thorough analysis.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Brain&lt;/strong&gt;: Acting as the core processing unit, this module perceives data from the Perception module with LLMs and utilizes Financial Chain-of-Thought (CoT) processes to generate structured instructions.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Action&lt;/strong&gt;: This module executes instructions from the Brain module, applying tools to translate analytical insights into actionable outcomes. Actions include trading, portfolio adjustments, generating reports, or sending alerts, thereby actively influencing the financial environment.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;FinRobot: Smart Scheduler&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/06fa0b78-ac53-48d3-8a6e-98d15386327e&quot; width=&quot;60%&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;The Smart Scheduler is central to ensuring model diversity and optimizing the integration and selection of the most appropriate LLM for each task.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Director Agent&lt;/strong&gt;: This component orchestrates the task assignment process, ensuring that tasks are allocated to agents based on their performance metrics and suitability for specific tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agent Registration&lt;/strong&gt;: Manages the registration and tracks the availability of agents within the system, facilitating an efficient task allocation process.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agent Adaptor&lt;/strong&gt;: Tailor agent functionalities to specific tasks, enhancing their performance and integration within the overall system.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task Manager&lt;/strong&gt;: Manages and stores different general and fine-tuned LLMs-based agents tailored for various financial tasks, updated periodically to ensure relevance and efficacy.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;File Structure&lt;/h2&gt; 
&lt;p&gt;The main folder &lt;strong&gt;finrobot&lt;/strong&gt; has three subfolders &lt;strong&gt;agents, data_source, functional&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;FinRobot
├── finrobot (main folder)
│   ├── agents
│   	├── agent_library.py
│   	└── workflow.py
│   ├── data_source
│   	├── finnhub_utils.py
│   	├── finnlp_utils.py
│   	├── fmp_utils.py
│   	├── sec_utils.py
│   	└── yfinance_utils.py
│   ├── functional
│   	├── analyzer.py
│   	├── charting.py
│   	├── coding.py
│   	├── quantitative.py
│   	├── reportlab.py
│   	└── text.py
│   ├── toolkits.py
│   └── utils.py
│
├── configs
├── experiments
├── tutorials_beginner (hands-on tutorial)
│   ├── agent_fingpt_forecaster.ipynb
│   └── agent_annual_report.ipynb 
├── tutorials_advanced (advanced tutorials for potential finrobot developers)
│   ├── agent_trade_strategist.ipynb
│   ├── agent_fingpt_forecaster.ipynb
│   ├── agent_annual_report.ipynb 
│   ├── lmm_agent_mplfinance.ipynb
│   └── lmm_agent_opt_smacross.ipynb
├── setup.py
├── OAI_CONFIG_LIST_sample
├── config_api_keys_sample
├── requirements.txt
└── README.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation:&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1. (Recommended) Create a new virtual environment&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;conda create --name finrobot python=3.10
conda activate finrobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;2. download the FinRobot repo use terminal or download it manually&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git clone https://github.com/AI4Finance-Foundation/FinRobot.git
cd FinRobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;3. install finrobot &amp;amp; dependencies from source or pypi&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;get our latest release from pypi&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U finrobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or install from this repo directly&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;4. modify OAI_CONFIG_LIST_sample file&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;1) rename OAI_CONFIG_LIST_sample to OAI_CONFIG_LIST
2) remove the four lines of comment within the OAI_CONFIG_LIST file
3) add your own openai api-key &amp;lt;your OpenAI API key here&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;5. modify config_api_keys_sample file&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;1) rename config_api_keys_sample to config_api_keys
2) remove the comment within the config_api_keys file
3) add your own finnhub-api &quot;YOUR_FINNHUB_API_KEY&quot;
4) add your own financialmodelingprep and sec-api keys &quot;YOUR_FMP_API_KEY&quot; and &quot;YOUR_SEC_API_KEY&quot; (for financial report generation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;6. start navigating the tutorials or the demos below:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# find these notebooks in tutorials
1) agent_annual_report.ipynb
2) agent_fingpt_forecaster.ipynb
3) agent_trade_strategist.ipynb
4) lmm_agent_mplfinance.ipynb
5) lmm_agent_opt_smacross.ipynb
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;h3&gt;1. Market Forecaster Agent (Predict Stock Movements Direction)&lt;/h3&gt; 
&lt;p&gt;Takes a company&#39;s ticker symbol, recent basic financials, and market news as input and predicts its stock movements.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Import&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import autogen
from finrobot.utils import get_current_date, register_keys_from_json
from finrobot.agents.workflow import SingleAssistant
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Config&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Read OpenAI API keys from a JSON file
llm_config = {
    &quot;config_list&quot;: autogen.config_list_from_json(
        &quot;../OAI_CONFIG_LIST&quot;,
        filter_dict={&quot;model&quot;: [&quot;gpt-4-0125-preview&quot;]},
    ),
    &quot;timeout&quot;: 120,
    &quot;temperature&quot;: 0,
}

# Register FINNHUB API keys
register_keys_from_json(&quot;../config_api_keys&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Run&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;company = &quot;NVDA&quot;

assitant = SingleAssistant(
    &quot;Market_Analyst&quot;,
    llm_config,
    # set to &quot;ALWAYS&quot; if you want to chat instead of simply receiving the prediciton
    human_input_mode=&quot;NEVER&quot;,
)
assitant.chat(
    f&quot;Use all the tools provided to retrieve information available for {company} upon {get_current_date()}. Analyze the positive developments and potential concerns of {company} &quot;
    &quot;with 2-4 most important factors respectively and keep them concise. Most factors should be inferred from company related news. &quot;
    f&quot;Then make a rough prediction (e.g. up/down by 2-3%) of the {company} stock price movement for next week. Provide a summary analysis to support your prediction.&quot;
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt;Result&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/812ec23a-9cb3-4fad-b716-78533ddcd9dc&quot; width=&quot;40%&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/9a2f9f48-b0e1-489c-8679-9a4c530f313c&quot; width=&quot;41%&quot;&gt; 
&lt;/div&gt; 
&lt;h3&gt;2. Financial Analyst Agent for Report Writing (Equity Research Report)&lt;/h3&gt; 
&lt;p&gt;Take a company&#39;s 10-k form, financial data, and market data as input and output an equity research report&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Import&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import os
import autogen
from textwrap import dedent
from finrobot.utils import register_keys_from_json
from finrobot.agents.workflow import SingleAssistantShadow
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Config&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;llm_config = {
    &quot;config_list&quot;: autogen.config_list_from_json(
        &quot;../OAI_CONFIG_LIST&quot;,
        filter_dict={
            &quot;model&quot;: [&quot;gpt-4-0125-preview&quot;],
        },
    ),
    &quot;timeout&quot;: 120,
    &quot;temperature&quot;: 0.5,
}
register_keys_from_json(&quot;../config_api_keys&quot;)

# Intermediate strategy modules will be saved in this directory
work_dir = &quot;../report&quot;
os.makedirs(work_dir, exist_ok=True)

assistant = SingleAssistantShadow(
    &quot;Expert_Investor&quot;,
    llm_config,
    max_consecutive_auto_reply=None,
    human_input_mode=&quot;TERMINATE&quot;,
)

&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Run&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;company = &quot;Microsoft&quot;
fyear = &quot;2023&quot;

message = dedent(
    f&quot;&quot;&quot;
    With the tools you&#39;ve been provided, write an annual report based on {company}&#39;s {fyear} 10-k report, format it into a pdf.
    Pay attention to the followings:
    - Explicitly explain your working plan before you kick off.
    - Use tools one by one for clarity, especially when asking for instructions. 
    - All your file operations should be done in &quot;{work_dir}&quot;. 
    - Display any image in the chat once generated.
    - All the paragraphs should combine between 400 and 450 words, don&#39;t generate the pdf until this is explicitly fulfilled.
&quot;&quot;&quot;
)

assistant.chat(message, use_cache=True, max_turns=50,
               summary_method=&quot;last_msg&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt;Result&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/d2d999e0-dc0e-4196-aca1-218f5fadcc5b&quot; width=&quot;60%&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/3a21873f-9498-4d73-896b-3740bf6d116d&quot; width=&quot;60%&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Financial CoT&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Gather Preliminary Data&lt;/strong&gt;: 10-K report, market data, financial ratios&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Analyze Financial Statements&lt;/strong&gt;: balance sheet, income statement, cash flow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Company Overview and Performance&lt;/strong&gt;: company description, business highlights, segment analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Risk Assessment&lt;/strong&gt;: assess risks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Financial Performance Visualization&lt;/strong&gt;: plot PE ratio and EPS&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Synthesize Findings into Paragraphs&lt;/strong&gt;: combine all parts into a coherent summary&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generate PDF Report&lt;/strong&gt;: use tools to generate PDF automatically&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quality Assurance&lt;/strong&gt;: check word counts&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;3. Trade Strategist Agent with multimodal capabilities&lt;/h3&gt; 
&lt;h2&gt;AI Agent Papers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Stanford University + Microsoft Research] &lt;a href=&quot;https://arxiv.org/abs/2401.03568&quot;&gt;Agent AI: Surveying the Horizons of Multimodal Interaction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Stanford University] &lt;a href=&quot;https://arxiv.org/abs/2304.03442&quot;&gt;Generative Agents: Interactive Simulacra of Human Behavior&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Fudan NLP Group] &lt;a href=&quot;https://arxiv.org/abs/2309.07864&quot;&gt;The Rise and Potential of Large Language Model Based Agents: A Survey&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Fudan NLP Group] &lt;a href=&quot;https://github.com/WooooDyy/LLM-Agent-Paper-List&quot;&gt;LLM-Agent-Paper-List&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Tsinghua University] &lt;a href=&quot;https://arxiv.org/abs/2312.11970&quot;&gt;Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Renmin University] &lt;a href=&quot;https://arxiv.org/pdf/2308.11432.pdf&quot;&gt;A Survey on Large Language Model-based Autonomous Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Nanyang Technological University] &lt;a href=&quot;https://arxiv.org/abs/2402.18485&quot;&gt;FinAgent: A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;AI Agent Blogs and Videos&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Medium] &lt;a href=&quot;https://medium.com/humansdotai/an-introduction-to-ai-agents-e8c4afd2ee8f&quot;&gt;An Introduction to AI Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Medium] &lt;a href=&quot;https://medium.com/@aitrendorbit/unmasking-the-best-character-ai-chatbots-2024-351de43792f4#the-best-character-ai-chatbots&quot;&gt;Unmasking the Best Character AI Chatbots | 2024&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[big-picture] &lt;a href=&quot;https://blog.big-picture.com/en/chatgpt-next-level-meet-10-autonomous-ai-agents-auto-gpt-babyagi-agentgpt-microsoft-jarvis-chaosgpt-friends/&quot;&gt;ChatGPT, Next Level: Meet 10 Autonomous AI Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[TowardsDataScience] &lt;a href=&quot;https://towardsdatascience.com/navigating-the-world-of-llm-agents-a-beginners-guide-3b8d499db7a9&quot;&gt;Navigating the World of LLM Agents: A Beginner’s Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[YouTube] &lt;a href=&quot;https://www.youtube.com/watch?v=iVbN95ica_k&quot;&gt;Introducing Devin - The &quot;First&quot; AI Agent Software Engineer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;AI Agent Open-Source Framework &amp;amp; Tool&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Significant-Gravitas/AutoGPT&quot;&gt;AutoGPT (163k stars)&lt;/a&gt; is a tool for everyone to use, aiming to democratize AI, making it accessible for everyone to use and build upon.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/langchain-ai/langchain&quot;&gt;LangChain (87.4k stars)&lt;/a&gt; is a framework for developing context-aware applications powered by language models, enabling them to connect to sources of context and rely on the model&#39;s reasoning capabilities for responses and actions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/geekan/MetaGPT&quot;&gt;MetaGPT (41k stars)&lt;/a&gt; is a multi-agent open-source framework that assigns different roles to GPTs, forming a collaborative software entity to execute complex tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/langgenius/dify&quot;&gt;dify (34.1.7k stars)&lt;/a&gt; is an LLM application development platform. It integrates the concepts of Backend as a Service and LLMOps, covering the core tech stack required for building generative AI-native applications, including a built-in RAG engine&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/autogen&quot;&gt;AutoGen (27.4k stars)&lt;/a&gt; is a framework for developing LLM applications with conversational agents that collaborate to solve tasks. These agents are customizable, support human interaction, and operate in modes combining LLMs, human inputs, and tools.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/OpenBMB/ChatDev&quot;&gt;ChatDev (24.1k stars)&lt;/a&gt; is a framework that focuses on developing conversational AI Agents capable of dialogue and question-answering. It provides a range of pre-trained models and interactive interfaces, facilitating the development of customized chat Agents for users.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yoheinakajima/babyagi&quot;&gt;BabyAGI (19.5k stars)&lt;/a&gt; is an AI-powered task management system, dedicated to building AI Agents with preliminary general intelligence.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/joaomdmoura/crewAI&quot;&gt;CrewAI (16k stars)&lt;/a&gt; is a framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/TransformerOptimus/SuperAGI&quot;&gt;SuperAGI (14.8k stars)&lt;/a&gt; is a dev-first open-source autonomous AI agent framework enabling developers to build, manage &amp;amp; run useful autonomous agents.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/labring/FastGPT&quot;&gt;FastGPT (14.6k stars)&lt;/a&gt; is a knowledge-based platform built on the LLM, offers out-of-the-box data processing and model invocation capabilities, allows for workflow orchestration through Flow visualization.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/OpenBMB/XAgent&quot;&gt;XAgent (7.8k stars)&lt;/a&gt; is an open-source experimental Large Language Model (LLM) driven autonomous agent that can automatically solve various tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dataelement/bisheng&quot;&gt;Bisheng (7.8k stars)&lt;/a&gt; is a leading open-source platform for developing LLM applications.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/OpenBMB/XAgent&quot;&gt;Voyager (5.3k stars)&lt;/a&gt; An Open-Ended Embodied Agent with Large Language Models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/camel-ai/camel&quot;&gt;CAMEL (4.7k stars)&lt;/a&gt; is a framework that offers a comprehensive set of tools and algorithms for building multimodal AI Agents, enabling them to handle various data forms such as text, images, and speech.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/langfuse/langfuse&quot;&gt;Langfuse (4.3k stars)&lt;/a&gt; is a language fusion framework that can integrate the language abilities of multiple AI Agents, enabling them to simultaneously possess multilingual understanding and generation capabilities.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citing FinRobot&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@inproceedings{
zhou2024finrobot,
title={FinRobot: {AI} Agent for Equity Research and Valuation with Large Language Models},
author={Tianyu Zhou and Pinqiao Wang and Yilin Wu and Hongyang Yang},
booktitle={ICAIF 2024: The 1st Workshop on Large Language Models and Generative AI for Finance},
year={2024}
}

@article{yang2024finrobot,
  title={FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models},
  author={Yang, Hongyang and Zhang, Boyu and Wang, Neng and Guo, Cheng and Zhang, Xiaoli and Lin, Likun and Wang, Junlin and Zhou, Tianyu and Guan, Mao and Zhang, Runjia and others},
  journal={arXiv preprint arXiv:2405.14767},
  year={2024}
}

@inproceedings{han2024enhancing,
  title={Enhancing Investment Analysis: Optimizing AI-Agent Collaboration in Financial Research},
  author={Han, Xuewen and Wang, Neng and Che, Shangkun and Yang, Hongyang and Zhang, Kunpeng and Xu, Sean Xin},
  booktitle={ICAIF 2024: Proceedings of the 5th ACM International Conference on AI in Finance},
  pages={538--546},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: The codes and documents provided herein are released under the Apache-2.0 license. They should not be construed as financial counsel or recommendations for live trading. It is imperative to exercise caution and consult with qualified financial professionals prior to any trading or investment actions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>evidentlyai/evidently</title>
      <link>https://github.com/evidentlyai/evidently</link>
      <description>&lt;p&gt;Evidently is ​​an open-source ML and LLM observability framework. Evaluate, test, and monitor any AI-powered system or data pipeline. From tabular data to Gen AI. 100+ metrics.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&quot;center&quot;&gt;Evidently&lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;b&gt;An open-source framework to evaluate, test and monitor ML and LLM-powered systems.&lt;/b&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://pepy.tech/project/evidently&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://pepy.tech/badge/evidently&quot; alt=&quot;PyPi Downloads&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/evidentlyai/evidently/raw/main/LICENSE&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/evidentlyai/evidently&quot; alt=&quot;License&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/evidently/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/evidently&quot; alt=&quot;PyPi&quot;&gt;&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/evidentlyai/evidently/main/docs/images/gh_header.png&quot; alt=&quot;Evidently&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://docs.evidentlyai.com&quot;&gt;Documentation&lt;/a&gt; | &lt;a href=&quot;https://discord.gg/xZjKRaNp8b&quot;&gt;Discord Community&lt;/a&gt; | &lt;a href=&quot;https://evidentlyai.com/blog&quot;&gt;Blog&lt;/a&gt; | &lt;a href=&quot;https://twitter.com/EvidentlyAI&quot;&gt;Twitter&lt;/a&gt; | &lt;a href=&quot;https://www.evidentlyai.com/register&quot;&gt;Evidently Cloud&lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;🆕&lt;/span&gt; New release&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Evidently 0.4.25&lt;/strong&gt;. LLM evaluation -&amp;gt; &lt;a href=&quot;https://docs.evidentlyai.com/tutorials-and-examples/tutorial-llm&quot;&gt;Tutorial&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;📊&lt;/span&gt; What is Evidently?&lt;/h1&gt; 
&lt;p&gt;Evidently is an open-source Python library for ML and LLM evaluation and observability. It helps evaluate, test, and monitor AI-powered systems and data pipelines from experimentation to production.&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🔡 Works with tabular, text data, and embeddings.&lt;/li&gt; 
 &lt;li&gt;✨ Supports predictive and generative systems, from classification to RAG.&lt;/li&gt; 
 &lt;li&gt;📚 100+ built-in metrics from data drift detection to LLM judges.&lt;/li&gt; 
 &lt;li&gt;🛠️ Python interface for custom metrics and tests.&amp;nbsp;&lt;/li&gt; 
 &lt;li&gt;🚦 Both offline evals and live monitoring.&lt;/li&gt; 
 &lt;li&gt;💻 Open architecture: easily export data and integrate with existing tools.&amp;nbsp;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Evidently is very modular. You can start with one-off evaluations using &lt;code&gt;Reports&lt;/code&gt; or &lt;code&gt;Test Suites&lt;/code&gt; in Python or get a real-time monitoring &lt;code&gt;Dashboard&lt;/code&gt; service.&lt;/p&gt; 
&lt;h2&gt;1. Reports&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Reports&lt;/strong&gt; compute various data, ML and LLM quality metrics. You can start with Presets or customize.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Out-of-the-box interactive visuals.&lt;/li&gt; 
 &lt;li&gt;Best for exploratory analysis and debugging.&lt;/li&gt; 
 &lt;li&gt;Get results in Python, export as JSON, Python dictionary, HTML, DataFrame, or view in monitoring UI.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Reports&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/evidentlyai/evidently/main/docs/book/.gitbook/assets/main/reports-min.png&quot; alt=&quot;Report example&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;2. Test Suites&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Test Suites&lt;/strong&gt; check for defined conditions on metric values and return a pass or fail result.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Best for regression testing, CI/CD checks, or data validation pipelines.&lt;/li&gt; 
 &lt;li&gt;Zero setup option: auto-generate test conditions from the reference dataset.&lt;/li&gt; 
 &lt;li&gt;Simple syntax to set custom test conditions as &lt;code&gt;gt&lt;/code&gt; (greater than), &lt;code&gt;lt&lt;/code&gt; (less than), etc.&lt;/li&gt; 
 &lt;li&gt;Get results in Python, export as JSON, Python dictionary, HTML, DataFrame, or view in monitoring UI.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Test Suite&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/evidentlyai/evidently/main/docs/book/.gitbook/assets/main/tests.gif&quot; alt=&quot;Test example&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;3. Monitoring Dashboard&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Monitoring UI&lt;/strong&gt; service helps visualize metrics and test results over time.&lt;/p&gt; 
&lt;p&gt;You can choose:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Self-host the open-source version. &lt;a href=&quot;https://demo.evidentlyai.com&quot;&gt;Live demo&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Sign up for &lt;a href=&quot;https://www.evidentlyai.com/register&quot;&gt;Evidently Cloud&lt;/a&gt; (Recommended).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Evidently Cloud offers a generous free tier and extra features like user management, alerting, and no-code evals.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dashboard&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/evidentlyai/evidently/main/docs/book/.gitbook/assets/main/dashboard.gif&quot; alt=&quot;Dashboard example&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;&lt;span&gt;👩💻&lt;/span&gt; Install Evidently&lt;/h1&gt; 
&lt;p&gt;Evidently is available as a PyPI package. To install it using pip package manager, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;pip install evidently
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install Evidently using conda installer, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;conda install -c conda-forge evidently
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;&lt;span&gt;▶&lt;/span&gt; Getting started&lt;/h1&gt; 
&lt;h3&gt;Option 1: Test Suites&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This is a simple Hello World. Check the Tutorials for more: &lt;a href=&quot;https://docs.evidentlyai.com/tutorials-and-examples/tutorial_reports_tests&quot;&gt;Tabular data&lt;/a&gt; or &lt;a href=&quot;https://docs.evidentlyai.com/tutorials-and-examples/tutorial-llm&quot;&gt;LLM evaluation&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Import the &lt;strong&gt;Test Suite&lt;/strong&gt;, evaluation Preset and toy tabular dataset.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import pandas as pd

from sklearn import datasets

from evidently.test_suite import TestSuite
from evidently.test_preset import DataStabilityTestPreset

iris_data = datasets.load_iris(as_frame=True)
iris_frame = iris_data.frame
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Split the &lt;code&gt;DataFrame&lt;/code&gt; into reference and current. Run the &lt;strong&gt;Data Stability&lt;/strong&gt; Test Suite that will automatically generate checks on column value ranges, missing values, etc. from the reference. Get the output in Jupyter notebook:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data_stability= TestSuite(tests=[
    DataStabilityTestPreset(),
])
data_stability.run(current_data=iris_frame.iloc[:60], reference_data=iris_frame.iloc[60:], column_mapping=None)
data_stability
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also save an HTML file. You&#39;ll need to open it from the destination folder.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data_stability.save_html(&quot;file.html&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To get the output as JSON:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data_stability.json()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can choose other Presets, individual Tests and set conditions.&lt;/p&gt; 
&lt;h3&gt;Option 2: Reports&lt;/h3&gt; 
&lt;p&gt;Import the &lt;strong&gt;Report&lt;/strong&gt;, evaluation Preset and toy tabular dataset.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import pandas as pd

from sklearn import datasets

from evidently.report import Report
from evidently.metric_preset import DataDriftPreset

iris_data = datasets.load_iris(as_frame=True)
iris_frame = iris_data.frame
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the &lt;strong&gt;Data Drift&lt;/strong&gt; Report that will compare column distributions between &lt;code&gt;current&lt;/code&gt; and &lt;code&gt;reference&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data_drift_report = Report(metrics=[
    DataDriftPreset(),
])

data_drift_report.run(current_data=iris_frame.iloc[:60], reference_data=iris_frame.iloc[60:], column_mapping=None)
data_drift_report

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Save the report as HTML. You&#39;ll later need to open it from the destination folder.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data_drift_report.save_html(&quot;file.html&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To get the output as JSON:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data_drift_report.json()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can choose other Presets and individual Metrics, including LLM evaluations for text data.&lt;/p&gt; 
&lt;h3&gt;Option 3: ML monitoring dashboard&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This launches a demo project in the Evidently UI. Check tutorials for &lt;a href=&quot;https://docs.evidentlyai.com/tutorials-and-examples/tutorial-monitoring&quot;&gt;Self-hosting&lt;/a&gt; or &lt;a href=&quot;https://docs.evidentlyai.com/tutorials-and-examples/tutorial-cloud&quot;&gt;Evidently Cloud&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Recommended step: create a virtual environment and activate it.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install virtualenv
virtualenv venv
source venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After installing Evidently (&lt;code&gt;pip install evidently&lt;/code&gt;), run the Evidently UI with the demo projects:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;evidently ui --demo-projects all
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Access Evidently UI service in your browser. Go to the &lt;strong&gt;localhost:8000&lt;/strong&gt;.&lt;/p&gt; 
&lt;h1&gt;🚦 What can you evaluate?&lt;/h1&gt; 
&lt;p&gt;Evidently has 100+ built-in evals. You can also add custom ones. Each metric has an optional visualization: you can use it in &lt;code&gt;Reports&lt;/code&gt;, &lt;code&gt;Test Suites&lt;/code&gt;, or plot on a &lt;code&gt;Dashboard&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Here are examples of things you can check:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;🔡 Text descriptors&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;📝 LLM outputs&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Length, sentiment, toxicity, language, special symbols, regular expression matches, etc.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Semantic similarity, retrieval relevance, summarization quality, etc. with model- and LLM-based evals.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;🛢 Data quality&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;📊 Data distribution drift&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Missing values, duplicates, min-max ranges, new categorical values, correlations, etc.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;20+ statistical tests and distance metrics to compare shifts in data distribution.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;🎯 Classification&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;📈 Regression&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Accuracy, precision, recall, ROC AUC, confusion matrix, bias, etc.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;MAE, ME, RMSE, error distribution, error normality, error bias, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;🗂 Ranking (inc. RAG)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;🛒 Recommendations&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;NDCG, MAP, MRR, Hit Rate, etc.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Serendipity, novelty, diversity, popularity bias, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;&lt;span&gt;💻&lt;/span&gt; Contributions&lt;/h1&gt; 
&lt;p&gt;We welcome contributions! Read the &lt;a href=&quot;https://raw.githubusercontent.com/evidentlyai/evidently/main/CONTRIBUTING.md&quot;&gt;Guide&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;📚&lt;/span&gt; Documentation&lt;/h1&gt; 
&lt;p&gt;For more information, refer to a complete &lt;a href=&quot;https://docs.evidentlyai.com&quot;&gt;Documentation&lt;/a&gt;. You can start with the tutorials:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.evidentlyai.com/tutorials-and-examples/tutorial_reports_tests&quot;&gt;Get Started with Tabular and ML Evaluation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.evidentlyai.com/tutorials-and-examples/tutorial-llm&quot;&gt;Get Started with LLM Evaluation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.evidentlyai.com/tutorials-and-examples/tutorial-monitoring&quot;&gt;Self-hosting ML monitoring Dashboard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.evidentlyai.com/tutorials-and-examples/tutorial-cloud&quot;&gt;Cloud ML monitoring Dashboard&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See more examples in the &lt;a href=&quot;%5Bhttps://docs.evidentlyai.com/tutorials-and-examples%5D(https://docs.evidentlyai.com/tutorials-and-examples/examples)&quot;&gt;Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;How-to guides&lt;/h2&gt; 
&lt;p&gt;Explore the &lt;a href=&quot;https://github.com/evidentlyai/evidently/tree/main/examples/how_to_questions&quot;&gt;How-to guides&lt;/a&gt; to understand specific features in Evidently.&lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;✅&lt;/span&gt; Discord Community&lt;/h1&gt; 
&lt;p&gt;If you want to chat and connect, join our &lt;a href=&quot;https://discord.gg/xZjKRaNp8b&quot;&gt;Discord community&lt;/a&gt;!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>causify-ai/tutorials</title>
      <link>https://github.com/causify-ai/tutorials</link>
      <description>&lt;p&gt;Causify system tutorials&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tutorials&lt;/h1&gt;</description>
    </item>
    
    <item>
      <title>modelscope/facechain</title>
      <link>https://github.com/modelscope/facechain</link>
      <description>&lt;p&gt;FaceChain is a deep-learning toolchain for generating your Digital-Twin.&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; &lt;br&gt; &lt;img src=&quot;https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif&quot; width=&quot;400&quot;&gt; &lt;br&gt; &lt;/p&gt;
&lt;h1&gt;FaceChain&lt;/h1&gt; 
&lt;p&gt; &lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://trendshift.io/repositories/1185&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/1185&quot; alt=&quot;modelscope%2Ffacechain | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;More Technology Details of FaceChain-FACT train-free portrait generation can be seen in &lt;a href=&quot;https://arxiv.org/abs/2410.12312&quot;&gt;Paper&lt;/a&gt;. (October 17th, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;Our work &lt;a href=&quot;https://arxiv.org/abs/2410.10587&quot;&gt;TopoFR&lt;/a&gt; got accepted to NeurIPS 2024 ! (September 26th, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;We provide training scripts for new styles, offering an automatic training for new style LoRas as well as the corresponding style prompts, along with the one click call in Infinite Style Portrait generation tab! (July 3rd, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;🚀🚀🚀 We are launching [FACT] into the main branch, offering a 10-second impressive speed and seamless integration with standard ready-to-use LoRas and ControlNets, along with improved instruction-following capabilities ! The original train-based FaceChain is moved to (&lt;a href=&quot;https://github.com/modelscope/facechain/tree/v3.0.0&quot;&gt;https://github.com/modelscope/facechain/tree/v3.0.0&lt;/a&gt; ). (May 28th, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;Our work &lt;a href=&quot;https://arxiv.org/abs/2403.01901&quot;&gt;FaceChain-ImagineID&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2403.06775&quot;&gt;FaceChain-SuDe&lt;/a&gt; got accepted to CVPR 2024 ! (February 27th, 2024 UTC)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;如果您熟悉中文，可以阅读&lt;a href=&quot;https://raw.githubusercontent.com/modelscope/facechain/main/README_ZH.md&quot;&gt;中文版本的README&lt;/a&gt;。&lt;/p&gt; 
&lt;p&gt;FaceChain is a novel framework for generating identity-preserved human portraits. In the newest FaceChain FACT (Face Adapter with deCoupled Training) version, with only 1 photo and 10 seconds, you can generate personal portraits in different settings (multiple styles now supported!). FaceChain has both high controllability and authenticity in portrait generation, including text-to-image and inpainting based pipelines, and is seamlessly compatible with ControlNet and LoRAs. You may generate portraits via FaceChain&#39;s Python scripts, or via the familiar Gradio interface, or via sd webui. FaceChain is powered by &lt;a href=&quot;https://github.com/modelscope/modelscope&quot;&gt;ModelScope&lt;/a&gt;.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; ModelScope Studio &lt;a href=&quot;https://modelscope.cn/studios/CVstudio/FaceChain-FACT&quot;&gt;🤖&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&amp;nbsp; ｜API &lt;a href=&quot;https://help.aliyun.com/zh/dashscope/developer-reference/facechain-quick-start&quot;&gt;🔥&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&amp;nbsp; | SD WebUI | HuggingFace Space &lt;a href=&quot;https://huggingface.co/spaces/modelscope/FaceChain-FACT&quot;&gt;🤗&lt;/a&gt;&amp;nbsp; &lt;/p&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;a href=&quot;https://facechain-fact.github.io/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Project-Page-Green&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/DHqEl0qwi-M?si=y6VpInXdhIX0HpbI&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/modelscope/facechain/main/resources/git_cover.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;More Technology Details of FaceChain-FACT train-free portrait generation can be seen in &lt;a href=&quot;https://arxiv.org/abs/2410.12312&quot;&gt;Paper&lt;/a&gt;. (October 17th, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;Our work &lt;a href=&quot;https://arxiv.org/abs/2410.10587&quot;&gt;TopoFR&lt;/a&gt; got accepted to NeurIPS 2024 ! (September 26th, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;We provide training scripts for new styles, offering an automatic training for new style LoRas as well as the corresponding style prompts, along with the one click call in Infinite Style Portrait generation tab! (July 3rd, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;🚀🚀🚀 We are launching [FACT], offering a 10-second impressive speed and seamless integration with standard ready-to-use LoRas and ControlNets, along with improved instruction-following capabilities ! (May 28th, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;Our work &lt;a href=&quot;https://arxiv.org/abs/2403.01901&quot;&gt;FaceChain-ImagineID&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2403.06775&quot;&gt;FaceChain-SuDe&lt;/a&gt; got accepted to CVPR 2024 ! (February 27th, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;🏆🏆🏆Alibaba Annual Outstanding Open Source Project, Alibaba Annual Open Source Pioneer (Yang Liu, Baigui Sun). (January 20th, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;Our work &lt;a href=&quot;https://github.com/henryqin1997/InfoBatch&quot;&gt;InfoBatch&lt;/a&gt; co-authored with NUS team got accepted to ICLR 2024(Oral)! (January 16th, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;🏆OpenAtom&#39;s 2023 Rapidly Growing Open Source Projects Award. (December 20th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Add SDXL pipeline🔥🔥🔥, image detail is improved obviously. (November 22th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Support super resolution🔥🔥🔥, provide multiple resolution choice (512&lt;em&gt;512, 768&lt;/em&gt;768, 1024&lt;em&gt;1024, 2048&lt;/em&gt;2048). (November 13th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;🏆FaceChain has been selected in the &lt;a href=&quot;https://www.benchcouncil.org/evaluation/opencs/annual.html#Institutions&quot;&gt;BenchCouncil Open100 (2022-2023)&lt;/a&gt; annual ranking. (November 8th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Add virtual try-on module. (October 27th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Add wanx version &lt;a href=&quot;https://tongyi.aliyun.com/wanxiang/app/portrait-gallery&quot;&gt;online free app&lt;/a&gt;. (October 26th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;🏆1024 Programmer&#39;s Day AIGC Application Tool Most Valuable Business Award. (2023-10-24, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Support FaceChain in stable-diffusion-webui🔥🔥🔥. (October 13th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;High performance inpainting for single &amp;amp; double person, Simplify User Interface. (September 09th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;More Technology Details can be seen in &lt;a href=&quot;https://arxiv.org/abs/2308.14256&quot;&gt;Paper&lt;/a&gt;. (August 30th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Add validate &amp;amp; ensemble for Lora training, and InpaintTab(hide in gradio for now). (August 28th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Add pose control module. (August 27th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Add robust face lora training module, enhance the performance of one pic training &amp;amp; style-lora blending. (August 27th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;HuggingFace Space is available now! You can experience FaceChain directly with &lt;a href=&quot;https://huggingface.co/spaces/modelscope/FaceChain&quot;&gt;🤗&lt;/a&gt; (August 25th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Add awesome prompts! Refer to: &lt;a href=&quot;https://raw.githubusercontent.com/modelscope/facechain/main/resources/awesome-prompts-facechain.txt&quot;&gt;awesome-prompts-facechain&lt;/a&gt; (August 18th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Support a series of new style models in a plug-and-play fashion. (August 16th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Support customizable prompts. (August 16th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Colab notebook is available now! You can experience FaceChain directly with &lt;a href=&quot;https://colab.research.google.com/github/modelscope/facechain/blob/main/facechain_demo.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;. (August 15th, 2023 UTC)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;To-Do List&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;full-body digital humans&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;p&gt;Please cite FaceChain and FaceChain-FACT in your publications if it helps your research&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{liu2023facechain,
  title={FaceChain: A Playground for Identity-Preserving Portrait Generation},
  author={Liu, Yang and Yu, Cheng and Shang, Lei and Wu, Ziheng and 
          Wang, Xingjun and Zhao, Yuze and Zhu, Lin and Cheng, Chen and 
          Chen, Weitao and Xu, Chao and Xie, Haoyu and Yao, Yuan and 
          Zhou,  Wenmeng and Chen Yingda and Xie, Xuansong and Sun, Baigui},
  journal={arXiv preprint arXiv:2308.14256},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;@article{yu2024facechain,
  title={FaceChain-FACT: Face Adapter with Decoupled Training for Identity-preserved Personalization},
  author={Yu, Cheng and Xie, Haoyu and Shang, Lei and Liu, Yang and Dan, Jun and Sun, Baigui and Bo, Liefeng},
  journal={arXiv preprint arXiv:2410.12312},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;h2&gt;Compatibility Verification&lt;/h2&gt; 
&lt;p&gt;We have verified e2e execution on the following environment:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;python: py3.8, py3.10&lt;/li&gt; 
 &lt;li&gt;pytorch: torch2.0.0, torch2.0.1&lt;/li&gt; 
 &lt;li&gt;CUDA: 11.7&lt;/li&gt; 
 &lt;li&gt;CUDNN: 8+&lt;/li&gt; 
 &lt;li&gt;OS: Ubuntu 20.04, CentOS 7.9&lt;/li&gt; 
 &lt;li&gt;GPU: Nvidia-A10 24G&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Memory Optimization&lt;/h2&gt; 
&lt;p&gt;Jemalloc are recommanded to install for optimizing the memory from above 30G to below 20G. Here is an example for installing Jemalloc in Modelscope notebook.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;apt-get install -y libjemalloc-dev
export LD_PRELOAD=/lib/x86_64-linux-gnu/libjemalloc.so
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation Guide&lt;/h2&gt; 
&lt;p&gt;The following installation methods are supported:&lt;/p&gt; 
&lt;h3&gt;1. ModelScope notebook【recommended】&lt;/h3&gt; 
&lt;p&gt;The ModelScope Notebook offers a free-tier that allows ModelScope user to run the FaceChain application with minimum setup, refer to &lt;a href=&quot;https://modelscope.cn/my/mynotebook/preset&quot;&gt;ModelScope Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# Step1: 我的notebook -&amp;gt; PAI-DSW -&amp;gt; GPU环境
# Note: Please use: ubuntu20.04-py38-torch2.0.1-tf1.15.5-modelscope1.8.1

# Step2: Entry the Notebook cell，clone FaceChain from github:
!GIT_LFS_SKIP_SMUDGE=1 git clone https://github.com/modelscope/facechain.git --depth 1

# Step3: Change the working directory to facechain, and install the dependencies:
import os
os.chdir(&#39;/mnt/workspace/facechain&#39;)    # You may change to your own path
print(os.getcwd())

!pip3 install gradio==3.47.1
!pip3 install controlnet_aux==0.0.6
!pip3 install python-slugify
!pip3 install diffusers==0.29.0
!pip3 install peft==0.11.1
!pip3 install modelscope -U
!pip3 install datasets==2.16

# Step4: Start the app service, click &quot;public URL&quot; or &quot;local URL&quot;, upload your images to 
# train your own model and then generate your digital twin.
!python3 app.py

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you may also purchase a &lt;a href=&quot;https://www.aliyun.com/activity/bigdata/pai/dsw&quot;&gt;PAI-DSW&lt;/a&gt; instance (using A10 resource), with the option of ModelScope image to run FaceChain following similar steps.&lt;/p&gt; 
&lt;h3&gt;2. Docker&lt;/h3&gt; 
&lt;p&gt;If you are familiar with using docker, we recommend to use this way:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# Step1: Prepare the environment with GPU on local or cloud, we recommend to use Alibaba Cloud ECS, refer to: https://www.aliyun.com/product/ecs

# Step2: Download the docker image (for installing docker engine, refer to https://docs.docker.com/engine/install/）
# For China Mainland users:
docker pull registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.1
# For users outside China Mainland:
docker pull registry.us-west-1.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.1

# Step3: run the docker container
docker run -it --name facechain -p 7860:7860 --gpus all registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.1 /bin/bash
# Note: you may need to install the nvidia-container-runtime, follow the instructions:
# 1. Install nvidia-container-runtime：https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
# 2. sudo systemctl restart docker

# Step4: Install the gradio in the docker container:
pip3 install gradio==3.47.1
pip3 install controlnet_aux==0.0.6
pip3 install python-slugify
pip3 install diffusers==0.29.0
pip3 install peft==0.11.1
pip3 install modelscope -U
pip3 install datasets==2.16

# Step5 clone facechain from github
GIT_LFS_SKIP_SMUDGE=1 git clone https://github.com/modelscope/facechain.git --depth 1
cd facechain
python3 app.py
# Note: FaceChain currently assume single-GPU, if your environment has multiple GPU, please use the following instead:
# CUDA_VISIBLE_DEVICES=0 python3 app.py

# Step6
Run the app server: click &quot;public URL&quot; --&amp;gt; in the form of: https://xxx.gradio.live
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. stable-diffusion-webui&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Select the &lt;code&gt;Extensions Tab&lt;/code&gt;, then choose &lt;code&gt;Install From URL&lt;/code&gt; (official plugin integration is integrated, please install from URL currently). &lt;img src=&quot;https://raw.githubusercontent.com/modelscope/facechain/main/resources/sdwebui_install.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Switch to &lt;code&gt;Installed&lt;/code&gt;, check the FaceChain plugin, then click &lt;code&gt;Apply and restart UI&lt;/code&gt;. It may take a while for installing the dependencies and downloading the models. Make sure that the &quot;CUDA Toolkit&quot; is installed correctly, otherwise the &quot;mmcv&quot; package cannot be successfully installed. &lt;img src=&quot;https://raw.githubusercontent.com/modelscope/facechain/main/resources/sdwebui_restart.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;After the page refreshes, the appearance of the &lt;code&gt;FaceChain&lt;/code&gt; Tab indicates a successful installation. &lt;img src=&quot;https://raw.githubusercontent.com/modelscope/facechain/main/resources/sdwebui_success.jpg&quot; alt=&quot;image&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;Script Execution&lt;/h1&gt; 
&lt;p&gt;FaceChain supports direct inference in the python environment. When inferring for Infinite Style Portrait generation, please edit the code in run_inference.py:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Use pose control, default False
use_pose_model = False
# The path of the input image containing ID information for portrait generation
input_img_path = &#39;poses/man/pose2.png&#39;
# The path of the image for pose control, only effective when using pose control
pose_image = &#39;poses/man/pose1.png&#39;
# The number of images to generate in inference
num_generate = 5
# The weight for the style model, see styles for detail
multiplier_style = 0.25
# Specify a folder to save the generated images, this parameter can be modified as needed
output_dir = &#39;./generated&#39;
# The index of the chosen base model, see facechain/constants.py for detail
base_model_idx = 0
# The index of the style model, see styles for detail
style_idx = 0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then execute:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python run_inference.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can find the generated personal digital image photos in the &lt;code&gt;output_dir&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;When inferring for Fixed Templates Portrait generation, please edit the code in run_inference_inpaint.py.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Number of faces for the template image
num_faces = 1
# Index of face for inpainting, counting from left to right
selected_face = 1
# The strength for inpainting, you do not need to change the parameter
strength = 0.6
# The path of the template image
inpaint_img = &#39;poses/man/pose1.png&#39;
# The path of the input image containing ID information for portrait generation
input_img_path = &#39;poses/man/pose2.png&#39;
# The number of images to generate in inference
num_generate = 1
# Specify a folder to save the generated images, this parameter can be modified as needed
output_dir = &#39;./generated_inpaint&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then execute:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python run_inference_inpaint.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can find the generated personal digital image photos in the &lt;code&gt;output_dir&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Algorithm Introduction&lt;/h1&gt; 
&lt;p&gt;The capability of AI portraits generation comes from the large generative models like Stable Diffusion and its fine-tuning techniques. Due to the strong generalization capability of large models, it is possible to perform downstream tasks by fine-tuning on specific types of data and tasks, while preserving the model&#39;s overall ability of text following and image generation. The technical foundation of train-based and train-free AI portraits generation comes from applying different fine-tuning tasks to generative models. Currently, most existing AI portraits tools adopt a two-stage “train then generate” pipeline, where the fine-tuning task is “to generate portrait photos of a fixed character ID”, and the corresponding training data are multiple images of the fixed character ID. The effectiveness of such train-based pipeline depends on the scale of the training data, thus requiring certain image data support and training time, which also increases the cost for users.&lt;/p&gt; 
&lt;p&gt;Different from train-based pipeline, train-free pipeline adjusts the fine-tuning task to “generate portrait photos of a specified character ID”, meaning that the character ID image (face photo) is used as an additional input, and the output is a portrait photo preserving the input ID. Such a pipeline completely separates offline training from online inference, allowing users to generate portraits directly based on the fine-tuned model with only one photo in just 10 seconds, avoiding the cost for extensive data and training time. The fine-tuning task of train-free AI portraits generation is based on the adapter module. Face photos are processed through an image encoder with fixed weights and a parameter-efficient feature projection layer to obtain aligned features, and are then fed into the U-Net model of Stable Diffusion through attention mechanism similar as text conditions. At this point, face information as an independent branch condition is fed into the model alongside text information for inference, thereby enabling the generated images to maintain ID fidelity.&lt;/p&gt; 
&lt;p&gt;The basic algorithm based on face adapter is capable of achieving train-free AI portraits, but still requires certain adjustments to further improve its effectiveness. Existing train-free portrait tools generally suffer from the following issues: poor image quality of portraits, inadequate text following and style retention abilities in portraits, poor controllability and richness of portrait faces, and poor compatibility with extensions like ControlNet and style Lora. To address these issues, FaceChain attribute them to the fact that the fine-tuning tasks for existing train-free AI portrait tools have coupled with too much information beyond character IDs, and propose FaceChain Face Adapter with Decoupled Training (FaceChain FACT) to solve these problems. By fine-tuning the Stable Diffusion model on millions of portrait data, FaceChain FACT can achieve high-quality portrait image generation for specified character IDs. The entire framework of FaceChain FACT is shown in the figure below.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/modelscope/facechain/main/resources/framework.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt; 
&lt;p&gt;The decoupled training of FaceChain FACT consists of two parts: decoupling face from image, and decoupling ID from face. Existing methods often treat denoising portrait images as the fine-tuning task, which makes the model hard to accurately focus on the face area, thereby affecting the text-to-image ability of the base Stable Diffusion model. FaceChain FACT draws on the sequential processing and regional control advantages of face-swapping algorithms and implements the fine-tuning method for decoupling faces from images from both structural and training strategy aspects. Structurally, unlike existing methods that use a parallel cross-attention mechanism to process face and text information, FaceChain FACT adopts a sequential processing approach as an independent adapter layer inserted into the original Stable Diffusion&#39;s blocks. This way, face adaptation acts as an independent step similar to face-swapping during the denoising process, avoiding interference between face and text conditions. In terms of training strategy, besides the original MSE loss function, FaceChain FACT introduces the Face Adapting Incremental Regularization (FAIR) loss function, which controls the feature increment of the face adaptation step in the adapter layer to focus on the face region. During inference, users can flexibly adjust the generated effects by modifying the weight of the face adapter, balancing fidelity and generalization of the face while maintaining the text-to-image ability of Stable Diffusion. The FAIR loss function is formulated as follows:&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/modelscope/facechain/main/resources/FAIR.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Furthermore, addressing the issue of poor controllability and richness of generated faces, FaceChain FACT proposes a training method for decoupling ID from face, so that the portrait process only preserves the character ID rather than the entire face. Firstly, to better extract the ID information from the face while maintaining certain key facial details, and to better adapt to the structure of Stable Diffusion, FaceChain FACT employs a face feature extractor named &lt;a href=&quot;https://github.com/DanJun6737/TransFace&quot;&gt;TransFace&lt;/a&gt; based on the Transformer architecture, which is pre-trained on a large-scale face dataset. All tokens from the penultimate layer are subsequently fed into a simple attention query model for feature projection, thereby ensuring that the extracted ID features meet the aforementioned requirements. Additionally, during the training process, FaceChain FACT uses the Classifier Free Guidance (CFG) method to perform random shuffle and drop for different portrait images of the same ID, thus ensuring that the input face images and the target images used for denoising may have different faces with the same ID, thus further preventing the model from overfitting to non-ID information of the face. As such, FaceChain FACT possesses high compatibility with the massive exquisite styles of FaceChain, which is shown as follows.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/modelscope/facechain/main/resources/generated_examples.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;Model List&lt;/h2&gt; 
&lt;p&gt;The models used in FaceChain:&lt;/p&gt; 
&lt;p&gt;[1] Face recognition model TransFace：&lt;a href=&quot;https://www.modelscope.cn/models/iic/cv_vit_face-recognition&quot;&gt;https://www.modelscope.cn/models/iic/cv_vit_face-recognition&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[2] Face detection model DamoFD：&lt;a href=&quot;https://modelscope.cn/models/damo/cv_ddsar_face-detection_iclr23-damofd&quot;&gt;https://modelscope.cn/models/damo/cv_ddsar_face-detection_iclr23-damofd&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[3] Human parsing model M2FP：&lt;a href=&quot;https://modelscope.cn/models/damo/cv_resnet101_image-multiple-human-parsing&quot;&gt;https://modelscope.cn/models/damo/cv_resnet101_image-multiple-human-parsing&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[4] Skin retouching model ABPN：&lt;a href=&quot;https://www.modelscope.cn/models/damo/cv_unet_skin_retouching_torch&quot;&gt;https://www.modelscope.cn/models/damo/cv_unet_skin_retouching_torch&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[5] Face fusion model：&lt;a href=&quot;https://www.modelscope.cn/models/damo/cv_unet_face_fusion_torch&quot;&gt;https://www.modelscope.cn/models/damo/cv_unet_face_fusion_torch&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[6] FaceChain FACT model: &lt;a href=&quot;https://www.modelscope.cn/models/yucheng1996/FaceChain-FACT&quot;&gt;https://www.modelscope.cn/models/yucheng1996/FaceChain-FACT&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[7] Face attribute recognition model FairFace: &lt;a href=&quot;https://modelscope.cn/models/damo/cv_resnet34_face-attribute-recognition_fairface&quot;&gt;https://modelscope.cn/models/damo/cv_resnet34_face-attribute-recognition_fairface&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;More Information&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/modelscope/modelscope/&quot;&gt;ModelScope library&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;​ ModelScope Library provides the foundation for building the model-ecosystem of ModelScope, including the interface and implementation to integrate various models into ModelScope.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88&quot;&gt;Contribute models to ModelScope&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;This project is licensed under the &lt;a href=&quot;https://github.com/modelscope/modelscope/raw/master/LICENSE&quot;&gt;Apache License (Version 2.0)&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>jackfrued/Python-100-Days</title>
      <link>https://github.com/jackfrued/Python-100-Days</link>
      <description>&lt;p&gt;Python - 100天从新手到大师&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Python - 100天从新手到大师&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;作者&lt;/strong&gt;：骆昊&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：如果访问 GitHub 比较慢的话，可以关注我的知乎号（&lt;a href=&quot;https://www.zhihu.com/people/jackfrued&quot;&gt;&lt;strong&gt;Python-Jack&lt;/strong&gt;&lt;/a&gt;），上面的&lt;a href=&quot;https://zhuanlan.zhihu.com/c_1216656665569013760&quot;&gt;“&lt;strong&gt;从零开始学Python&lt;/strong&gt;”&lt;/a&gt;专栏（对应本项目前 20 天的内容）比较适合初学者，其他的专栏如“&lt;a href=&quot;https://www.zhihu.com/column/c_1620074540456964096&quot;&gt;&lt;strong&gt;数据思维和统计思维&lt;/strong&gt;&lt;/a&gt;”、“&lt;a href=&quot;https://www.zhihu.com/column/c_1217746527315496960&quot;&gt;&lt;strong&gt;基于Python的数据分析&lt;/strong&gt;&lt;/a&gt;”、“&lt;a href=&quot;https://www.zhihu.com/column/c_1628900668109946880&quot;&gt;&lt;strong&gt;说走就走的AI之旅&lt;/strong&gt;&lt;/a&gt;”等也在持续创作和更新中，欢迎大家关注、点赞和评论。如果希望免费学习打卡或者参与问题讨论，可以加入下面的 QQ 交流群（三个群加一个即可），请不要重复加群，也不要在群里发布广告和其他色情、低俗或敏感内容。如果有付费学习或付费咨询的需求，可以添加我的私人微信（微信号：&lt;strong&gt;jackfrued&lt;/strong&gt;），备注好自己的称呼和需求，我会为大家提供力所能及的帮助。&lt;/p&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/res/python_study_qq_group.png&quot; style=&quot;zoom:30%;&quot;&gt; 
 &lt;p&gt;本项目对应的部分视频已经同步到 &lt;a href=&quot;https://space.bilibili.com/1177252794&quot;&gt;Bilibili&lt;/a&gt;，有兴趣的小伙伴可以点赞、投币、关注，一键三连支持一下！&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Python应用领域和职业发展分析&lt;/h3&gt; 
&lt;p&gt;简单的说，Python是一个“优雅”、“明确”、“简单”的编程语言。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;学习曲线低，非专业人士也能上手&lt;/li&gt; 
 &lt;li&gt;开源系统，拥有强大的生态圈&lt;/li&gt; 
 &lt;li&gt;解释型语言，完美的平台可移植性&lt;/li&gt; 
 &lt;li&gt;动态类型语言，支持面向对象和函数式编程&lt;/li&gt; 
 &lt;li&gt;代码规范程度高，可读性强&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Python在以下领域都有用武之地。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;后端开发 - Python / Java / Go / PHP&lt;/li&gt; 
 &lt;li&gt;DevOps - Python / Shell / Ruby&lt;/li&gt; 
 &lt;li&gt;数据采集 - Python / C++ / Java&lt;/li&gt; 
 &lt;li&gt;量化交易 - Python / C++ / R&lt;/li&gt; 
 &lt;li&gt;数据科学 - Python / R / Julia / Matlab&lt;/li&gt; 
 &lt;li&gt;机器学习 - Python / R / C++ / Julia&lt;/li&gt; 
 &lt;li&gt;自动化测试 - Python / Shell&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;作为一名Python开发者，根据个人的喜好和职业规划，可以选择的就业领域也非常多。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python后端开发工程师（服务器、云平台、数据接口）&lt;/li&gt; 
 &lt;li&gt;Python运维工程师（自动化运维、SRE、DevOps）&lt;/li&gt; 
 &lt;li&gt;Python数据分析师（数据分析、商业智能、数字化运营）&lt;/li&gt; 
 &lt;li&gt;Python数据科学家（机器学习、深度学习、算法专家）&lt;/li&gt; 
 &lt;li&gt;Python爬虫工程师（不推荐此赛道！！！）&lt;/li&gt; 
 &lt;li&gt;Python测试工程师（自动化测试、测试开发）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：目前，&lt;strong&gt;数据科学赛道是非常热门的方向&lt;/strong&gt;，因为不管是互联网行业还是传统行业都已经积累了大量的数据，各行各业都需要数据科学家从已有的数据中发现更多的商业价值，从而为企业的决策提供数据的支撑，这就是所谓的数据驱动决策。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;给初学者的几个建议：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Make English as your working language.&lt;/strong&gt; （让英语成为你的工作语言）&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Practice makes perfect.&lt;/strong&gt; （熟能生巧）&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;All experience comes from the mistakes you&#39;ve made.&lt;/strong&gt; （所有的经验都源于你犯过的错误）&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Don&#39;t be a freeloader.&lt;/strong&gt; （不要当伸手党）&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Either outstanding or out.&lt;/strong&gt; （要么出众，要么出局）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Day01~20 - Python语言基础&lt;/h3&gt; 
&lt;h4&gt;Day01 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/01.%E5%88%9D%E8%AF%86Python.md&quot;&gt;初识Python&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Python简介 
  &lt;ul&gt; 
   &lt;li&gt;Python编年史&lt;/li&gt; 
   &lt;li&gt;Python优缺点&lt;/li&gt; 
   &lt;li&gt;Python应用领域&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;安装Python环境 
  &lt;ul&gt; 
   &lt;li&gt;Windows环境&lt;/li&gt; 
   &lt;li&gt;macOS环境&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day02 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/02.%E7%AC%AC%E4%B8%80%E4%B8%AAPython%E7%A8%8B%E5%BA%8F.md&quot;&gt;第一个Python程序&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;编写代码的工具&lt;/li&gt; 
 &lt;li&gt;你好世界&lt;/li&gt; 
 &lt;li&gt;注释你的代码&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day03 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/03.Python%E8%AF%AD%E8%A8%80%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F.md&quot;&gt;Python语言中的变量&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;一些常识&lt;/li&gt; 
 &lt;li&gt;变量和类型&lt;/li&gt; 
 &lt;li&gt;变量命名&lt;/li&gt; 
 &lt;li&gt;变量的使用&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day04 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/04.Python%E8%AF%AD%E8%A8%80%E4%B8%AD%E7%9A%84%E8%BF%90%E7%AE%97%E7%AC%A6.md&quot;&gt;Python语言中的运算符&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;算术运算符&lt;/li&gt; 
 &lt;li&gt;赋值运算符&lt;/li&gt; 
 &lt;li&gt;比较运算符和逻辑运算符&lt;/li&gt; 
 &lt;li&gt;运算符和表达式应用 
  &lt;ul&gt; 
   &lt;li&gt;华氏和摄氏温度转换&lt;/li&gt; 
   &lt;li&gt;计算圆的周长和面积&lt;/li&gt; 
   &lt;li&gt;判断闰年&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day05 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/05.%E5%88%86%E6%94%AF%E7%BB%93%E6%9E%84.md&quot;&gt;分支结构&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;使用if和else构造分支结构&lt;/li&gt; 
 &lt;li&gt;使用match和case构造分支结构&lt;/li&gt; 
 &lt;li&gt;分支结构的应用 
  &lt;ul&gt; 
   &lt;li&gt;分段函数求值&lt;/li&gt; 
   &lt;li&gt;百分制成绩转换成等级&lt;/li&gt; 
   &lt;li&gt;计算三角形的周长和面积&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day06 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/06.%E5%BE%AA%E7%8E%AF%E7%BB%93%E6%9E%84.md&quot;&gt;循环结构&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;for-in循环&lt;/li&gt; 
 &lt;li&gt;while循环&lt;/li&gt; 
 &lt;li&gt;break和continue&lt;/li&gt; 
 &lt;li&gt;嵌套的循环结构&lt;/li&gt; 
 &lt;li&gt;循环结构的应用 
  &lt;ul&gt; 
   &lt;li&gt;判断素数&lt;/li&gt; 
   &lt;li&gt;最大公约数&lt;/li&gt; 
   &lt;li&gt;猜数字游戏&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day07 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/07.%E5%88%86%E6%94%AF%E5%92%8C%E5%BE%AA%E7%8E%AF%E7%BB%93%E6%9E%84%E5%AE%9E%E6%88%98.md&quot;&gt;分支和循环结构实战&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;例子1：100以内的素数&lt;/li&gt; 
 &lt;li&gt;例子2：斐波那契数列&lt;/li&gt; 
 &lt;li&gt;例子3：寻找水仙花数&lt;/li&gt; 
 &lt;li&gt;例子4：百钱百鸡问题&lt;/li&gt; 
 &lt;li&gt;例子5：CRAPS赌博游戏&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day08 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/08.%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%88%97%E8%A1%A8-1.md&quot;&gt;常用数据结构之列表-1&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;创建列表&lt;/li&gt; 
 &lt;li&gt;列表的运算&lt;/li&gt; 
 &lt;li&gt;元素的遍历&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day09 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/09.%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%88%97%E8%A1%A8-2.md&quot;&gt;常用数据结构之列表-2&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;列表的方法 
  &lt;ul&gt; 
   &lt;li&gt;添加和删除元素&lt;/li&gt; 
   &lt;li&gt;元素位置和频次&lt;/li&gt; 
   &lt;li&gt;元素排序和反转&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;列表生成式&lt;/li&gt; 
 &lt;li&gt;嵌套列表&lt;/li&gt; 
 &lt;li&gt;列表的应用&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day10 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/10.%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%85%83%E7%BB%84.md&quot;&gt;常用数据结构之元组&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;元组的定义和运算&lt;/li&gt; 
 &lt;li&gt;打包和解包操作&lt;/li&gt; 
 &lt;li&gt;交换变量的值&lt;/li&gt; 
 &lt;li&gt;元组和列表的比较&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day11 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/11.%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%AD%97%E7%AC%A6%E4%B8%B2.md&quot;&gt;常用数据结构之字符串&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;字符串的定义 
  &lt;ul&gt; 
   &lt;li&gt;转义字符&lt;/li&gt; 
   &lt;li&gt;原始字符串&lt;/li&gt; 
   &lt;li&gt;字符的特殊表示&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;字符串的运算 
  &lt;ul&gt; 
   &lt;li&gt;拼接和重复&lt;/li&gt; 
   &lt;li&gt;比较运算&lt;/li&gt; 
   &lt;li&gt;成员运算&lt;/li&gt; 
   &lt;li&gt;获取字符串长度&lt;/li&gt; 
   &lt;li&gt;索引和切片&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;字符的遍历&lt;/li&gt; 
 &lt;li&gt;字符串的方法 
  &lt;ul&gt; 
   &lt;li&gt;大小写相关操作&lt;/li&gt; 
   &lt;li&gt;查找操作&lt;/li&gt; 
   &lt;li&gt;性质判断&lt;/li&gt; 
   &lt;li&gt;格式化&lt;/li&gt; 
   &lt;li&gt;修剪操作&lt;/li&gt; 
   &lt;li&gt;替换操作&lt;/li&gt; 
   &lt;li&gt;拆分与合并&lt;/li&gt; 
   &lt;li&gt;编码与解码&lt;/li&gt; 
   &lt;li&gt;其他方法&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day12 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/12.%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%9B%86%E5%90%88.md&quot;&gt;常用数据结构之集合&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;创建集合&lt;/li&gt; 
 &lt;li&gt;元素的变量&lt;/li&gt; 
 &lt;li&gt;集合的运算 
  &lt;ul&gt; 
   &lt;li&gt;成员运算&lt;/li&gt; 
   &lt;li&gt;二元运算&lt;/li&gt; 
   &lt;li&gt;比较运算&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;集合的方法&lt;/li&gt; 
 &lt;li&gt;不可变集合&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day13 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/13.%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%AD%97%E5%85%B8.md&quot;&gt;常用数据结构之字典&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;创建和使用字典&lt;/li&gt; 
 &lt;li&gt;字典的运算&lt;/li&gt; 
 &lt;li&gt;字典的方法&lt;/li&gt; 
 &lt;li&gt;字典的应用&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day14 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/14.%E5%87%BD%E6%95%B0%E5%92%8C%E6%A8%A1%E5%9D%97.md&quot;&gt;函数和模块&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;定义函数&lt;/li&gt; 
 &lt;li&gt;函数的参数 
  &lt;ul&gt; 
   &lt;li&gt;位置参数和关键字参数&lt;/li&gt; 
   &lt;li&gt;参数的默认值&lt;/li&gt; 
   &lt;li&gt;可变参数&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;用模块管理函数&lt;/li&gt; 
 &lt;li&gt;标准库中的模块和函数&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day15 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/15.%E5%87%BD%E6%95%B0%E5%BA%94%E7%94%A8%E5%AE%9E%E6%88%98.md&quot;&gt;函数应用实战&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;例子1：随机验证码&lt;/li&gt; 
 &lt;li&gt;例子2：判断素数&lt;/li&gt; 
 &lt;li&gt;例子3：最大公约数和最小公倍数&lt;/li&gt; 
 &lt;li&gt;例子4：数据统计&lt;/li&gt; 
 &lt;li&gt;例子5：双色球随机选号&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day16 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/16.%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8%E8%BF%9B%E9%98%B6.md&quot;&gt;函数使用进阶&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;高阶函数&lt;/li&gt; 
 &lt;li&gt;Lambda函数&lt;/li&gt; 
 &lt;li&gt;偏函数&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day17 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/17.%E5%87%BD%E6%95%B0%E9%AB%98%E7%BA%A7%E5%BA%94%E7%94%A8.md&quot;&gt;函数高级应用&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;装饰器&lt;/li&gt; 
 &lt;li&gt;递归调用&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day18 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/18.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8.md&quot;&gt;面向对象编程入门&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;类和对象&lt;/li&gt; 
 &lt;li&gt;定义类&lt;/li&gt; 
 &lt;li&gt;创建和使用对象&lt;/li&gt; 
 &lt;li&gt;初始化方法&lt;/li&gt; 
 &lt;li&gt;面向对象的支柱&lt;/li&gt; 
 &lt;li&gt;面向对象案例 
  &lt;ul&gt; 
   &lt;li&gt;例子1：数字时钟&lt;/li&gt; 
   &lt;li&gt;例子2：平面上的点&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day19 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/19.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6.md&quot;&gt;面向对象编程进阶&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;可见性和属性装饰器&lt;/li&gt; 
 &lt;li&gt;动态属性&lt;/li&gt; 
 &lt;li&gt;静态方法和类方法&lt;/li&gt; 
 &lt;li&gt;继承和多态&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day20 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/20.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E5%BA%94%E7%94%A8.md&quot;&gt;面向对象编程应用&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;扑克游戏&lt;/li&gt; 
 &lt;li&gt;工资结算系统&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Day21~30 - Python语言应用&lt;/h3&gt; 
&lt;h4&gt;Day21 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day21-30/21.%E6%96%87%E4%BB%B6%E8%AF%BB%E5%86%99%E5%92%8C%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86.md&quot;&gt;文件读写和异常处理&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;打开和关闭文件&lt;/li&gt; 
 &lt;li&gt;读写文本文件&lt;/li&gt; 
 &lt;li&gt;异常处理机制&lt;/li&gt; 
 &lt;li&gt;上下文管理器语法&lt;/li&gt; 
 &lt;li&gt;读写二进制文件&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day22 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day21-30/22.%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E5%92%8C%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96.md&quot;&gt;对象的序列化和反序列化&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;JSON概述&lt;/li&gt; 
 &lt;li&gt;读写JSON格式的数据&lt;/li&gt; 
 &lt;li&gt;包管理工具pip&lt;/li&gt; 
 &lt;li&gt;使用网络API获取数据&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day23 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/23.Python%E8%AF%BB%E5%86%99CSV%E6%96%87%E4%BB%B6.md&quot;&gt;Python读写CSV文件&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;CSV文件介绍&lt;/li&gt; 
 &lt;li&gt;将数据写入CSV文件&lt;/li&gt; 
 &lt;li&gt;从CSV文件读取数据&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day24 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day21-30/24.%E7%94%A8Python%E8%AF%BB%E5%86%99Excel%E6%96%87%E4%BB%B6-1.md&quot;&gt;Python读写Excel文件-1&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Excel简介&lt;/li&gt; 
 &lt;li&gt;读Excel文件&lt;/li&gt; 
 &lt;li&gt;写Excel文件&lt;/li&gt; 
 &lt;li&gt;调整样式&lt;/li&gt; 
 &lt;li&gt;公式计算&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day25 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day21-30/25.Python%E8%AF%BB%E5%86%99Excel%E6%96%87%E4%BB%B6-2.md&quot;&gt;Python读写Excel文件-2&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Excel简介&lt;/li&gt; 
 &lt;li&gt;读Excel文件&lt;/li&gt; 
 &lt;li&gt;写Excel文件&lt;/li&gt; 
 &lt;li&gt;调整样式&lt;/li&gt; 
 &lt;li&gt;生成统计图表&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day26 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day21-30/26.Python%E6%93%8D%E4%BD%9CWord%E5%92%8CPowerPoint%E6%96%87%E4%BB%B6.md&quot;&gt;Python操作Word和PowerPoint文件&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;操作Word文档&lt;/li&gt; 
 &lt;li&gt;生成PowerPoint&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day27 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day21-30/27.Python%E6%93%8D%E4%BD%9CPDF%E6%96%87%E4%BB%B6.md&quot;&gt;Python操作PDF文件&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;从PDF中提取文本&lt;/li&gt; 
 &lt;li&gt;旋转和叠加页面&lt;/li&gt; 
 &lt;li&gt;加密PDF文件&lt;/li&gt; 
 &lt;li&gt;批量添加水印&lt;/li&gt; 
 &lt;li&gt;创建PDF文件&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day28 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day21-30/28.Python%E5%A4%84%E7%90%86%E5%9B%BE%E5%83%8F.md&quot;&gt;Python处理图像&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;入门知识&lt;/li&gt; 
 &lt;li&gt;用Pillow处理图像&lt;/li&gt; 
 &lt;li&gt;使用Pillow绘图&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day29 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day21-30/29.Python%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%E5%92%8C%E7%9F%AD%E4%BF%A1.md&quot;&gt;Python发送邮件和短信&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;发送电子邮件&lt;/li&gt; 
 &lt;li&gt;发送短信&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day30 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day21-30/30.%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%9A%84%E5%BA%94%E7%94%A8.md&quot;&gt;正则表达式的应用&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;正则表达式相关知识&lt;/li&gt; 
 &lt;li&gt;Python对正则表达式的支持 
  &lt;ul&gt; 
   &lt;li&gt;例子1：输入验证&lt;/li&gt; 
   &lt;li&gt;例子2：内容提取&lt;/li&gt; 
   &lt;li&gt;例子3：内容替换&lt;/li&gt; 
   &lt;li&gt;例子4：长句拆分&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Day31~35 - 其他相关内容&lt;/h3&gt; 
&lt;h4&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day31-35/31.Python%E8%AF%AD%E8%A8%80%E8%BF%9B%E9%98%B6.md&quot;&gt;Python语言进阶&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;重要知识点&lt;/li&gt; 
 &lt;li&gt;数据结构和算法&lt;/li&gt; 
 &lt;li&gt;函数的使用方式&lt;/li&gt; 
 &lt;li&gt;面向对象相关知识&lt;/li&gt; 
 &lt;li&gt;迭代器和生成器&lt;/li&gt; 
 &lt;li&gt;并发编程&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day31-35/32-33.Web%E5%89%8D%E7%AB%AF%E5%85%A5%E9%97%A8.md&quot;&gt;Web前端入门&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;用HTML标签承载页面内容&lt;/li&gt; 
 &lt;li&gt;用CSS渲染页面&lt;/li&gt; 
 &lt;li&gt;用JavaScript处理交互式行为&lt;/li&gt; 
 &lt;li&gt;Vue.js入门&lt;/li&gt; 
 &lt;li&gt;Element的使用&lt;/li&gt; 
 &lt;li&gt;Bootstrap的使用&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day31-35/34-35.%E7%8E%A9%E8%BD%ACLinux%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.md&quot;&gt;玩转Linux操作系统&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;操作系统发展史和Linux概述&lt;/li&gt; 
 &lt;li&gt;Linux基础命令&lt;/li&gt; 
 &lt;li&gt;Linux中的实用程序&lt;/li&gt; 
 &lt;li&gt;Linux的文件系统&lt;/li&gt; 
 &lt;li&gt;Vim编辑器的应用&lt;/li&gt; 
 &lt;li&gt;环境变量和Shell编程&lt;/li&gt; 
 &lt;li&gt;软件的安装和服务的配置&lt;/li&gt; 
 &lt;li&gt;网络访问和管理&lt;/li&gt; 
 &lt;li&gt;其他相关内容&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Day36~45 - 数据库基础和进阶&lt;/h3&gt; 
&lt;h4&gt;Day36 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/36.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E5%92%8CMySQL%E6%A6%82%E8%BF%B0.md&quot;&gt;关系型数据库和MySQL概述&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;关系型数据库概述&lt;/li&gt; 
 &lt;li&gt;MySQL简介&lt;/li&gt; 
 &lt;li&gt;安装MySQL&lt;/li&gt; 
 &lt;li&gt;MySQL基本命令&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day37 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/37.SQL%E8%AF%A6%E8%A7%A3%E4%B9%8BDDL.md&quot;&gt;SQL详解之DDL&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;建库建表&lt;/li&gt; 
 &lt;li&gt;删除表和修改表&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day38 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/38.SQL%E8%AF%A6%E8%A7%A3%E4%B9%8BDML.md&quot;&gt;SQL详解之DML&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;insert操作&lt;/li&gt; 
 &lt;li&gt;delete操作&lt;/li&gt; 
 &lt;li&gt;update操作&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day39 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/39.SQL%E8%AF%A6%E8%A7%A3%E4%B9%8BDQL.md&quot;&gt;SQL详解之DQL&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;投影和别名&lt;/li&gt; 
 &lt;li&gt;筛选数据&lt;/li&gt; 
 &lt;li&gt;空值处理&lt;/li&gt; 
 &lt;li&gt;去重&lt;/li&gt; 
 &lt;li&gt;排序&lt;/li&gt; 
 &lt;li&gt;聚合函数&lt;/li&gt; 
 &lt;li&gt;嵌套查询&lt;/li&gt; 
 &lt;li&gt;分组操作&lt;/li&gt; 
 &lt;li&gt;表连接 
  &lt;ul&gt; 
   &lt;li&gt;笛卡尔积&lt;/li&gt; 
   &lt;li&gt;内连接&lt;/li&gt; 
   &lt;li&gt;自然连接&lt;/li&gt; 
   &lt;li&gt;外连接&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;窗口函数 
  &lt;ul&gt; 
   &lt;li&gt;定义窗口&lt;/li&gt; 
   &lt;li&gt;排名函数&lt;/li&gt; 
   &lt;li&gt;取数函数&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day40 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/40.SQL%E8%AF%A6%E8%A7%A3%E4%B9%8BDCL.md&quot;&gt;SQL详解之DCL&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;创建用户&lt;/li&gt; 
 &lt;li&gt;授予权限&lt;/li&gt; 
 &lt;li&gt;召回权限&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day41 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/41.MySQL%E6%96%B0%E7%89%B9%E6%80%A7.md&quot;&gt;MySQL新特性&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;JSON类型&lt;/li&gt; 
 &lt;li&gt;窗口函数&lt;/li&gt; 
 &lt;li&gt;公共表表达式&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Day42 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/42.%E8%A7%86%E5%9B%BE%E3%80%81%E5%87%BD%E6%95%B0%E5%92%8C%E8%BF%87%E7%A8%8B.md&quot;&gt;视图、函数和过程&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;视图 
  &lt;ul&gt; 
   &lt;li&gt;使用场景&lt;/li&gt; 
   &lt;li&gt;创建视图&lt;/li&gt; 
   &lt;li&gt;使用限制&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;函数 
  &lt;ul&gt; 
   &lt;li&gt;内置函数&lt;/li&gt; 
   &lt;li&gt;用户自定义函数（UDF）&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;过程 
  &lt;ul&gt; 
   &lt;li&gt;创建过程&lt;/li&gt; 
   &lt;li&gt;调用过程&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day43 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/43.%E7%B4%A2%E5%BC%95.md&quot;&gt;索引&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;执行计划&lt;/li&gt; 
 &lt;li&gt;索引的原理&lt;/li&gt; 
 &lt;li&gt;创建索引 
  &lt;ul&gt; 
   &lt;li&gt;普通索引&lt;/li&gt; 
   &lt;li&gt;唯一索引&lt;/li&gt; 
   &lt;li&gt;前缀索引&lt;/li&gt; 
   &lt;li&gt;复合索引&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;注意事项&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day44 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/44.Python%E6%8E%A5%E5%85%A5MySQL%E6%95%B0%E6%8D%AE%E5%BA%93.md&quot;&gt;Python接入MySQL数据库&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;安装三方库&lt;/li&gt; 
 &lt;li&gt;创建连接&lt;/li&gt; 
 &lt;li&gt;获取游标&lt;/li&gt; 
 &lt;li&gt;执行SQL语句&lt;/li&gt; 
 &lt;li&gt;通过游标抓取数据&lt;/li&gt; 
 &lt;li&gt;事务提交和回滚&lt;/li&gt; 
 &lt;li&gt;释放连接&lt;/li&gt; 
 &lt;li&gt;编写ETL脚本&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day45 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/45.%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E5%92%8CHiveSQL.md&quot;&gt;大数据平台和HiveSQL&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Hadoop生态圈&lt;/li&gt; 
 &lt;li&gt;Hive概述&lt;/li&gt; 
 &lt;li&gt;准备工作&lt;/li&gt; 
 &lt;li&gt;数据类型&lt;/li&gt; 
 &lt;li&gt;DDL操作&lt;/li&gt; 
 &lt;li&gt;DML操作&lt;/li&gt; 
 &lt;li&gt;数据查询&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Day46~60 - 实战Django&lt;/h3&gt; 
&lt;h4&gt;Day46 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/46.Django%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B.md&quot;&gt;Django快速上手&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Web应用工作机制&lt;/li&gt; 
 &lt;li&gt;HTTP请求和响应&lt;/li&gt; 
 &lt;li&gt;Django框架概述&lt;/li&gt; 
 &lt;li&gt;5分钟快速上手&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day47 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/47.%E6%B7%B1%E5%85%A5%E6%A8%A1%E5%9E%8B.md&quot;&gt;深入模型&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;关系型数据库配置&lt;/li&gt; 
 &lt;li&gt;使用ORM完成对模型的CRUD操作&lt;/li&gt; 
 &lt;li&gt;管理后台的使用&lt;/li&gt; 
 &lt;li&gt;Django模型最佳实践&lt;/li&gt; 
 &lt;li&gt;模型定义参考&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day48 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/48.%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%92%8CAjax%E8%AF%B7%E6%B1%82.md&quot;&gt;静态资源和Ajax请求&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;加载静态资源&lt;/li&gt; 
 &lt;li&gt;Ajax概述&lt;/li&gt; 
 &lt;li&gt;用Ajax实现投票功能&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day49 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/49.Cookie%E5%92%8CSession.md&quot;&gt;Cookie和Session&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;实现用户跟踪&lt;/li&gt; 
 &lt;li&gt;cookie和session的关系&lt;/li&gt; 
 &lt;li&gt;Django框架对session的支持&lt;/li&gt; 
 &lt;li&gt;视图函数中的cookie读写操作&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day50 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/50.%E5%88%B6%E4%BD%9C%E6%8A%A5%E8%A1%A8.md&quot;&gt;报表和日志&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;通过&lt;code&gt;HttpResponse&lt;/code&gt;修改响应头&lt;/li&gt; 
 &lt;li&gt;使用&lt;code&gt;StreamingHttpResponse&lt;/code&gt;处理大文件&lt;/li&gt; 
 &lt;li&gt;使用&lt;code&gt;xlwt&lt;/code&gt;生成Excel报表&lt;/li&gt; 
 &lt;li&gt;使用&lt;code&gt;reportlab&lt;/code&gt;生成PDF报表&lt;/li&gt; 
 &lt;li&gt;使用ECharts生成前端图表&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day51 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/51.%E6%97%A5%E5%BF%97%E5%92%8C%E8%B0%83%E8%AF%95%E5%B7%A5%E5%85%B7%E6%A0%8F.md&quot;&gt;日志和调试工具栏&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;配置日志&lt;/li&gt; 
 &lt;li&gt;配置Django-Debug-Toolbar&lt;/li&gt; 
 &lt;li&gt;优化ORM代码&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day52 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/52.%E4%B8%AD%E9%97%B4%E4%BB%B6%E7%9A%84%E5%BA%94%E7%94%A8.md&quot;&gt;中间件的应用&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;什么是中间件&lt;/li&gt; 
 &lt;li&gt;Django框架内置的中间件&lt;/li&gt; 
 &lt;li&gt;自定义中间件及其应用场景&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day53 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/53.%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8.md&quot;&gt;前后端分离开发入门&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;返回JSON格式的数据&lt;/li&gt; 
 &lt;li&gt;用Vue.js渲染页面&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day54 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/54.RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E5%85%A5%E9%97%A8.md&quot;&gt;RESTful架构和DRF入门&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;REST概述&lt;/li&gt; 
 &lt;li&gt;DRF库使用入门&lt;/li&gt; 
 &lt;li&gt;前后端分离开发&lt;/li&gt; 
 &lt;li&gt;JWT的应用&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day55 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/55.RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E8%BF%9B%E9%98%B6.md&quot;&gt;RESTful架构和DRF进阶&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;使用CBV&lt;/li&gt; 
 &lt;li&gt;数据分页&lt;/li&gt; 
 &lt;li&gt;数据筛选&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day56 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/56.%E4%BD%BF%E7%94%A8%E7%BC%93%E5%AD%98.md&quot;&gt;使用缓存&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;网站优化第一定律&lt;/li&gt; 
 &lt;li&gt;在Django项目中使用Redis提供缓存服务&lt;/li&gt; 
 &lt;li&gt;在视图函数中读写缓存&lt;/li&gt; 
 &lt;li&gt;使用装饰器实现页面缓存&lt;/li&gt; 
 &lt;li&gt;为数据接口提供缓存服务&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day57 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/57.%E6%8E%A5%E5%85%A5%E4%B8%89%E6%96%B9%E5%B9%B3%E5%8F%B0.md&quot;&gt;接入三方平台&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;文件上传表单控件和图片文件预览&lt;/li&gt; 
 &lt;li&gt;服务器端如何处理上传的文件&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day58 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/58.%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%E5%92%8C%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1.md&quot;&gt;异步任务和定时任务&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;网站优化第二定律&lt;/li&gt; 
 &lt;li&gt;配置消息队列服务&lt;/li&gt; 
 &lt;li&gt;在项目中使用Celery实现任务异步化&lt;/li&gt; 
 &lt;li&gt;在项目中使用Celery实现定时任务&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day59 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/59.%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95.md&quot;&gt;单元测试&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;Day60 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/60.%E9%A1%B9%E7%9B%AE%E4%B8%8A%E7%BA%BF.md&quot;&gt;项目上线&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Python中的单元测试&lt;/li&gt; 
 &lt;li&gt;Django框架对单元测试的支持&lt;/li&gt; 
 &lt;li&gt;使用版本控制系统&lt;/li&gt; 
 &lt;li&gt;配置和使用uWSGI&lt;/li&gt; 
 &lt;li&gt;动静分离和Nginx配置&lt;/li&gt; 
 &lt;li&gt;配置HTTPS&lt;/li&gt; 
 &lt;li&gt;配置域名解析&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Day61~65 - 网络数据采集&lt;/h3&gt; 
&lt;h4&gt;Day61 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day61-65/61.%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E6%A6%82%E8%BF%B0.md&quot;&gt;网络数据采集概述&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;网络爬虫的概念及其应用领域&lt;/li&gt; 
 &lt;li&gt;网络爬虫的合法性探讨&lt;/li&gt; 
 &lt;li&gt;开发网络爬虫的相关工具&lt;/li&gt; 
 &lt;li&gt;一个爬虫程序的构成&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day62 - 数据抓取和解析&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day61-65/62.%E7%94%A8Python%E8%8E%B7%E5%8F%96%E7%BD%91%E7%BB%9C%E8%B5%84%E6%BA%90-1.md&quot;&gt;使用&lt;code&gt;requests&lt;/code&gt;三方库实现数据抓取&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day61-65/62.%E7%94%A8Python%E8%A7%A3%E6%9E%90HTML%E9%A1%B5%E9%9D%A2-2.md&quot;&gt;页面解析的三种方式&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;正则表达式解析&lt;/li&gt; 
   &lt;li&gt;XPath解析&lt;/li&gt; 
   &lt;li&gt;CSS选择器解析&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day63 - Python中的并发编程&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day61-65/63.Python%E4%B8%AD%E7%9A%84%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-1.md&quot;&gt;多线程&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day61-65/63.Python%E4%B8%AD%E7%9A%84%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-2.md&quot;&gt;多进程&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day61-65/63.Python%E4%B8%AD%E7%9A%84%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-3.md&quot;&gt;异步I/O&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day64 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day61-65/64.%E4%BD%BF%E7%94%A8Selenium%E6%8A%93%E5%8F%96%E7%BD%91%E9%A1%B5%E5%8A%A8%E6%80%81%E5%86%85%E5%AE%B9.md&quot;&gt;使用Selenium抓取网页动态内容&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;安装Selenium&lt;/li&gt; 
 &lt;li&gt;加载页面&lt;/li&gt; 
 &lt;li&gt;查找元素和模拟用户行为&lt;/li&gt; 
 &lt;li&gt;隐式等待和显示等待&lt;/li&gt; 
 &lt;li&gt;执行JavaScript代码&lt;/li&gt; 
 &lt;li&gt;Selenium反爬破解&lt;/li&gt; 
 &lt;li&gt;设置无头浏览器&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day65 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day61-65/65.%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6Scrapy%E7%AE%80%E4%BB%8B.md&quot;&gt;爬虫框架Scrapy简介&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Scrapy核心组件&lt;/li&gt; 
 &lt;li&gt;Scrapy工作流程&lt;/li&gt; 
 &lt;li&gt;安装Scrapy和创建项目&lt;/li&gt; 
 &lt;li&gt;编写蜘蛛程序&lt;/li&gt; 
 &lt;li&gt;编写中间件和管道程序&lt;/li&gt; 
 &lt;li&gt;Scrapy配置文件&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Day66~80 - Python数据分析&lt;/h3&gt; 
&lt;h4&gt;Day66 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/66.%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%A6%82%E8%BF%B0.md&quot;&gt;数据分析概述&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;数据分析师的职责&lt;/li&gt; 
 &lt;li&gt;数据分析师的技能栈&lt;/li&gt; 
 &lt;li&gt;数据分析相关库&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day67 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/67.%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87.md&quot;&gt;环境准备&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;安装和使用anaconda 
  &lt;ul&gt; 
   &lt;li&gt;conda相关命令&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;安装和使用jupyter-lab 
  &lt;ul&gt; 
   &lt;li&gt;安装和启动&lt;/li&gt; 
   &lt;li&gt;使用小技巧&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day68 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/68.NumPy%E7%9A%84%E5%BA%94%E7%94%A8-1.md&quot;&gt;NumPy的应用-1&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;创建数组对象&lt;/li&gt; 
 &lt;li&gt;数组对象的属性&lt;/li&gt; 
 &lt;li&gt;数组对象的索引运算 
  &lt;ul&gt; 
   &lt;li&gt;普通索引&lt;/li&gt; 
   &lt;li&gt;花式索引&lt;/li&gt; 
   &lt;li&gt;布尔索引&lt;/li&gt; 
   &lt;li&gt;切片索引&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;案例：使用数组处理图像&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day69 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/69.NumPy%E7%9A%84%E5%BA%94%E7%94%A8-2.md&quot;&gt;NumPy的应用-2&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;数组对象的相关方法 
  &lt;ul&gt; 
   &lt;li&gt;获取描述性统计信息&lt;/li&gt; 
   &lt;li&gt;其他相关方法&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day70 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/70.NumPy%E7%9A%84%E5%BA%94%E7%94%A8-3.md&quot;&gt;NumPy的应用-3&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;数组的运算 
  &lt;ul&gt; 
   &lt;li&gt;数组跟标量的运算&lt;/li&gt; 
   &lt;li&gt;数组跟数组的运算&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;通用一元函数&lt;/li&gt; 
 &lt;li&gt;通用二元函数&lt;/li&gt; 
 &lt;li&gt;广播机制&lt;/li&gt; 
 &lt;li&gt;Numpy常用函数&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day71 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/71.NumPy%E7%9A%84%E5%BA%94%E7%94%A8-4.md&quot;&gt;NumPy的应用-4&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;向量&lt;/li&gt; 
 &lt;li&gt;行列式&lt;/li&gt; 
 &lt;li&gt;矩阵&lt;/li&gt; 
 &lt;li&gt;多项式&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day72 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/72.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApandas-1.md&quot;&gt;深入浅出pandas-1&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;创建Series对象&lt;/li&gt; 
 &lt;li&gt;Series对象的运算&lt;/li&gt; 
 &lt;li&gt;Series对象的属性和方法&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day73 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/73.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApandas-2.md&quot;&gt;深入浅出pandas-2&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;创建DataFrame对象&lt;/li&gt; 
 &lt;li&gt;DataFrame对象的属性和方法&lt;/li&gt; 
 &lt;li&gt;读写DataFrame中的数据&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day74 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/74.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApandas-3.md&quot;&gt;深入浅出pandas-3&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;数据重塑 
  &lt;ul&gt; 
   &lt;li&gt;数据拼接&lt;/li&gt; 
   &lt;li&gt;数据合并&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;数据清洗 
  &lt;ul&gt; 
   &lt;li&gt;缺失值&lt;/li&gt; 
   &lt;li&gt;重复值&lt;/li&gt; 
   &lt;li&gt;异常值&lt;/li&gt; 
   &lt;li&gt;预处理&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day75 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/75.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApandas-4.md&quot;&gt;深入浅出pandas-4&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;数据透视 
  &lt;ul&gt; 
   &lt;li&gt;获取描述性统计信息&lt;/li&gt; 
   &lt;li&gt;排序和头部值&lt;/li&gt; 
   &lt;li&gt;分组聚合&lt;/li&gt; 
   &lt;li&gt;透视表和交叉表&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;数据呈现&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day76 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/76.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApandas-5.md&quot;&gt;深入浅出pandas-5&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;计算同比环比&lt;/li&gt; 
 &lt;li&gt;窗口计算&lt;/li&gt; 
 &lt;li&gt;相关性判定&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day77 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/77.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApandas-6.md&quot;&gt;深入浅出pandas-6&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;索引的使用 
  &lt;ul&gt; 
   &lt;li&gt;范围索引&lt;/li&gt; 
   &lt;li&gt;分类索引&lt;/li&gt; 
   &lt;li&gt;多级索引&lt;/li&gt; 
   &lt;li&gt;间隔索引&lt;/li&gt; 
   &lt;li&gt;日期时间索引&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day78 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/78.%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96-1.md&quot;&gt;数据可视化-1&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;安装和导入matplotlib&lt;/li&gt; 
 &lt;li&gt;创建画布&lt;/li&gt; 
 &lt;li&gt;创建坐标系&lt;/li&gt; 
 &lt;li&gt;绘制图表 
  &lt;ul&gt; 
   &lt;li&gt;折线图&lt;/li&gt; 
   &lt;li&gt;散点图&lt;/li&gt; 
   &lt;li&gt;柱状图&lt;/li&gt; 
   &lt;li&gt;饼状图&lt;/li&gt; 
   &lt;li&gt;直方图&lt;/li&gt; 
   &lt;li&gt;箱线图&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;显示和保存图表&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day79 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/79.%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96-2.md&quot;&gt;数据可视化-2&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;高阶图表 
  &lt;ul&gt; 
   &lt;li&gt;气泡图&lt;/li&gt; 
   &lt;li&gt;面积图&lt;/li&gt; 
   &lt;li&gt;雷达图&lt;/li&gt; 
   &lt;li&gt;玫瑰图&lt;/li&gt; 
   &lt;li&gt;3D图表&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day80 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/80.%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96-3.md&quot;&gt;数据可视化-3&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Seaborn&lt;/li&gt; 
 &lt;li&gt;Pyecharts&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Day81~90 - 机器学习&lt;/h3&gt; 
&lt;h4&gt;Day81 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/81.%E6%B5%85%E8%B0%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.md&quot;&gt;浅谈机器学习&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;人工智能发展史&lt;/li&gt; 
 &lt;li&gt;什么是机器学习&lt;/li&gt; 
 &lt;li&gt;机器学习应用领域&lt;/li&gt; 
 &lt;li&gt;机器学习的分类&lt;/li&gt; 
 &lt;li&gt;机器学习的步骤&lt;/li&gt; 
 &lt;li&gt;第一次机器学习&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day82 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/82.k%E6%9C%80%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95.md&quot;&gt;k最近邻算法&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;距离的度量&lt;/li&gt; 
 &lt;li&gt;数据集介绍&lt;/li&gt; 
 &lt;li&gt;kNN分类的实现&lt;/li&gt; 
 &lt;li&gt;模型评估&lt;/li&gt; 
 &lt;li&gt;参数调优&lt;/li&gt; 
 &lt;li&gt;kNN回归的实现&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day83 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/83.%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.md&quot;&gt;决策树和随机森林&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;决策树的构建 
  &lt;ul&gt; 
   &lt;li&gt;特征选择&lt;/li&gt; 
   &lt;li&gt;数据分裂&lt;/li&gt; 
   &lt;li&gt;树的剪枝&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;实现决策树模型&lt;/li&gt; 
 &lt;li&gt;随机森林概述&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day84 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/84.%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95.md&quot;&gt;朴素贝叶斯算法&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;贝叶斯定理&lt;/li&gt; 
 &lt;li&gt;朴素贝叶斯&lt;/li&gt; 
 &lt;li&gt;算法原理 
  &lt;ul&gt; 
   &lt;li&gt;训练阶段&lt;/li&gt; 
   &lt;li&gt;预测阶段&lt;/li&gt; 
   &lt;li&gt;代码实现&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;算法优缺点&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day85 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/85.%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B.md&quot;&gt;回归模型&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;回归模型的分类&lt;/li&gt; 
 &lt;li&gt;回归系数的计算&lt;/li&gt; 
 &lt;li&gt;新数据集介绍&lt;/li&gt; 
 &lt;li&gt;线性回归代码实现&lt;/li&gt; 
 &lt;li&gt;回归模型的评估&lt;/li&gt; 
 &lt;li&gt;引入正则化项&lt;/li&gt; 
 &lt;li&gt;线性回归另一种实现&lt;/li&gt; 
 &lt;li&gt;多项式回归&lt;/li&gt; 
 &lt;li&gt;逻辑回归&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day86 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/86.K-Means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95.md&quot;&gt;K-Means聚类算法&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;算法原理&lt;/li&gt; 
 &lt;li&gt;数学描述&lt;/li&gt; 
 &lt;li&gt;代码实现&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day87 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/87.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95.md&quot;&gt;集成学习算法&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;算法分类&lt;/li&gt; 
 &lt;li&gt;AdaBoost&lt;/li&gt; 
 &lt;li&gt;GBDT&lt;/li&gt; 
 &lt;li&gt;XGBoost&lt;/li&gt; 
 &lt;li&gt;LightGBM&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day88 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/88.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.md&quot;&gt;神经网络模型&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;基本构成&lt;/li&gt; 
 &lt;li&gt;工作原理&lt;/li&gt; 
 &lt;li&gt;代码实现&lt;/li&gt; 
 &lt;li&gt;模型优缺点&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day89 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/89.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8.md&quot;&gt;自然语言处理入门&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;词袋模型&lt;/li&gt; 
 &lt;li&gt;词向量&lt;/li&gt; 
 &lt;li&gt;NPLM和RNN&lt;/li&gt; 
 &lt;li&gt;Seq2Seq&lt;/li&gt; 
 &lt;li&gt;Transformer&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day90 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/90.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98.md&quot;&gt;机器学习实战&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;数据探索&lt;/li&gt; 
 &lt;li&gt;特征工程&lt;/li&gt; 
 &lt;li&gt;模型训练&lt;/li&gt; 
 &lt;li&gt;模型评估&lt;/li&gt; 
 &lt;li&gt;模型部署&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Day91~99 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100&quot;&gt;团队项目开发&lt;/a&gt;&lt;/h3&gt; 
&lt;h4&gt;第91天：&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/91.%E5%9B%A2%E9%98%9F%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.md&quot;&gt;团队项目开发的问题和解决方案&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;软件过程模型&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;经典过程模型（瀑布模型）&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;可行性分析（研究做还是不做），输出《可行性分析报告》。&lt;/li&gt; 
     &lt;li&gt;需求分析（研究做什么），输出《需求规格说明书》和产品界面原型图。&lt;/li&gt; 
     &lt;li&gt;概要设计和详细设计，输出概念模型图（ER图）、物理模型图、类图、时序图等。&lt;/li&gt; 
     &lt;li&gt;编码 / 测试。&lt;/li&gt; 
     &lt;li&gt;上线 / 维护。&lt;/li&gt; 
    &lt;/ul&gt; &lt;p&gt;瀑布模型最大的缺点是无法拥抱需求变化，整套流程结束后才能看到产品，团队士气低落。&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;敏捷开发（Scrum）- 产品所有者、Scrum Master、研发人员 - Sprint&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;产品的Backlog（用户故事、产品原型）。&lt;/li&gt; 
     &lt;li&gt;计划会议（评估和预算）。&lt;/li&gt; 
     &lt;li&gt;日常开发（站立会议、番茄工作法、结对编程、测试先行、代码重构……）。&lt;/li&gt; 
     &lt;li&gt;修复bug（问题描述、重现步骤、测试人员、被指派人）。&lt;/li&gt; 
     &lt;li&gt;发布版本。&lt;/li&gt; 
     &lt;li&gt;评审会议（Showcase，用户需要参与）。&lt;/li&gt; 
     &lt;li&gt;回顾会议（对当前迭代周期做一个总结）。&lt;/li&gt; 
    &lt;/ul&gt; 
    &lt;blockquote&gt; 
     &lt;p&gt;补充：敏捷软件开发宣言&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;strong&gt;个体和互动&lt;/strong&gt; 高于 流程和工具&lt;/li&gt; 
      &lt;li&gt;&lt;strong&gt;工作的软件&lt;/strong&gt; 高于 详尽的文档&lt;/li&gt; 
      &lt;li&gt;&lt;strong&gt;客户合作&lt;/strong&gt; 高于 合同谈判&lt;/li&gt; 
      &lt;li&gt;&lt;strong&gt;响应变化&lt;/strong&gt; 高于 遵循计划&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/blockquote&gt; &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/res/agile-scrum-sprint-cycle.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt; 
    &lt;blockquote&gt; 
     &lt;p&gt;角色：产品所有者（决定做什么，能对需求拍板的人）、团队负责人（解决各种问题，专注如何更好的工作，屏蔽外部对开发团队的影响）、开发团队（项目执行人员，具体指开发人员和测试人员）。&lt;/p&gt; 
    &lt;/blockquote&gt; 
    &lt;blockquote&gt; 
     &lt;p&gt;准备工作：商业案例和资金、合同、憧憬、初始产品需求、初始发布计划、入股、组建团队。&lt;/p&gt; 
    &lt;/blockquote&gt; 
    &lt;blockquote&gt; 
     &lt;p&gt;敏捷团队通常人数为8-10人。&lt;/p&gt; 
    &lt;/blockquote&gt; 
    &lt;blockquote&gt; 
     &lt;p&gt;工作量估算：将开发任务量化，包括原型、Logo设计、UI设计、前端开发等，尽量把每个工作分解到最小任务量，最小任务量标准为工作时间不能超过两天，然后估算总体项目时间。把每个任务都贴在看板上面，看板上分三部分：to do（待完成）、in progress（进行中）和done（已完成）。&lt;/p&gt; 
    &lt;/blockquote&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;项目团队组建&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;团队的构成和角色&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/res/company_architecture.png&quot; alt=&quot;company_architecture&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;编程规范和代码审查（&lt;code&gt;flake8&lt;/code&gt;、&lt;code&gt;pylint&lt;/code&gt;）&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/res/pylint.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Python中的一些“惯例”（请参考&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/%E7%95%AA%E5%A4%96%E7%AF%87/Python%E7%BC%96%E7%A8%8B%E6%83%AF%E4%BE%8B.md&quot;&gt;《Python惯例-如何编写Pythonic的代码》&lt;/a&gt;）&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;影响代码可读性的原因：&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;代码注释太少或者没有注释&lt;/li&gt; 
     &lt;li&gt;代码破坏了语言的最佳实践&lt;/li&gt; 
     &lt;li&gt;反模式编程（意大利面代码、复制-黏贴编程、自负编程、……）&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;团队开发工具介绍&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;版本控制：Git、Mercury&lt;/li&gt; 
   &lt;li&gt;缺陷管理：&lt;a href=&quot;https://about.gitlab.com/&quot;&gt;Gitlab&lt;/a&gt;、&lt;a href=&quot;http://www.redmine.org.cn/&quot;&gt;Redmine&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;敏捷闭环工具：&lt;a href=&quot;https://www.zentao.net/&quot;&gt;禅道&lt;/a&gt;、&lt;a href=&quot;https://www.atlassian.com/software/jira/features&quot;&gt;JIRA&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;持续集成：&lt;a href=&quot;https://jenkins.io/&quot;&gt;Jenkins&lt;/a&gt;、&lt;a href=&quot;https://travis-ci.org/&quot;&gt;Travis-CI&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;请参考&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/91.%E5%9B%A2%E9%98%9F%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.md&quot;&gt;《团队项目开发的问题和解决方案》&lt;/a&gt;。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h5&gt;项目选题和理解业务&lt;/h5&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;选题范围设定&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;CMS（用户端）：新闻聚合网站、问答/分享社区、影评/书评网站等。&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;MIS（用户端+管理端）：KMS、KPI考核系统、HRS、CRM系统、供应链系统、仓储管理系统等。&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;App后台（管理端+数据接口）：二手交易类、报刊杂志类、小众电商类、新闻资讯类、旅游类、社交类、阅读类等。&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;其他类型：自身行业背景和工作经验、业务容易理解和把控。&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;需求理解、模块划分和任务分配&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;需求理解：头脑风暴和竞品分析。&lt;/li&gt; 
   &lt;li&gt;模块划分：画思维导图（XMind），每个模块是一个枝节点，每个具体的功能是一个叶节点（用动词表述），需要确保每个叶节点无法再生出新节点，确定每个叶子节点的重要性、优先级和工作量。&lt;/li&gt; 
   &lt;li&gt;任务分配：由项目负责人根据上面的指标为每个团队成员分配任务。&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/res/requirements_by_xmind.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;制定项目进度表（每日更新）&lt;/p&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th&gt;模块&lt;/th&gt; 
     &lt;th&gt;功能&lt;/th&gt; 
     &lt;th&gt;人员&lt;/th&gt; 
     &lt;th&gt;状态&lt;/th&gt; 
     &lt;th&gt;完成&lt;/th&gt; 
     &lt;th&gt;工时&lt;/th&gt; 
     &lt;th&gt;计划开始&lt;/th&gt; 
     &lt;th&gt;实际开始&lt;/th&gt; 
     &lt;th&gt;计划结束&lt;/th&gt; 
     &lt;th&gt;实际结束&lt;/th&gt; 
     &lt;th&gt;备注&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td&gt;评论&lt;/td&gt; 
     &lt;td&gt;添加评论&lt;/td&gt; 
     &lt;td&gt;王大锤&lt;/td&gt; 
     &lt;td&gt;正在进行&lt;/td&gt; 
     &lt;td&gt;50%&lt;/td&gt; 
     &lt;td&gt;4&lt;/td&gt; 
     &lt;td&gt;2018/8/7&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;2018/8/7&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;删除评论&lt;/td&gt; 
     &lt;td&gt;王大锤&lt;/td&gt; 
     &lt;td&gt;等待&lt;/td&gt; 
     &lt;td&gt;0%&lt;/td&gt; 
     &lt;td&gt;2&lt;/td&gt; 
     &lt;td&gt;2018/8/7&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;2018/8/7&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;查看评论&lt;/td&gt; 
     &lt;td&gt;白元芳&lt;/td&gt; 
     &lt;td&gt;正在进行&lt;/td&gt; 
     &lt;td&gt;20%&lt;/td&gt; 
     &lt;td&gt;4&lt;/td&gt; 
     &lt;td&gt;2018/8/7&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;2018/8/7&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;需要进行代码审查&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;评论投票&lt;/td&gt; 
     &lt;td&gt;白元芳&lt;/td&gt; 
     &lt;td&gt;等待&lt;/td&gt; 
     &lt;td&gt;0%&lt;/td&gt; 
     &lt;td&gt;4&lt;/td&gt; 
     &lt;td&gt;2018/8/8&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;2018/8/8&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;OOAD和数据库设计&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;UML（统一建模语言）的类图&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/res/uml-class-diagram.png&quot; alt=&quot;uml&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;通过模型创建表（正向工程），例如在Django项目中可以通过下面的命令创建二维表。&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-Shell&quot;&gt;python manage.py makemigrations app
python manage.py migrate
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;使用PowerDesigner绘制物理模型图。&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/res/power-designer-pdm.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;通过数据表创建模型（反向工程），例如在Django项目中可以通过下面的命令生成模型。&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-Shell&quot;&gt;python manage.py inspectdb &amp;gt; app/models.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;第92天：&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/92.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%E8%AF%A6%E8%A7%A3.md&quot;&gt;Docker容器技术详解&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Docker简介&lt;/li&gt; 
 &lt;li&gt;安装Docker&lt;/li&gt; 
 &lt;li&gt;使用Docker创建容器（Nginx、MySQL、Redis、Gitlab、Jenkins）&lt;/li&gt; 
 &lt;li&gt;构建Docker镜像（Dockerfile的编写和相关指令）&lt;/li&gt; 
 &lt;li&gt;容器编排（Docker-compose）&lt;/li&gt; 
 &lt;li&gt;集群管理（Kubernetes）&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;第93天：&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/93.MySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96.md&quot;&gt;MySQL性能优化&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;基本原则&lt;/li&gt; 
 &lt;li&gt;InnoDB引擎&lt;/li&gt; 
 &lt;li&gt;索引的使用和注意事项&lt;/li&gt; 
 &lt;li&gt;数据分区&lt;/li&gt; 
 &lt;li&gt;SQL优化&lt;/li&gt; 
 &lt;li&gt;配置优化&lt;/li&gt; 
 &lt;li&gt;架构优化&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;第94天：&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/94.%E7%BD%91%E7%BB%9CAPI%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1.md&quot;&gt;网络API接口设计&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;设计原则 
  &lt;ul&gt; 
   &lt;li&gt;关键问题&lt;/li&gt; 
   &lt;li&gt;其他问题&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;文档撰写&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;第95天：[使用Django开发商业项目](./Day91-100/95.使用Django开发商业项 目.md)&lt;/h4&gt; 
&lt;h5&gt;项目开发中的公共问题&lt;/h5&gt; 
&lt;ol&gt; 
 &lt;li&gt;数据库的配置（多数据库、主从复制、数据库路由）&lt;/li&gt; 
 &lt;li&gt;缓存的配置（分区缓存、键设置、超时设置、主从复制、故障恢复（哨兵））&lt;/li&gt; 
 &lt;li&gt;日志的配置&lt;/li&gt; 
 &lt;li&gt;分析和调试（Django-Debug-ToolBar）&lt;/li&gt; 
 &lt;li&gt;好用的Python模块（日期计算、图像处理、数据加密、三方API）&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h5&gt;REST API设计&lt;/h5&gt; 
&lt;ol&gt; 
 &lt;li&gt;RESTful架构 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;http://www.ruanyifeng.com/blog/2011/09/restful.html&quot;&gt;理解RESTful架构&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://www.ruanyifeng.com/blog/2014/05/restful_api.html&quot;&gt;RESTful API设计指南&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://www.ruanyifeng.com/blog/2018/10/restful-api-best-practices.html&quot;&gt;RESTful API最佳实践&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;API接口文档的撰写 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;http://rap2.taobao.org/&quot;&gt;RAP2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://yapi.demo.qunar.com/&quot;&gt;YAPI&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.django-rest-framework.org/&quot;&gt;django-REST-framework&lt;/a&gt;的应用&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h5&gt;项目中的重点难点剖析&lt;/h5&gt; 
&lt;ol&gt; 
 &lt;li&gt;使用缓存缓解数据库压力 - Redis&lt;/li&gt; 
 &lt;li&gt;使用消息队列做解耦合和削峰 - Celery + RabbitMQ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;第96天：&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/96.%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E5%92%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95.md&quot;&gt;软件测试和自动化测试&lt;/a&gt;&lt;/h4&gt; 
&lt;h5&gt;单元测试&lt;/h5&gt; 
&lt;ol&gt; 
 &lt;li&gt;测试的种类&lt;/li&gt; 
 &lt;li&gt;编写单元测试（&lt;code&gt;unittest&lt;/code&gt;、&lt;code&gt;pytest&lt;/code&gt;、&lt;code&gt;nose2&lt;/code&gt;、&lt;code&gt;tox&lt;/code&gt;、&lt;code&gt;ddt&lt;/code&gt;、……）&lt;/li&gt; 
 &lt;li&gt;测试覆盖率（&lt;code&gt;coverage&lt;/code&gt;）&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h5&gt;Django项目部署&lt;/h5&gt; 
&lt;ol&gt; 
 &lt;li&gt;部署前的准备工作 
  &lt;ul&gt; 
   &lt;li&gt;关键设置（SECRET_KEY / DEBUG / ALLOWED_HOSTS / 缓存 / 数据库）&lt;/li&gt; 
   &lt;li&gt;HTTPS / CSRF_COOKIE_SECUR / SESSION_COOKIE_SECURE&lt;/li&gt; 
   &lt;li&gt;日志相关配置&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Linux常用命令回顾&lt;/li&gt; 
 &lt;li&gt;Linux常用服务的安装和配置&lt;/li&gt; 
 &lt;li&gt;uWSGI/Gunicorn和Nginx的使用 
  &lt;ul&gt; 
   &lt;li&gt;Gunicorn和uWSGI的比较 
    &lt;ul&gt; 
     &lt;li&gt;对于不需要大量定制化的简单应用程序，Gunicorn是一个不错的选择，uWSGI的学习曲线比Gunicorn要陡峭得多，Gunicorn的默认参数就已经能够适应大多数应用程序。&lt;/li&gt; 
     &lt;li&gt;uWSGI支持异构部署。&lt;/li&gt; 
     &lt;li&gt;由于Nginx本身支持uWSGI，在线上一般都将Nginx和uWSGI捆绑在一起部署，而且uWSGI属于功能齐全且高度定制的WSGI中间件。&lt;/li&gt; 
     &lt;li&gt;在性能上，Gunicorn和uWSGI其实表现相当。&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;使用虚拟化技术（Docker）部署测试环境和生产环境&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h5&gt;性能测试&lt;/h5&gt; 
&lt;ol&gt; 
 &lt;li&gt;AB的使用&lt;/li&gt; 
 &lt;li&gt;SQLslap的使用&lt;/li&gt; 
 &lt;li&gt;sysbench的使用&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h5&gt;自动化测试&lt;/h5&gt; 
&lt;ol&gt; 
 &lt;li&gt;使用Shell和Python进行自动化测试&lt;/li&gt; 
 &lt;li&gt;使用Selenium实现自动化测试 
  &lt;ul&gt; 
   &lt;li&gt;Selenium IDE&lt;/li&gt; 
   &lt;li&gt;Selenium WebDriver&lt;/li&gt; 
   &lt;li&gt;Selenium Remote Control&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;测试工具Robot Framework介绍&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;第97天：&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/97.%E7%94%B5%E5%95%86%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E8%A6%81%E7%82%B9%E5%89%96%E6%9E%90.md&quot;&gt;电商网站技术要点剖析&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;商业模式和需求要点&lt;/li&gt; 
 &lt;li&gt;物理模型设计&lt;/li&gt; 
 &lt;li&gt;第三方登录&lt;/li&gt; 
 &lt;li&gt;缓存预热和查询缓存&lt;/li&gt; 
 &lt;li&gt;购物车的实现&lt;/li&gt; 
 &lt;li&gt;支付功能集成&lt;/li&gt; 
 &lt;li&gt;秒杀和超卖问题&lt;/li&gt; 
 &lt;li&gt;静态资源管理&lt;/li&gt; 
 &lt;li&gt;全文检索方案&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;第98天：&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/98.%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E4%B8%8A%E7%BA%BF%E5%92%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98.md&quot;&gt;项目部署上线和性能调优&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;MySQL数据库调优&lt;/li&gt; 
 &lt;li&gt;Web服务器性能优化 
  &lt;ul&gt; 
   &lt;li&gt;Nginx负载均衡配置&lt;/li&gt; 
   &lt;li&gt;Keepalived实现高可用&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;代码性能调优 
  &lt;ul&gt; 
   &lt;li&gt;多线程&lt;/li&gt; 
   &lt;li&gt;异步化&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;静态资源访问优化 
  &lt;ul&gt; 
   &lt;li&gt;云存储&lt;/li&gt; 
   &lt;li&gt;CDN&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;第99天：&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/99.%E9%9D%A2%E8%AF%95%E4%B8%AD%E7%9A%84%E5%85%AC%E5%85%B1%E9%97%AE%E9%A2%98.md&quot;&gt;面试中的公共问题&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;计算机基础&lt;/li&gt; 
 &lt;li&gt;Python基础&lt;/li&gt; 
 &lt;li&gt;Web框架相关&lt;/li&gt; 
 &lt;li&gt;爬虫相关问题&lt;/li&gt; 
 &lt;li&gt;数据分析&lt;/li&gt; 
 &lt;li&gt;项目相关&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;第100天 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/100.%E8%A1%A5%E5%85%85%E5%86%85%E5%AE%B9.md&quot;&gt;补充内容&lt;/a&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;面试宝典&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Python 面试宝典&lt;/li&gt; 
   &lt;li&gt;SQL 面试宝典（数据分析师）&lt;/li&gt; 
   &lt;li&gt;商业分析面试宝典&lt;/li&gt; 
   &lt;li&gt;机器学习面试宝典&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;机器学习数学基础&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;深度学习&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;计算机视觉&lt;/li&gt; 
   &lt;li&gt;大语言模型&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>aws-samples/amazon-bedrock-workshop</title>
      <link>https://github.com/aws-samples/amazon-bedrock-workshop</link>
      <description>&lt;p&gt;This is a workshop designed for Amazon Bedrock a foundational model service.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Amazon Bedrock Workshop &lt;a href=&quot;https://github.com/dwyl/esta/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat&quot; alt=&quot;contributions welcome&quot;&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;This hands-on workshop, aimed at developers and solution builders, introduces how to leverage foundation models (FMs) through &lt;a href=&quot;https://aws.amazon.com/bedrock/&quot;&gt;Amazon Bedrock&lt;/a&gt;. This code goes alongside the self-paced or instructor lead workshop here - &lt;a href=&quot;https://catalog.us-east-1.prod.workshops.aws/amazon-bedrock/en-US&quot;&gt;https://catalog.us-east-1.prod.workshops.aws/amazon-bedrock/en-US&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Please follow the prerequisites listed in the link above or ask your AWS workshop instructor how to get started.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Amazon Bedrock is a fully managed service that provides access to FMs from third-party providers and Amazon; available via an API. With Bedrock, you can choose from a variety of models to find the one that’s best suited for your use case.&lt;/p&gt; 
&lt;p&gt;Within this series of labs, you&#39;ll explore some of the most common usage patterns we are seeing with our customers for Generative AI. We will show techniques for generating text and images, creating value for organizations by improving productivity. This is achieved by leveraging foundation models to help in composing emails, summarizing text, answering questions, building chatbots, and creating images. While the focus of this workshop is for you to gain hands-on experience implementing these patterns via Bedrock APIs and SDKs, you will also have the option of exploring integrations with open-source packages like &lt;a href=&quot;https://python.langchain.com/docs/get_started/introduction&quot;&gt;LangChain&lt;/a&gt; and &lt;a href=&quot;https://faiss.ai/index.html&quot;&gt;FAISS&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Labs include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;01 - Text Generation&lt;/strong&gt; [Estimated time to complete - 15 mins] &lt;a href=&quot;https://&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Test-pass-2ea44f&quot; alt=&quot;Test - pass&quot;&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Text, code generation with Bedrock&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;02 - Knowledge bases and RAG&lt;/strong&gt; [Estimated time to complete - 35 mins] &lt;a href=&quot;https://&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Test-pass-2ea44f&quot; alt=&quot;Test - pass&quot;&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Managed RAG retrieve and generate example&lt;/li&gt; 
   &lt;li&gt;Langchain RAG retrieve and generate example&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;03 - Model customization&lt;/strong&gt; [Estimated time to complete - 30 mins] &lt;a href=&quot;https://&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Test-pass-2ea44f&quot; alt=&quot;Test - pass&quot;&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fine tuning Titan lite, Llama2&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt; - &lt;em&gt;You must run this on your own AWS account, and this will not work on AWS Workshop Studio!&lt;/em&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;04 - Image and Multimodal&lt;/strong&gt; [Estimated time to complete - 30 mins] &lt;a href=&quot;https://&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Test-pass-2ea44f&quot; alt=&quot;Test - pass&quot;&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Bedrock Titan image generator&lt;/li&gt; 
   &lt;li&gt;Bedrock Stable Diffusion XL&lt;/li&gt; 
   &lt;li&gt;Bedrock Titan Multimodal embeddings&lt;/li&gt; 
   &lt;li&gt;Nova Reel and Canvas notebooks&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;05 - Agents&lt;/strong&gt; [Estimated time to complete - 30 mins] &lt;a href=&quot;https://&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Test-pass-2ea44f&quot; alt=&quot;Test - pass&quot;&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Customer service agent&lt;/li&gt; 
   &lt;li&gt;Insurance claims agent&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;06 - Open source examples (optional)&lt;/strong&gt; [Estimated time to complete - 30 mins] &lt;a href=&quot;https://&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Test-fail-red&quot; alt=&quot;Test - fail&quot;&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Langchain Text Generation examples&lt;/li&gt; 
   &lt;li&gt;Langchain KB RAG examples&lt;/li&gt; 
   &lt;li&gt;Langchain Chatbot examples&lt;/li&gt; 
   &lt;li&gt;NVIDIA NeMo Guardrails examples&lt;/li&gt; 
   &lt;li&gt;NodeJS Bedrock examples&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/main/imgs/11-overview.png&quot; alt=&quot;imgs/11-overview&quot; title=&quot;Overview of the different labs in the workshop&quot;&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;You can also refer to these &lt;a href=&quot;https://catalog.us-east-1.prod.workshops.aws/workshops/a4bdb007-5600-4368-81c5-ff5b4154f518/en-US&quot;&gt;Step-by-step guided instructions on the workshop website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;h3&gt;Choose a notebook environment&lt;/h3&gt; 
&lt;p&gt;This workshop is presented as a series of &lt;strong&gt;Python notebooks&lt;/strong&gt;, which you can run from the environment of your choice:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For a fully-managed environment with rich AI/ML features, we&#39;d recommend using &lt;a href=&quot;https://aws.amazon.com/sagemaker/studio/&quot;&gt;SageMaker Studio&lt;/a&gt;. To get started quickly, you can refer to the &lt;a href=&quot;https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-quick-start.html&quot;&gt;instructions for domain quick setup&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For a fully-managed but more basic experience, you could instead &lt;a href=&quot;https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html&quot;&gt;create a SageMaker Notebook Instance&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;If you prefer to use your existing (local or other) notebook environment, make sure it has &lt;a href=&quot;https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html&quot;&gt;credentials for calling AWS&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Enable AWS IAM permissions for Bedrock&lt;/h3&gt; 
&lt;p&gt;The AWS identity you assume from your notebook environment (which is the &lt;a href=&quot;https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html&quot;&gt;&lt;em&gt;Studio/notebook Execution Role&lt;/em&gt;&lt;/a&gt; from SageMaker, or could be a role or IAM User for self-managed notebooks), must have sufficient &lt;a href=&quot;https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html&quot;&gt;AWS IAM permissions&lt;/a&gt; to call the Amazon Bedrock service.&lt;/p&gt; 
&lt;p&gt;To grant Bedrock access to your identity, you can:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open the &lt;a href=&quot;https://us-east-1.console.aws.amazon.com/iam/home?#&quot;&gt;AWS IAM Console&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Find your &lt;a href=&quot;https://us-east-1.console.aws.amazon.com/iamv2/home?#/roles&quot;&gt;Role&lt;/a&gt; (if using SageMaker or otherwise assuming an IAM Role), or else &lt;a href=&quot;https://us-east-1.console.aws.amazon.com/iamv2/home?#/users&quot;&gt;User&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;em&gt;Add Permissions &amp;gt; Create Inline Policy&lt;/em&gt; to attach new inline permissions, open the &lt;em&gt;JSON&lt;/em&gt; editor and paste in the below example policy:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Sid&quot;: &quot;BedrockFullAccess&quot;,
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: [&quot;bedrock:*&quot;],
            &quot;Resource&quot;: &quot;*&quot;
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;⚠️ &lt;strong&gt;Note:&lt;/strong&gt; With Amazon SageMaker, your notebook execution role will typically be &lt;em&gt;separate&lt;/em&gt; from the user or role that you log in to the AWS Console with. If you&#39;d like to explore the AWS Console for Amazon Bedrock, you&#39;ll need to grant permissions to your Console user/role too. You can run the notebooks anywhere as long as you have access to the AWS Bedrock service and have appropriate credentials&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For more information on the fine-grained action and resource permissions in Bedrock, check out the Bedrock Developer Guide.&lt;/p&gt; 
&lt;h3&gt;Clone and use the notebooks&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ℹ️ &lt;strong&gt;Note:&lt;/strong&gt; In SageMaker Studio, you can open a &quot;System Terminal&quot; to run these commands by clicking &lt;em&gt;File &amp;gt; New &amp;gt; Terminal&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Once your notebook environment is set up, clone this workshop repository into it.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;sudo yum install -y unzip
git clone https://github.com/aws-samples/amazon-bedrock-workshop.git
cd amazon-bedrock-workshop
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href=&quot;http://hits.dwyl.com/aws-samples/amazon-bedrock-workshop&quot;&gt;&lt;img src=&quot;https://hits.dwyl.com/aws-samples/amazon-bedrock-workshop.svg?style=flat-square&amp;amp;show=unique&quot; alt=&quot;HitCount&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You&#39;re now ready to explore the lab notebooks! Start with &lt;a href=&quot;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/main/00_Prerequisites/bedrock_basics.ipynb&quot;&gt;00_Prerequisites/bedrock_basics.ipynb&lt;/a&gt; for details on how to install the Bedrock SDKs, create a client, and start calling the APIs from Python. Here is the directory structure at a high level:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Directory structure:
└── aws-samples-amazon-bedrock-workshop/
    ├── README.md
    ├── CODE_OF_CONDUCT.md
    ├── CONTRIBUTING.md
    ├── LICENSE
    ├── RELEASE_NOTES.md
    ├── 00_Prerequisites/
    │   ├── README.md
    │   ├── Getting_started_with_Converse_API.ipynb
    │   └── bedrock_basics.ipynb
    ├── 01_Text_generation/
    │   ├── README.md
    │   ├── 01_text_and_code_generation_w_bedrock.ipynb
    │   ├── emails/
    │   │   ├── 00_treasure_island.txt
    │   │   └── 01_return.txt
    │   └── images/
    │       └── nova/
    ├── 02_KnowledgeBases_and_RAG/
    │   ├── README.md
    │   ├── 0_create_ingest_documents_test_kb.ipynb
    │   ├── 1_managed-rag-kb-retrieve-generate-api.ipynb
    │   ├── 2_Langchain-rag-retrieve-api-mistral-and-claude-3-haiku.ipynb
    │   ├── 3_Langchain-rag-retrieve-api-claude-3.ipynb
    │   ├── 4_CLEAN_UP.ipynb
    │   ├── requirements.txt
    │   ├── utility.py
    │   └── images/
    ├── 03_Model_customization/
    │   ├── README.md
    │   ├── 00_setup.ipynb
    │   ├── 01_fine-tuning-titan-lite.ipynb
    │   ├── 02_fine-tuning_llama2.ipynb
    │   ├── 03_continued_pretraining_titan_text.ipynb
    │   └── 04_cleanup.ipynb
    ├── 04_Image_and_Multimodal/
    │   ├── README.md
    │   ├── bedrock-titan-multimodal-embeddings.ipynb
    │   ├── nova-canvas-notebook.ipynb
    │   ├── nova-reel-notebook.ipynb
    │   ├── AmazonNova/
    │   │   ├── NovaCanvas/
    │   │   │   ├── 00-NovaCanvas-prerequisites.ipynb
    │   │   │   ├── 01-text-to-image.ipynb
    │   │   │   ├── 02-image-inpainting.ipynb
    │   │   │   ├── 03-image-outpainting.ipynb
    │   │   │   ├── 04-background-removal.ipynb
    │   │   │   ├── 05-image-variation.ipynb
    │   │   │   ├── 06-image-conditioning.ipynb
    │   │   │   ├── 07-color-conditioning.ipynb
    │   │   │   ├── utils.py
    │   │   │   └── images/
    │   │   └── NovaReel/
    │   │       ├── 00-NovaReel-prerequisites.ipynb
    │   │       ├── 01-text-to-video.ipynb
    │   │       ├── 02-image-to-video.ipynb
    │   │       ├── video_gen_util.py
    │   │       └── images/
    │   └── images/
    │       └── octank_color_palette.JPG
    ├── 05_Agents/
    │   ├── README.md
    │   ├── 00_inline_agents.ipynb
    │   ├── 01_create_agent.ipynb
    │   ├── 02_associate_knowledge_base_to_agent.ipynb
    │   ├── 03_invoke_agent.ipynb
    │   ├── 04_clean_up_agent_resources.ipynb
    │   ├── agent.py
    │   ├── knowledge_base.py
    │   ├── requirements.txt
    │   ├── images/
    │   └── kb_documents/
    ├── 06_OpenSource_examples/
    │   ├── README.md
    │   ├── advance-langgraph-multi-agent-setup.ipynb
    │   ├── find-relevant-information-using-RAG.ipynb
    │   ├── intermediate-langgraph-agent-setup-w-tools.ipynb
    │   ├── ragas-agent-evaluation.ipynb
    │   ├── requirements.txt
    │   ├── simple-crewai-agent-setup.ipynb
    │   ├── simple-langgraph-agent-setup.ipynb
    │   ├── utils.py
    │   ├── data/
    │   │   ├── section_doc_store.pkl
    │   │   ├── section_vector_store.pkl
    │   │   ├── synthetic_travel_data.csv
    │   │   └── travel_guides/
    │   ├── images/
    │   └── text-generation-with-langchain/
    │       ├── 01_zero_shot_generation.ipynb
    │       ├── 02_code_interpret_w_langchain.ipynb
    │       ├── 03_code_translate_w_langchain.ipynb
    │       ├── 04_long text summarization using LCEL chains on Langchain.ipynb
    │       ├── images/
    │       └── letters/
    │           └── 2022-letter.txt
    ├── 07_Cross_Region_Inference/
    │   ├── README.md
    │   └── Getting_started_with_Cross-region_Inference.ipynb
    ├── imgs/
    └── .github/
        └── ISSUE_TEMPLATE/
            └── bug_report.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://star-history.com/#aws-samples/amazon-bedrock-workshop&amp;amp;Date&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=aws-samples/amazon-bedrock-workshop&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;👥 Contributors&lt;/h1&gt; 
&lt;p&gt;Thanks to our awesome contributors! 🚀🚀🚀&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/aws-samples/amazon-bedrock-workshop/graphs/contributors&quot;&gt;&lt;img src=&quot;https://contrib.rocks/image?repo=aws-samples/amazon-bedrock-workshop&amp;amp;max=2000&quot; alt=&quot;contributors&quot;&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NirDiamant/GenAI_Agents</title>
      <link>https://github.com/NirDiamant/GenAI_Agents</link>
      <description>&lt;p&gt;This repository provides tutorials and implementations for various Generative AI Agent techniques, from basic to advanced. It serves as a comprehensive guide for building intelligent, interactive AI systems.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;http://makeapullrequest.com&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.linkedin.com/in/nir-diamant-759323134/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/LinkedIn-Connect-blue&quot; alt=&quot;LinkedIn&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/NirDiamantAI&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/NirDiamantAI?label=Follow%20@NirDiamantAI&amp;amp;style=social&quot; alt=&quot;Twitter&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/cA6Aa4uyDX&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20our%20community-7289da?style=flat-square&amp;amp;logo=discord&amp;amp;logoColor=white&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;🌟 &lt;strong&gt;Support This Project:&lt;/strong&gt; Your sponsorship fuels innovation in GenAI agent development. &lt;strong&gt;&lt;a href=&quot;https://github.com/sponsors/NirDiamant&quot;&gt;Become a sponsor&lt;/a&gt;&lt;/strong&gt; to help maintain and expand this valuable resource!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;GenAI Agents: Comprehensive Repository for Development and Implementation 🚀&lt;/h1&gt; 
&lt;p&gt;Welcome to one of the most extensive and dynamic collections of Generative AI (GenAI) agent tutorials and implementations available today. This repository serves as a comprehensive resource for learning, building, and sharing GenAI agents, ranging from simple conversational bots to complex, multi-agent systems.&lt;/p&gt; 
&lt;h2&gt;📫 Stay Updated!&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;🚀&lt;br&gt;&lt;b&gt;Cutting-edge&lt;br&gt;Updates&lt;/b&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;💡&lt;br&gt;&lt;b&gt;Expert&lt;br&gt;Insights&lt;/b&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;🎯&lt;br&gt;&lt;b&gt;Top 0.1%&lt;br&gt;Content&lt;/b&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;p&gt;&lt;a href=&quot;https://diamantai.substack.com/?r=336pe4&amp;amp;utm_campaign=pub-share-checklist&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/NirDiamant/GenAI_Agents/main/images/subscribe-button.svg?sanitize=true&quot; alt=&quot;Subscribe to DiamantAI Newsletter&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Join over 15,000 of AI enthusiasts getting unique cutting-edge insights and free tutorials!&lt;/em&gt; &lt;em&gt;&lt;strong&gt;Plus, subscribers get exclusive early access and special 33% discounts to my book and the upcoming RAG Techniques course!&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href=&quot;https://diamantai.substack.com/?r=336pe4&amp;amp;utm_campaign=pub-share-checklist&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/NirDiamant/GenAI_Agents/main/images/substack_image.png&quot; alt=&quot;DiamantAI&#39;s newsletter&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Generative AI agents are at the forefront of artificial intelligence, revolutionizing the way we interact with and leverage AI technologies. This repository is designed to guide you through the development journey, from basic agent implementations to advanced, cutting-edge systems.&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td&gt; &lt;h3&gt;📚 Learn to Build Your First AI Agent&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://diamantai.substack.com/p/your-first-ai-agent-simpler-than&quot;&gt;Your First AI Agent: Simpler Than You Think&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This detailed blog post complements the repository by providing a complete A-Z walkthrough with in-depth explanations of core concepts, step-by-step implementation, and the theory behind AI agents. It&#39;s designed to be incredibly simple to follow while covering everything you need to know to build your first working agent from scratch.&lt;/p&gt; &lt;p&gt;&lt;em&gt;💡 Plus: Subscribe to the newsletter for exclusive early access to tutorials and special discounts on upcoming courses and books!&lt;/em&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;Our goal is to provide a valuable resource for everyone - from beginners taking their first steps in AI to seasoned practitioners pushing the boundaries of what&#39;s possible. By offering a range of examples from foundational to complex, we aim to facilitate learning, experimentation, and innovation in the rapidly evolving field of GenAI agents.&lt;/p&gt; 
&lt;p&gt;Furthermore, this repository serves as a platform for showcasing innovative agent creations. Whether you&#39;ve developed a novel agent architecture or found an innovative application for existing techniques, we encourage you to share your work with the community.&lt;/p&gt; 
&lt;h2&gt;Related Projects&lt;/h2&gt; 
&lt;p&gt;📚 Dive into my &lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_Techniques&quot;&gt;comprehensive guide on RAG techniques&lt;/a&gt;&lt;/strong&gt; to learn about integrating external knowledge into AI systems, enhancing their capabilities with up-to-date and relevant information retrieval.&lt;/p&gt; 
&lt;p&gt;🖋️ Explore my &lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/Prompt_Engineering&quot;&gt;Prompt Engineering Techniques guide&lt;/a&gt;&lt;/strong&gt; for an extensive collection of prompting strategies, from fundamental concepts to advanced methods, improving your ability to communicate effectively with AI language models.&lt;/p&gt; 
&lt;h2&gt;A Community-Driven Knowledge Hub&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;This repository grows stronger with your contributions!&lt;/strong&gt; Join our vibrant Discord community — the central hub for shaping and advancing this project together 🤝&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://discord.gg/cA6Aa4uyDX&quot;&gt;GenAI Agents Discord Community&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Whether you&#39;re a novice eager to learn or an expert ready to share your knowledge, your insights can shape the future of GenAI agents. Join us to propose ideas, get feedback, and collaborate on innovative implementations. For contribution guidelines, please refer to our &lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/strong&gt; file. Let&#39;s advance GenAI agent technology together!&lt;/p&gt; 
&lt;p&gt;🔗 For discussions on GenAI, agents, or to explore knowledge-sharing opportunities, feel free to &lt;strong&gt;&lt;a href=&quot;https://www.linkedin.com/in/nir-diamant-759323134/&quot;&gt;connect on LinkedIn&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;🎓 Learn to build GenAI agents from beginner to advanced levels&lt;/li&gt; 
 &lt;li&gt;🧠 Explore a wide range of agent architectures and applications&lt;/li&gt; 
 &lt;li&gt;📚 Step-by-step tutorials and comprehensive documentation&lt;/li&gt; 
 &lt;li&gt;🛠️ Practical, ready-to-use agent implementations&lt;/li&gt; 
 &lt;li&gt;🌟 Regular updates with the latest advancements in GenAI&lt;/li&gt; 
 &lt;li&gt;🤝 Share your own agent creations with the community&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;GenAI Agent Implementations&lt;/h2&gt; 
&lt;p&gt;Explore our extensive list of GenAI agent implementations, sorted by categories:&lt;/p&gt; 
&lt;h3&gt;🌱 Beginner-Friendly Agents&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Simple Conversational Agent&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/simple_conversational_agent.ipynb&quot;&gt;LangChain&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/simple_conversational_agent-pydanticai.ipynb&quot;&gt;PydanticAI&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A context-aware conversational AI maintains information across interactions, enabling more natural dialogues.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Integrates a language model, prompt template, and history manager to generate contextual responses and track conversation sessions.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/simple_question_answering_agent.ipynb&quot;&gt;Simple Question Answering Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Answering (QA) agent using LangChain and OpenAI&#39;s language model understands user queries and provides relevant, concise answers.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Combines OpenAI&#39;s GPT model, a prompt template, and an LLMChain to process user questions and generate AI-driven responses in a streamlined manner.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Simple Data Analysis Agent&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/simple_data_analysis_agent_notebook.ipynb&quot;&gt;LangChain&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/simple_data_analysis_agent_notebook-pydanticai.ipynb&quot;&gt;PydanticAI&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;An AI-powered data analysis agent interprets and answers questions about datasets using natural language, combining language models with data manipulation tools for intuitive data exploration.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Integrates a language model, data manipulation framework, and agent framework to process natural language queries and perform data analysis on a synthetic dataset, enabling accessible insights for non-technical users.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;🔧 Framework Tutorial: LangGraph&lt;/h3&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/langgraph-tutorial.ipynb&quot;&gt;Introduction to LangGraph: Building Modular AI Workflows&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;This tutorial introduces LangGraph, a powerful framework for creating modular, graph-based AI workflows. Learn how to leverage LangGraph to build more complex and flexible AI agents that can handle multi-step processes efficiently.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Step-by-step guide on using LangGraph to create a StateGraph workflow. The tutorial covers key concepts such as state management, node creation, and graph compilation. It demonstrates these principles by constructing a simple text analysis pipeline, serving as a foundation for more advanced agent architectures.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/your-first-ai-agent-simpler-than?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false&quot;&gt;Blog Post&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;🎓 Educational and Research Agents&lt;/h3&gt; 
&lt;ol start=&quot;5&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/Academic_Task_Learning_Agent_LangGraph.ipynb&quot;&gt;ATLAS: Academic Task and Learning Agent System&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;ATLAS demonstrates how to build an intelligent multi-agent system that transforms academic support through AI-powered assistance. The system leverages LangGraph&#39;s workflow framework to coordinate multiple specialized agents that provide personalized academic planning, note-taking, and advisory support.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Implements a state-managed multi-agent architecture using four specialized agents (Coordinator, Planner, Notewriter, and Advisor) working in concert through LangGraph&#39;s workflow framework. The system features sophisticated workflows for profile analysis and academic support, with continuous adaptation based on student performance and feedback.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=yxowMLL2dDI&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/atlas-when-artificial-intelligence?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false&quot;&gt;Blog Post&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/scientific_paper_agent_langgraph.ipynb&quot;&gt;Scientific Paper Agent - Literature Review&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;An intelligent research assistant that helps users navigate, understand, and analyze scientific literature through an orchestrated workflow. The system combines academic APIs with sophisticated paper processing techniques to automate literature review tasks, enabling researchers to efficiently extract insights from academic papers while maintaining research rigor and quality control.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Leverages LangGraph to create a five-node workflow system including decision making, planning, tool execution, and quality validation nodes. The system integrates the CORE API for paper access, PDFplumber for document processing, and advanced language models for analysis. Key features include a retry mechanism for robust paper downloads, structured data handling through Pydantic models, and quality-focused improvement cycles with human-in-the-loop validation options.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://youtu.be/Bc4YtpHY6Ws&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/nexus-ai-the-revolutionary-research?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false&quot;&gt;Blog Post&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/chiron_learning_agent_langgraph.ipynb&quot;&gt;Chiron - A Feynman-Enhanced Learning Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;An adaptive learning agent that guides users through educational content using a structured checkpoint system and Feynman-style teaching. The system processes learning materials (either user-provided or web-retrieved), verifies understanding through interactive checkpoints, and provides simplified explanations when needed, creating a personalized learning experience that mimics one-on-one tutoring.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Uses LangGraph to orchestrate a learning workflow that includes checkpoint definition, context building, understanding verification, and Feynman teaching nodes. The system integrates web search for dynamic content retrieval, employs semantic chunking for context processing, and manages embeddings for relevant information retrieval. Key features include a 70% understanding threshold for progression, interactive human-in-the-loop validation, and structured output through Pydantic models for consistent data handling.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=qsdiTGkB8mk&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;💼 Business and Professional Agents&lt;/h3&gt; 
&lt;ol start=&quot;8&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/customer_support_agent_langgraph.ipynb&quot;&gt;Customer Support Agent (LangGraph)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;An intelligent customer support agent using LangGraph categorizes queries, analyzes sentiment, and provides appropriate responses or escalates issues.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Utilizes LangGraph to create a workflow combining state management, query categorization, sentiment analysis, and response generation.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/essay_grading_system_langgraph.ipynb&quot;&gt;Essay Grading Agent (LangGraph)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;An automated essay grading system using LangGraph and an LLM model evaluates essays based on relevance, grammar, structure, and depth of analysis.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Utilizes a state graph to define the grading workflow, incorporating separate grading functions for each criterion.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/simple_travel_planner_langgraph.ipynb&quot;&gt;Travel Planning Agent (LangGraph)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A Travel Planner using LangGraph demonstrates how to build a stateful, multi-step conversational AI application that collects user input and generates personalized travel itineraries.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Utilizes StateGraph to define the application flow, incorporates custom PlannerState for process management.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/agent_hackathon_genAI_career_assistant.ipynb&quot;&gt;GenAI Career Assistant Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;The GenAI Career Assistant demonstrates how to create a multi-agent system that provides personalized guidance for careers in Generative AI. Using LangGraph and Gemini LLM, the system delivers customized learning paths, resume assistance, interview preparation, and job search support.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Leverages a multi-agent architecture using LangGraph to coordinate specialized agents (Learning, Resume, Interview, Job Search) through TypedDict-based state management. The system employs sophisticated query categorization and routing while integrating with external tools like DuckDuckGo for job searches and dynamic content generation.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=IcKh0ltXO_8&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/project_manager_assistant_agent.ipynb&quot;&gt;Project Manager Assistant Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;An AI agent designed to assist in project management tasks by automating the process of creating actionable tasks from project descriptions, identifying dependencies, scheduling work, and assigning tasks to team members based on expertise. The system includes risk assessment and self-reflection capabilities to optimize project plans through multiple iterations, aiming to minimize overall project risk.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Leverages LangGraph to orchestrate a workflow of specialized nodes including task generation, dependency mapping, scheduling, allocation, and risk assessment. Each node uses GPT-4o-mini for structured outputs following Pydantic models. The system implements a feedback loop for self-improvement, where risk scores trigger reflection cycles that generate insights to optimize the project plan. Visualization tools display Gantt charts of the generated schedules across iterations.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=R7YWjzg3LpI&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/ClauseAI.ipynb&quot;&gt;Contract Analysis Assistant (ClauseAI)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;ClauseAI demonstrates how to build an AI-powered contract analysis system using a multi-agent approach. The system employs specialized AI agents for different aspects of contract review, from clause analysis to compliance checking, and leverages LangGraph for workflow orchestration and Pinecone for efficient clause retrieval and comparison.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Implements a sophisticated state-based workflow using LangGraph to coordinate multiple AI agents through contract analysis stages. The system features Pydantic models for data validation, vector storage with Pinecone for clause comparison, and LLM-based analysis for generating comprehensive contract reports. The implementation includes parallel processing capabilities and customizable report generation based on user requirements.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=rP8uv_tXuSI&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/e2e_testing_agent.ipynb&quot;&gt;E2E Testing Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;The E2E Testing Agent demonstrates how to build an AI-powered system that converts natural language test instructions into executable end-to-end web tests. Using LangGraph for workflow orchestration and Playwright for browser automation, the system enables users to specify test cases in plain English while handling the complexity of test generation and execution.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Implements a structured workflow using LangGraph to coordinate test generation, validation, and execution. The system features TypedDict state management, integration with Playwright for browser automation, and LLM-based code generation for converting natural language instructions into executable test scripts. The implementation includes DOM state analysis, error handling, and comprehensive test reporting.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jPXtpzcCtyA&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;🎨 Creative and Content Generation Agents&lt;/h3&gt; 
&lt;ol start=&quot;15&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/gif_animation_generator_langgraph.ipynb&quot;&gt;GIF Animation Generator Agent (LangGraph)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A GIF animation generator that integrates LangGraph for workflow management, GPT-4 for text generation, and DALL-E for image creation, producing custom animations from user prompts.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Utilizes LangGraph to orchestrate a workflow that generates character descriptions, plots, and image prompts using GPT-4, creates images with DALL-E 3, and assembles them into GIFs using PIL. Employs asynchronous programming for efficient parallel processing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/tts_poem_generator_agent_langgraph.ipynb&quot;&gt;TTS Poem Generator Agent (LangGraph)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;An advanced text-to-speech (TTS) agent using LangGraph and OpenAI&#39;s APIs classifies input text, processes it based on content type, and generates corresponding speech output.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Utilizes LangGraph to orchestrate a workflow that classifies input text using GPT models, applies content-specific processing, and converts the processed text to speech using OpenAI&#39;s TTS API. The system adapts its output based on the identified content type (general, poem, news, or joke).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/music_compositor_agent_langgraph.ipynb&quot;&gt;Music Compositor Agent (LangGraph)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;An AI Music Compositor using LangGraph and OpenAI&#39;s language models generates custom musical compositions based on user input. The system processes the input through specialized components, each contributing to the final musical piece, which is then converted to a playable MIDI file.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;LangGraph orchestrates a workflow that transforms user input into a musical composition, using ChatOpenAI (GPT-4) to generate melody, harmony, and rhythm, which are then style-adapted. The final AI-generated composition is converted to a MIDI file using music21 and can be played back using pygame.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/ContentIntelligence.ipynb&quot;&gt;Content Intelligence: Multi-Platform Content Generation Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Content Intelligence demonstrates how to build an advanced content generation system that transforms input text into platform-optimized content across multiple social media channels. The system employs LangGraph for workflow orchestration to analyze content, conduct research, and generate tailored content while maintaining brand consistency across different platforms.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Implements a sophisticated workflow using LangGraph to coordinate multiple specialized nodes (Summary, Research, Platform-Specific) through the content generation process. The system features TypedDict and Pydantic models for state management, integration with Tavily Search for research enhancement, and platform-specific content generation using GPT-4. The implementation includes parallel processing for multiple platforms and customizable content templates.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=DPMtPbKmWnU&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/business_meme_generator.ipynb&quot;&gt;Business Meme Generator Using LangGraph and Memegen.link&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;The Business Meme Generator demonstrates how to create an AI-powered system that generates contextually relevant memes based on company website analysis. Using LangGraph for workflow orchestration, the system combines Groq&#39;s Llama model for text analysis and the Memegen.link API to automatically produce brand-aligned memes for digital marketing.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Implements a state-managed workflow using LangGraph to coordinate website content analysis, meme concept generation, and image creation. The system features Pydantic models for data validation, asynchronous processing with aiohttp, and integration with external APIs (Groq, Memegen.link) to create a complete meme generation pipeline with customizable templates.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://youtu.be/lsdDaGmkSCw?si=oF3CGfhbRqz1_Vm8&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/murder_mystery_agent_langgraph.ipynb&quot;&gt;Murder Mystery Game with LLM Agents&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A text-based detective game that utilizes autonomous LLM agents as interactive characters in a procedurally generated murder mystery. Drawing inspiration from the UNBOUNDED paper, the system creates unique scenarios each time, with players taking on the role of Sherlock Holmes to solve the case through character interviews and deductive reasoning.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Leverages two LangGraph workflows - a main game loop for story/character generation and game progression, and a conversation sub-graph for character interactions. The system uses a combination of LLM-powered narrative generation, character AI, and structured game mechanics to create an immersive investigative experience with replayable storylines.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=_3cJYlk2EmA&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;📊 Analysis and Information Processing Agents&lt;/h3&gt; 
&lt;ol start=&quot;21&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/memory_enhanced_conversational_agent.ipynb&quot;&gt;Memory-Enhanced Conversational Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A memory-enhanced conversational AI agent incorporates short-term and long-term memory systems to maintain context within conversations and across multiple sessions, improving interaction quality and personalization.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Integrates a language model with separate short-term and long-term memory stores, utilizes a prompt template incorporating both memory types, and employs a memory manager for storage and retrieval. The system includes an interaction loop that updates and utilizes memories for each response.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/multi_agent_collaboration_system.ipynb&quot;&gt;Multi-Agent Collaboration System&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A multi-agent collaboration system combining historical research with data analysis, leveraging large language models to simulate specialized agents working together to answer complex historical questions.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Utilizes a base Agent class to create specialized HistoryResearchAgent and DataAnalysisAgent, orchestrated by a HistoryDataCollaborationSystem. The system follows a five-step process: historical context provision, data needs identification, historical data provision, data analysis, and final synthesis.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/self_improving_agent.ipynb&quot;&gt;Self-Improving Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A Self-Improving Agent using LangChain engages in conversations, learns from interactions, and continuously improves its performance over time through reflection and adaptation.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Integrates a language model with chat history management, response generation, and a reflection mechanism. The system employs a learning system that incorporates insights from reflection to enhance future performance, creating a continuous improvement loop.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/task_oriented_agent.ipynb&quot;&gt;Task-Oriented Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A language model application using LangChain that summarizes text and translates the summary to Spanish, combining custom functions, structured tools, and an agent for efficient text processing.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Utilizes custom functions for summarization and translation, wrapped as structured tools. Employs a prompt template to guide the agent, which orchestrates the use of tools. An agent executor manages the process, taking input text and producing both an English summary and its Spanish translation.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/search_the_internet_and_summarize.ipynb&quot;&gt;Internet Search and Summarize Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;An intelligent web research assistant that combines web search capabilities with AI-powered summarization, automating the process of gathering information from the internet and distilling it into concise, relevant summaries.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Integrates a web search module using DuckDuckGo&#39;s API, a result parser, and a text summarization engine leveraging OpenAI&#39;s language models. The system performs site-specific or general searches, extracts relevant content, generates concise summaries, and compiles attributed results for efficient information retrieval and synthesis.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/research_team_autogen.ipynb&quot;&gt;Multi agent research team - Autogen&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;This technique explores a multi-agent system for collaborative research using the AutoGen library. It employs agents to solve tasks collaboratively, focusing on efficient execution and quality assurance. The system enhances research by distributing tasks among specialized agents.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Agents are configured with specific roles using the GPT-4 model, including admin, developer, planner, executor, and quality assurance. Interaction management ensures orderly communication with defined transitions. Task execution involves collaborative planning, coding, execution, and quality checking, demonstrating a scalable framework for various domains.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/yanivvak/dream-team&quot;&gt;comprehensive solution with UI&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/build-your-dream-team-with-autogen/ba-p/4157961&quot;&gt;Blogpost&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/sales_call_analyzer_agent.ipynb&quot;&gt;Sales Call Analyzer&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;An intelligent system that automates the analysis of sales call recordings by combining audio transcription with advanced natural language processing. The analyzer transcribes audio using OpenAI&#39;s Whisper, processes the text using NLP techniques, and generates comprehensive reports including sentiment analysis, key phrases, pain points, and actionable recommendations to improve sales performance.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Utilizes multiple components in a structured workflow: OpenAI Whisper for audio transcription, CrewAI for task automation and agent management, and LangChain for orchestrating the analysis pipeline. The system processes audio through a series of steps from transcription to detailed analysis, leveraging custom agents and tasks to generate structured JSON reports containing insights about customer sentiment, sales opportunities, and recommended improvements.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=SKAt_PvznDw&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/Weather_Disaster_Management_AI_AGENT.ipynb&quot;&gt;Weather Emergency &amp;amp; Response System&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A comprehensive system demonstrating two agent graph implementations for weather emergency response: a real-time graph processing live weather data, and a hybrid graph combining real and simulated data for testing high-severity scenarios. The system handles complete workflow from data gathering through emergency plan generation, with automated notifications and human verification steps.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Utilizes LangGraph for orchestrating complex workflows with state management, integrating OpenWeatherMap API for real-time data, and Gemini for analysis and response generation. The system incorporates email notifications, social media monitoring simulation, and severity-based routing with configurable human verification for low/medium severity events.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=AgiOAJl_apw&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/self_healing_code.ipynb&quot;&gt;Self-Healing Codebase System&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;An intelligent system that automatically detects, diagnoses, and fixes runtime code errors using LangGraph workflow orchestration and ChromaDB vector storage. The system maintains a memory of encountered bugs and their fixes through vector embeddings, enabling pattern recognition for similar errors across the codebase.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Utilizes a state-based graph workflow that processes function definitions and runtime arguments through specialized nodes for error detection, code analysis, and fix generation. Incorporates ChromaDB for vector-based storage of bug patterns and fixes, with automated search and retrieval capabilities for similar error patterns, while maintaining code execution safety through structured validation steps.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ga7ShvIXOvE&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/database_discovery_fleet.ipynb&quot;&gt;DataScribe: AI-Powered Schema Explorer&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;An intelligent agent system that enables intuitive exploration and querying of relational databases through natural language interactions. The system utilizes a fleet of specialized agents, coordinated by a stateful Supervisor, to handle schema discovery, query planning, and data analysis tasks while maintaining contextual understanding through vector-based relationship graphs.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Leverages LangGraph for orchestrating a multi-agent workflow including discovery, inference, and planning agents, with NetworkX for relationship graph visualization and management. The system incorporates dynamic state management through TypedDict classes, maintains database context between sessions using a db_graph attribute, and includes safety measures to prevent unauthorized database modifications.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/memory-agent-tutorial.ipynb&quot;&gt;Memory-Enhanced Email Agent (LangGraph &amp;amp; LangMem)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;An intelligent email assistant that combines three types of memory (semantic, episodic, and procedural) to create a system that improves over time. The agent can triage incoming emails, draft contextually appropriate responses using stored knowledge, and enhance its performance based on user feedback.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Leverages LangGraph for workflow orchestration and LangMem for sophisticated memory management across multiple memory types. The system implements a triage workflow with memory-enhanced decision making, specialized tools for email composition and calendar management, and a self-improvement mechanism that updates its own prompts based on feedback and past performance.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;**&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/building-an-ai-agent-with-memory?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false&quot;&gt;Blog Post&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;📰 News and Information Agents&lt;/h3&gt; 
&lt;ol start=&quot;32&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/news_tldr_langgraph.ipynb&quot;&gt;News TL;DR using LangGraph&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A news summarization system that generates concise TL;DR summaries of current events based on user queries. The system leverages large language models for decision making and summarization while integrating with news APIs to access up-to-date content, allowing users to quickly catch up on topics of interest through generated bullet-point summaries.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Utilizes LangGraph to orchestrate a workflow combining multiple components: GPT-4o-mini for generating search terms and article summaries, NewsAPI for retrieving article metadata, BeautifulSoup for web scraping article content, and Asyncio for concurrent processing. The system follows a structured pipeline from query processing through article selection and summarization, managing the flow between components to produce relevant TL;DRs of current news articles.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=0fRxW6miybI&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/stop-reading-start-understanding?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false&quot;&gt;Blog Post&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/ainsight_langgraph.ipynb&quot;&gt;AInsight: AI/ML Weekly News Reporter&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;AInsight demonstrates how to build an intelligent news aggregation and summarization system using a multi-agent architecture. The system employs three specialized agents (NewsSearcher, Summarizer, Publisher) to automatically collect, process and summarize AI/ML news for general audiences through LangGraph-based workflow orchestration.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Implements a state-managed multi-agent system using LangGraph to coordinate the news collection (Tavily API), technical content summarization (GPT-4), and report generation processes. The system features modular architecture with TypedDict-based state management, external API integration, and markdown report generation with customizable templates.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=kH5S1is2D_0&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/journalism_focused_ai_assistant_langgraph.ipynb&quot;&gt;Journalism-Focused AI Assistant&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A specialized AI assistant that helps journalists tackle modern journalistic challenges like misinformation, bias, and information overload. The system integrates fact-checking, tone analysis, summarization, and grammar review tools to enhance the accuracy and efficiency of journalistic work while maintaining ethical reporting standards.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Leverages LangGraph to orchestrate a workflow of specialized components including language models for analysis and generation, web search integration via DuckDuckGo&#39;s API, document parsing tools like PyMuPDFLoader and WebBaseLoader, text splitting with RecursiveCharacterTextSplitter, and structured JSON outputs. Each component works together through a unified workflow to analyze content, verify facts, detect bias, extract quotes, and generate comprehensive reports.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/blog_writer_swarm.ipynb&quot;&gt;Blog Writer (Open AI Swarm)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A multi-agent system for collaborative blog post creation using OpenAI&#39;s Swarm package. It leverages specialized agents to perform research, planning, writing, and editing tasks efficiently.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Utilizes OpenAI&#39;s Swarm Package to manage agent interactions. Includes an admin, researcher, planner, writer, and editor, each with specific roles. The system follows a structured workflow: topic setting, outlining, research, drafting, and editing. This approach enhances content creation through task distribution, specialization, and collaborative problem-solving.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/openai/swarm&quot;&gt;Swarm Repo&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/generate_podcast_agent_langgraph.ipynb&quot;&gt;Podcast Internet Search and Generate Agent 🎙️&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A two step agent that first searches the internet for a given topic and then generates a podcast on the topic found. The search step uses a search agent and search function to find the most relevant information. The second step uses a podcast generation agent and generation function to create a podcast on the topic found.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Utilizes LangGraph to orchestrate a two-step workflow. The first step involves a search agent and function to gather information from the internet. The second step uses a podcast generation agent and function to create a podcast based on the gathered information.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;🛍️ Shopping and Product Analysis Agents&lt;/h3&gt; 
&lt;ol start=&quot;37&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/ShopGenie.ipynb&quot;&gt;ShopGenie - Redefining Online Shopping Customer Experience&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;An AI-powered shopping assistant that helps customers make informed purchasing decisions even without domain expertise. The system analyzes product information from multiple sources, compares specifications and reviews, identifies the best option based on user needs, and delivers recommendations through email with supporting video reviews, creating a comprehensive shopping experience.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Uses LangGraph to orchestrate a workflow combining Tavily for web search, Llama-3.1-70B for structured data analysis and product comparison, and YouTube API for review video retrieval. The system processes search results through multiple nodes including schema mapping, product comparison, review identification, and email generation. Key features include structured Pydantic models for consistent data handling, retry mechanisms for robust API interactions, and email delivery through SMTP for sharing recommendations.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Js0sK0u53dQ&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/car_buyer_agent_langgraph.ipynb&quot;&gt;Car Buyer AI Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;The Smart Product Buyer AI Agent demonstrates how to build an intelligent system that assists users in making informed purchasing decisions. Using LangGraph and LLM-based intelligence, the system processes user requirements, scrapes product listings from websites like AutoTrader, and provides detailed analysis and recommendations for car purchases.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Implements a state-based workflow using LangGraph to coordinate user interaction, web scraping, and decision support. The system features TypedDict state management, async web scraping with Playwright, and integrates with external APIs for comprehensive product analysis. The implementation includes a Gradio interface for real-time chat interaction and modular scraper architecture for easy extension to additional product categories.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=I61I1fp0qys&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;🎯 Task Management and Productivity Agents&lt;/h3&gt; 
&lt;ol start=&quot;39&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/taskifier.ipynb&quot;&gt;Taskifier - Intelligent Task Allocation &amp;amp; Management&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;An intelligent task management system that analyzes user work styles and creates personalized task breakdown strategies, born from the observation that procrastination often stems from task ambiguity among students and early-career professionals. The system evaluates historical work patterns, gathers relevant task information through web search, and generates customized step-by-step approaches to optimize productivity and reduce workflow paralysis.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Leverages LangGraph for orchestrating a multi-step workflow including work style analysis, information gathering via Tavily API, and customized plan generation. The system maintains state through the process, integrating historical work pattern data with fresh task research to output detailed, personalized task execution plans aligned with the user&#39;s natural working style.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=1W_p_RVi9KE&amp;amp;t=25s&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/grocery_management_agents_system.ipynb&quot;&gt;Grocery Management Agents System&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A multi-agent system built with CrewAI that automates grocery management tasks including receipt interpretation, expiration date tracking, inventory management, and recipe recommendations. The system uses specialized agents to extract data from receipts, estimate product shelf life, track consumption, and suggest recipes to minimize food waste.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Implements four specialized agents using CrewAI - a Receipt Interpreter that extracts item details from receipts, an Expiration Date Estimator that determines shelf life using online sources, a Grocery Tracker that maintains inventory based on consumption, and a Recipe Recommender that suggests meals using available ingredients. Each agent has specific tools and tasks orchestrated through a crew workflow.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=FlMu5pKSaHI&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;🔍 Quality Assurance and Testing Agents&lt;/h3&gt; 
&lt;ol start=&quot;41&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/graph_inspector_system_langgraph.ipynb&quot;&gt;LangGraph-Based Systems Inspector&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A comprehensive testing and validation tool for LangGraph-based applications that automatically analyzes system architecture, generates test cases, and identifies potential vulnerabilities through multi-agent inspection. The inspector employs specialized AI testers to evaluate different aspects of the system, from basic functionality to security concerns and edge cases.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Integrates LangGraph for workflow orchestration, multiple LLM-powered testing agents, and a structured evaluation pipeline that includes static analysis, test case generation, and results verification. The system uses Pydantic for data validation, NetworkX for graph representation, and implements a modular architecture that allows for parallel test execution and comprehensive result analysis.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=fQd6lXc-Y9A&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/langgraph-systems-inspector-an-ai?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false&quot;&gt;Blog Post&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/EU_Green_Compliance_FAQ_Bot.ipynb&quot;&gt;EU Green Deal FAQ Bot&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;The EU Green Deal FAQ Bot demonstrates how to build a RAG-based AI agent that helps businesses understand EU green deal policies. The system processes complex regulatory documents into manageable chunks and provides instant, accurate answers to common questions about environmental compliance, emissions reporting, and waste management requirements.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Implements a sophisticated RAG pipeline using FAISS vectorstore for document storage, semantic chunking for preprocessing, and multiple specialized agents (Retriever, Summarizer, Evaluator) for query processing. The system features query rephrasing for improved accuracy, cross-reference with gold Q&amp;amp;A datasets for answer validation, and comprehensive evaluation metrics to ensure response quality and relevance.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Av0kBQjwU-Y&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/systematic_review_of_scientific_articles.ipynb&quot;&gt;Systematic Review Automation System + Paper Draft Creation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A comprehensive system for automating academic systematic reviews using a directed graph architecture and LangChain components. The system generates complete, publication-ready systematic review papers, automatically processing everything from literature search through final draft generation with multiple revision cycles.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Utilizes a state-based graph workflow that handles paper search and selection (up to 3 papers), PDF processing, and generates a complete academic paper with all standard sections (abstract, introduction, methods, results, conclusions, references). The system incorporates multiple revision cycles with automated critique and improvement phases, all orchestrated through LangGraph state management.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=qi35mGGkCtg&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;🌟 Special Advanced Technique 🌟&lt;/h3&gt; 
&lt;ol start=&quot;44&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/Controllable-RAG-Agent&quot;&gt;Sophisticated Controllable Agent for Complex RAG Tasks 🤖&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;An advanced RAG solution designed to tackle complex questions that simple semantic similarity-based retrieval cannot solve. This approach uses a sophisticated deterministic graph as the &quot;brain&quot; 🧠 of a highly controllable autonomous agent, capable of answering non-trivial questions from your own data.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;• Implement a multi-step process involving question anonymization, high-level planning, task breakdown, adaptive information retrieval and question answering, continuous re-planning, and rigorous answer verification to ensure grounded and accurate responses.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;To begin exploring and building GenAI agents:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone this repository: &lt;pre&gt;&lt;code&gt;git clone https://github.com/NirDiamant/GenAI_Agents.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Navigate to the technique you&#39;re interested in: &lt;pre&gt;&lt;code&gt;cd all_agents_tutorials/technique-name
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Follow the detailed implementation guide in each technique&#39;s notebook.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! If you have a new technique or improvement to suggest:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create your feature branch: &lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Commit your changes: &lt;code&gt;git commit -m &#39;Add some AmazingFeature&#39;&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Push to the branch: &lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Open a pull request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/graphs/contributors&quot;&gt;&lt;img src=&quot;https://contrib.rocks/image?repo=NirDiamant/GenAI_Agents&quot; alt=&quot;Contributors&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under a custom non-commercial license - see the &lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/GenAI_Agents/main/LICENSE&quot;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;⭐️ If you find this repository helpful, please consider giving it a star!&lt;/p&gt; 
&lt;p&gt;Keywords: GenAI, Generative AI, Agents, NLP, AI, Machine Learning, Natural Language Processing, LLM, Conversational AI, Task-Oriented AI&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ed-donner/llm_engineering</title>
      <link>https://github.com/ed-donner/llm_engineering</link>
      <description>&lt;p&gt;Repo to accompany my mastering LLM engineering course&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLM Engineering - Master AI and LLMs&lt;/h1&gt; 
&lt;h2&gt;Your 8 week journey to proficiency starts today&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/ed-donner/llm_engineering/main/voyage.jpg&quot; alt=&quot;Voyage&quot;&gt;&lt;/p&gt; 
&lt;p&gt;I&#39;m so happy you&#39;re joining me on this path. We&#39;ll be building immensely satisfying projects in the coming weeks. Some will be easy, some will be challenging, many will ASTOUND you! The projects build on each other so you develop deeper and deeper expertise each week. One thing&#39;s for sure: you&#39;re going to have a lot of fun along the way.&lt;/p&gt; 
&lt;h3&gt;Before you begin&lt;/h3&gt; 
&lt;p&gt;I&#39;m here to help you be most successful with your learning! If you hit any snafus, or if you have any ideas on how I can improve the course, please do reach out in the platform or by emailing me direct (&lt;a href=&quot;mailto:ed@edwarddonner.com&quot;&gt;ed@edwarddonner.com&lt;/a&gt;). It&#39;s always great to connect with people on LinkedIn to build up the community - you&#39;ll find me here:&lt;br&gt; &lt;a href=&quot;https://www.linkedin.com/in/eddonner/&quot;&gt;https://www.linkedin.com/in/eddonner/&lt;/a&gt;&lt;br&gt; And this is new to me, but I&#39;m also trying out X/Twitter at &lt;a href=&quot;https://x.com/edwarddonner&quot;&gt;@edwarddonner&lt;/a&gt; - if you&#39;re on X, please show me how it&#39;s done 😂&lt;/p&gt; 
&lt;p&gt;Resources to accompany the course, including the slides and useful links, are here:&lt;br&gt; &lt;a href=&quot;https://edwarddonner.com/2024/11/13/llm-engineering-resources/&quot;&gt;https://edwarddonner.com/2024/11/13/llm-engineering-resources/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Instant Gratification instructions for Week 1, Day 1&lt;/h2&gt; 
&lt;h3&gt;Important note: see my warning about Llama3.3 below - it&#39;s too large for home computers! Stick with llama3.2! Several students have missed this warning...&lt;/h3&gt; 
&lt;p&gt;We will start the course by installing Ollama so you can see results immediately!&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download and install Ollama from &lt;a href=&quot;https://ollama.com&quot;&gt;https://ollama.com&lt;/a&gt; noting that on a PC you might need to have administrator permissions for the install to work properly&lt;/li&gt; 
 &lt;li&gt;On a PC, start a Command prompt / Powershell (Press Win + R, type &lt;code&gt;cmd&lt;/code&gt;, and press Enter). On a Mac, start a Terminal (Applications &amp;gt; Utilities &amp;gt; Terminal).&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;ollama run llama3.2&lt;/code&gt; or for smaller machines try &lt;code&gt;ollama run llama3.2:1b&lt;/code&gt; - &lt;strong&gt;please note&lt;/strong&gt; steer clear of Meta&#39;s latest model llama3.3 because at 70B parameters that&#39;s way too large for most home computers!&lt;/li&gt; 
 &lt;li&gt;If this doesn&#39;t work: you may need to run &lt;code&gt;ollama serve&lt;/code&gt; in another Powershell (Windows) or Terminal (Mac), and try step 3 again. On a PC, you may need to be running in an Admin instance of Powershell.&lt;/li&gt; 
 &lt;li&gt;And if that doesn&#39;t work on your box, I&#39;ve set up this on the cloud. This is on Google Colab, which will need you to have a Google account to sign in, but is free: &lt;a href=&quot;https://colab.research.google.com/drive/1-_f5XZPsChvfU1sJ0QqCePtIuc55LSdu?usp=sharing&quot;&gt;https://colab.research.google.com/drive/1-_f5XZPsChvfU1sJ0QqCePtIuc55LSdu?usp=sharing&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Any problems, please contact me!&lt;/p&gt; 
&lt;h2&gt;Then, Setup instructions&lt;/h2&gt; 
&lt;p&gt;After we do the Ollama quick project, and after I introduce myself and the course, we get to work with the full environment setup.&lt;/p&gt; 
&lt;p&gt;Hopefully I&#39;ve done a decent job of making these guides bulletproof - but please contact me right away if you hit roadblocks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PC people please follow the instructions in &lt;a href=&quot;https://raw.githubusercontent.com/ed-donner/llm_engineering/main/SETUP-PC.md&quot;&gt;SETUP-PC.md&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Mac people please follow the instructions in &lt;a href=&quot;https://raw.githubusercontent.com/ed-donner/llm_engineering/main/SETUP-mac.md&quot;&gt;SETUP-mac.md&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Linux people please follow the instructions in &lt;a href=&quot;https://raw.githubusercontent.com/ed-donner/llm_engineering/main/SETUP-linux.md&quot;&gt;SETUP-linux.md&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The are also PDF versions of the setup instructions in this folder if you&#39;d prefer.&lt;/p&gt; 
&lt;h3&gt;An important point on API costs (which are optional! No need to spend if you don&#39;t wish)&lt;/h3&gt; 
&lt;p&gt;During the course, I&#39;ll suggest you try out the leading models at the forefront of progress, known as the Frontier models. I&#39;ll also suggest you run open-source models using Google Colab. These services have some charges, but I&#39;ll keep cost minimal - like, a few cents at a time. And I&#39;ll provide alternatives if you&#39;d prefer not to use them.&lt;/p&gt; 
&lt;p&gt;Please do monitor your API usage to ensure you&#39;re comfortable with spend; I&#39;ve included links below. There&#39;s no need to spend anything more than a couple of dollars for the entire course. Some AI providers such as OpenAI require a minimum credit like $5 or local equivalent; we should only spend a fraction of it, and you&#39;ll have plenty of opportunity to put it to good use in your own projects. During Week 7 you have an option to spend a bit more if you&#39;re enjoying the process - I spend about $10 myself and the results make me very happy indeed! But it&#39;s not necessary in the least; the important part is that you focus on learning.&lt;/p&gt; 
&lt;h3&gt;Free alternative to Paid APIs&lt;/h3&gt; 
&lt;p&gt;Early in the course, I show you an alternative if you&#39;d rather not spend anything on APIs:&lt;br&gt; Any time that we have code like:&lt;br&gt; &lt;code&gt;openai = OpenAI()&lt;/code&gt;&lt;br&gt; You can use this as a direct replacement:&lt;br&gt; &lt;code&gt;openai = OpenAI(base_url=&#39;http://localhost:11434/v1&#39;, api_key=&#39;ollama&#39;)&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Below is a full example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# You need to do this one time on your computer
!ollama pull llama3.2

from openai import OpenAI
MODEL = &quot;llama3.2&quot;
openai = OpenAI(base_url=&quot;http://localhost:11434/v1&quot;, api_key=&quot;ollama&quot;)

response = openai.chat.completions.create(
 model=MODEL,
 messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is 2 + 2?&quot;}]
)

print(response.choices[0].message.content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How this Repo is organized&lt;/h3&gt; 
&lt;p&gt;There are folders for each of the &quot;weeks&quot;, representing modules of the class, culminating in a powerful autonomous Agentic AI solution in Week 8 that draws on many of the prior weeks.&lt;br&gt; Follow the setup instructions above, then open the Week 1 folder and prepare for joy.&lt;/p&gt; 
&lt;h3&gt;The most important part&lt;/h3&gt; 
&lt;p&gt;The mantra of the course is: the best way to learn is by &lt;strong&gt;DOING&lt;/strong&gt;. I don&#39;t type all the code during the course; I execute it for you to see the results. You should work along with me or after each lecture, running each cell, inspecting the objects to get a detailed understanding of what&#39;s happening. Then tweak the code and make it your own. There are juicy challenges for you throughout the course. I&#39;d love it if you wanted to submit a Pull Request for your code (instructions &lt;a href=&quot;https://chatgpt.com/share/677a9cb5-c64c-8012-99e0-e06e88afd293&quot;&gt;here&lt;/a&gt;) and I can make your solutions available to others so we share in your progress; as an added benefit, you&#39;ll be recognized in GitHub for your contribution to the repo. While the projects are enjoyable, they are first and foremost designed to be &lt;em&gt;educational&lt;/em&gt;, teaching you business skills that can be put into practice in your work.&lt;/p&gt; 
&lt;h2&gt;Starting in Week 3, we&#39;ll also be using Google Colab for running with GPUs&lt;/h2&gt; 
&lt;p&gt;You should be able to use the free tier or minimal spend to complete all the projects in the class. I personally signed up for Colab Pro+ and I&#39;m loving it - but it&#39;s not required.&lt;/p&gt; 
&lt;p&gt;Learn about Google Colab and set up a Google account (if you don&#39;t already have one) &lt;a href=&quot;https://colab.research.google.com/&quot;&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The colab links are in the Week folders and also here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For week 3 day 1, this Google Colab shows what &lt;a href=&quot;https://colab.research.google.com/drive/1DjcrYDZldAXKJ08x1uYIVCtItoLPk1Wr?usp=sharing&quot;&gt;colab can do&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For week 3 day 2, here is a colab for the HuggingFace &lt;a href=&quot;https://colab.research.google.com/drive/1aMaEw8A56xs0bRM4lu8z7ou18jqyybGm?usp=sharing&quot;&gt;pipelines API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For week 3 day 3, here&#39;s the colab on &lt;a href=&quot;https://colab.research.google.com/drive/1WD6Y2N7ctQi1X9wa6rpkg8UfyA4iSVuz?usp=sharing&quot;&gt;Tokenizers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For week 3 day 4, we go to a colab with HuggingFace &lt;a href=&quot;https://colab.research.google.com/drive/1hhR9Z-yiqjUe7pJjVQw4c74z_V3VchLy?usp=sharing&quot;&gt;models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For week 3 day 5, we return to colab to make our &lt;a href=&quot;https://colab.research.google.com/drive/1KSMxOCprsl1QRpt_Rq0UqCAyMtPqDQYx?usp=sharing&quot;&gt;Meeting Minutes product&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For week 7, we will use these Colab books: &lt;a href=&quot;https://colab.research.google.com/drive/15rqdMTJwK76icPBxNoqhI7Ww8UM-Y7ni?usp=sharing&quot;&gt;Day 1&lt;/a&gt; | &lt;a href=&quot;https://colab.research.google.com/drive/1T72pbfZw32fq-clQEp-p8YQ4_qFKv4TP?usp=sharing&quot;&gt;Day 2&lt;/a&gt; | &lt;a href=&quot;https://colab.research.google.com/drive/1csEdaECRtjV_1p9zMkaKKjCpYnltlN3M?usp=sharing&quot;&gt;Days 3 and 4&lt;/a&gt; | &lt;a href=&quot;https://colab.research.google.com/drive/1igA0HF0gvQqbdBD4GkcK3GpHtuDLijYn?usp=sharing&quot;&gt;Day 5&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Monitoring API charges&lt;/h3&gt; 
&lt;p&gt;You can keep your API spend very low throughout this course; you can monitor spend at the dashboards: &lt;a href=&quot;https://platform.openai.com/usage&quot;&gt;here&lt;/a&gt; for OpenAI, &lt;a href=&quot;https://console.anthropic.com/settings/cost&quot;&gt;here&lt;/a&gt; for Anthropic and &lt;a href=&quot;https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com/cost&quot;&gt;here&lt;/a&gt; for Google Gemini.&lt;/p&gt; 
&lt;p&gt;The charges for the exercsies in this course should always be quite low, but if you&#39;d prefer to keep them minimal, then be sure to always choose the cheapest versions of models:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;For OpenAI: Always use model &lt;code&gt;gpt-4o-mini&lt;/code&gt; in the code instead of &lt;code&gt;gpt-4o&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;For Anthropic: Always use model &lt;code&gt;claude-3-haiku-20240307&lt;/code&gt; in the code instead of the other Claude models&lt;/li&gt; 
 &lt;li&gt;During week 7, look out for my instructions for using the cheaper dataset&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Please do message me or email me at &lt;a href=&quot;mailto:ed@edwarddonner.com&quot;&gt;ed@edwarddonner.com&lt;/a&gt; if this doesn&#39;t work or if I can help with anything. I can&#39;t wait to hear how you get on.&lt;/p&gt; 
&lt;table style=&quot;margin: 0; text-align: left;&quot;&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td style=&quot;width: 150px; height: 150px; vertical-align: middle;&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/ed-donner/llm_engineering/main/resources.jpg&quot; width=&quot;150&quot; height=&quot;150&quot; style=&quot;display: block;&quot;&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;h2 style=&quot;color:#f71;&quot;&gt;Other resources&lt;/h2&gt; &lt;span style=&quot;color:#f71;&quot;&gt;I&#39;ve put together this webpage with useful resources for the course. This includes links to all the slides.&lt;br&gt; &lt;a href=&quot;https://edwarddonner.com/2024/11/13/llm-engineering-resources/&quot;&gt;https://edwarddonner.com/2024/11/13/llm-engineering-resources/&lt;/a&gt;&lt;br&gt; Please keep this bookmarked, and I&#39;ll continue to add more useful links there over time. &lt;/span&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>greyhatguy007/Machine-Learning-Specialization-Coursera</title>
      <link>https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera</link>
      <description>&lt;p&gt;Contains Solutions and Notes for the Machine Learning Specialization By Stanford University and Deeplearning.ai - Coursera (2022) by Prof. Andrew NG&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Specialization Coursera&lt;/h1&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/greyhatguy007/Machine-Learning-Specialization-Coursera/main/resources/title-head.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Contains Solutions and Notes for the &lt;a href=&quot;https://www.coursera.org/specializations/machine-learning-introduction/?utm_medium=coursera&amp;amp;utm_source=home-page&amp;amp;utm_campaign=mlslaunch2022IN&quot;&gt;Machine Learning Specialization&lt;/a&gt; by Andrew NG on Coursera&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note : If you would like to have a deeper understanding of the concepts by understanding all the math required, have a look at &lt;a href=&quot;https://github.com/greyhatguy007/Mathematics-for-Machine-Learning-and-Data-Science-Specialization-Coursera&quot;&gt;Mathematics for Machine Learning and Data Science&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;Course 1 : &lt;a href=&quot;https://www.coursera.org/learn/machine-learning?specialization=machine-learning-introduction&quot;&gt;Supervised Machine Learning: Regression and Classification &lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1&quot;&gt;Week 1&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Practice%20quiz%20-%20Regression&quot;&gt;Practice quiz: Regression&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Practice%20quiz%20-%20Supervised%20vs%20unsupervised%20learning&quot;&gt;Practice quiz: Supervised vs unsupervised learning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Practice%20quiz%20-%20Train%20the%20model%20with%20gradient%20descent&quot;&gt;Practice quiz: Train the model with gradient descent&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Optional%20Labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Optional%20Labs/C1_W1_Lab03_Model_Representation_Soln.ipynb&quot;&gt;Model Representation&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Optional%20Labs/C1_W1_Lab04_Cost_function_Soln.ipynb&quot;&gt;Cost Function&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Optional%20Labs/C1_W1_Lab05_Gradient_Descent_Soln.ipynb&quot;&gt;Gradient Descent&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2&quot;&gt;Week 2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Practice%20quiz%20-%20Gradient%20descent%20in%20practice&quot;&gt;Practice quiz: Gradient descent in practice&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Practice%20quiz%20-%20Multiple%20linear%20regression&quot;&gt;Practice quiz: Multiple linear regression&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb&quot;&gt;Numpy Vectorization&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab02_Multiple_Variable_Soln.ipynb&quot;&gt;Multi Variate Regression&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln.ipynb&quot;&gt;Feature Scaling&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab04_FeatEng_PolyReg_Soln.ipynb&quot;&gt;Feature Engineering&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab05_Sklearn_GD_Soln.ipynb&quot;&gt;Sklearn Gradient Descent&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab05_Sklearn_GD_Soln.ipynb&quot;&gt;Sklearn Normal Method&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/C1W2A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/C1W2A1/C1_W2_Linear_Regression.ipynb&quot;&gt;Linear Regression&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3&quot;&gt;Week 3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Practice%20quiz%20-%20Cost%20function%20for%20logistic%20regression&quot;&gt;Practice quiz: Cost function for logistic regression&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Practice%20quiz%20-%20Gradient%20descent%20for%20logistic%20regression&quot;&gt;Practice quiz: Gradient descent for logistic regression&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab01_Classification_Soln.ipynb&quot;&gt;Classification&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab02_Sigmoid_function_Soln.ipynb&quot;&gt;Sigmoid Function&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab03_Decision_Boundary_Soln.ipynb&quot;&gt;Decision Boundary&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab04_LogisticLoss_Soln.ipynb&quot;&gt;Logistic Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab05_Cost_Function_Soln.ipynb&quot;&gt;Cost Function&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab06_Gradient_Descent_Soln.ipynb&quot;&gt;Gradient Descent&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab07_Scikit_Learn_Soln.ipynb&quot;&gt;Scikit Learn - Logistic Regression&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab08_Overfitting_Soln.ipynb&quot;&gt;Overfitting&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab09_Regularization_Soln.ipynb&quot;&gt;Regularization&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/C1W3A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/C1W3A1/C1_W3_Logistic_Regression.ipynb&quot;&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href=&quot;https://coursera.org/share/195768f3c1a83e42298d3f61dae99d01&quot;&gt;Certificate Of Completion&lt;/a&gt;&lt;/h4&gt; 
&lt;br&gt; 
&lt;h2&gt;Course 2 : &lt;a href=&quot;https://www.coursera.org/learn/advanced-learning-algorithms?specialization=machine-learning-introduction&quot;&gt;Advanced Learning Algorithms&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1&quot;&gt;Week 1&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/Practice%20quiz%20-%20Neural%20networks%20intuition&quot;&gt;Practice quiz: Neural networks intuition&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/Practice%20quiz%20-%20Neural%20network%20model&quot;&gt;Practice quiz: Neural network model&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/Practice%20quiz%20-%20TensorFlow%20implementation&quot;&gt;Practice quiz: TensorFlow implementation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/Practice-Quiz-Neural-Networks-Implementation-in-python&quot;&gt;Practice quiz : Neural Networks Implementation in Numpy&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/optional-labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/optional-labs/C2_W1_Lab01_Neurons_and_Layers.ipynb&quot;&gt;Neurons and Layers&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/optional-labs/C2_W1_Lab02_CoffeeRoasting_TF.ipynb&quot;&gt;Coffee Roasting&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/optional-labs/C2_W1_Lab02_CoffeeRoasting_TF.ipynb&quot;&gt;Coffee Roasting Using Numpy&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/C2W1A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/C2W1A1/C2_W1_Assignment.ipynb&quot;&gt;Neural Networks for Binary Classification&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;br&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2&quot;&gt;Week 2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/Practice-Quiz-Neural-Network-Training&quot;&gt;Practice quiz : Neural Networks Training&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/Practice-Quiz-Activation-Functions&quot;&gt;Practice quiz : Activation Functions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/Practice-quiz-Multiclass-Classification&quot;&gt;Practice quiz : Multiclass Classification&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/Practice-Quiz-Additional-Neural-Network-Concepts&quot;&gt;Practice quiz : Additional Neural Networks Concepts&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/optional-labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/optional-labs/C2_W2_Relu.ipynb&quot;&gt;RElu&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/optional-labs/C2_W2_SoftMax.ipynb&quot;&gt;Softmax&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/optional-labs/C2_W2_Multiclass_TF.ipynb&quot;&gt;Multiclass Classification&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/C2W2A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/C2W2A1/C2_W2_Assignment.ipynb&quot;&gt;Neural Networks For Handwritten Digit Recognition - Multiclass&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3&quot;&gt;Week 3&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/Practice-Quiz-Advice-for-applying-machine-learning&quot;&gt;Practice quiz : Advice for Applying Machine Learning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/practice-quiz-bias-and-variance&quot;&gt;Practice quiz : Bias and Variance&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/practice-quiz-machine-learning-development-process&quot;&gt;Practice quiz : Machine Learning Development Process&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/C2W3A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/C2W3A1/C2_W3_Assignment.ipynb&quot;&gt;Advice for Applied Machine Learning&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4&quot;&gt;Week 4&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/practice-quiz-decision-trees&quot;&gt;Practice quiz : Decision Trees&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/practice-quiz-decision-tree-learning&quot;&gt;Practice quiz : Decision Trees Learning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/practice-quiz-tree-ensembles&quot;&gt;Practice quiz : Decision Trees Ensembles&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/C2W4A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/C2W4A1/C2_W4_Decision_Tree_with_Markdown.ipynb&quot;&gt;Decision Trees&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href=&quot;https://coursera.org/share/c9a7766b0c6eab27db2e955376d29bf7&quot;&gt;Certificate of Completion&lt;/a&gt;&lt;/h4&gt; 
&lt;br&gt; 
&lt;h2&gt;Course 3 : &lt;a href=&quot;https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning?specialization=machine-learning-introduction&quot;&gt;Unsupervised Learning, Recommenders, Reinforcement Learning&lt;/a&gt;&lt;/h2&gt; 
&lt;br&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1&quot;&gt;Week 1&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/Practice%20Quiz%20-%20Clustering&quot;&gt;Practice quiz : Clustering&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/Practice%20Quiz%20-%20Anomaly%20Detection&quot;&gt;Practice quiz : Anomaly Detection&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/C3W1A&quot;&gt;Programming Assignments&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/C3W1A/C3W1A1/C3_W1_KMeans_Assignment.ipynb&quot;&gt;K means&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/C3W1A/C3W1A2/C3_W1_Anomaly_Detection.ipynb&quot;&gt;Anomaly Detection&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2&quot;&gt;Week 2&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/Practice%20Quiz%20-%20Collaborative%20Filtering&quot;&gt;Practice quiz : Collaborative Filtering&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/Practice%20Quiz%20-%20Recommender%20systems%20implementation&quot;&gt;Practice quiz : Recommender systems implementation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/Practice%20Quiz%20-%20Content-based%20filtering&quot;&gt;Practice quiz : Content-based filtering&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/C3W2&quot;&gt;Programming Assignments&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/C3W2/C3W2A1/C3_W2_Collaborative_RecSys_Assignment.ipynb&quot;&gt;Collaborative Filtering RecSys&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/C3W2/C3W2A2/C3_W2_RecSysNN_Assignment.ipynb&quot;&gt;RecSys using Neural Networks&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3&quot;&gt;Week 3&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/Practice%20Quiz%20-%20Reinforcement%20learning%20introduction&quot;&gt;Practice quiz : Reinforcement learning introduction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/Practice%20Quiz%20-%20State-action%20value%20function&quot;&gt;Practice Quiz : State-action value function&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/Practice%20Quiz%20-%20Continuous%20state%20spaces&quot;&gt;Practice Quiz : Continuous state spaces&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/C3W3A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/C3W3A1/C3_W3_A1_Assignment.ipynb&quot;&gt;Deep Q-Learning - Lunar Lander&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href=&quot;https://coursera.org/share/5bf5ee456b0c806df9b8622067b47ca6&quot;&gt;Certificate of Completion&lt;/a&gt;&lt;/h4&gt; 
&lt;h3&gt;&lt;a href=&quot;https://coursera.org/share/a15ac6426f90924491a542850700a759&quot;&gt;Specialization Certificate&lt;/a&gt;&lt;/h3&gt; 
&lt;br&gt; 
&lt;br&gt; 
&lt;hr&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;h3&gt;Stargazers over time&lt;/h3&gt; 
 &lt;p&gt;&lt;a href=&quot;https://starchart.cc/greyhatguy007/Machine-Learning-Specialization-Coursera&quot;&gt;&lt;img src=&quot;https://starchart.cc/greyhatguy007/Machine-Learning-Specialization-Coursera.svg?variant=adaptive&quot; alt=&quot;Stargazers over time&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://hits.seeyoufarm.com&quot;&gt;&lt;img src=&quot;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fgreyhatguy007%2FMachine-Learning-Specialization-Coursera&amp;amp;count_bg=%2379C83D&amp;amp;title_bg=%23555555&amp;amp;icon=&amp;amp;icon_color=%23E7E7E7&amp;amp;title=hits&amp;amp;edge_flat=false&quot; alt=&quot;Hits&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h3&gt;Course Review :&lt;/h3&gt; 
&lt;p&gt;This Course is a best place towards becoming a Machine Learning Engineer. Even if you&#39;re an expert, many algorithms are covered in depth such as decision trees which may help in further improvement of skills.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Special thanks to &lt;a href=&quot;https://www.andrewng.org/&quot;&gt;Professor Andrew Ng&lt;/a&gt; for structuring and tailoring this Course.&lt;/strong&gt;&lt;/p&gt; 
&lt;br&gt; 
&lt;hr&gt; 
&lt;h4&gt;An insight of what you might be able to accomplish at the end of this specialization :&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;i&gt;Write an unsupervised learning algorithm to &lt;strong&gt;Land the Lunar Lander&lt;/strong&gt; Using Deep Q-Learning&lt;/i&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The Rover was trained to land correctly on the surface, correctly between the flags as indicators after many unsuccessful attempts in learning how to do it.&lt;/li&gt; 
   &lt;li&gt;The final landing after training the agent using appropriate parameters :&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://user-images.githubusercontent.com/77543865/182395635-703ae199-ba79-4940-86eb-23dd90093ab3.mp4&quot;&gt;https://user-images.githubusercontent.com/77543865/182395635-703ae199-ba79-4940-86eb-23dd90093ab3.mp4&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;i&gt;Write an algorithm for a &lt;strong&gt;Movie Recommender System&lt;/strong&gt;&lt;/i&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;A movie database is collected based on its genre.&lt;/li&gt; 
   &lt;li&gt;A content based filtering and collaborative filtering algorithm is trained and the movie recommender system is implemented.&lt;/li&gt; 
   &lt;li&gt;It gives movie recommendentations based on the movie genre.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/77543865/182398093-c7387754-34a9-4044-b842-0085060c3525.png&quot; alt=&quot;movie_recommendation&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;i&gt; And Much More !! &lt;/i&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Concluding, this is a course which I would recommend everyone to take. Not just because you learn many new stuffs, but also the assignments are real life examples which are &lt;em&gt;exciting to complete&lt;/em&gt;.&lt;/p&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;strong&gt;Happy Learning :))&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google-gemini/gemma-cookbook</title>
      <link>https://github.com/google-gemini/gemma-cookbook</link>
      <description>&lt;p&gt;A collection of guides and examples for the Gemma open models from Google.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to the Gemma Cookbook&lt;/h1&gt; 
&lt;p&gt;This is a collection of guides and examples for &lt;a href=&quot;https://ai.google.dev/gemma/&quot;&gt;Google Gemma&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Get started with the Gemma models&lt;/h2&gt; 
&lt;p&gt;Gemma is a family of lightweight, generative artificial intelligence (AI) open models, built from the same research and technology used to create the Gemini models. The Gemma model family includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Gemma&lt;br&gt; The core models of the Gemma family. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/core/model_card&quot;&gt;Gemma&lt;/a&gt;&lt;br&gt; For a variety of text generation tasks and can be further tuned for specific use cases&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/core/model_card_2&quot;&gt;Gemma 2&lt;/a&gt;&lt;br&gt; Higher-performing and more efficient, available in 2B, 9B, 27B parameter sizes&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/core/model_card_3&quot;&gt;Gemma 3&lt;/a&gt;&lt;br&gt; Longer context window and handling text and image input, available in 1B, 4B, 12B and 27B parameter sizes&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Gemma variants 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/codegemma&quot;&gt;CodeGemma&lt;/a&gt;&lt;br&gt; Fine-tuned for a variety of coding tasks&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/paligemma/model-card&quot;&gt;PaliGemma&lt;/a&gt;&lt;br&gt; Vision Language Model&lt;br&gt; For a deeper analysis of images and provide useful insights&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/paligemma/model-card-2&quot;&gt;PaliGemma 2&lt;/a&gt;&lt;br&gt; VLM which incorporates the capabilities of the Gemma 2 models&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/recurrentgemma&quot;&gt;RecurrentGemma&lt;/a&gt;&lt;br&gt; Based on &lt;a href=&quot;https://arxiv.org/abs/2402.19427&quot;&gt;Griffin&lt;/a&gt; architecture&lt;br&gt; For a variety of text generation tasks&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/shieldgemma/model_card&quot;&gt;ShieldGemma&lt;/a&gt;&lt;br&gt; Fine-tuned for evaluating the safety of text prompt input and text output responses against a set of defined safety policies&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/shieldgemma/model_card_2&quot;&gt;ShieldGemma 2&lt;/a&gt;&lt;br&gt; Fine-tuned on Gemma 3&#39;s 4B IT checkpoint for image safety classification&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/datagemma&quot;&gt;DataGemma&lt;/a&gt;&lt;br&gt; Fine-tuned for using Data Commons to address AI hallucinations&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can find the Gemma models on the Hugging Face Hub, Kaggle, Google Cloud Vertex AI Model Garden, and &lt;a href=&quot;https://ai.nvidia.com&quot;&gt;ai.nvidia.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Table of Notebooks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/gemma-cookbook/main/Gemma/README.md&quot;&gt;Gemma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/gemma-cookbook/main/CodeGemma/README.md&quot;&gt;CodeGemma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/gemma-cookbook/main/PaliGemma/README.md&quot;&gt;PaliGemma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/gemma-cookbook/main/Workshops/README.md&quot;&gt;Workshops and technical talks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/gemma-cookbook/main/Demos/README.md&quot;&gt;Showcase complex end-to-end use cases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/GoogleCloudPlatform/generative-ai/tree/main/open-models&quot;&gt;Gemma on Google Cloud&lt;/a&gt; : GCP open models has additional notebooks for using Gemma&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Get help&lt;/h2&gt; 
&lt;p&gt;Ask a Gemma cookbook-related question on the &lt;a href=&quot;https://discuss.ai.google.dev/c/gemma/10&quot;&gt;developer forum&lt;/a&gt;, or open an &lt;a href=&quot;https://github.com/google-gemini/gemma-cookbook/issues&quot;&gt;issue&lt;/a&gt; on GitHub.&lt;/p&gt; 
&lt;h2&gt;Wish list&lt;/h2&gt; 
&lt;p&gt;If you want to see additional cookbooks implemented for specific features/integrations, please open a new issue with &lt;a href=&quot;https://github.com/google-gemini/gemma-cookbook/issues/new?template=feature_request.yml&quot;&gt;“Feature Request” template&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to make contributions to the Gemma Cookbook project, you are welcome to pick any idea in the &lt;a href=&quot;https://github.com/google-gemini/gemma-cookbook/labels/wishlist&quot;&gt;“Wish List”&lt;/a&gt; and implement it.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are always welcome. Please read &lt;a href=&quot;https://github.com/google-gemini/gemma-cookbook/raw/main/CONTRIBUTING.md&quot;&gt;contributing&lt;/a&gt; before implementation.&lt;/p&gt; 
&lt;p&gt;Thank you for developing with Gemma! We’re excited to see what you create.&lt;/p&gt; 
&lt;h2&gt;Translation of this repository&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/doggy8088/gemma-cookbook&quot;&gt;Traditional Chinese&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xiaoxiong1006/gemma-cookbook&quot;&gt;Simplified Chinese&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Azure-Samples/AI-Gateway</title>
      <link>https://github.com/Azure-Samples/AI-Gateway</link>
      <description>&lt;p&gt;APIM ❤️ OpenAI - this repo contains a set of experiments on using GenAI capabilities of Azure API Management with Azure OpenAI and other services&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;APIM ❤️ OpenAI - 🧪 Labs for the &lt;a href=&quot;https://techcommunity.microsoft.com/t5/azure-integration-services-blog/introducing-genai-gateway-capabilities-in-azure-api-management/ba-p/4146525&quot;&gt;GenAI Gateway capabilities&lt;/a&gt; of &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/api-management-key-concepts&quot;&gt;Azure API Management&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/firstcontributions/open-source-badges&quot;&gt;&lt;img src=&quot;https://firstcontributions.github.io/open-source-badges/badges/open-source-v1/open-source.svg?sanitize=true&quot; alt=&quot;Open Source Love&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What&#39;s new ✨&lt;/h2&gt; 
&lt;p&gt;➕ the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/finops-framework/finops-framework.ipynb&quot;&gt;&lt;strong&gt;FinOps Framework&lt;/strong&gt;&lt;/a&gt; lab to manage AI budgets effectively 💰&lt;br&gt; ➕ &lt;strong&gt;Agentic ✨&lt;/strong&gt; experiments with &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-context-protocol/model-context-protocol.ipynb&quot;&gt;&lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;&lt;/a&gt;.&lt;br&gt; ➕ &lt;strong&gt;Agentic ✨&lt;/strong&gt; experiments with &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/openai-agents/openai-agents.ipynb&quot;&gt;&lt;strong&gt;OpenAI Agents SDK&lt;/strong&gt;&lt;/a&gt;.&lt;br&gt; ➕ &lt;strong&gt;Agentic ✨&lt;/strong&gt; experiments with &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-agent-service/ai-agent-service.ipynb&quot;&gt;&lt;strong&gt;AI Agent Service&lt;/strong&gt;&lt;/a&gt; from &lt;a href=&quot;https://azure.microsoft.com/en-us/products/ai-foundry&quot;&gt;Azure AI Foundry&lt;/a&gt;.&lt;br&gt; ➕ the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-foundry-deepseek/ai-foundry-deepseek.ipynb&quot;&gt;&lt;strong&gt;AI Foundry Deepseek&lt;/strong&gt;&lt;/a&gt; lab with Deepseek R1 model from &lt;a href=&quot;https://azure.microsoft.com/en-us/products/ai-foundry&quot;&gt;Azure AI Foundry&lt;/a&gt;.&lt;br&gt; ➕ the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/zero-to-production/zero-to-production.ipynb&quot;&gt;&lt;strong&gt;Zero-to-Production&lt;/strong&gt;&lt;/a&gt; lab with an iterative policy exploration to fine-tune the optimal production configuration.&lt;br&gt; ➕ the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing-tf/backend-pool-load-balancing-tf.ipynb&quot;&gt;&lt;strong&gt;Terraform flavor of backend pool load balancing&lt;/strong&gt;&lt;/a&gt; lab.&lt;br&gt; ➕ the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-foundry-sdk/ai-foundry-sdk.ipynb&quot;&gt;&lt;strong&gt;AI Foundry SDK&lt;/strong&gt;&lt;/a&gt; lab.&lt;br&gt; ➕ the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/content-filtering.ipynb&quot;&gt;&lt;strong&gt;Content filtering&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/prompt-shielding.ipynb&quot;&gt;&lt;strong&gt;Prompt shielding&lt;/strong&gt;&lt;/a&gt; labs.&lt;br&gt; ➕ the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-routing/model-routing.ipynb&quot;&gt;&lt;strong&gt;Model routing&lt;/strong&gt;&lt;/a&gt; lab with OpenAI model based routing.&lt;br&gt; ➕ the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/prompt-flow/prompt-flow.ipynb&quot;&gt;&lt;strong&gt;Prompt flow&lt;/strong&gt;&lt;/a&gt; lab to try the &lt;a href=&quot;https://learn.microsoft.com/azure/ai-studio/how-to/prompt-flow&quot;&gt;Azure AI Studio Prompt Flow&lt;/a&gt; with Azure API Management.&lt;br&gt; ➕ &lt;code&gt;priority&lt;/code&gt; and &lt;code&gt;weight&lt;/code&gt; parameters to the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb&quot;&gt;&lt;strong&gt;Backend pool load balancing&lt;/strong&gt;&lt;/a&gt; lab.&lt;br&gt; ➕ the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/streaming.ipynb&quot;&gt;&lt;strong&gt;Streaming&lt;/strong&gt;&lt;/a&gt; tool to test OpenAI streaming with Azure API Management.&lt;br&gt; ➕ the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/tools/tracing.ipynb&quot;&gt;&lt;strong&gt;Tracing&lt;/strong&gt;&lt;/a&gt; tool to debug and troubleshoot OpenAI APIs using &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/api-management-howto-api-inspector&quot;&gt;Azure API Management tracing capability&lt;/a&gt;.&lt;br&gt; ➕ image processing to the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/GPT-4o-inferencing/GPT-4o-inferencing.ipynb&quot;&gt;&lt;strong&gt;GPT-4o inferencing&lt;/strong&gt;&lt;/a&gt; lab.&lt;br&gt; ➕ the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/function-calling/function-calling.ipynb&quot;&gt;&lt;strong&gt;Function calling&lt;/strong&gt;&lt;/a&gt; lab with a sample API on Azure Functions.&lt;/p&gt; 
&lt;h2&gt;Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-genai-gateway&quot;&gt;🧠 GenAI Gateway&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-labs-with-ai-agents&quot;&gt;🧪 Labs with AI Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-labs-with-the-inference-api&quot;&gt;🧪 Labs with the Inference API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-labs-based-on-azure-openai&quot;&gt;🧪 Labs based on Azure OpenAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-getting-started&quot;&gt;🚀 Getting started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-roll-out-to-production&quot;&gt;⛵ Roll-out to production&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-supporting-tools&quot;&gt;🔨 Supporting tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-well-architected-framework&quot;&gt;🏛️ Well-Architected Framework&lt;/a&gt; 
  &lt;!-- markdownlint-disable-line MD051 --&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-show-and-tell&quot;&gt;🎒 Show and tell&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-other-resources&quot;&gt;🥇 Other Resources&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The rapid pace of AI advances demands experimentation-driven approaches for organizations to remain at the forefront of the industry. With AI steadily becoming a game-changer for an array of sectors, maintaining a fast-paced innovation trajectory is crucial for businesses aiming to leverage its full potential.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AI services&lt;/strong&gt; are predominantly accessed via &lt;strong&gt;APIs&lt;/strong&gt;, underscoring the essential need for a robust and efficient API management strategy. This strategy is instrumental for maintaining control and governance over the consumption of &lt;strong&gt;AI services&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;With the expanding horizons of &lt;strong&gt;AI services&lt;/strong&gt; and their seamless integration with &lt;strong&gt;APIs&lt;/strong&gt;, there is a considerable demand for a comprehensive &lt;strong&gt;AI Gateway&lt;/strong&gt; pattern, which broadens the core principles of API management. Aiming to accelerate the experimentation of advanced use cases and pave the road for further innovation in this rapidly evolving field. The well-architected principles of the &lt;strong&gt;AI Gateway&lt;/strong&gt; provides a framework for the confident deployment of &lt;strong&gt;Intelligent Apps&lt;/strong&gt; into production.&lt;/p&gt; 
&lt;h2&gt;🧠 GenAI Gateway&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/ai-gateway.gif&quot; alt=&quot;AI-Gateway flow&quot;&gt;&lt;/p&gt; 
&lt;p&gt;This repo explores the &lt;strong&gt;AI Gateway&lt;/strong&gt; pattern through a series of experimental labs. The &lt;a href=&quot;https://techcommunity.microsoft.com/t5/azure-integration-services-blog/introducing-genai-gateway-capabilities-in-azure-api-management/ba-p/4146525&quot;&gt;GenAI Gateway capabilities&lt;/a&gt; of &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/api-management-key-concepts&quot;&gt;Azure API Management&lt;/a&gt; plays a crucial role within these labs, handling AI services APIs, with security, reliability, performance, overall operational efficiency and cost controls. The primary focus is on &lt;a href=&quot;https://learn.microsoft.com/azure/ai-services/openai/overview&quot;&gt;Azure OpenAI&lt;/a&gt;, which sets the standard reference for Large Language Models (LLM). However, the same principles and design patterns could potentially be applied to any LLM.&lt;/p&gt; 
&lt;p&gt;Acknowledging the rising dominance of Python, particularly in the realm of AI, along with the powerful experimental capabilities of Jupyter notebooks, the following labs are structured around Jupyter notebooks, with step-by-step instructions with Python scripts, &lt;a href=&quot;https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep&quot;&gt;Bicep&lt;/a&gt; files and &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/api-management-howto-policies&quot;&gt;Azure API Management policies&lt;/a&gt;:&lt;/p&gt; 
&lt;h2&gt;🧪 Labs with AI Agents&lt;/h2&gt; 
&lt;!-- Model Context Protocol (MCP) --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-context-protocol/model-context-protocol.ipynb&quot;&gt;&lt;strong&gt;🧪 Model Context Protocol (MCP)&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to experiment the &lt;a href=&quot;https://modelcontextprotocol.io/&quot;&gt;Model Context Protocol&lt;/a&gt; with Azure API Management to enable plug &amp;amp; play of tools to LLMs. Leverages the &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/api-management/credentials-overview&quot;&gt;credential manager&lt;/a&gt; for managing OAuth 2.0 tokens to backend tools and &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/api-management/validate-jwt-policy&quot;&gt;client token validation&lt;/a&gt; to ensure end-to-end authentication and authorization.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-context-protocol/model-context-protocol.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/model-context-protocol-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-context-protocol/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-context-protocol/inference-policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-context-protocol/model-context-protocol.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- OpenAI Agents --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/openai-agents/openai-agents.ipynb&quot;&gt;&lt;strong&gt;🧪 OpenAI Agents&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the &lt;a href=&quot;https://openai.github.io/openai-agents-python/&quot;&gt;OpenAI Agents&lt;/a&gt; with Azure OpenAI models and API based tools controlled by Azure API Management.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/openai-agents/openai-agents.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/openai-agents-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/openai-agents/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/openai-agents/inference-policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/openai-agents/openai-agents.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- AI Agent Service --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-agent-service/ai-agent-service.ipynb&quot;&gt;&lt;strong&gt;🧪 AI Agent Service&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Use this playground to explore the &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/ai-services/agents/overview&quot;&gt;Azure AI Agent Service&lt;/a&gt;, leveraging Azure API Management to control multiple services, including Azure OpenAI models, Logic Apps Workflows, and OpenAPI-based APIs.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-agent-service/ai-agent-service.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/ai-agent-service-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-agent-service/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-agent-service/policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-agent-service/ai-agent-service.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Function calling --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/function-calling/function-calling.ipynb&quot;&gt;&lt;strong&gt;🧪 Function calling&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the OpenAI &lt;a href=&quot;https://learn.microsoft.com/azure/ai-services/openai/how-to/function-calling?tabs=non-streaming%2Cpython&quot;&gt;function calling&lt;/a&gt; feature with an Azure Functions API that is also managed by Azure API Management.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/function-calling/function-calling.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/function-calling-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/function-calling/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/function-calling/policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/function-calling/function-calling.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;🧪 Labs with the Inference API&lt;/h2&gt; 
&lt;!-- AI Foundry Deepseek --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-foundry-deepseek/ai-foundry-deepseek.ipynb&quot;&gt;&lt;strong&gt;🧪 AI Foundry Deepseek&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/deepseek-r1-is-now-available-on-azure-ai-foundry-and-github/&quot;&gt;Deepseek R1 model&lt;/a&gt; via the AI Model Inference from &lt;a href=&quot;https://azure.microsoft.com/en-us/products/ai-foundry&quot;&gt;Azure AI Foundry&lt;/a&gt;. This lab uses the &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/ai-foundry/model-inference/how-to/inference?tabs=python&quot;&gt;Azure AI Model Inference API&lt;/a&gt; and two APIM LLM policies: &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/api-management/llm-token-limit-policy&quot;&gt;llm-token-limit&lt;/a&gt; and &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/api-management/llm-emit-token-metric-policy&quot;&gt;llm-emit-token-metric&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-foundry-deepseek/ai-foundry-deepseek.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/ai-foundry-deepseek-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-foundry-deepseek/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-foundry-deepseek/policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-foundry-deepseek/ai-foundry-deepseek.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- SLM self-hosting --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/slm-self-hosting/slm-self-hosting.ipynb&quot;&gt;&lt;strong&gt;🧪 SLM self-hosting&lt;/strong&gt;&lt;/a&gt; (phy-3)&lt;/h3&gt; 
&lt;p&gt;Playground to try the self-hosted &lt;a href=&quot;https://azure.microsoft.com/blog/introducing-phi-3-redefining-whats-possible-with-slms/&quot;&gt;phy-3 Small Language Model (SLM)&lt;/a&gt; through the &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/self-hosted-gateway-overview&quot;&gt;Azure API Management self-hosted gateway&lt;/a&gt; with OpenAI API compatibility.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/slm-self-hosting/slm-self-hosting.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/slm-self-hosting-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/slm-self-hosting/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/slm-self-hosting/policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/slm-self-hosting/slm-self-hosting.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;🧪 Labs based on Azure OpenAI&lt;/h2&gt; 
&lt;!--FinOps framework --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/finops-framework/finops-framework.ipynb&quot;&gt;&lt;strong&gt;🧪 FinOps Framework&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;This playground leverages the &lt;a href=&quot;https://www.finops.org/framework/&quot;&gt;FinOps Framework&lt;/a&gt; and Azure API Management to control AI costs. It uses the &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/api-management/azure-openai-token-limit-policy&quot;&gt;token limit&lt;/a&gt; policy for each &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-add-products?tabs=azure-portal&amp;amp;pivots=interactive&quot;&gt;product&lt;/a&gt; and integrates &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-overview&quot;&gt;Azure Monitor alerts&lt;/a&gt; with &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-logic-apps?tabs=send-email&quot;&gt;Logic Apps&lt;/a&gt; to automatically disable APIM &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/api-management/api-management-subscriptions&quot;&gt;subscriptions&lt;/a&gt; that exceed cost quotas.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/finops-framework/finops-framework.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/finops-framework-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/finops-framework/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/finops-framework/openai-policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/finops-framework/finops-framework.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Backend pool load balancing --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb&quot;&gt;&lt;strong&gt;🧪 Backend pool load balancing&lt;/strong&gt;&lt;/a&gt; - Available with &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb&quot;&gt;Bicep&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing-tf/backend-pool-load-balancing-tf.ipynb&quot;&gt;Terraform&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the built-in load balancing &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/backends?tabs=bicep&quot;&gt;backend pool functionality of Azure API Management&lt;/a&gt; to either a list of Azure OpenAI endpoints or mock servers.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/backend-pool-load-balancing-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing/policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Token rate limiting --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-rate-limiting/token-rate-limiting.ipynb&quot;&gt;&lt;strong&gt;🧪 Token rate limiting&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/azure-openai-token-limit-policy&quot;&gt;token rate limiting policy&lt;/a&gt; to one or more Azure OpenAI endpoints. When the token usage is exceeded, the caller receives a 429.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-rate-limiting/token-rate-limiting.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/token-rate-limiting-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-rate-limiting/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-rate-limiting/policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-rate-limiting/token-rate-limiting.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Token metrics emitting --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-metrics-emitting/token-metrics-emitting.ipynb&quot;&gt;&lt;strong&gt;🧪 Token metrics emitting&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/azure-openai-emit-token-metric-policy&quot;&gt;emit token metric policy&lt;/a&gt;. The policy sends metrics to Application Insights about consumption of large language model tokens through Azure OpenAI Service APIs.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-metrics-emitting/token-metrics-emitting.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/token-metrics-emitting-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-metrics-emitting/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-metrics-emitting/policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-metrics-emitting/token-metrics-emitting.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Semantic caching --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/semantic-caching/semantic-caching.ipynb&quot;&gt;&lt;strong&gt;🧪 Semantic caching&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/azure-openai-semantic-cache-lookup-policy&quot;&gt;semantic caching policy&lt;/a&gt;. Uses vector proximity of the prompt to previous requests and a specified similarity score threshold.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/semantic-caching/semantic-caching.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/semantic-caching-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/semantic-caching/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/semantic-caching/policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/semantic-caching/semantic-caching.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Access controlling --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/access-controlling/access-controlling.ipynb&quot;&gt;&lt;strong&gt;🧪 Access controlling&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/api-management-authenticate-authorize-azure-openai#oauth-20-authorization-using-identity-provider&quot;&gt;OAuth 2.0 authorization feature&lt;/a&gt; using identity provider to enable more fine-grained access to OpenAPI APIs by particular users or client.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/access-controlling/access-controlling.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/access-controlling-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/access-controlling/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/access-controlling/policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/access-controlling/access-controlling.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- zero-to-production --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/zero-to-production/zero-to-production.ipynb&quot;&gt;&lt;strong&gt;🧪 Zero-to-Production&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to create a combination of several policies in an iterative approach. We start with load balancing, then progressively add token emitting, rate limiting, and, eventually, semantic caching. Each of these sets of policies is derived from other labs in this repo.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/zero-to-production/zero-to-production.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/zero-to-production-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/zero-to-production/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/zero-to-production/policy-3.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/zero-to-production/zero-to-production.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- GPT-4o inferencing --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/GPT-4o-inferencing/GPT-4o-inferencing.ipynb&quot;&gt;&lt;strong&gt;🧪 GPT-4o inferencing&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the new GPT-4o model. GPT-4o (&quot;o&quot; for &quot;omni&quot;) is designed to handle a combination of text, audio, and video inputs, and can generate outputs in text, audio, and image formats.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/GPT-4o-inferencing/GPT-4o-inferencing.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/GPT-4o-inferencing-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/GPT-4o-inferencing/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/GPT-4o-inferencing/policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/GPT-4o-inferencing/GPT-4o-inferencing.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Model Routing --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-routing/model-routing.ipynb&quot;&gt;&lt;strong&gt;🧪 Model Routing&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try routing to a backend based on Azure OpenAI model and version.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-routing/model-routing.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/model-routing-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-routing/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-routing/policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-routing/model-routing.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Vector searching --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/vector-searching/vector-searching.ipynb&quot;&gt;&lt;strong&gt;🧪 Vector searching&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the &lt;a href=&quot;https://learn.microsoft.com/azure/search/retrieval-augmented-generation-overview&quot;&gt;Retrieval Augmented Generation (RAG) pattern&lt;/a&gt; with Azure AI Search, Azure OpenAI embeddings and Azure OpenAI completions.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/vector-searching/vector-searching.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/vector-searching-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/vector-searching/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/vector-searching/policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/vector-searching/vector-searching.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Built-in logging --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/built-in-logging/built-in-logging.ipynb&quot;&gt;&lt;strong&gt;🧪 Built-in logging&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/observability&quot;&gt;buil-in logging capabilities of Azure API Management&lt;/a&gt;. Logs requests into App Insights to track details and token usage.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/built-in-logging/built-in-logging.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/built-in-logging-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/built-in-logging/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/built-in-logging/policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/built-in-logging/built-in-logging.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Message storing --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/message-storing/message-storing.ipynb&quot;&gt;&lt;strong&gt;🧪 Message storing&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to test storing message details into Cosmos DB through the &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/log-to-eventhub-policy&quot;&gt;Log to event hub&lt;/a&gt; policy. With the policy we can control which data will be stored in the DB (prompt, completion, model, region, tokens etc.).&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/message-storing/message-storing.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/message-storing-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/message-storing/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/message-storing/policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/message-storing/message-storing.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Prompt flow --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/prompt-flow/prompt-flow.ipynb&quot;&gt;&lt;strong&gt;🧪 Prompt flow&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the &lt;a href=&quot;https://learn.microsoft.com/azure/ai-studio/how-to/prompt-flow&quot;&gt;Azure AI Studio Prompt Flow&lt;/a&gt; with Azure API Management.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/prompt-flow/prompt-flow.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/prompt-flow-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/prompt-flow/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/prompt-flow/policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/prompt-flow/prompt-flow.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Content Filtering --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/content-filtering.ipynb&quot;&gt;&lt;strong&gt;🧪 Content Filtering&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try integrating Azure API Management with &lt;a href=&quot;https://learn.microsoft.com/azure/ai-services/content-safety/overview&quot;&gt;Azure AI Content Safety&lt;/a&gt; to filter potentially offensive, risky, or undesirable content.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/content-filtering.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/content-filtering-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/content-filtering-policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/content-filtering.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Prompt Shielding --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/prompt-shielding.ipynb&quot;&gt;&lt;strong&gt;🧪 Prompt Shielding&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try Prompt Shields from Azure AI Content Safety service that analyzes LLM inputs and detects User Prompt attacks and Document attacks, which are two common types of adversarial inputs.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/prompt-shielding.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/content-filtering-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/main.bicep&quot;&gt;🦾 Bicep&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/prompt-shield-policy.xml&quot;&gt;⚙️ Policy&lt;/a&gt; ➕ &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/prompt-shielding.ipynb&quot;&gt;🧾 Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Backlog of Labs&lt;/h2&gt; 
&lt;p&gt;This is a list of potential future labs to be developed.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Real Time API&lt;/li&gt; 
 &lt;li&gt;Semantic Kernel with Agents&lt;/li&gt; 
 &lt;li&gt;Logic Apps RAG&lt;/li&gt; 
 &lt;li&gt;PII handling&lt;/li&gt; 
 &lt;li&gt;Gemini&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Kindly use &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/discussions/9&quot;&gt;the feedback discussion&lt;/a&gt; so that we can continuously improve with your experiences, suggestions, ideas or lab requests.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;🚀 Getting Started&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.python.org/&quot;&gt;Python 3.12 or later version&lt;/a&gt; installed&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://code.visualstudio.com/&quot;&gt;VS Code&lt;/a&gt; installed with the &lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter&quot;&gt;Jupyter notebook extension&lt;/a&gt; enabled&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://code.visualstudio.com/docs/python/environments#_creating-environments&quot;&gt;Python environment&lt;/a&gt; with the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/requirements.txt&quot;&gt;requirements.txt&lt;/a&gt; or run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; in your terminal&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/free/&quot;&gt;An Azure Subscription&lt;/a&gt; with &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#contributor&quot;&gt;Contributor&lt;/a&gt; + &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#role-based-access-control-administrator&quot;&gt;RBAC Administrator&lt;/a&gt; or &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#owner&quot;&gt;Owner&lt;/a&gt; roles&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://learn.microsoft.com/cli/azure/install-azure-cli&quot;&gt;Azure CLI&lt;/a&gt; installed and &lt;a href=&quot;https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively&quot;&gt;Signed into your Azure subscription&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quickstart&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone this repo and configure your local machine with the prerequisites. Or just create a &lt;a href=&quot;https://codespaces.new/Azure-Samples/AI-Gateway/tree/main&quot;&gt;GitHub Codespace&lt;/a&gt; and run it on the browser or in VS Code.&lt;/li&gt; 
 &lt;li&gt;Navigate through the available labs and select one that best suits your needs. For starters we recommend the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-rate-limiting/token-rate-limiting.ipynb&quot;&gt;token rate limiting&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Open the notebook and run the provided steps.&lt;/li&gt; 
 &lt;li&gt;Tailor the experiment according to your requirements. If you wish to contribute to our collective work, we would appreciate your &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/CONTRIBUTING.MD&quot;&gt;submission of a pull request&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] 🪲 Please feel free to open a new &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/issues/new&quot;&gt;issue&lt;/a&gt; if you find something that should be fixed or enhanced.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;⛵ Roll-out to production&lt;/h2&gt; 
&lt;p&gt;We recommend the guidelines and best practices from the &lt;a href=&quot;https://github.com/Azure-Samples/ai-hub-gateway-solution-accelerator&quot;&gt;AI Hub Gateway Landing Zone&lt;/a&gt; to implement a central AI API gateway to empower various line-of-business units in an organization to leverage Azure AI services.&lt;/p&gt; 
&lt;h2&gt;🔨 Supporting Tools&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/tools/mock-server/mock-server.ipynb&quot;&gt;AI-Gateway Mock server&lt;/a&gt; is designed to mimic the behavior and responses of the OpenAI API, thereby creating an efficient simulation environment suitable for testing and development purposes on the integration with Azure API Management and other use cases. The &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/tools/mock-server/app.py&quot;&gt;app.py&lt;/a&gt; can be customized to tailor the Mock server to specific use cases.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/tools/tracing.ipynb&quot;&gt;Tracing&lt;/a&gt; - Invoke OpenAI API with trace enabled and returns the tracing information.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/streaming.ipynb&quot;&gt;Streaming&lt;/a&gt; - Invoke OpenAI API with stream enabled and returns response in chunks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🏛️ Well-Architected Framework&lt;/h2&gt; 
&lt;p&gt;The &lt;a href=&quot;https://learn.microsoft.com/azure/well-architected/what-is-well-architected-framework&quot;&gt;Azure Well-Architected Framework&lt;/a&gt; is a design framework that can improve the quality of a workload. The following table maps labs with the Well-Architected Framework pillars to set you up for success through architectural experimentation.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Lab&lt;/th&gt; 
   &lt;th&gt;Security&lt;/th&gt; 
   &lt;th&gt;Reliability&lt;/th&gt; 
   &lt;th&gt;Performance&lt;/th&gt; 
   &lt;th&gt;Operations&lt;/th&gt; 
   &lt;th&gt;Costs&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/request-forwarding/request-forwarding.ipynb&quot;&gt;Request forwarding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Zero trust, keyless approach with manage identities and Azure API Management security features&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-circuit-breaking/backend-circuit-breaking.ipynb&quot;&gt;Backend circuit breaking&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Zero trust, keyless approach with manage identities and Azure API Management security features&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Controls the availability of the OpenAI endpoint with the circuit breaker feature&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb&quot;&gt;Backend pool load balancing&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Zero trust, keyless approach with manage identities and Azure API Management security features&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;To ensure resilience, the request is distributed to two or more endpoints with the built-in feature&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Load balances the requests to increase performance with the built-in feature&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/advanced-load-balancing/advanced-load-balancing.ipynb&quot;&gt;Advanced load balancing&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Zero trust, keyless approach with manage identities and Azure API Management security features&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;To ensure resilience, the request is distributed to two or more endpoints with a custom policy&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Load balances the requests to increase performance with a custom policy&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/response-streaming/response-streaming.ipynb&quot;&gt;Response streaming&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Zero trust, keyless approach with manage identities and Azure API Management security features&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;To get responses sooner, you can &#39;stream&#39; the completion as it&#39;s being generated&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/vector-searching/vector-searching.ipynb&quot;&gt;Vector searching&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Zero trust, keyless approach with manage identities and Azure API Management security features&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;To ensure resilience, the request is distributed to two or more endpoints with the built-in feature&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Load balances the requests to increase performance with the built-in feature&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/built-in-logging/built-in-logging.ipynb&quot;&gt;Built-in logging&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Zero trust, keyless approach with manage identities and Azure API Management security features&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;To ensure resilience, the request is distributed to two or more endpoints with the built-in feature&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Load balances the requests to increase performance with the built-in feature&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Requests are logged to enable monitoring, alerting and automatic remediation&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Relation between Azure API Management subscription and token consumption allows cost control&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/slm-self-hosting/slm-self-hosting.ipynb&quot;&gt;SLM self-hosting&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Self hosting the model might improve the security posture with network restrictions&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Performance might be improved with full control to the self-hosted model&quot;&gt;⭐&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Check the &lt;a href=&quot;https://learn.microsoft.com/azure/well-architected/service-guides/azure-openai&quot;&gt;Azure Well-Architected Framework perspective on Azure OpenAI Service&lt;/a&gt; for aditional guidance.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;🎒 Show and tell&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Install the &lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=evilz.vscode-reveal&quot;&gt;VS Code Reveal extension&lt;/a&gt;, open AI-GATEWAY.md and click on &#39;slides&#39; at the botton to present the AI Gateway without leaving VS Code. Or just open the &lt;a href=&quot;https://view.officeapps.live.com/op/view.aspx?src=https%3A%2F%2Fraw.githubusercontent.com%2FAzure-Samples%2FAI-Gateway%2Fmain%2FAI-GATEWAY.pptx&amp;amp;wdOrigin=BROWSELINK&quot;&gt;AI-GATEWAY.pptx&lt;/a&gt; for a plain old PowerPoint experience.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;🥇 Other resources&lt;/h2&gt; 
&lt;p&gt;Numerous reference architectures, best practices and starter kits are available on this topic. Please refer to the resources provided if you need comprehensive solutions or a landing zone to initiate your project. We suggest leveraging the AI-Gateway labs to discover additional capabilities that can be integrated into the reference architectures.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genai-gateway&quot;&gt;GenAI Gateway Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/apim/genai/sample-app&quot;&gt;Azure OpenAI&amp;nbsp;+&amp;nbsp;APIM Sample&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://techcommunity.microsoft.com/t5/apps-on-azure-blog/ai-api-better-together-benefits-amp-best-practices-using-apis/ba-p/4157120&quot;&gt;AI+API better together: Benefits &amp;amp; Best Practices using APIs for AI workloads&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genai-gateway&quot;&gt;Designing and implementing a gateway solution with Azure OpenAI resources&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Azure/aoai-apim&quot;&gt;Azure OpenAI Using PTUs/TPMs With API Management - Using the Scaling Special Sauce&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/AzureOpenAI-with-APIM&quot;&gt;Manage Azure OpenAI using APIM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Azure/enterprise-azureai&quot;&gt;Setting up Azure OpenAI as a central capability with Azure API Management&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Azure/intro-to-intelligent-apps&quot;&gt;Introduction to Building AI Apps&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We believe that there may be valuable content that we are currently unaware of. We would greatly appreciate any suggestions or recommendations to enhance this list.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;🌐 WW GBB initiative&lt;/h3&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/gbb.png&quot; alt=&quot;GBB&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;Disclaimer&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] This software is provided for demonstration purposes only. It is not intended to be relied upon for any purpose. The creators of this software make no representations or warranties of any kind, express or implied, about the completeness, accuracy, reliability, suitability or availability with respect to the software or the information, products, services, or related graphics contained in the software for any purpose. Any reliance you place on such information is therefore strictly at your own risk.&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>panaversity/learn-agentic-ai</title>
      <link>https://github.com/panaversity/learn-agentic-ai</link>
      <description>&lt;p&gt;Learn Agentic AI using OpenAI Agents SDK, Memory, MCP, Knowledge Graphs, LangGraph, and Autogen&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Learn Agentic AI&lt;/h1&gt; 
&lt;p&gt;This repo is part of the &lt;a href=&quot;https://docs.google.com/document/d/15usu1hkrrRLRjcq_3nCTT-0ljEcgiC44iSdvdqrCprk/edit?usp=sharing&quot;&gt;Panaversity Certified Agentic &amp;amp; Robotic AI Engineer&lt;/a&gt; program. It covers AI-201 and AI-202 courses.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/panaversity/learn-agentic-ai/main/toptrend.webp&quot; alt=&quot;Agentic AI Top Trend&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;Watch The NVIDIA CEO Jensen Huang Keynote at CES 2025&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=k82RwXqZHY8&quot; title=&quot;NVIDIA CEO Jensen Huang Keynote at CES 2025&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/panaversity/learn-agentic-ai/main/hr.jpeg&quot; alt=&quot;HR for Agents&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Reference:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/alexwang2911_aiagents-robotics-technology-activity-7282829390445453314-QLeS&quot;&gt;https://www.linkedin.com/posts/alexwang2911_aiagents-robotics-technology-activity-7282829390445453314-QLeS&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;AI-201: Fundamentals of Agentic AI - From Foundations to Autonomous Agents&lt;/h3&gt; 
&lt;p&gt;AI 201 Fundamentals of Agentic AI we cover chapters: 00-06&lt;/p&gt; 
&lt;p&gt;Kickstart your journey into Agentic AI! This course provides a rapid yet comprehensive introduction to Conversational, Generative, and Agentic AI. You&#39;ll master the foundational concepts using &lt;strong&gt;OpenAI Agents SDK&lt;/strong&gt;, then immediately build practical Conversational AI applications to understand human-AI interaction firsthand. The focus quickly shifts to Agentic Design Patterns, which you&#39;ll implement using OpenAI Agents SDK to create truly autonomous AI agents. You&#39;ll become proficient with OpenAI Agents SDK, developing agents ready for real-world tasks. Furthermore, you&#39;ll gain the unique skills to construct Model Context Protocol (MCP) servers and agents, enabling you to build next-generation augmented LLMs. Finally, we&#39;ll explore the groundbreaking potential of Agentic Payments, envisioning the future of AI in finance.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL0vKVrkG4hWovpr0FX6Gs-06hfsPDEUe6&quot;&gt;AI-201 Video Playlist&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Note: These videos are for additional learning, and do not cover all the material taught in the onsite classes.&lt;/p&gt; 
&lt;h3&gt;AI-202: Advanced Agentic AI Engineering - Master Enterprise-Scale AI Agent Development&lt;/h3&gt; 
&lt;p&gt;AI 202 Advanced Agentic AI we cover chapters: 06, 7a, 8, 8a, 9, 9a, 10, 10a, 11, and 12&lt;/p&gt; 
&lt;p&gt;Ready to engineer truly sophisticated AI agent systems? AI-202 builds upon your AI-201 foundation to propel you into advanced Agentic AI engineering. You&#39;ll master powerful frameworks like Microsoft AutoGen to construct complex agents for intricate tasks and advanced decision-making. Focusing on Agent-to-Agent communication and orchestration, you&#39;ll develop enterprise-ready multi-agent solutions. You&#39;ll build robust Model Context Protocol (MCP) servers, and then craft dynamic, user-centric agentic frontends with Next.js and TypeScript. The course culminates in a professional project where you&#39;ll design and deploy a complete enterprise-grade agentic solution, showcasing your mastery of cutting-edge AI technologies and your readiness for the forefront of the field.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>salesforce/LAVIS</title>
      <link>https://github.com/salesforce/LAVIS</link>
      <description>&lt;p&gt;LAVIS - A One-stop Library for Language-Vision Intelligence&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; &lt;br&gt; &lt;img src=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/docs/_static/logo_final.png&quot; width=&quot;400&quot;&gt; &lt;br&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://github.com/salesforce/LAVIS/releases&quot;&gt;&lt;img alt=&quot;Latest Release&quot; src=&quot;https://img.shields.io/github/release/salesforce/LAVIS.svg?sanitize=true&quot;&gt;&lt;/a&gt; 
 &lt;a href=&quot;https://opensource.salesforce.com/LAVIS/index.html&quot;&gt; &lt;img alt=&quot;docs&quot; src=&quot;https://github.com/salesforce/LAVIS/actions/workflows/docs.yaml/badge.svg?sanitize=true&quot;&gt; &lt;/a&gt;
 &lt;a href=&quot;https://opensource.org/licenses/BSD-3-Clause&quot;&gt; &lt;img alt=&quot;license&quot; src=&quot;https://img.shields.io/badge/License-BSD_3--Clause-blue.svg?sanitize=true&quot;&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://pepy.tech/project/salesforce-lavis&quot;&gt; &lt;img alt=&quot;Downloads&quot; src=&quot;https://pepy.tech/badge/salesforce-lavis&quot;&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://opensource.salesforce.com/LAVIS//latest/benchmark.html&quot;&gt;Benchmark&lt;/a&gt;, 
 &lt;a href=&quot;https://arxiv.org/abs/2209.09019&quot;&gt;Technical Report&lt;/a&gt;, 
 &lt;a href=&quot;https://opensource.salesforce.com/LAVIS//latest/index.html&quot;&gt;Documentation&lt;/a&gt;, 
 &lt;a href=&quot;https://github.com/salesforce/LAVIS/tree/main/examples&quot;&gt;Jupyter Notebook Examples&lt;/a&gt;, 
 &lt;a href=&quot;https://blog.salesforceairesearch.com/lavis-language-vision-library/&quot;&gt;Blog&lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;LAVIS - A Library for Language-Vision Intelligence&lt;/h1&gt; 
&lt;h2&gt;What&#39;s New: 🎉&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Model Release] November 2023, released implementation of &lt;strong&gt;X-InstructBLIP&lt;/strong&gt; &lt;br&gt; &lt;a href=&quot;https://arxiv.org/pdf/2311.18799.pdf&quot;&gt;Paper&lt;/a&gt;, &lt;a href=&quot;https://github.com/salesforce/LAVIS/tree/main/projects/xinstructblip&quot;&gt;Project Page&lt;/a&gt;, &lt;a href=&quot;https://artemisp.github.io/X-InstructBLIP-page/&quot;&gt;Website&lt;/a&gt;, &lt;a href=&quot;https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/xinstructblip/demo/run_demo.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A simple, yet effective, cross-modality framework built atop frozen LLMs that allows the integration of various modalities (image, video, audio, 3D) without extensive modality-specific customization.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Model Release] July 2023, released implementation of &lt;strong&gt;BLIP-Diffusion&lt;/strong&gt; &lt;br&gt; &lt;a href=&quot;https://arxiv.org/abs/2305.06500&quot;&gt;Paper&lt;/a&gt;, &lt;a href=&quot;https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion&quot;&gt;Project Page&lt;/a&gt;, &lt;a href=&quot;https://dxli94.github.io/BLIP-Diffusion-website/&quot;&gt;Website&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A text-to-image generation model that trains 20x than DreamBooth. Also facilitates zero-shot subject-driven generation and editing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Model Release] May 2023, released implementation of &lt;strong&gt;InstructBLIP&lt;/strong&gt; &lt;br&gt; &lt;a href=&quot;https://arxiv.org/abs/2305.06500&quot;&gt;Paper&lt;/a&gt;, &lt;a href=&quot;https://github.com/salesforce/LAVIS/tree/main/projects/instructblip&quot;&gt;Project Page&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A new vision-language instruction-tuning framework using BLIP-2 models, achieving state-of-the-art zero-shot generalization performance on a wide range of vision-language tasks.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Model Release] Jan 2023, released implementation of &lt;strong&gt;BLIP-2&lt;/strong&gt; &lt;br&gt; &lt;a href=&quot;https://arxiv.org/abs/2301.12597&quot;&gt;Paper&lt;/a&gt;, &lt;a href=&quot;https://github.com/salesforce/LAVIS/tree/main/projects/blip2&quot;&gt;Project Page&lt;/a&gt;, &lt;a href=&quot;https://colab.research.google.com/github/salesforce/LAVIS/blob/main/examples/blip2_instructed_generation.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A generic and efficient pre-training strategy that easily harvests development of pretrained vision models and large language models (LLMs) for vision-language pretraining. BLIP-2 beats Flamingo on zero-shot VQAv2 (&lt;strong&gt;65.0&lt;/strong&gt; vs &lt;strong&gt;56.3&lt;/strong&gt;), establishing new state-of-the-art on zero-shot captioning (on NoCaps &lt;strong&gt;121.6&lt;/strong&gt; CIDEr score vs previous best &lt;strong&gt;113.2&lt;/strong&gt;). In addition, equipped with powerful LLMs (e.g. OPT, FlanT5), BLIP-2 also unlocks the new &lt;strong&gt;zero-shot instructed vision-to-language generation&lt;/strong&gt; capabilities for various interesting applications!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;Jan 2023, LAVIS is now available on &lt;a href=&quot;https://pypi.org/project/salesforce-lavis/&quot;&gt;PyPI&lt;/a&gt; for installation!&lt;/li&gt; 
 &lt;li&gt;[Model Release] Dec 2022, released implementation of &lt;strong&gt;Img2LLM-VQA&lt;/strong&gt; (&lt;strong&gt;CVPR 2023&lt;/strong&gt;, &lt;em&gt;&quot;From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models&quot;&lt;/em&gt;, by Jiaxian Guo et al) &lt;br&gt; &lt;a href=&quot;https://arxiv.org/pdf/2212.10846.pdf&quot;&gt;Paper&lt;/a&gt;, &lt;a href=&quot;https://github.com/salesforce/LAVIS/tree/main/projects/img2llm-vqa&quot;&gt;Project Page&lt;/a&gt;, &lt;a href=&quot;https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/img2llm-vqa/img2llm_vqa.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A plug-and-play module that enables off-the-shelf use of Large Language Models (LLMs) for visual question answering (VQA). Img2LLM-VQA surpasses Flamingo on zero-shot VQA on VQAv2 (61.9 vs 56.3), while in contrast requiring no end-to-end training!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Model Release] Oct 2022, released implementation of &lt;strong&gt;PNP-VQA&lt;/strong&gt; (&lt;strong&gt;EMNLP Findings 2022&lt;/strong&gt;, &lt;em&gt;&quot;Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training&quot;&lt;/em&gt;, by Anthony T.M.H. et al), &lt;br&gt; &lt;a href=&quot;https://arxiv.org/abs/2210.08773&quot;&gt;Paper&lt;/a&gt;, &lt;a href=&quot;https://github.com/salesforce/LAVIS/tree/main/projects/pnp-vqa&quot;&gt;Project Page&lt;/a&gt;, &lt;a href=&quot;https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/pnp-vqa/pnp_vqa.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A modular zero-shot VQA framework that requires no PLMs training, achieving SoTA zero-shot VQA performance.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Technical Report and Citing LAVIS&lt;/h2&gt; 
&lt;p&gt;You can find more details in our &lt;a href=&quot;https://arxiv.org/abs/2209.09019&quot;&gt;technical report&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;If you&#39;re using LAVIS in your research or applications, please cite it using this BibTeX&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inproceedings{li-etal-2023-lavis,
    title = &quot;{LAVIS}: A One-stop Library for Language-Vision Intelligence&quot;,
    author = &quot;Li, Dongxu  and
      Li, Junnan  and
      Le, Hung  and
      Wang, Guangsen  and
      Savarese, Silvio  and
      Hoi, Steven C.H.&quot;,
    booktitle = &quot;Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)&quot;,
    month = jul,
    year = &quot;2023&quot;,
    address = &quot;Toronto, Canada&quot;,
    publisher = &quot;Association for Computational Linguistics&quot;,
    url = &quot;https://aclanthology.org/2023.acl-demo.3&quot;,
    pages = &quot;31--41&quot;,
    abstract = &quot;We introduce LAVIS, an open-source deep learning library for LAnguage-VISion research and applications. LAVIS aims to serve as a one-stop comprehensive library that brings recent advancements in the language-vision field accessible for researchers and practitioners, as well as fertilizing future research and development. It features a unified interface to easily access state-of-the-art image-language, video-language models and common datasets. LAVIS supports training, evaluation and benchmarking on a rich variety of tasks, including multimodal classification, retrieval, captioning, visual question answering, dialogue and pre-training. In the meantime, the library is also highly extensible and configurable, facilitating future development and customization. In this technical report, we describe design principles, key components and functionalities of the library, and also present benchmarking results across common language-vision tasks.&quot;,
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#installation&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#getting-started&quot;&gt;Getting Started&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#model-zoo&quot;&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#image-captioning&quot;&gt;Image Captioning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#visual-question-answering-vqa&quot;&gt;Visual question answering (VQA)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#unified-feature-extraction-interface&quot;&gt;Unified Feature Extraction Interface&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#load-datasets&quot;&gt;Load Datasets&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#jupyter-notebook-examples&quot;&gt;Jupyter Notebook Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#resources-and-tools&quot;&gt;Resources and Tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#documentations&quot;&gt;Documentations&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#ethical-and-responsible-use&quot;&gt;Ethical and Responsible Use&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#technical-report-and-citing-lavis&quot;&gt;Technical Report and Citing LAVIS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#license&quot;&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;LAVIS is a Python deep learning library for LAnguage-and-VISion intelligence research and applications. This library aims to provide engineers and researchers with a one-stop solution to rapidly develop models for their specific multimodal scenarios, and benchmark them across standard and customized datasets. It features a unified interface design to access&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;10+&lt;/strong&gt; tasks (retrieval, captioning, visual question answering, multimodal classification etc.);&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;20+&lt;/strong&gt; datasets (COCO, Flickr, Nocaps, Conceptual Commons, SBU, etc.);&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;30+&lt;/strong&gt; pretrained weights of state-of-the-art foundation language-vision models and their task-specific adaptations, including &lt;a href=&quot;https://arxiv.org/pdf/2107.07651.pdf&quot;&gt;ALBEF&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2201.12086.pdf&quot;&gt;BLIP&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2112.09583.pdf&quot;&gt;ALPRO&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2103.00020.pdf&quot;&gt;CLIP&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;br&gt; &lt;img src=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/assets/demo-6.png&quot;&gt; &lt;br&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Key features of LAVIS include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified and Modular Interface&lt;/strong&gt;: facilitating to easily leverage and repurpose existing modules (datasets, models, preprocessors), also to add new modules.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy Off-the-shelf Inference and Feature Extraction&lt;/strong&gt;: readily available pre-trained models let you take advantage of state-of-the-art multimodal understanding and generation capabilities on your own data.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reproducible Model Zoo and Training Recipes&lt;/strong&gt;: easily replicate and extend state-of-the-art models on existing and new tasks.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dataset Zoo and Automatic Downloading Tools&lt;/strong&gt;: it can be a hassle to prepare the many language-vision datasets. LAVIS provides automatic downloading scripts to help prepare a large variety of datasets and their annotations.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The following table shows the supported tasks, datasets and models in our library. This is a continuing effort and we are working on further growing the list.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;Tasks&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Supported Models&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Supported Datasets&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Image-text Pre-training&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;ALBEF, BLIP&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;COCO, VisualGenome, SBU ConceptualCaptions&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Image-text Retrieval&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;ALBEF, BLIP, CLIP&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;COCO, Flickr30k&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Text-image Retrieval&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;ALBEF, BLIP, CLIP&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;COCO, Flickr30k&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Visual Question Answering&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;ALBEF, BLIP&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;VQAv2, OKVQA, A-OKVQA&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Image Captioning&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;BLIP&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;COCO, NoCaps&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Image Classification&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;CLIP&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;ImageNet&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Natural Language Visual Reasoning (NLVR)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;ALBEF, BLIP&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;NLVR2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Visual Entailment (VE)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;ALBEF&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;SNLI-VE&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Visual Dialogue&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;BLIP&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;VisDial&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Video-text Retrieval&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;BLIP, ALPRO&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;MSRVTT, DiDeMo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Text-video Retrieval&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;BLIP, ALPRO&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;MSRVTT, DiDeMo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Video Question Answering (VideoQA)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;BLIP, ALPRO&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;MSRVTT, MSVD&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Video Dialogue&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;VGD-GPT&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;AVSD&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Multimodal Feature Extraction&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;ALBEF, CLIP, BLIP, ALPRO&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;customized&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Text-to-image Generation&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;[COMING SOON]&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;(Optional) Creating conda environment&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;conda create -n lavis python=3.8
conda activate lavis
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;install from &lt;a href=&quot;https://pypi.org/project/salesforce-lavis/&quot;&gt;PyPI&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install salesforce-lavis
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Or, for development, you may build from source&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/salesforce/LAVIS.git
cd LAVIS
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Model Zoo&lt;/h3&gt; 
&lt;p&gt;Model zoo summarizes supported models in LAVIS, to view:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from lavis.models import model_zoo
print(model_zoo)
# ==================================================
# Architectures                  Types
# ==================================================
# albef_classification           ve
# albef_feature_extractor        base
# albef_nlvr                     nlvr
# albef_pretrain                 base
# albef_retrieval                coco, flickr
# albef_vqa                      vqav2
# alpro_qa                       msrvtt, msvd
# alpro_retrieval                msrvtt, didemo
# blip_caption                   base_coco, large_coco
# blip_classification            base
# blip_feature_extractor         base
# blip_nlvr                      nlvr
# blip_pretrain                  base
# blip_retrieval                 coco, flickr
# blip_vqa                       vqav2, okvqa, aokvqa
# clip_feature_extractor         ViT-B-32, ViT-B-16, ViT-L-14, ViT-L-14-336, RN50
# clip                           ViT-B-32, ViT-B-16, ViT-L-14, ViT-L-14-336, RN50
# gpt_dialogue                   base
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Let’s see how to use models in LAVIS to perform inference on example data. We first load a sample image from local.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch
from PIL import Image
# setup device to use
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
# load sample image
raw_image = Image.open(&quot;docs/_static/merlion.png&quot;).convert(&quot;RGB&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This example image shows &lt;a href=&quot;https://en.wikipedia.org/wiki/Merlion&quot;&gt;Merlion park&lt;/a&gt; (&lt;a href=&quot;https://theculturetrip.com/asia/singapore/articles/what-exactly-is-singapores-merlion-anyway/&quot;&gt;source&lt;/a&gt;), a landmark in Singapore.&lt;/p&gt; 
&lt;h3&gt;Image Captioning&lt;/h3&gt; 
&lt;p&gt;In this example, we use the BLIP model to generate a caption for the image. To make inference even easier, we also associate each pre-trained model with its preprocessors (transforms), accessed via &lt;code&gt;load_model_and_preprocess()&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch
from lavis.models import load_model_and_preprocess
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
# loads BLIP caption base model, with finetuned checkpoints on MSCOCO captioning dataset.
# this also loads the associated image processors
model, vis_processors, _ = load_model_and_preprocess(name=&quot;blip_caption&quot;, model_type=&quot;base_coco&quot;, is_eval=True, device=device)
# preprocess the image
# vis_processors stores image transforms for &quot;train&quot; and &quot;eval&quot; (validation / testing / inference)
image = vis_processors[&quot;eval&quot;](raw_image).unsqueeze(0).to(device)
# generate caption
model.generate({&quot;image&quot;: image})
# [&#39;a large fountain spewing water into the air&#39;]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Visual question answering (VQA)&lt;/h3&gt; 
&lt;p&gt;BLIP model is able to answer free-form questions about images in natural language. To access the VQA model, simply replace the &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;model_type&lt;/code&gt; arguments passed to &lt;code&gt;load_model_and_preprocess()&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from lavis.models import load_model_and_preprocess
model, vis_processors, txt_processors = load_model_and_preprocess(name=&quot;blip_vqa&quot;, model_type=&quot;vqav2&quot;, is_eval=True, device=device)
# ask a random question.
question = &quot;Which city is this photo taken?&quot;
image = vis_processors[&quot;eval&quot;](raw_image).unsqueeze(0).to(device)
question = txt_processors[&quot;eval&quot;](question)
model.predict_answers(samples={&quot;image&quot;: image, &quot;text_input&quot;: question}, inference_method=&quot;generate&quot;)
# [&#39;singapore&#39;]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Unified Feature Extraction Interface&lt;/h3&gt; 
&lt;p&gt;LAVIS provides a unified interface to extract features from each architecture. To extract features, we load the feature extractor variants of each model. The multimodal feature can be used for multimodal classification. The low-dimensional unimodal features can be used to compute cross-modal similarity.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from lavis.models import load_model_and_preprocess
model, vis_processors, txt_processors = load_model_and_preprocess(name=&quot;blip_feature_extractor&quot;, model_type=&quot;base&quot;, is_eval=True, device=device)
caption = &quot;a large fountain spewing water into the air&quot;
image = vis_processors[&quot;eval&quot;](raw_image).unsqueeze(0).to(device)
text_input = txt_processors[&quot;eval&quot;](caption)
sample = {&quot;image&quot;: image, &quot;text_input&quot;: [text_input]}

features_multimodal = model.extract_features(sample)
print(features_multimodal.multimodal_embeds.shape)
# torch.Size([1, 12, 768]), use features_multimodal[:,0,:] for multimodal classification tasks

features_image = model.extract_features(sample, mode=&quot;image&quot;)
features_text = model.extract_features(sample, mode=&quot;text&quot;)
print(features_image.image_embeds.shape)
# torch.Size([1, 197, 768])
print(features_text.text_embeds.shape)
# torch.Size([1, 12, 768])

# low-dimensional projected features
print(features_image.image_embeds_proj.shape)
# torch.Size([1, 197, 256])
print(features_text.text_embeds_proj.shape)
# torch.Size([1, 12, 256])
similarity = features_image.image_embeds_proj[:,0,:] @ features_text.text_embeds_proj[:,0,:].t()
print(similarity)
# tensor([[0.2622]])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Load Datasets&lt;/h3&gt; 
&lt;p&gt;LAVIS inherently supports a wide variety of common language-vision datasets by providing &lt;a href=&quot;https://opensource.salesforce.com/LAVIS//latest/benchmark&quot;&gt;automatic download tools&lt;/a&gt; to help download and organize these datasets. After downloading, to load the datasets, use the following code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from lavis.datasets.builders import dataset_zoo
dataset_names = dataset_zoo.get_names()
print(dataset_names)
# [&#39;aok_vqa&#39;, &#39;coco_caption&#39;, &#39;coco_retrieval&#39;, &#39;coco_vqa&#39;, &#39;conceptual_caption_12m&#39;,
#  &#39;conceptual_caption_3m&#39;, &#39;didemo_retrieval&#39;, &#39;flickr30k&#39;, &#39;imagenet&#39;, &#39;laion2B_multi&#39;,
#  &#39;msrvtt_caption&#39;, &#39;msrvtt_qa&#39;, &#39;msrvtt_retrieval&#39;, &#39;msvd_caption&#39;, &#39;msvd_qa&#39;, &#39;nlvr&#39;,
#  &#39;nocaps&#39;, &#39;ok_vqa&#39;, &#39;sbu_caption&#39;, &#39;snli_ve&#39;, &#39;vatex_caption&#39;, &#39;vg_caption&#39;, &#39;vg_vqa&#39;]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After downloading the images, we can use &lt;code&gt;load_dataset()&lt;/code&gt; to obtain the dataset.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from lavis.datasets.builders import load_dataset
coco_dataset = load_dataset(&quot;coco_caption&quot;)
print(coco_dataset.keys())
# dict_keys([&#39;train&#39;, &#39;val&#39;, &#39;test&#39;])
print(len(coco_dataset[&quot;train&quot;]))
# 566747
print(coco_dataset[&quot;train&quot;][0])
# {&#39;image&#39;: &amp;lt;PIL.Image.Image image mode=RGB size=640x480&amp;gt;,
#  &#39;text_input&#39;: &#39;A woman wearing a net on her head cutting a cake. &#39;,
#  &#39;image_id&#39;: 0}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you already host a local copy of the dataset, you can pass in the &lt;code&gt;vis_path&lt;/code&gt; argument to change the default location to load images.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;coco_dataset = load_dataset(&quot;coco_caption&quot;, vis_path=YOUR_LOCAL_PATH)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Jupyter Notebook Examples&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://github.com/salesforce/LAVIS/tree/main/examples&quot;&gt;examples&lt;/a&gt; for more inference examples, e.g. captioning, feature extraction, VQA, GradCam, zeros-shot classification.&lt;/p&gt; 
&lt;h2&gt;Resources and Tools&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;: see &lt;a href=&quot;https://opensource.salesforce.com/LAVIS//latest/benchmark&quot;&gt;Benchmark&lt;/a&gt; for instructions to evaluate and train supported models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dataset Download and Browsing&lt;/strong&gt;: see &lt;a href=&quot;https://opensource.salesforce.com/LAVIS//latest/benchmark&quot;&gt;Dataset Download&lt;/a&gt; for instructions and automatic tools on download common language-vision datasets.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GUI Demo&lt;/strong&gt;: to run the demo locally, run &lt;code&gt;bash run_scripts/run_demo.sh&lt;/code&gt; and then follow the instruction on the prompts to view in browser. A web demo is coming soon.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Documentations&lt;/h2&gt; 
&lt;p&gt;For more details and advanced usages, please refer to &lt;a href=&quot;https://opensource.salesforce.com/LAVIS//latest/index.html#&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Ethical and Responsible Use&lt;/h2&gt; 
&lt;p&gt;We note that models in LAVIS provide no guarantees on their multimodal abilities; incorrect or biased predictions may be observed. In particular, the datasets and pretrained models utilized in LAVIS may contain socioeconomic biases which could result in misclassification and other unwanted behaviors such as offensive or inappropriate speech. We strongly recommend that users review the pre-trained models and overall system in LAVIS before practical adoption. We plan to improve the library by investigating and mitigating these potential biases and inappropriate behaviors in the future.&lt;/p&gt; 
&lt;h2&gt;Contact us&lt;/h2&gt; 
&lt;p&gt;If you have any questions, comments or suggestions, please do not hesitate to contact us at &lt;a href=&quot;mailto:lavis@salesforce.com&quot;&gt;lavis@salesforce.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/LICENSE.txt&quot;&gt;BSD 3-Clause License&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MLEveryday/practicalAI-cn</title>
      <link>https://github.com/MLEveryday/practicalAI-cn</link>
      <description>&lt;p&gt;AI实战-practicalAI 中文版&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI实战-&lt;a href=&quot;https://github.com/LisonEvf/practicalAI-cn&quot;&gt;practicalAI&lt;/a&gt; 中文版&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/launch-Google%20Colab-orange.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/LisonEvf/practicalAI-cn/raw/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-MIT-brightgreen.svg?sanitize=true&quot; alt=&quot;MIT&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/GokuMohandas&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Author-GokuMohandas-blue.svg?sanitize=true&quot; alt=&quot;Author&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/MLEveryday/practicalAI-cn&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Fork-MLEveryday/practicalAI--cn-yellow.svg?sanitize=true&quot; alt=&quot;Fork&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;让你有能力使用机器学习从数据中获取有价值的见解。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🔥 使用 &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt; 实现基本的机器学习算法和深度神经网络。&lt;/li&gt; 
 &lt;li&gt;🖥️ 不需要任何设置，在浏览器中使用 &lt;a href=&quot;https://colab.research.google.com/&quot;&gt;Google Colab&lt;/a&gt; 运行所有程序。&lt;/li&gt; 
 &lt;li&gt;📦 不仅仅是教程，而是学习产品级的面向对象机器学习编程。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Notebooks&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;基础&lt;/th&gt; 
   &lt;th&gt;深度学习&lt;/th&gt; 
   &lt;th&gt;进阶&lt;/th&gt; 
   &lt;th&gt;主题&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;📓 &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/00_Notebooks.ipynb&quot;&gt;Notebooks&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;🔥 &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/07_PyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;📚 &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/14_Advanced_RNNs.ipynb&quot;&gt;高级循环神经网络 Advanced RNNs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;📸 &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/15_Computer_Vision.ipynb&quot;&gt;计算机视觉 Computer Vision&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🐍 &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/01_Python.ipynb&quot;&gt;Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;🎛️ &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/08_Multilayer_Perceptron.ipynb&quot;&gt;多层感知 Multilayer Perceptrons&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;🏎️ Highway and Residual Networks&lt;/td&gt; 
   &lt;td&gt;⏰ 时间序列分析 Time Series Analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🔢 &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/02_NumPy.ipynb&quot;&gt;NumPy&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;🔎 &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/09_Data_and_Models.ipynb&quot;&gt;数据和模型 Data &amp;amp; Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;🔮 自编码器 Autoencoders&lt;/td&gt; 
   &lt;td&gt;🏘️ Topic Modeling&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🐼 &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/03_Pandas.ipynb&quot;&gt;Pandas&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;📦 &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/10_Object_Oriented_ML.ipynb&quot;&gt;面向对象的机器学习 Object-Oriented ML&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;🎭 生成对抗网络 Generative Adversarial Networks&lt;/td&gt; 
   &lt;td&gt;🛒 推荐系统 Recommendation Systems&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;📈 &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/04_Linear_Regression.ipynb&quot;&gt;线性回归 Linear Regression&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;🖼️ &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/11_Convolutional_Neural_Networks.ipynb&quot;&gt;卷积神经网络 Convolutional Neural Networks&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;🐝 空间变换模型 Spatial Transformer Networks&lt;/td&gt; 
   &lt;td&gt;🗣️ 预训练语言模型 Pretrained Language Modeling&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;📊 &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/05_Logistic_Regression.ipynb&quot;&gt;逻辑回归 Logistic Regression&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;📝 &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/12_Embeddings.ipynb&quot;&gt;嵌入层 Embeddings&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;🤷 多任务学习 Multitask Learning&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🌳 &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/06_Random_Forests.ipynb&quot;&gt;随机森林 Random Forests&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;📗 &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/13_Recurrent_Neural_Networks.ipynb&quot;&gt;递归神经网络 Recurrent Neural Networks&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;🎯 Low Shot Learning&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;💥 k-均值聚类 KMeans Clustering&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;🍒 强化学习 Reinforcement Learning&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;查看 notebooks&lt;/h2&gt; 
&lt;p&gt;如果不需要运行 notebooks，使用 Jupyter nbviewer 就可以方便地查看它们。&lt;/p&gt; 
&lt;p&gt;将 &lt;code&gt;https://github.com/&lt;/code&gt; 替换为 &lt;code&gt;https://nbviewer.jupyter.org/github/&lt;/code&gt; ，或者打开 &lt;code&gt;https://nbviewer.jupyter.org&lt;/code&gt; 并输入 notebook 的 URL。&lt;/p&gt; 
&lt;h2&gt;运行 notebooks&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;在本项目的 &lt;a href=&quot;https://raw.githubusercontent.com/MLEveryday/practicalAI-cn/master/notebooks/&quot;&gt;&lt;code&gt;notebooks&lt;/code&gt;&lt;/a&gt; 文件夹获取 notebook；&lt;/li&gt; 
 &lt;li&gt;你可以在 Google Colab（推荐）或本地电脑运行这些 notebook；&lt;/li&gt; 
 &lt;li&gt;点击一个 notebook，然后替换URL地址中 &lt;code&gt;https://github.com/&lt;/code&gt; 为 &lt;code&gt;https://colab.research.google.com/github/&lt;/code&gt; ，或者使用这个 &lt;a href=&quot;https://chrome.google.com/webstore/detail/open-in-colab/iogfkhleblhcpcekbiedikdehleodpjo&quot;&gt;Chrome扩展&lt;/a&gt; 一键完成；&lt;/li&gt; 
 &lt;li&gt;登录你自己的 Google 账户；&lt;/li&gt; 
 &lt;li&gt;点击工具栏上的 &lt;code&gt;复制到云端硬盘&lt;/code&gt;，会在一个新的标签页打开 notebook；&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/MLEveryday/practicalAI-cn/master/images/copy_to_drive.png&quot;&gt; 
&lt;ol start=&quot;5&quot;&gt; 
 &lt;li&gt;通过去掉标题中的&lt;code&gt;副本&lt;/code&gt;完成 notebook 重命名；&lt;/li&gt; 
 &lt;li&gt;运行代码、修改等，所有这些都会自动保存到你的个人 Google Drive。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;贡献 notebooks&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;修改后下载 Google Colab notebook 为 .ipynb 文件；&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/MLEveryday/practicalAI-cn/master/images/download_ipynb.png&quot;&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;转到 &lt;a href=&quot;https://github.com/LisonEvf/practicalAI-cn/tree/master/notebooks&quot;&gt;https://github.com/LisonEvf/practicalAI-cn/tree/master/notebooks&lt;/a&gt; ；&lt;/li&gt; 
 &lt;li&gt;点击 &lt;code&gt;Upload files&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/MLEveryday/practicalAI-cn/master/images/upload.png&quot;&gt; 
&lt;ol start=&quot;5&quot;&gt; 
 &lt;li&gt;上传这个 .ipynb 文件；&lt;/li&gt; 
 &lt;li&gt;写一个详细详细的提交标题和说明；&lt;/li&gt; 
 &lt;li&gt;适当命名你的分支；&lt;/li&gt; 
 &lt;li&gt;点击 &lt;code&gt;Propose changes&lt;/code&gt;。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/MLEveryday/practicalAI-cn/master/images/commit.png&quot;&gt; 
&lt;h2&gt;贡献列表&lt;/h2&gt; 
&lt;p&gt;欢迎任何人参与和完善。&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Notebook&lt;/th&gt; 
   &lt;th&gt;译者&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;00_Notebooks.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/amusi&quot;&gt;@amusi&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;01_Python.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/amusi&quot;&gt;@amusi&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;02_NumPy.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/amusi&quot;&gt;@amusi&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;03_Pandas.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/amusi&quot;&gt;@amusi&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;04_Linear_Regression.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/jasonhhao&quot;&gt;@jasonhhao&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;05_Logistic_Regression.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/jasonhhao&quot;&gt;@jasonhhao&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;06_Random_Forests.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/jasonhhao&quot;&gt;@jasonhhao&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;07_PyTorch.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/amusi&quot;&gt;@amusi&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;08_Multilayer_Perceptron.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/zhyongquan&quot;&gt;@zhyongquan&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;09_Data_and_Models.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/zhyongquan&quot;&gt;@zhyongquan&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10_Object_Oriented_ML.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/zhyongquan&quot;&gt;@zhyongquan&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11_Convolutional_Neural_Networks.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12_Embeddings.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/wengJJ&quot;&gt;@wengJJ&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13_Recurrent_Neural_Networks.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14_Advanced_RNNs.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;15_Computer_Vision.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>causify-ai/kaizenflow</title>
      <link>https://github.com/causify-ai/kaizenflow</link>
      <description>&lt;p&gt;KaizenFlow is a framework for Bayesian reasoning and AI/ML stream computing&lt;/p&gt;&lt;hr&gt;&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/causify-ai/kaizenflow/master/#kaizen-technologies-internal-documentation&quot;&gt;Kaizen Technologies internal documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- tocstop --&gt; 
&lt;h1&gt;Kaizen Technologies internal documentation&lt;/h1&gt; 
&lt;p&gt;Welcome to the internal documentation pages of Kaizen Technologies.&lt;/p&gt; 
&lt;p&gt;To start exploring the repository refer to &lt;a href=&quot;https://raw.githubusercontent.com/causify-ai/kaizenflow/master/all.workflow.explanation.md&quot;&gt;workflows reference&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/AI-For-Beginners</title>
      <link>https://github.com/microsoft/AI-For-Beginners</link>
      <description>&lt;p&gt;12 Weeks, 24 Lessons, AI for All!&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/microsoft/AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub license&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/AI-For-Beginners/graphs/contributors/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/microsoft/AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub contributors&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/AI-For-Beginners/issues/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/microsoft/AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub issues&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/AI-For-Beginners/pulls/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr/microsoft/AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub pull-requests&quot;&gt;&lt;/a&gt; &lt;a href=&quot;http://makeapullrequest.com&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://GitHub.com/microsoft/AI-For-Beginners/watchers/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/watchers/microsoft/AI-For-Beginners.svg?style=social&amp;amp;label=Watch&quot; alt=&quot;GitHub watchers&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/AI-For-Beginners/network/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/microsoft/AI-For-Beginners.svg?style=social&amp;amp;label=Fork&quot; alt=&quot;GitHub forks&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/AI-For-Beginners/stargazers/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/microsoft/AI-For-Beginners.svg?style=social&amp;amp;label=Star&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://mybinder.org/v2/gh/microsoft/ai-for-beginners/HEAD&quot;&gt;&lt;img src=&quot;https://mybinder.org/badge_logo.svg?sanitize=true&quot; alt=&quot;Binder&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://gitter.im/Microsoft/ai-for-beginners?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&quot;&gt;&lt;img src=&quot;https://badges.gitter.im/Microsoft/ai-for-beginners.svg?sanitize=true&quot; alt=&quot;Gitter&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/zxKYvhSnVp?WT.mc_id=academic-000002-leestott&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/ByRwuEEgH4&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Artificial Intelligence for Beginners - A Curriculum&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/sketchnotes/ai-overview.png&quot; alt=&quot; Sketchnote by (@girlie_mac) &quot;&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;AI For Beginners - &lt;em&gt;Sketchnote by &lt;a href=&quot;https://twitter.com/girlie_mac&quot;&gt;@girlie_mac&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Explore the world of &lt;strong&gt;Artificial Intelligence&lt;/strong&gt; (AI) with our 12-week, 24-lesson curriculum! It includes practical lessons, quizzes, and labs. The curriculum is beginner-friendly and covers tools like TensorFlow and PyTorch, as well as ethics in AI&lt;/p&gt; 
&lt;h2&gt;What you will learn&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://soshnikov.com/courses/ai-for-beginners/mindmap.html&quot;&gt;Mindmap of the Course&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;In this curriculum, you will learn:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Different approaches to Artificial Intelligence, including the &quot;good old&quot; symbolic approach with &lt;strong&gt;Knowledge Representation&lt;/strong&gt; and reasoning (&lt;a href=&quot;https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence&quot;&gt;GOFAI&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Neural Networks&lt;/strong&gt; and &lt;strong&gt;Deep Learning&lt;/strong&gt;, which are at the core of modern AI. We will illustrate the concepts behind these important topics using code in two of the most popular frameworks - &lt;a href=&quot;http://Tensorflow.org&quot;&gt;TensorFlow&lt;/a&gt; and &lt;a href=&quot;http://pytorch.org&quot;&gt;PyTorch&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Neural Architectures&lt;/strong&gt; for working with images and text. We will cover recent models but may be a bit lacking in the state-of-the-art.&lt;/li&gt; 
 &lt;li&gt;Less popular AI approaches, such as &lt;strong&gt;Genetic Algorithms&lt;/strong&gt; and &lt;strong&gt;Multi-Agent Systems&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;What we will not cover in this curriculum:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href=&quot;https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Find all additional resources for this course in our Microsoft Learn collection&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;Business cases of using &lt;strong&gt;AI in Business&lt;/strong&gt;. Consider taking &lt;a href=&quot;https://docs.microsoft.com/learn/paths/introduction-ai-for-business-users/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Introduction to AI for business users&lt;/a&gt; learning path on Microsoft Learn, or &lt;a href=&quot;https://www.microsoft.com/ai/ai-business-school/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;AI Business School&lt;/a&gt;, developed in cooperation with &lt;a href=&quot;https://www.insead.edu/&quot;&gt;INSEAD&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Classic Machine Learning&lt;/strong&gt;, which is well described in our &lt;a href=&quot;http://github.com/Microsoft/ML-for-Beginners&quot;&gt;Machine Learning for Beginners Curriculum&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Practical AI applications built using &lt;strong&gt;&lt;a href=&quot;https://azure.microsoft.com/services/cognitive-services/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Cognitive Services&lt;/a&gt;&lt;/strong&gt;. For this, we recommend that you start with modules Microsoft Learn for &lt;a href=&quot;https://docs.microsoft.com/learn/paths/create-computer-vision-solutions-azure-cognitive-services/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;vision&lt;/a&gt;, &lt;a href=&quot;https://docs.microsoft.com/learn/paths/explore-natural-language-processing/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;natural language processing&lt;/a&gt;, &lt;strong&gt;&lt;a href=&quot;https://learn.microsoft.com/en-us/training/paths/develop-ai-solutions-azure-openai/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Generative AI with Azure OpenAI Service&lt;/a&gt;&lt;/strong&gt; and others.&lt;/li&gt; 
 &lt;li&gt;Specific ML &lt;strong&gt;Cloud Frameworks&lt;/strong&gt;, such as &lt;a href=&quot;https://azure.microsoft.com/services/machine-learning/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Azure Machine Learning&lt;/a&gt;, &lt;a href=&quot;https://learn.microsoft.com/en-us/training/paths/get-started-fabric/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Microsoft Fabric&lt;/a&gt;, or &lt;a href=&quot;https://docs.microsoft.com/learn/paths/data-engineer-azure-databricks?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Azure Databricks&lt;/a&gt;. Consider using &lt;a href=&quot;https://docs.microsoft.com/learn/paths/build-ai-solutions-with-azure-ml-service/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Build and operate machine learning solutions with Azure Machine Learning&lt;/a&gt; and &lt;a href=&quot;https://docs.microsoft.com/learn/paths/build-operate-machine-learning-solutions-azure-databricks/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Build and Operate Machine Learning Solutions with Azure Databricks&lt;/a&gt; learning paths.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Conversational AI&lt;/strong&gt; and &lt;strong&gt;Chat Bots&lt;/strong&gt;. There is a separate &lt;a href=&quot;https://docs.microsoft.com/learn/paths/create-conversational-ai-solutions/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Create conversational AI solutions&lt;/a&gt; learning path, and you can also refer to &lt;a href=&quot;https://soshnikov.com/azure/hello-bot-conversational-ai-on-microsoft-platform/&quot;&gt;this blog post&lt;/a&gt; for more detail.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep Mathematics&lt;/strong&gt; behind deep learning. For this, we would recommend &lt;a href=&quot;https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618&quot;&gt;Deep Learning&lt;/a&gt; by Ian Goodfellow, Yoshua Bengio and Aaron Courville, which is also available online at &lt;a href=&quot;https://www.deeplearningbook.org/&quot;&gt;https://www.deeplearningbook.org/&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For a gentle introduction to &lt;em&gt;AI in the Cloud&lt;/em&gt; topics you may consider taking the &lt;a href=&quot;https://docs.microsoft.com/learn/paths/get-started-with-artificial-intelligence-on-azure/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Get started with artificial intelligence on Azure&lt;/a&gt; Learning Path.&lt;/p&gt; 
&lt;h1&gt;Content&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Lesson Link&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;PyTorch/Keras/TensorFlow&lt;/th&gt; 
   &lt;th&gt;Lab&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;0&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/0-course-setup/setup.md&quot;&gt;Course Setup&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/0-course-setup/how-to-run.md&quot;&gt;Setup Your Development Environment&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;I&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/1-Intro/README.md&quot;&gt;&lt;strong&gt;Introduction to AI&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;01&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/1-Intro/README.md&quot;&gt;Introduction and History of AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;II&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;Symbolic AI&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;02&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/2-Symbolic/README.md&quot;&gt;Knowledge Representation and Expert Systems&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/2-Symbolic/Animals.ipynb&quot;&gt;Expert Systems&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/2-Symbolic/FamilyOntology.ipynb&quot;&gt;Ontology&lt;/a&gt; /&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/2-Symbolic/MSConceptGraph.ipynb&quot;&gt;Concept Graph&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;III&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/README.md&quot;&gt;&lt;strong&gt;Introduction to Neural Networks&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;03&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/03-Perceptron/README.md&quot;&gt;Perceptron&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/3-NeuralNetworks/03-Perceptron/Perceptron.ipynb&quot;&gt;Notebook&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/03-Perceptron/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;04&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/04-OwnFramework/README.md&quot;&gt;Multi-Layered Perceptron and Creating our own Framework&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb&quot;&gt;Notebook&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/04-OwnFramework/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;05&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/README.md&quot;&gt;Intro to Frameworks (PyTorch/TensorFlow) and Overfitting&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb&quot;&gt;Keras&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;IV&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/README.md&quot;&gt;&lt;strong&gt;Computer Vision&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.microsoft.com/learn/modules/intro-computer-vision-pytorch/?WT.mc_id=academic-77998-cacaste&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://docs.microsoft.com/learn/modules/intro-computer-vision-TensorFlow/?WT.mc_id=academic-77998-cacaste&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Explore Computer Vision on Microsoft Azure&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;06&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/06-IntroCV/README.md&quot;&gt;Intro to Computer Vision. OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/06-IntroCV/OpenCV.ipynb&quot;&gt;Notebook&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/06-IntroCV/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;07&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/README.md&quot;&gt;Convolutional Neural Networks&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/CNN_Architectures.md&quot;&gt;CNN Architectures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/07-ConvNets/ConvNetsPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; /&lt;a href=&quot;https://microsoft.github.io/AI-For-Beginners/lessons/4-ComputerVision/07-ConvNets/ConvNetsTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;08&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/README.md&quot;&gt;Pre-trained Networks and Transfer Learning&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/TrainingTricks.md&quot;&gt;Training Tricks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/08-TransferLearning/TransferLearningPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;09&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/09-Autoencoders/README.md&quot;&gt;Autoencoders and VAEs&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;10&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/10-GANs/README.md&quot;&gt;Generative Adversarial Networks &amp;amp; Artistic Style Transfer&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/10-GANs/GANPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/10-GANs/GANTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;11&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/11-ObjectDetection/README.md&quot;&gt;Object Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/11-ObjectDetection/ObjectDetection.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/11-ObjectDetection/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;12&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/12-Segmentation/README.md&quot;&gt;Semantic Segmentation. U-Net&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/12-Segmentation/SemanticSegmentationPytorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;(https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/4-ComputerVision/12-Segmentation/SemanticSegmentationTF.ipynb)&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;V&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/README.md&quot;&gt;&lt;strong&gt;Natural Language Processing&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste&quot;&gt;PyTorch&lt;/a&gt; /&lt;a href=&quot;https://docs.microsoft.com/learn/modules/intro-natural-language-processing-TensorFlow/?WT.mc_id=academic-77998-cacaste&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Explore Natural Language Processing on Microsoft Azure&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;13&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/13-TextRep/README.md&quot;&gt;Text Representation. Bow/TF-IDF&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;14&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/14-Embeddings/README.md&quot;&gt;Semantic word embeddings. Word2Vec and GloVe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;15&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/15-LanguageModeling/README.md&quot;&gt;Language Modeling. Training your own embeddings&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/15-LanguageModeling/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;16&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/16-RNN/README.md&quot;&gt;Recurrent Neural Networks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/16-RNN/RNNPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/16-RNN/RNNTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;17&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/17-GenerativeNetworks/README.md&quot;&gt;Generative Recurrent Networks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://microsoft.github.io/AI-For-Beginners/lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.md&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://microsoft.github.io/AI-For-Beginners/lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.md&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/17-GenerativeNetworks/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;18&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/18-Transformers/READMEtransformers.md&quot;&gt;Transformers. BERT.&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; /&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/18-Transformers/TransformersTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;19&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/19-NER/README.md&quot;&gt;Named Entity Recognition&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://microsoft.github.io/AI-For-Beginners/lessons/5-NLP/19-NER/NER-TF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/19-NER/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;20&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/20-LangModels/READMELargeLang.md&quot;&gt;Large Language Models, Prompt Programming and Few-Shot Tasks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://microsoft.github.io/AI-For-Beginners/lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;VI&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;Other AI Techniques&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;21&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/21-GeneticAlgorithms/README.md&quot;&gt;Genetic Algorithms&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/6-Other/21-GeneticAlgorithms/Genetic.ipynb&quot;&gt;Notebook&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;22&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/22-DeepRL/README.md&quot;&gt;Deep Reinforcement Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; /&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/22-DeepRL/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;23&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/23-MultiagentSystems/README.md&quot;&gt;Multi-Agent Systems&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;VII&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;AI Ethics&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;24&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/7-Ethics/README.md&quot;&gt;AI Ethics and Responsible AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.microsoft.com/learn/paths/responsible-ai-business-principles/?WT.mc_id=academic-77998-cacaste&quot;&gt;Microsoft Learn: Responsible AI Principles&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;IX&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;Extras&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;25&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/X-Extras/X1-MultiModal/README.md&quot;&gt;Multi-Modal Networks, CLIP and VQGAN&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/X-Extras/X1-MultiModal/Clip.ipynb&quot;&gt;Notebook&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Each lesson contains&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pre-reading material&lt;/li&gt; 
 &lt;li&gt;Executable Jupyter Notebooks, which are often specific to the framework (&lt;strong&gt;PyTorch&lt;/strong&gt; or &lt;strong&gt;TensorFlow&lt;/strong&gt;). The executable notebook also contains a lot of theoretical material, so to understand the topic you need to go through at least one version of the notebook (either PyTorch or TensorFlow).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Labs&lt;/strong&gt; available for some topics, which give you an opportunity to try applying the material you have learned to a specific problem.&lt;/li&gt; 
 &lt;li&gt;Some sections contain links to &lt;a href=&quot;https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum&quot;&gt;&lt;strong&gt;MS Learn&lt;/strong&gt;&lt;/a&gt; modules that cover related topics.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;We have created a &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/0-course-setup/setup.md&quot;&gt;setup lesson&lt;/a&gt; to help you with setting up your development environment. - For Educators, we have created a &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/0-course-setup/for-teachers.md&quot;&gt;curricula setup lesson&lt;/a&gt; for you too!&lt;/li&gt; 
 &lt;li&gt;How to &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/0-course-setup/how-to-run.md&quot;&gt;Run the code in a VSCode or a Codepace&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Follow these steps:&lt;/p&gt; 
&lt;p&gt;Fork the Repository: Click on the &quot;Fork&quot; button at the top-right corner of this page.&lt;/p&gt; 
&lt;p&gt;Clone the Repository: &lt;code&gt;git clone https://github.com/microsoft/AI-For-Beginners.git&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Don&#39;t forget to star (🌟) this repo to find it easier later.&lt;/p&gt; 
&lt;h2&gt;Meet other Learners&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href=&quot;https://aka.ms/genai-discord?WT.mc_id=academic-105485-bethanycheum&quot;&gt;official AI Discord server&lt;/a&gt; to meet and network with other learners taking this course and get support.&lt;/p&gt; 
&lt;h2&gt;Quizzes&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;A note about quizzes&lt;/strong&gt;: All quizzes are contained in the Quiz-app folder in etc\quiz-app, They are linked from within the lessons the quiz app can be run locally or deployed to Azure; follow the instruction in the &lt;code&gt;quiz-app&lt;/code&gt; folder. They are gradually being localized.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Help Wanted&lt;/h2&gt; 
&lt;p&gt;Do you have suggestions or found spelling or code errors? Raise an issue or create a pull request.&lt;/p&gt; 
&lt;h2&gt;Special Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;✍️ Primary Author:&lt;/strong&gt; &lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry Soshnikov&lt;/a&gt;, PhD&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔥 Editor:&lt;/strong&gt; &lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen Looper&lt;/a&gt;, PhD&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🎨 Sketchnote illustrator:&lt;/strong&gt; &lt;a href=&quot;https://twitter.com/girlie_mac&quot;&gt;Tomomi Imura&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;✅ Quiz Creator:&lt;/strong&gt; &lt;a href=&quot;https://github.com/CinnamonXI&quot;&gt;Lateefah Bello&lt;/a&gt;, &lt;a href=&quot;https://studentambassadors.microsoft.com/&quot;&gt;MLSA&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🙏 Core Contributors:&lt;/strong&gt; &lt;a href=&quot;https://github.com/Pe4enIks&quot;&gt;Evgenii Pishchik&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Other Curricula&lt;/h2&gt; 
&lt;p&gt;Our team produces other curricula! Check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genai-beginners&quot;&gt;Generative AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Generative-AI-for-beginners-dotnet&quot;&gt;Generative AI for Beginners .NET&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/generative-ai-with-javascript&quot;&gt;Generative AI with JavaScript&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-beginners&quot;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/datascience-beginners&quot;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ml-beginners&quot;&gt;ML for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Security-101&quot;&gt;Cybersecurity for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/webdev-beginners&quot;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/iot-beginners&quot;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/xr-development-for-beginners&quot;&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Mastering-GitHub-Copilot-for-Paired-Programming&quot;&gt;Mastering GitHub Copilot for Paired Programming&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers&quot;&gt;Mastering GitHub Copilot for C#/.NET Developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/CopilotAdventures&quot;&gt;Choose Your Own Copilot Adventure&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>
