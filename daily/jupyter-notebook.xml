<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="data:text/xsl;base64,<?xml version="1.0" encoding="utf-8"?><xsl:stylesheet version="3.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform" xmlns:atom="http://www.w3.org/2005/Atom"><xsl:output method="html" version="1.0" encoding="UTF-8" indent="yes"/><xsl:template match="/"><xsl:variable name="title"><xsl:value-of select="/rss/channel/title"/></xsl:variable><xsl:variable name="description"><xsl:value-of select="/rss/channel/description"/></xsl:variable><xsl:variable name="link"><xsl:value-of select="/rss/channel/link"/></xsl:variable><html class="dark scroll-smooth"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="referrer" content="unsafe-url"/><title><xsl:value-of select="$title"/></title><style>*,:after,:before{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-gradient-from-position: ;--tw-gradient-via-position: ;--tw-gradient-to-position: ;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgba(59,130,246,.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: ;--tw-contain-size: ;--tw-contain-layout: ;--tw-contain-paint: ;--tw-contain-style: }::backdrop{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-gradient-from-position: ;--tw-gradient-via-position: ;--tw-gradient-to-position: ;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgba(59,130,246,.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: ;--tw-contain-size: ;--tw-contain-layout: ;--tw-contain-paint: ;--tw-contain-style: }
        
        /*! tailwindcss v3.4.17 | MIT License | https://tailwindcss.com*/*,:after,:before{box-sizing:border-box;border:0 solid #e7e7f0}:after,:before{--tw-content:""}:host,html{line-height:1.5;-webkit-text-size-adjust:100%;-moz-tab-size:4;-o-tab-size:4;tab-size:4;font-family:ui-sans-serif,system-ui,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;font-feature-settings:normal;font-variation-settings:normal;-webkit-tap-highlight-color:transparent}body{margin:0;line-height:inherit}hr{height:0;color:inherit;border-top-width:1px}abbr:where([title]){-webkit-text-decoration:underline dotted;text-decoration:underline dotted}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}b,strong{font-weight:bolder}code,kbd,pre,samp{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-feature-settings:normal;font-variation-settings:normal;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}table{text-indent:0;border-color:inherit;border-collapse:collapse}button,input,optgroup,select,textarea{font-family:inherit;font-feature-settings:inherit;font-variation-settings:inherit;font-size:100%;font-weight:inherit;line-height:inherit;letter-spacing:inherit;color:inherit;margin:0;padding:0}button,select{text-transform:none}button,input:where([type=button]),input:where([type=reset]),input:where([type=submit]){-webkit-appearance:button;background-color:transparent;background-image:none}:-moz-focusring{outline:auto}:-moz-ui-invalid{box-shadow:none}progress{vertical-align:baseline}::-webkit-inner-spin-button,::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}summary{display:list-item}blockquote,dd,dl,figure,h1,h2,h3,h4,h5,h6,hr,p,pre{margin:0}fieldset{margin:0}fieldset,legend{padding:0}menu,ol,ul{list-style:none;margin:0;padding:0}dialog{padding:0}textarea{resize:vertical}input::-moz-placeholder,textarea::-moz-placeholder{opacity:1;color:#a8a8b8}input::placeholder,textarea::placeholder{opacity:1;color:#a8a8b8}[role=button],button{cursor:pointer}:disabled{cursor:default}audio,canvas,embed,iframe,img,object,svg,video{display:block;vertical-align:middle}img,video{max-width:100%;height:auto}[hidden]:where(:not([hidden=until-found])){display:none}:root{--card-radius:0.75rem;--btn-radius:var(--card-radius);--badge-radius:var(--btn-radius);--input-radius:var(--btn-radius);--avatar-radius:9999px;--annonce-radius:var(--avatar-radius);--ui-border-color:#1f1f31;--btn-border:#1f1f31;--badge-border:var(--btn-border);--input-border:var(--ui-border-color);--ui-disabled-border:#121220;--ui-error-border:#e11d48;--ui-success-border:#65a30d;--input-outline:#4f46e5;--ui-bg:rgb(18 18 32/var(--ui-bg-opacity));--ui-soft-bg:#1f1f31;--overlay-bg:rgba(2,2,13,.25);--input-bg:var(--ui-soft-bg);--ui-disabled-bg:#121220;--card-padding:1.5rem;--display-text-color:#fff;--title-text-color:var(--display-text-color);--body-text-color:#d6d6e1;--caption-text-color:#6e6e81;--placeholder-text-color:#4d4d5f;--ui-bg-opacity:1;color:var(--body-text-color)}*,.border{border-color:var(--ui-border-color)}button:disabled{border:none!important;background:var(--ui-disabled-bg)!important;background-image:none!important;box-shadow:none!important;color:var(--placeholder-text-color)!important;pointer-events:none!important}button:disabled:before{content:var(--tw-content);display:none}a:focus-visible,button:focus-visible{outline-width:2px;outline-offset:2px;outline-color:#4f46e5}a:focus-visible:focus-visible,button:focus-visible:focus-visible{outline-style:solid}input:user-invalid,select:user-invalid,textarea:user-invalid{--input-border:var(--ui-error-border);--ui-border-color:var(--ui-error-border);--input-outline:var(--ui-error-border);--title-text-color:#fb7185}[data-rounded=none]{--card-radius:0px;--avatar-radius:0px}[data-rounded=default]{--card-radius:0.25rem}[data-rounded=small]{--card-radius:0.125rem}[data-rounded=medium]{--card-radius:0.375rem}[data-rounded=large]{--card-radius:0.5rem}[data-rounded=xlarge]{--card-radius:0.75rem}[data-rounded="2xlarge"]{--card-radius:1rem;--input-radius:0.75rem}[data-rounded="3xlarge"]{--card-radius:1.5rem;--input-radius:0.75rem}[data-rounded=full]{--card-radius:1.5rem;--btn-radius:9999px;--input-radius:1rem}[data-shade=glassy]{--ui-bd-blur:40px;--ui-bg-opacity:0.75;--ui-bg:rgb(58 58 75/var(--ui-bg-opacity));--ui-border-color:rgba(250,250,254,.1);--ui-soft-bg:rgba(77,77,95,.5)}[data-shade="800"]{--ui-border-color:#3a3a4b;--ui-bg:#1f1f31;--ui-soft-bg:#121220}[data-shade="900"]{--ui-border-color:#1f1f31;--ui-bg:#121220;--ui-soft-bg:#1f1f31}[data-shade="950"]{--ui-border-color:#1f1f31;--ui-bg:#02020d;--ui-soft-bg:#1f1f31}.container{width:100%}@media (min-width:640px){.container{max-width:640px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:1024px){.container{max-width:1024px}}@media (min-width:1280px){.container{max-width:1280px}}@media (min-width:1536px){.container{max-width:1536px}}.icon-\[tabler--rss\]{display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24'%3E%3Cpath fill='none' stroke='%23000' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M4 19a1 1 0 1 0 2 0 1 1 0 1 0-2 0M4 4a16 16 0 0 1 16 16M4 11a9 9 0 0 1 9 9'/%3E%3C/svg%3E")}.link{--tw-text-opacity:1;color:rgb(129 140 248/var(--tw-text-opacity,1));transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,-webkit-backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter,-webkit-backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1);transition-duration:.15s}.link.variant-ghost:hover,.link.variant-underlined{text-decoration-line:underline}.link.variant-animated{position:relative}.link.variant-animated:before{position:absolute;left:0;right:0;bottom:0;height:1px;transform-origin:right;--tw-scale-x:0;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y));transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,-webkit-backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter,-webkit-backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1);content:var(--tw-content);transition-duration:.2s}.link.variant-animated:hover:before{transform-origin:left;content:var(--tw-content);--tw-scale-x:1;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.link.intent-info{--tw-text-opacity:1;color:rgb(96 165 250/var(--tw-text-opacity,1))}.link.intent-neutral{--tw-text-opacity:1;color:rgb(255 255 255/var(--tw-text-opacity,1))}.link.variant-animated.intent-neutral:before{content:var(--tw-content);background-color:hsla(0,0%,100%,.5)}.link.variant-animated.intent-info:before{content:var(--tw-content);--tw-bg-opacity:1;background-color:rgb(37 99 235/var(--tw-bg-opacity,1))}.link.variant-animated.intent-primary:before{content:var(--tw-content);--tw-bg-opacity:1;background-color:rgb(79 70 229/var(--tw-bg-opacity,1))}.link.variant-ghost.intent-neutral,.link.variant-underlined.intent-neutral{text-decoration-color:hsla(0,0%,100%,.5)}.mx-auto{margin-left:auto;margin-right:auto}.my-2{margin-top:.5rem;margin-bottom:.5rem}.my-6{margin-top:1.5rem;margin-bottom:1.5rem}.mb-2{margin-bottom:.5rem}.mb-6{margin-bottom:1.5rem}.mb-8{margin-bottom:2rem}.ml-1{margin-left:.25rem}.ml-4{margin-left:1rem}.mr-2{margin-right:.5rem}.mt-1{margin-top:.25rem}.mt-2{margin-top:.5rem}.mt-3{margin-top:.75rem}.block{display:block}.inline-block{display:inline-block}.inline{display:inline}.flex{display:flex}.grid{display:grid}.hidden{display:none}.h-4{height:1rem}.h-8{height:2rem}.min-h-screen{min-height:100vh}.min-h-svh{min-height:100svh}.w-4{width:1rem}.w-8{width:2rem}.max-w-full{max-width:100%}.max-w-screen-lg{max-width:1024px}.flex-1{flex:1 1 0%}.cursor-pointer{cursor:pointer}.list-disc{list-style-type:disc}.grid-cols-1{grid-template-columns:repeat(1,minmax(0,1fr))}.flex-col{flex-direction:column}.items-center{align-items:center}.justify-between{justify-content:space-between}.gap-4{gap:1rem}.gap-6{gap:1.5rem}.gap-8{gap:2rem}.space-y-2>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-top:calc(.5rem*(1 - var(--tw-space-y-reverse)));margin-bottom:calc(.5rem*var(--tw-space-y-reverse))}.space-y-3>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-top:calc(.75rem*(1 - var(--tw-space-y-reverse)));margin-bottom:calc(.75rem*var(--tw-space-y-reverse))}.space-y-4>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-top:calc(1rem*(1 - var(--tw-space-y-reverse)));margin-bottom:calc(1rem*var(--tw-space-y-reverse))}.space-y-6>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-top:calc(1.5rem*(1 - var(--tw-space-y-reverse)));margin-bottom:calc(1.5rem*var(--tw-space-y-reverse))}.scroll-smooth{scroll-behavior:smooth}.truncate{overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.bg-gray-925{--tw-bg-opacity:1;background-color:rgb(9 9 21/var(--tw-bg-opacity,1))}.bg-gradient-to-r{background-image:linear-gradient(to right,var(--tw-gradient-stops))}.from-primary-600{--tw-gradient-from:#4f46e5 var(--tw-gradient-from-position);--tw-gradient-to:rgba(79,70,229,0) var(--tw-gradient-to-position);--tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to)}.to-accent-400{--tw-gradient-to:#e879f9 var(--tw-gradient-to-position)}.bg-clip-text{-webkit-background-clip:text;background-clip:text}.p-1{padding:.25rem}.px-4{padding-left:1rem;padding-right:1rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.py-4{padding-top:1rem;padding-bottom:1rem}.py-6{padding-top:1.5rem;padding-bottom:1.5rem}.pl-5{padding-left:1.25rem}.pt-2{padding-top:.5rem}.text-center{text-align:center}.font-sans{font-family:ui-sans-serif,system-ui,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}.text-2xl{font-size:1.5rem;line-height:2rem}.text-lg{font-size:1.125rem;line-height:1.75rem}.text-sm{font-size:.875rem;line-height:1.25rem}.text-xs{font-size:.75rem;line-height:1rem}.font-bold{font-weight:700}.font-medium{font-weight:500}.font-semibold{font-weight:600}.leading-normal{line-height:1.5}.text-gray-400{--tw-text-opacity:1;color:rgb(168 168 184/var(--tw-text-opacity,1))}.text-gray-500{--tw-text-opacity:1;color:rgb(110 110 129/var(--tw-text-opacity,1))}.text-transparent{color:transparent}.antialiased{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}.transition{transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,-webkit-backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter,-webkit-backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1);transition-duration:.15s}.text-title{color:var(--title-text-color)}.text-body{color:var(--body-text-color)}.\!text-caption{color:var(--caption-text-color)!important}.text-caption{color:var(--caption-text-color)}.dark{--display-text-color:#fff;--title-text-color:var(--display-text-color);--caption-text-color:#6e6e81;--body-text-color:#d6d6e1;--placeholder-text-color:#4d4d5f;--ui-border-color:#232323}[data-shade="900"]:where(.dark,.dark *),[data-shade="925"]:where(.dark,.dark *),[data-shade="950"]:where(.dark,.dark *){--ui-border-color:#383838}.hover\:text-gray-300:hover{--tw-text-opacity:1;color:rgb(214 214 225/var(--tw-text-opacity,1))}.group[open] .group-open\:rotate-180{--tw-rotate:180deg;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}@media (min-width:768px){.md\:grid-cols-2{grid-template-columns:repeat(2,minmax(0,1fr))}.md\:p-4{padding:1rem}.md\:px-6{padding-left:1.5rem;padding-right:1.5rem}.md\:pt-6{padding-top:1.5rem}}@media (min-width:1024px){.lg\:grid-cols-3{grid-template-columns:repeat(3,minmax(0,1fr))}.lg\:dark\:bg-gray-900:is(.dark *){--tw-bg-opacity:1;background-color:rgb(18 18 32/var(--tw-bg-opacity,1))}}</style></head><body class="bg-gray-925 min-h-screen min-h-svh font-sans leading-normal antialiased lg:dark:bg-gray-900"><main class="min-w-screen container mx-auto flex min-h-screen max-w-screen-lg flex-col px-4 py-6 md:px-6"><header class="space-y-2 pt-2 md:pt-6"><a title="{$title}" href="{$link}" target="_blank" rel="noopener noreferrer"><h1 class="flex text-2xl"><span class="icon-[tabler--rss] mr-2 h-8 w-8"/><span class="lg2:text-3xl from-primary-600 to-accent-400 inline-block bg-gradient-to-r bg-clip-text font-bold text-transparent"><xsl:value-of select="$title" disable-output-escaping="yes"/></span></h1></a><p class="text-body pt-2 text-lg py-4"><xsl:value-of select="$description" disable-output-escaping="yes"/></p><p class="text-caption text-sm">
              This RSS feed for the
              <a class="link intent-neutral variant-animated !text-caption font-bold" title="{$title}" href="{$link}" target="_blank" rel="noopener noreferrer"><xsl:value-of select="$title"/></a>
              website.
            </p><p class="text-body text-sm hidden" id="subscribe-links">
              You can subscribe this RSS feed by
              <a class="link intent-neutral variant-animated font-bold" title="Feedly" data-href="https://feedly.com/i/subscription/feed/" target="_blank" rel="noopener noreferrer">Feedly</a>,
              <a class="link intent-neutral variant-animated font-bold" title="Inoreader" data-href="https://www.inoreader.com/feed/" target="_blank" rel="noopener noreferrer">Inoreader</a>,
              <a class="link intent-neutral variant-animated font-bold" title="Newsblur" data-href="https://www.newsblur.com/?url=" target="_blank" rel="noopener noreferrer">Newsblur</a>,
              <a class="link intent-neutral variant-animated font-bold" title="Follow" data-href="follow://add?url=" rel="noopener noreferrer">Follow</a>,
              <a class="link intent-neutral variant-animated font-bold" title="RSS Reader" data-href="feed:" data-raw="true" rel="noopener noreferrer">RSS Reader</a>
              or
              <a class="link intent-neutral variant-animated font-bold" title="{$title} 's feed source" data-href="" data-raw="true" rel="noopener noreferrer">View Source</a>.
            </p><script>
              document.addEventListener('DOMContentLoaded', function () {
                document.querySelectorAll('a[data-href]').forEach(function (a) {
                  const url = new URL(location.href)
                  const feed = url.searchParams.get('url') || location.href
                  const raw = a.getAttribute('data-raw')
                  if (raw) {
                    a.href = a.getAttribute('data-href') + feed
                  } else {
                    a.href = a.getAttribute('data-href') + encodeURIComponent(feed)
                  }
                })
                document.getElementById('subscribe-links').classList.remove('hidden')
              })
            </script></header><hr class="my-6"/><section class="flex-1 space-y-6 p-1 md:p-4"><xsl:choose><xsl:when test="/rss/channel/item"><xsl:for-each select="/rss/channel/item"><article class="space-y-2"><details><summary class="max-w-full truncate"><xsl:if test="title"><h2 class="text-title inline cursor-pointer text-lg font-semibold"><xsl:value-of select="title" disable-output-escaping="yes"/></h2></xsl:if><xsl:if test="pubDate"><time class="text-caption ml-4 mt-1 block text-sm"><xsl:value-of select="pubDate"/></time></xsl:if></summary><div class="text-body px-4 py-2"><p class="my-2"><xsl:choose><xsl:when test="description"><xsl:value-of select="description" disable-output-escaping="yes"/></xsl:when></xsl:choose></p><xsl:if test="link"><a class="link variant-animated intent-neutral font-bold" href="{link}" target="_blank" rel="noopener noreferrer">
                            Read More
                          </a></xsl:if></div></details></article></xsl:for-each></xsl:when><xsl:when test="/atom:feed/atom:entry"><xsl:for-each select="/atom:feed/atom:entry"><article class="space-y-2"><details><summary class="max-w-full truncate"><xsl:if test="atom:title"><h2 class="text-title inline cursor-pointer text-lg font-semibold"><xsl:value-of select="atom:title" disable-output-escaping="yes"/></h2></xsl:if><xsl:if test="atom:updated"><time class="text-caption ml-4 mt-1 block text-sm"><xsl:value-of select="atom:updated"/></time></xsl:if></summary><div class="text-body px-4 py-2"><p class="my-2"><xsl:choose><xsl:when test="atom:summary"><xsl:value-of select="atom:summary" disable-output-escaping="yes"/></xsl:when><xsl:when test="atom:content"><xsl:value-of select="atom:content" disable-output-escaping="yes"/></xsl:when></xsl:choose></p><xsl:if test="atom:link/@href"><a class="link variant-animated intent-neutral font-bold" href="{atom:link/@href}" target="_blank" rel="noopener noreferrer">
                            Read More
                          </a></xsl:if></div></details></article></xsl:for-each></xsl:when></xsl:choose></section><hr class="my-6"/><footer class="text-gray-400"><div class="container mx-auto px-4"><div class="mb-8"><h3 class="text-lg font-semibold text-title mb-6">Popular Feed Collections</h3><div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6"><div class="space-y-3"><a href="https://redreamality.com/GitHubTrendingRSS/weekly/python.xml" class="block hover:text-gray-300"><div class="font-medium">🐍 Python TrendWatch</div><div class="text-xs text-gray-500">AI, ML &amp; Data Science Innovation Feed</div></a><a href="https://redreamality.com/GitHubTrendingRSS/weekly/cuda.xml" class="block hover:text-gray-300"><div class="font-medium">⚡ CUDA Accelerator</div><div class="text-xs text-gray-500">GPU Computing &amp; Deep Learning Updates</div></a><a href="https://redreamality.com/GitHubTrendingRSS/weekly/matlab.xml" class="block hover:text-gray-300"><div class="font-medium">🧠 MATLAB TrendPulse</div><div class="text-xs text-gray-500">MEG, EEG and iEEG Research Feed</div></a></div><div class="space-y-3"><a href="https://redreamality.com/GitHubTrendingRSS/weekly/rust.xml" class="block hover:text-gray-300"><div class="font-medium">🦀 Rust Systems Feed</div><div class="text-xs text-gray-500">High-Performance &amp; Safe Systems Programming</div></a><a href="https://redreamality.com/GitHubTrendingRSS/weekly/go.xml" class="block hover:text-gray-300"><div class="font-medium">🚀 Go Infrastructure</div><div class="text-xs text-gray-500">Cloud Native &amp; DevOps Excellence</div></a><a href="https://redreamality.com/GitHubTrendingRSS/weekly/typescript.xml" class="block hover:text-gray-300"><div class="font-medium">📱 TypeScript Ecosystem</div><div class="text-xs text-gray-500">Modern Web &amp; App Development</div></a></div><div class="space-y-3"><a href="https://redreamality.com/GitHubTrendingRSS/daily/adblock-filter-list.xml" class="block hover:text-gray-300"><div class="font-medium">🛡️ Privacy Shield</div><div class="text-xs text-gray-500">AdBlock &amp; Security Updates</div></a><a href="https://redreamality.com/GitHubTrendingRSS/daily/all.xml" class="block hover:text-gray-300"><div class="font-medium">🌟 Global TechRadar</div><div class="text-xs text-gray-500">Cross-Language Innovation Pulse, add it to your RSS reader rsshub://github/trending/monthly/any/zh</div></a></div></div></div><div class="grid grid-cols-1 md:grid-cols-2 gap-8 mb-8"><div class="space-y-4"><h3 class="text-lg font-semibold text-title">Getting Started</h3><div class="grid grid-cols-1 gap-4"><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">📖 Feed Integration Guide</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">1. Choose Your RSS Reader:</p><ul class="list-disc pl-5 space-y-2"><li>Feedly: Professional choice for cross-platform sync</li><li>Inoreader: Advanced filtering capabilities</li><li>NetNewsWire: Perfect for macOS/iOS users</li><li>FreshRSS: Self-hosted option with full control</li></ul><p class="mt-3 mb-2">2. Add Our Feeds:</p><ul class="list-disc pl-5 space-y-2"><li>Copy the feed URL (e.g., rsshub://github/trending/monthly/any/zh)</li><li>In your RSS reader, look for "Add Feed" or "Subscribe"</li><li>Paste the URL and customize update frequency</li></ul></div></details><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">🎯 Custom Feed Creation</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Create Your Perfect Feed:</p><ul class="list-disc pl-5 space-y-2"><li>Language-specific: /GitHubTrendingRSS/[frequency]/[language].xml</li><li>Topic-focused: Combine multiple language feeds</li><li>Custom time ranges: daily, weekly, or monthly updates</li><li>Regional feeds: Focus on specific developer communities</li></ul><p class="mt-3">Pro tip: Use tags in your RSS reader to organize feeds by topic, language, or priority.</p></div></details><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">⚡ Feed Management Tips</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Optimize Your Feed Reading:</p><ul class="list-disc pl-5 space-y-2"><li>Set update frequencies based on feed importance</li><li>Use folders to group related feeds (e.g., AI/ML, Web Dev)</li><li>Enable notifications only for high-priority feeds</li><li>Archive valuable resources for future reference</li></ul><p class="mt-3">Advanced Features:</p><ul class="list-disc pl-5 space-y-2"><li>Filter feeds using keywords to focus on specific topics</li><li>Set up IFTTT integrations for automated workflows</li><li>Export/backup your feed collection regularly</li></ul></div></details></div></div><div class="space-y-4"><h3 class="text-lg font-semibold text-title">Common Questions</h3><div class="grid grid-cols-1 gap-4"><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">🤔 About Github Radar</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Github Radar is your intelligent curator for:</p><ul class="list-disc pl-5 space-y-2"><li>Trending repositories across all programming languages</li><li>Language-specific innovation and updates</li><li>Regional development trends and patterns</li><li>Open source community movements</li></ul><p class="mt-3">Our mission is to help developers stay updated with minimal effort through smart feed curation and organization.</p></div></details><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">📊 Feed Frequency Options</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Choose Your Update Rhythm:</p><ul class="list-disc pl-5 space-y-2"><li>Daily: Perfect for fast-moving technologies and security updates</li><li>Weekly: Ideal for maintaining awareness without overwhelm</li><li>Monthly: Best for long-term trend analysis and strategic planning</li></ul><p class="mt-3">Customize by combining different frequencies for different topics based on your needs.</p></div></details><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">🔧 Technical Support</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Supported RSS Readers:</p><ul class="list-disc pl-5 space-y-2"><li>Desktop: NetNewsWire, Reeder, FeedReader</li><li>Mobile: Feedly, Inoreader, NewsBlur</li><li>Self-hosted: FreshRSS, Tiny Tiny RSS</li><li>Browser-based: Feedbro, RSS Feed Reader</li></ul><p class="mt-3">Common Issues:</p><ul class="list-disc pl-5 space-y-2"><li>Feed not updating? Check your reader's refresh settings</li><li>Missing content? Verify your internet connection</li><li>Format issues? Try re-subscribing to the feed</li></ul></div></details></div></div></div><div class="text-center text-sm"><p class="mt-2">Acknowledgement: Page decorated by <a href="https://github.com/ccbikai/RSS.Beauty"><svg class="inline-block w-4 h-4 ml-1" viewBox="0 0 16 16" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg></a> RSS.Beauty</p></div></div></footer></main></body></html></xsl:template></xsl:stylesheet>"?>
<rss version="2.0">
  <channel>
    <title>GitHub Jupyter Notebook Daily Trending</title>
    <description>Daily Trending of Jupyter Notebook in GitHub</description>
    <pubDate>Sat, 29 Mar 2025 02:26:11 GMT</pubDate>
    <link>http://redreamality.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>langchain-ai/langchain</title>
      <link>https://github.com/langchain-ai/langchain</link>
      <description>&lt;p&gt;ðŸ¦œðŸ”— Build context-aware reasoning applications&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;docs/static/img/logo-dark.svg&quot;&gt; 
 &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;docs/static/img/logo-light.svg&quot;&gt; 
 &lt;img alt=&quot;LangChain Logo&quot; src=&quot;https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/static/img/logo-dark.svg?sanitize=true&quot; width=&quot;80%&quot;&gt; 
&lt;/picture&gt; 
&lt;div&gt; 
 &lt;br&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/langchain-ai/langchain/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/release/langchain-ai/langchain?style=flat-square&quot; alt=&quot;Release Notes&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/langchain-ai/langchain/actions/workflows/check_diffs.yml&quot;&gt;&lt;img src=&quot;https://github.com/langchain-ai/langchain/actions/workflows/check_diffs.yml/badge.svg?sanitize=true&quot; alt=&quot;CI&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/l/langchain-core?style=flat-square&quot; alt=&quot;PyPI - License&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypistats.org/packages/langchain-core&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/dm/langchain-core?style=flat-square&quot; alt=&quot;PyPI - Downloads&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://star-history.com/#langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/langchain-ai/langchain?style=flat-square&quot; alt=&quot;GitHub star chart&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/langchain-ai/langchain/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-raw/langchain-ai/langchain?style=flat-square&quot; alt=&quot;Open Issues&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode&amp;amp;style=flat-square&quot; alt=&quot;Open in Dev Containers&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://codespaces.new/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://github.com/codespaces/badge.svg?sanitize=true&quot; alt=&quot;Open in GitHub Codespaces&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/langchainai&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&amp;amp;label=Follow%20%40LangChainAI&quot; alt=&quot;Twitter&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Looking for the JS/TS library? Check out &lt;a href=&quot;https://github.com/langchain-ai/langchainjs&quot;&gt;LangChain.js&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;LangChain is a framework for building LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development â€” all while future-proofing decisions as the underlying technology evolves.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U langchain
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To learn more about LangChain, check out &lt;a href=&quot;https://python.langchain.com/docs/introduction/&quot;&gt;the docs&lt;/a&gt;. If youâ€™re looking for more advanced customization or agent orchestration, check out &lt;a href=&quot;https://langchain-ai.github.io/langgraph/&quot;&gt;LangGraph&lt;/a&gt;, our framework for building controllable agent workflows.&lt;/p&gt; 
&lt;h2&gt;Why use LangChain?&lt;/h2&gt; 
&lt;p&gt;LangChain helps developers build applications powered by LLMs through a standard interface for models, embeddings, vector stores, and more.&lt;/p&gt; 
&lt;p&gt;Use LangChain for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time data augmentation&lt;/strong&gt;. Easily connect LLMs to diverse data sources and external / internal systems, drawing from LangChainâ€™s vast library of integrations with model providers, tools, vector stores, retrievers, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model interoperability&lt;/strong&gt;. Swap models in and out as your engineering team experiments to find the best choice for your applicationâ€™s needs. As the industry frontier evolves, adapt quickly â€” LangChainâ€™s abstractions keep you moving without losing momentum.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;LangChainâ€™s ecosystem&lt;/h2&gt; 
&lt;p&gt;While the LangChain framework can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools when building LLM applications.&lt;/p&gt; 
&lt;p&gt;To improve your LLM application development, pair LangChain with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;http://www.langchain.com/langsmith&quot;&gt;LangSmith&lt;/a&gt; - Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://langchain-ai.github.io/langgraph/&quot;&gt;LangGraph&lt;/a&gt; - Build agents that can reliably handle complex tasks with LangGraph, our low-level agent orchestration framework. LangGraph offers customizable architecture, long-term memory, and human-in-the-loop workflows â€” and is trusted in production by companies like LinkedIn, Uber, Klarna, and GitLab.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/#langgraph-platform&quot;&gt;LangGraph Platform&lt;/a&gt; - Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams â€” and iterate quickly with visual prototyping in &lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/&quot;&gt;LangGraph Studio&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Additional resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/tutorials/&quot;&gt;Tutorials&lt;/a&gt;: Simple walkthroughs with guided examples on getting started with LangChain.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/how_to/&quot;&gt;How-to Guides&lt;/a&gt;: Quick, actionable code snippets for topics such as tool calling, RAG use cases, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/concepts/&quot;&gt;Conceptual Guides&lt;/a&gt;: Explanations of key concepts behind the LangChain framework.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/api_reference/&quot;&gt;API Reference&lt;/a&gt;: Detailed reference on navigating base packages and integrations for LangChain.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>topoteretes/cognee</title>
      <link>https://github.com/topoteretes/cognee</link>
      <description>&lt;p&gt;Reliable LLM Memory for AI Applications and AI Agents&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://github.com/topoteretes/cognee&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png&quot; alt=&quot;Cognee Logo&quot; height=&quot;60&quot;&gt; &lt;/a&gt; 
 &lt;br&gt; 
 &lt;p&gt;cognee - memory layer for AI apps and Agents&lt;/p&gt; 
 &lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=1bezuvLwJmw&amp;amp;t=2s&quot;&gt;Demo&lt;/a&gt; . &lt;a href=&quot;https://cognee.ai&quot;&gt;Learn more&lt;/a&gt; Â· &lt;a href=&quot;https://discord.gg/NQPKmU5CCg&quot;&gt;Join Discord&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://GitHub.com/topoteretes/cognee/network/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&amp;amp;label=Fork&amp;amp;maxAge=2592000&quot; alt=&quot;GitHub forks&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/topoteretes/cognee/stargazers/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&amp;amp;label=Star&amp;amp;maxAge=2592000&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/topoteretes/cognee/commit/&quot;&gt;&lt;img src=&quot;https://badgen.net/github/commits/topoteretes/cognee&quot; alt=&quot;GitHub commits&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/topoteretes/cognee/tags/&quot;&gt;&lt;img src=&quot;https://badgen.net/github/tag/topoteretes/cognee&quot; alt=&quot;Github tag&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pepy.tech/project/cognee&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/cognee&quot; alt=&quot;Downloads&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/topoteretes/cognee/raw/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&amp;amp;colorB=000000&quot; alt=&quot;License&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/topoteretes/cognee/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&amp;amp;colorB=000000&quot; alt=&quot;Contributors&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;AI Agent responses you can rely on.&lt;/p&gt; 
 &lt;p&gt;Build dynamic Agent memory using scalable, modular ECL (Extract, Cognify, Load) pipelines.&lt;/p&gt; 
 &lt;p&gt;More on &lt;a href=&quot;https://docs.cognee.ai/use_cases&quot;&gt;use-cases&lt;/a&gt;.&lt;/p&gt; 
 &lt;div style=&quot;text-align: center&quot;&gt; 
  &lt;img src=&quot;https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee_benefits.png&quot; alt=&quot;Why cognee?&quot; width=&quot;100%&quot;&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Interconnect and retrieve your past conversations, documents, images and audio transcriptions&lt;/li&gt; 
 &lt;li&gt;Reduce hallucinations, developer effort, and cost.&lt;/li&gt; 
 &lt;li&gt;Load data to graph and vector databases using only Pydantic&lt;/li&gt; 
 &lt;li&gt;Manipulate your data while ingesting from 30+ data sources&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;p&gt;Get started quickly with a Google Colab &lt;a href=&quot;https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing&quot;&gt;notebook&lt;/a&gt; or &lt;a href=&quot;https://github.com/topoteretes/cognee-starter&quot;&gt;starter repo&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Your contributions are at the core of making this a true open source project. Any contributions you make are &lt;strong&gt;greatly appreciated&lt;/strong&gt;. See &lt;a href=&quot;https://raw.githubusercontent.com/topoteretes/cognee/dev/CONTRIBUTING.md&quot;&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;ðŸ“¦ Installation&lt;/h2&gt; 
&lt;p&gt;You can install Cognee using either &lt;strong&gt;pip&lt;/strong&gt;, &lt;strong&gt;poetry&lt;/strong&gt;, &lt;strong&gt;uv&lt;/strong&gt; or any other python package manager.&lt;/p&gt; 
&lt;h3&gt;With pip&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install cognee
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ðŸ’» Basic Usage&lt;/h2&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;import os
os.environ[&quot;LLM_API_KEY&quot;] = &quot;YOUR OPENAI_API_KEY&quot;

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also set the variables by creating .env file, using our &lt;a href=&quot;https://github.com/topoteretes/cognee/raw/main/.env.template&quot;&gt;template.&lt;/a&gt; To use different LLM providers, for more info check out our &lt;a href=&quot;https://docs.cognee.ai&quot;&gt;documentation&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Simple example&lt;/h3&gt; 
&lt;p&gt;This script will run the default pipeline:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import cognee
import asyncio


async def main():
    # Add text to cognee
    await cognee.add(&quot;Natural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval.&quot;)

    # Generate the knowledge graph
    await cognee.cognify()

    # Query the knowledge graph
    results = await cognee.search(&quot;Tell me about NLP&quot;)

    # Display the results
    for result in results:
        print(result)


if __name__ == &#39;__main__&#39;:
    asyncio.run(main())

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example output:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;       # ({&#39;id&#39;: UUID(&#39;bc338a39-64d6-549a-acec-da60846dd90d&#39;), &#39;updated_at&#39;: datetime.datetime(2024, 11, 21, 12, 23, 1, 211808, tzinfo=datetime.timezone.utc), &#39;name&#39;: &#39;natural language processing&#39;, &#39;description&#39;: &#39;An interdisciplinary subfield of computer science and information retrieval.&#39;}, {&#39;relationship_name&#39;: &#39;is_a_subfield_of&#39;, &#39;source_node_id&#39;: UUID(&#39;bc338a39-64d6-549a-acec-da60846dd90d&#39;), &#39;target_node_id&#39;: UUID(&#39;6218dbab-eb6a-5759-a864-b3419755ffe0&#39;), &#39;updated_at&#39;: datetime.datetime(2024, 11, 21, 12, 23, 15, 473137, tzinfo=datetime.timezone.utc)}, {&#39;id&#39;: UUID(&#39;6218dbab-eb6a-5759-a864-b3419755ffe0&#39;), &#39;updated_at&#39;: datetime.datetime(2024, 11, 21, 12, 23, 1, 211808, tzinfo=datetime.timezone.utc), &#39;name&#39;: &#39;computer science&#39;, &#39;description&#39;: &#39;The study of computation and information processing.&#39;})
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Graph visualization: &lt;a href=&quot;https://rawcdn.githack.com/topoteretes/cognee/refs/heads/add-visualization-readme/assets/graph_visualization.html&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/topoteretes/cognee/dev/assets/graph_visualization.png&quot; width=&quot;100%&quot; alt=&quot;Graph Visualization&quot;&gt;&lt;/a&gt; Open in &lt;a href=&quot;https://rawcdn.githack.com/topoteretes/cognee/refs/heads/add-visualization-readme/assets/graph_visualization.html&quot;&gt;browser&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more advanced usage, have a look at our &lt;a href=&quot;https://docs.cognee.ai&quot;&gt; documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Understand our architecture&lt;/h2&gt; 
&lt;div style=&quot;text-align: center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/topoteretes/cognee/dev/assets/cognee_diagram.png&quot; alt=&quot;cognee concept diagram&quot; width=&quot;100%&quot;&gt; 
&lt;/div&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;p&gt;What is AI memory:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/8b2a0050-5ec4-424c-b417-8269971503f0&quot;&gt;Learn about cognee&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Code of Conduct&lt;/h2&gt; 
&lt;p&gt;We are committed to making open source an enjoyable and respectful experience for our community. See &lt;a href=&quot;https://github.com/topoteretes/cognee/raw/main/CODE_OF_CONDUCT.md&quot;&gt;&lt;code&gt;CODE_OF_CONDUCT&lt;/code&gt;&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;ðŸ’« Contributors&lt;/h2&gt; 
&lt;a href=&quot;https://github.com/topoteretes/cognee/graphs/contributors&quot;&gt; &lt;img alt=&quot;contributors&quot; src=&quot;https://contrib.rocks/image?repo=topoteretes/cognee&quot;&gt; &lt;/a&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://star-history.com/#topoteretes/cognee&amp;amp;Date&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=topoteretes/cognee&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>shivammehta25/Matcha-TTS</title>
      <link>https://github.com/shivammehta25/Matcha-TTS</link>
      <description>&lt;p&gt;[ICASSP 2024] ðŸµ Matcha-TTS: A fast TTS architecture with conditional flow matching&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;h1&gt;ðŸµ Matcha-TTS: A fast TTS architecture with conditional flow matching&lt;/h1&gt; 
 &lt;h3&gt;&lt;a href=&quot;https://www.kth.se/profile/smehta&quot;&gt;Shivam Mehta&lt;/a&gt;, &lt;a href=&quot;https://www.kth.se/profile/ruibo&quot;&gt;Ruibo Tu&lt;/a&gt;, &lt;a href=&quot;https://www.kth.se/profile/beskow&quot;&gt;Jonas Beskow&lt;/a&gt;, &lt;a href=&quot;https://www.kth.se/profile/szekely&quot;&gt;Ã‰va SzÃ©kely&lt;/a&gt;, and &lt;a href=&quot;https://people.kth.se/~ghe/&quot;&gt;Gustav Eje Henter&lt;/a&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;a href=&quot;https://www.python.org/downloads/release/python-3100/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/-Python_3.10-blue?logo=python&amp;amp;logoColor=white&quot; alt=&quot;python&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PyTorch_2.0+-ee4c2c?logo=pytorch&amp;amp;logoColor=white&quot; alt=&quot;pytorch&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pytorchlightning.ai/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/-Lightning_2.0+-792ee5?logo=pytorchlightning&amp;amp;logoColor=white&quot; alt=&quot;lightning&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://hydra.cc/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Config-Hydra_1.3-89b8cd&quot; alt=&quot;hydra&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://black.readthedocs.io/en/stable/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Code%20Style-Black-black.svg?labelColor=gray&quot; alt=&quot;black&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pycqa.github.io/isort/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&amp;amp;labelColor=ef8336&quot; alt=&quot;isort&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p style=&quot;text-align: center;&quot;&gt; &lt;img src=&quot;https://shivammehta25.github.io/Matcha-TTS/images/logo.png&quot; height=&quot;128&quot;&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This is the official code implementation of ðŸµ Matcha-TTS [ICASSP 2024].&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;We propose ðŸµ Matcha-TTS, a new approach to non-autoregressive neural TTS, that uses &lt;a href=&quot;https://arxiv.org/abs/2210.02747&quot;&gt;conditional flow matching&lt;/a&gt; (similar to &lt;a href=&quot;https://arxiv.org/abs/2209.03003&quot;&gt;rectified flows&lt;/a&gt;) to speed up ODE-based speech synthesis. Our method:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Is probabilistic&lt;/li&gt; 
 &lt;li&gt;Has compact memory footprint&lt;/li&gt; 
 &lt;li&gt;Sounds highly natural&lt;/li&gt; 
 &lt;li&gt;Is very fast to synthesise from&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Check out our &lt;a href=&quot;https://shivammehta25.github.io/Matcha-TTS&quot;&gt;demo page&lt;/a&gt; and read &lt;a href=&quot;https://arxiv.org/abs/2309.03199&quot;&gt;our ICASSP 2024 paper&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://drive.google.com/drive/folders/17C_gYgEHOxI5ZypcfE_k1piKCtyR0isJ?usp=sharing&quot;&gt;Pre-trained models&lt;/a&gt; will be automatically downloaded with the CLI or gradio interface.&lt;/p&gt; 
&lt;p&gt;You can also &lt;a href=&quot;https://huggingface.co/spaces/shivammehta25/Matcha-TTS&quot;&gt;try ðŸµ Matcha-TTS in your browser on HuggingFace ðŸ¤— spaces&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Teaser video&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://youtu.be/xmvJkz3bqw0&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/xmvJkz3bqw0/hqdefault.jpg&quot; alt=&quot;Watch the video&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create an environment (suggested but optional)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;conda create -n matcha-tts python=3.10 -y
conda activate matcha-tts
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Install Matcha TTS using pip or from source&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install matcha-tts
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;from source&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install git+https://github.com/shivammehta25/Matcha-TTS.git
cd Matcha-TTS
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Run CLI / gradio app / jupyter notebook&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# This will download the required models
matcha-tts --text &quot;&amp;lt;INPUT TEXT&amp;gt;&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-tts-app
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or open &lt;code&gt;synthesis.ipynb&lt;/code&gt; on jupyter notebook&lt;/p&gt; 
&lt;h3&gt;CLI Arguments&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;To synthesise from given text, run:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-tts --text &quot;&amp;lt;INPUT TEXT&amp;gt;&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;To synthesise from a file, run:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-tts --file &amp;lt;PATH TO FILE&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;To batch synthesise from a file, run:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-tts --file &amp;lt;PATH TO FILE&amp;gt; --batched
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Additional arguments&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Speaking rate&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-tts --text &quot;&amp;lt;INPUT TEXT&amp;gt;&quot; --speaking_rate 1.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Sampling temperature&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-tts --text &quot;&amp;lt;INPUT TEXT&amp;gt;&quot; --temperature 0.667
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Euler ODE solver steps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-tts --text &quot;&amp;lt;INPUT TEXT&amp;gt;&quot; --steps 10
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Train with your own dataset&lt;/h2&gt; 
&lt;p&gt;Let&#39;s assume we are training with LJ Speech&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Download the dataset from &lt;a href=&quot;https://keithito.com/LJ-Speech-Dataset/&quot;&gt;here&lt;/a&gt;, extract it to &lt;code&gt;data/LJSpeech-1.1&lt;/code&gt;, and prepare the file lists to point to the extracted data like for &lt;a href=&quot;https://github.com/NVIDIA/tacotron2#setup&quot;&gt;item 5 in the setup of the NVIDIA Tacotron 2 repo&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone and enter the Matcha-TTS repository&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/shivammehta25/Matcha-TTS.git
cd Matcha-TTS
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Install the package from source&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt;Go to &lt;code&gt;configs/data/ljspeech.yaml&lt;/code&gt; and change&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;train_filelist_path: data/filelists/ljs_audio_text_train_filelist.txt
valid_filelist_path: data/filelists/ljs_audio_text_val_filelist.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;5&quot;&gt; 
 &lt;li&gt;Generate normalisation statistics with the yaml file of dataset configuration&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-data-stats -i ljspeech.yaml
# Output:
#{&#39;mel_mean&#39;: -5.53662231756592, &#39;mel_std&#39;: 2.1161014277038574}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Update these values in &lt;code&gt;configs/data/ljspeech.yaml&lt;/code&gt; under &lt;code&gt;data_statistics&lt;/code&gt; key.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;data_statistics:  # Computed for ljspeech dataset
  mel_mean: -5.536622
  mel_std: 2.116101
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;to the paths of your train and validation filelists.&lt;/p&gt; 
&lt;ol start=&quot;6&quot;&gt; 
 &lt;li&gt;Run the training script&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;make train-ljspeech
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python matcha/train.py experiment=ljspeech
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;for a minimum memory run&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python matcha/train.py experiment=ljspeech_min_memory
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;for multi-gpu training, run&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python matcha/train.py experiment=ljspeech trainer.devices=[0,1]
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;7&quot;&gt; 
 &lt;li&gt;Synthesise from the custom trained model&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-tts --text &quot;&amp;lt;INPUT TEXT&amp;gt;&quot; --checkpoint_path &amp;lt;PATH TO CHECKPOINT&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ONNX support&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Special thanks to &lt;a href=&quot;https://github.com/mush42&quot;&gt;@mush42&lt;/a&gt; for implementing ONNX export and inference support.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;It is possible to export Matcha checkpoints to &lt;a href=&quot;https://onnx.ai/&quot;&gt;ONNX&lt;/a&gt;, and run inference on the exported ONNX graph.&lt;/p&gt; 
&lt;h3&gt;ONNX export&lt;/h3&gt; 
&lt;p&gt;To export a checkpoint to ONNX, first install ONNX with&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install onnx
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;then run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python3 -m matcha.onnx.export matcha.ckpt model.onnx --n-timesteps 5
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optionally, the ONNX exporter accepts &lt;strong&gt;vocoder-name&lt;/strong&gt; and &lt;strong&gt;vocoder-checkpoint&lt;/strong&gt; arguments. This enables you to embed the vocoder in the exported graph and generate waveforms in a single run (similar to end-to-end TTS systems).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; that &lt;code&gt;n_timesteps&lt;/code&gt; is treated as a hyper-parameter rather than a model input. This means you should specify it during export (not during inference). If not specified, &lt;code&gt;n_timesteps&lt;/code&gt; is set to &lt;strong&gt;5&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: for now, torch&amp;gt;=2.1.0 is needed for export since the &lt;code&gt;scaled_product_attention&lt;/code&gt; operator is not exportable in older versions. Until the final version is released, those who want to export their models must install torch&amp;gt;=2.1.0 manually as a pre-release.&lt;/p&gt; 
&lt;h3&gt;ONNX Inference&lt;/h3&gt; 
&lt;p&gt;To run inference on the exported model, first install &lt;code&gt;onnxruntime&lt;/code&gt; using&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install onnxruntime
pip install onnxruntime-gpu  # for GPU inference
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;then use the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python3 -m matcha.onnx.infer model.onnx --text &quot;hey&quot; --output-dir ./outputs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also control synthesis parameters:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python3 -m matcha.onnx.infer model.onnx --text &quot;hey&quot; --output-dir ./outputs --temperature 0.4 --speaking_rate 0.9 --spk 0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run inference on &lt;strong&gt;GPU&lt;/strong&gt;, make sure to install &lt;strong&gt;onnxruntime-gpu&lt;/strong&gt; package, and then pass &lt;code&gt;--gpu&lt;/code&gt; to the inference command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python3 -m matcha.onnx.infer model.onnx --text &quot;hey&quot; --output-dir ./outputs --gpu
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you exported only Matcha to ONNX, this will write mel-spectrogram as graphs and &lt;code&gt;numpy&lt;/code&gt; arrays to the output directory. If you embedded the vocoder in the exported graph, this will write &lt;code&gt;.wav&lt;/code&gt; audio files to the output directory.&lt;/p&gt; 
&lt;p&gt;If you exported only Matcha to ONNX, and you want to run a full TTS pipeline, you can pass a path to a vocoder model in &lt;code&gt;ONNX&lt;/code&gt; format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python3 -m matcha.onnx.infer model.onnx --text &quot;hey&quot; --output-dir ./outputs --vocoder hifigan.small.onnx
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will write &lt;code&gt;.wav&lt;/code&gt; audio files to the output directory.&lt;/p&gt; 
&lt;h2&gt;Extract phoneme alignments from Matcha-TTS&lt;/h2&gt; 
&lt;p&gt;If the dataset is structured as&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;data/
â””â”€â”€ LJSpeech-1.1
    â”œâ”€â”€ metadata.csv
    â”œâ”€â”€ README
    â”œâ”€â”€ test.txt
    â”œâ”€â”€ train.txt
    â”œâ”€â”€ val.txt
    â””â”€â”€ wavs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can extract the phoneme level alignments from a Trained Matcha-TTS model using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python  matcha/utils/get_durations_from_trained_model.py -i dataset_yaml -c &amp;lt;checkpoint&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python  matcha/utils/get_durations_from_trained_model.py -i ljspeech.yaml -c matcha_ljspeech.ckpt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or simply:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;matcha-tts-get-durations -i ljspeech.yaml -c matcha_ljspeech.ckpt
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;h2&gt;Train using extracted alignments&lt;/h2&gt; 
&lt;p&gt;In the datasetconfig turn on load duration. Example: &lt;code&gt;ljspeech.yaml&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;load_durations: True
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or see an examples in configs/experiment/ljspeech_from_durations.yaml&lt;/p&gt; 
&lt;h2&gt;Citation information&lt;/h2&gt; 
&lt;p&gt;If you use our code or otherwise find this work useful, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;@inproceedings{mehta2024matcha,
  title={Matcha-{TTS}: A fast {TTS} architecture with conditional flow matching},
  author={Mehta, Shivam and Tu, Ruibo and Beskow, Jonas and Sz{\&#39;e}kely, {\&#39;E}va and Henter, Gustav Eje},
  booktitle={Proc. ICASSP},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;Since this code uses &lt;a href=&quot;https://github.com/ashleve/lightning-hydra-template&quot;&gt;Lightning-Hydra-Template&lt;/a&gt;, you have all the powers that come with it.&lt;/p&gt; 
&lt;p&gt;Other source code we would like to acknowledge:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/coqui-ai/TTS/tree/dev&quot;&gt;Coqui-TTS&lt;/a&gt;: For helping me figure out how to make cython binaries pip installable and encouragement&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face Diffusers&lt;/a&gt;: For their awesome diffusers library and its components&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/huawei-noah/Speech-Backbones/tree/main/Grad-TTS&quot;&gt;Grad-TTS&lt;/a&gt;: For the monotonic alignment search source code&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/DiffEqML/torchdyn&quot;&gt;torchdyn&lt;/a&gt;: Useful for trying other ODE solvers during research and development&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://nn.labml.ai/transformers/rope/index.html&quot;&gt;labml.ai&lt;/a&gt;: For the RoPE implementation&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>mrdbourke/pytorch-deep-learning</title>
      <link>https://github.com/mrdbourke/pytorch-deep-learning</link>
      <description>&lt;p&gt;Materials for the Learn PyTorch for Deep Learning: Zero to Mastery course.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Learn PyTorch for Deep Learning&lt;/h1&gt; 
&lt;p&gt;Welcome to the &lt;a href=&quot;https://dbourke.link/ZTMPyTorch&quot;&gt;Zero to Mastery Learn PyTorch for Deep Learning course&lt;/a&gt;, the second best place to learn PyTorch on the internet (the first being the &lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;PyTorch documentation&lt;/a&gt;).&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Update April 2023:&lt;/strong&gt; New &lt;a href=&quot;https://www.learnpytorch.io/pytorch_2_intro/&quot;&gt;tutorial for PyTorch 2.0&lt;/a&gt; is live! And because PyTorch 2.0 is an additive (new features) and backward-compatible release, all previous course materials will &lt;em&gt;still&lt;/em&gt; work with PyTorch 2.0.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://learnpytorch.io&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/misc-pytorch-course-launch-cover-white-text-black-background.jpg&quot; width=&quot;750&quot; alt=&quot;pytorch deep learning by zero to mastery cover photo with different sections of the course&quot;&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Contents of this page&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning#course-materialsoutline&quot;&gt;Course materials/outline&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning#about-this-course&quot;&gt;About this course&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning#status&quot;&gt;Status&lt;/a&gt; (the progress of the course creation)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning#log&quot;&gt;Log&lt;/a&gt; (a log of the course material creation process)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Course materials/outline&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ðŸ“– &lt;strong&gt;Online book version:&lt;/strong&gt; All of course materials are available in a readable online book at &lt;a href=&quot;https://learnpytorch.io&quot;&gt;learnpytorch.io&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;ðŸŽ¥ &lt;strong&gt;First five sections on YouTube:&lt;/strong&gt; Learn Pytorch in a day by watching the &lt;a href=&quot;https://youtu.be/Z_ikDlimN6A&quot;&gt;first 25-hours of material&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;ðŸ”¬ &lt;strong&gt;Course focus:&lt;/strong&gt; code, code, code, experiment, experiment, experiment.&lt;/li&gt; 
 &lt;li&gt;ðŸƒâ€â™‚ï¸ &lt;strong&gt;Teaching style:&lt;/strong&gt; &lt;a href=&quot;https://sive.rs/kimo&quot;&gt;https://sive.rs/kimo&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;ðŸ¤” &lt;strong&gt;Ask a question:&lt;/strong&gt; See the &lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/discussions&quot;&gt;GitHub Discussions page&lt;/a&gt; for existing questions/ask your own.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Section&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;What does it cover?&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Exercises &amp;amp; Extra-curriculum&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Slides&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/00_pytorch_fundamentals/&quot;&gt;00 - PyTorch Fundamentals&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Many fundamental PyTorch operations used for deep learning and neural networks.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/00_pytorch_fundamentals/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/00_pytorch_and_deep_learning_fundamentals.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/01_pytorch_workflow/&quot;&gt;01 - PyTorch Workflow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Provides an outline for approaching deep learning problems and building neural networks with PyTorch.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/01_pytorch_workflow/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/01_pytorch_workflow.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/02_pytorch_classification/&quot;&gt;02 - PyTorch Neural Network Classification&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Uses the PyTorch workflow from 01 to go through a neural network classification problem.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/02_pytorch_classification/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/02_pytorch_classification.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/03_pytorch_computer_vision/&quot;&gt;03 - PyTorch Computer Vision&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Let&#39;s see how PyTorch can be used for computer vision problems using the same workflow from 01 &amp;amp; 02.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/03_pytorch_computer_vision/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/03_pytorch_computer_vision.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/04_pytorch_custom_datasets/&quot;&gt;04 - PyTorch Custom Datasets&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;How do you load a custom dataset into PyTorch? Also we&#39;ll be laying the foundations in this notebook for our modular code (covered in 05).&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/04_pytorch_custom_datasets/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/04_pytorch_custom_datasets.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/05_pytorch_going_modular/&quot;&gt;05 - PyTorch Going Modular&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;PyTorch is designed to be modular, let&#39;s turn what we&#39;ve created into a series of Python scripts (this is how you&#39;ll often find PyTorch code in the wild).&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/05_pytorch_going_modular/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/05_pytorch_going_modular.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/06_pytorch_transfer_learning/&quot;&gt;06 - PyTorch Transfer Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Let&#39;s take a well performing pre-trained model and adjust it to one of our own problems.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/06_pytorch_transfer_learning/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/06_pytorch_transfer_learning.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/07_pytorch_experiment_tracking/&quot;&gt;07 - Milestone Project 1: PyTorch Experiment Tracking&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;We&#39;ve built a bunch of models... wouldn&#39;t it be good to track how they&#39;re all going?&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/07_pytorch_experiment_tracking/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/07_pytorch_experiment_tracking.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/08_pytorch_paper_replicating/&quot;&gt;08 - Milestone Project 2: PyTorch Paper Replicating&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;PyTorch is the most popular deep learning framework for machine learning research, let&#39;s see why by replicating a machine learning paper.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/08_pytorch_paper_replicating/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/08_pytorch_paper_replicating.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/09_pytorch_model_deployment/&quot;&gt;09 - Milestone Project 3: Model Deployment&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;So we&#39;ve built a working PyTorch model... how do we get it in the hands of others? Hint: deploy it to the internet.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/09_pytorch_model_deployment/#exercises&quot;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/09_pytorch_model_deployment.pdf&quot;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/pytorch_extra_resources/&quot;&gt;PyTorch Extra Resources&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;This course covers a large amount of PyTorch and deep learning but the field of machine learning is vast, inside here you&#39;ll find recommended books and resources for: PyTorch and deep learning, ML engineering, NLP (natural language processing), time series data, where to find datasets and more.&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/pytorch_cheatsheet/&quot;&gt;PyTorch Cheatsheet&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A very quick overview of some of the main features of PyTorch plus links to various resources where more can be found in the course and in the PyTorch documentation.&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnpytorch.io/pytorch_2_intro/&quot;&gt;A Quick PyTorch 2.0 Tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A fasssssst introduction to PyTorch 2.0, what&#39;s new and how to get started along with resources to learn more.&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Status&lt;/h2&gt; 
&lt;p&gt;All materials completed and videos published on Zero to Mastery!&lt;/p&gt; 
&lt;p&gt;See the project page for work-in-progress board - &lt;a href=&quot;https://github.com/users/mrdbourke/projects/1&quot;&gt;https://github.com/users/mrdbourke/projects/1&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Total video count:&lt;/strong&gt; 321&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Done skeleton code for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Done annotations (text) for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Done images for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Done keynotes for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Done exercises and solutions for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning#log&quot;&gt;log&lt;/a&gt; for almost daily updates.&lt;/p&gt; 
&lt;h2&gt;About this course&lt;/h2&gt; 
&lt;h3&gt;Who is this course for?&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;You:&lt;/strong&gt; Are a beginner in the field of machine learning or deep learning and would like to learn PyTorch.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;This course:&lt;/strong&gt; Teaches you PyTorch and many machine learning concepts in a hands-on, code-first way.&lt;/p&gt; 
&lt;p&gt;If you already have 1-year+ experience in machine learning, this course may help but it is specifically designed to be beginner-friendly.&lt;/p&gt; 
&lt;h3&gt;What are the prerequisites?&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;3-6 months coding Python.&lt;/li&gt; 
 &lt;li&gt;At least one beginner machine learning course (however this might be able to be skipped, resources are linked for many different topics).&lt;/li&gt; 
 &lt;li&gt;Experience using Jupyter Notebooks or Google Colab (though you can pick this up as we go along).&lt;/li&gt; 
 &lt;li&gt;A willingness to learn (most important).&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For 1 &amp;amp; 2, I&#39;d recommend the &lt;a href=&quot;https://dbourke.link/ZTMMLcourse&quot;&gt;Zero to Mastery Data Science and Machine Learning Bootcamp&lt;/a&gt;, it&#39;ll teach you the fundamentals of machine learning and Python (I&#39;m biased though, I also teach that course).&lt;/p&gt; 
&lt;h3&gt;How is the course taught?&lt;/h3&gt; 
&lt;p&gt;All of the course materials are available for free in an online book at &lt;a href=&quot;https://learnpytorch.io&quot;&gt;learnpytorch.io&lt;/a&gt;. If you like to read, I&#39;d recommend going through the resources there.&lt;/p&gt; 
&lt;p&gt;If you prefer to learn via video, the course is also taught in apprenticeship-style format, meaning I write PyTorch code, you write PyTorch code.&lt;/p&gt; 
&lt;p&gt;There&#39;s a reason the course motto&#39;s include &lt;em&gt;if in doubt, run the code&lt;/em&gt; and &lt;em&gt;experiment, experiment, experiment!&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;My whole goal is to help you to do one thing: learn machine learning by writing PyTorch code.&lt;/p&gt; 
&lt;p&gt;The code is all written via &lt;a href=&quot;https://colab.research.google.com&quot;&gt;Google Colab Notebooks&lt;/a&gt; (you could also use Jupyter Notebooks), an incredible free resource to experiment with machine learning.&lt;/p&gt; 
&lt;h3&gt;What will I get if I finish the course?&lt;/h3&gt; 
&lt;p&gt;There&#39;s certificates and all that jazz if you go through the videos.&lt;/p&gt; 
&lt;p&gt;But certificates are meh.&lt;/p&gt; 
&lt;p&gt;You can consider this course a machine learning momentum builder.&lt;/p&gt; 
&lt;p&gt;By the end, you&#39;ll have written hundreds of lines of PyTorch code.&lt;/p&gt; 
&lt;p&gt;And will have been exposed to many of the most important concepts in machine learning.&lt;/p&gt; 
&lt;p&gt;So when you go to build your own machine learning projects or inspect a public machine learning project made with PyTorch, it&#39;ll feel familiar and if it doesn&#39;t, at least you&#39;ll know where to look.&lt;/p&gt; 
&lt;h3&gt;What will I build in the course?&lt;/h3&gt; 
&lt;p&gt;We start with the barebone fundamentals of PyTorch and machine learning, so even if you&#39;re new to machine learning you&#39;ll be caught up to speed.&lt;/p&gt; 
&lt;p&gt;Then weâ€™ll explore more advanced areas including PyTorch neural network classification, PyTorch workflows, computer vision, custom datasets, experiment tracking, model deployment, and my personal favourite: transfer learning, a powerful technique for taking what one machine learning model has learned on another problem and applying it to your own!&lt;/p&gt; 
&lt;p&gt;Along the way, youâ€™ll build three milestone projects surrounding an overarching project called FoodVision, a neural network computer vision model to classify images of food.&lt;/p&gt; 
&lt;p&gt;These milestone projects will help you practice using PyTorch to cover important machine learning concepts and create a portfolio you can show employers and say &quot;here&#39;s what I&#39;ve done&quot;.&lt;/p&gt; 
&lt;h3&gt;How do I get started?&lt;/h3&gt; 
&lt;p&gt;You can read the materials on any device but this course is best viewed and coded along within a desktop browser.&lt;/p&gt; 
&lt;p&gt;The course uses a free tool called Google Colab. If you&#39;ve got no experience with it, I&#39;d go through the free &lt;a href=&quot;https://colab.research.google.com/notebooks/basic_features_overview.ipynb&quot;&gt;Introduction to Google Colab tutorial&lt;/a&gt; and then come back here.&lt;/p&gt; 
&lt;p&gt;To start:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Click on one of the notebook or section links above like &quot;&lt;a href=&quot;https://www.learnpytorch.io/00_pytorch_fundamentals/&quot;&gt;00. PyTorch Fundamentals&lt;/a&gt;&quot;.&lt;/li&gt; 
 &lt;li&gt;Click the &quot;Open in Colab&quot; button up the top.&lt;/li&gt; 
 &lt;li&gt;Press SHIFT+Enter a few times and see what happens.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;My question isn&#39;t answered&lt;/h3&gt; 
&lt;p&gt;Please leave a &lt;a href=&quot;https://github.com/mrdbourke/pytorch-deep-learning/discussions&quot;&gt;discussion&lt;/a&gt; or send me an email directly: daniel (at) mrdbourke (dot) com.&lt;/p&gt; 
&lt;h2&gt;Log&lt;/h2&gt; 
&lt;p&gt;Almost daily updates of what&#39;s happening.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;15 May 2023 - PyTorch 2.0 tutorial finished + videos added to ZTM/Udemy, see code: &lt;a href=&quot;https://www.learnpytorch.io/pytorch_2_intro/&quot;&gt;https://www.learnpytorch.io/pytorch_2_intro/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;13 Apr 2023 - update PyTorch 2.0 notebook&lt;/li&gt; 
 &lt;li&gt;30 Mar 2023 - update PyTorch 2.0 notebook with more info/clean code&lt;/li&gt; 
 &lt;li&gt;23 Mar 2023 - upgrade PyTorch 2.0 tutorial with annotations and images&lt;/li&gt; 
 &lt;li&gt;13 Mar 2023 - add starter code for PyTorch 2.0 tutorial&lt;/li&gt; 
 &lt;li&gt;18 Nov 2022 - add a reference for 3 most common errors in PyTorch + links to course sections for more: &lt;a href=&quot;https://www.learnpytorch.io/pytorch_most_common_errors/&quot;&gt;https://www.learnpytorch.io/pytorch_most_common_errors/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;9 Nov 2022 - add PyTorch cheatsheet for a very quick overview of the main features of PyTorch + links to course sections: &lt;a href=&quot;https://www.learnpytorch.io/pytorch_cheatsheet/&quot;&gt;https://www.learnpytorch.io/pytorch_cheatsheet/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;9 Nov 2022 - full course materials (300+ videos) are now live on Udemy! You can sign up here: &lt;a href=&quot;https://www.udemy.com/course/pytorch-for-deep-learning/?couponCode=ZTMGOODIES7&quot;&gt;https://www.udemy.com/course/pytorch-for-deep-learning/?couponCode=ZTMGOODIES7&lt;/a&gt; (launch deal code valid for 3-4 days from this line)&lt;/li&gt; 
 &lt;li&gt;4 Nov 2022 - add a notebook for PyTorch Cheatsheet in &lt;code&gt;extras/&lt;/code&gt; (a simple overview of many of the most important functionality of PyTorch)&lt;/li&gt; 
 &lt;li&gt;2 Oct 2022 - all videos for section 08 and 09 published (100+ videos for the last two sections)!&lt;/li&gt; 
 &lt;li&gt;30 Aug 2022 - recorded 15 videos for 09, total videos: 321, finished section 09 videos!!!! ... even bigger than 08!!&lt;/li&gt; 
 &lt;li&gt;29 Aug 2022 - recorded 16 videos for 09, total videos: 306&lt;/li&gt; 
 &lt;li&gt;28 Aug 2022 - recorded 11 videos for 09, total videos: 290&lt;/li&gt; 
 &lt;li&gt;27 Aug 2022 - recorded 16 videos for 09, total videos: 279&lt;/li&gt; 
 &lt;li&gt;26 Aug 2022 - add finishing touchs to notebook 09, add slides for 09, create solutions and exercises for 09&lt;/li&gt; 
 &lt;li&gt;25 Aug 2022 - add annotations and cleanup 09, remove TK&#39;s, cleanup images, make slides for 09&lt;/li&gt; 
 &lt;li&gt;24 Aug 2022 - add annotations to 09, main takeaways, exercises and extra-curriculum done&lt;/li&gt; 
 &lt;li&gt;23 Aug 2022 - add annotations to 09, add plenty of images/slides&lt;/li&gt; 
 &lt;li&gt;22 Aug 2022 - add annotations to 09, start working on slides/images&lt;/li&gt; 
 &lt;li&gt;20 Aug 2022 - add annotations to 09&lt;/li&gt; 
 &lt;li&gt;19 Aug 2022 - add annotations to 09, check out the awesome demos!&lt;/li&gt; 
 &lt;li&gt;18 Aug 2022 - add annotations to 09&lt;/li&gt; 
 &lt;li&gt;17 Aug 2022 - add annotations to 09&lt;/li&gt; 
 &lt;li&gt;16 Aug 2022 - add annotations to 09&lt;/li&gt; 
 &lt;li&gt;15 Aug 2022 - add annotations to 09&lt;/li&gt; 
 &lt;li&gt;13 Aug 2022 - add annotations to 09&lt;/li&gt; 
 &lt;li&gt;12 Aug 2022 - add demo files for notebook 09 to &lt;code&gt;demos/&lt;/code&gt;, start annotating notebook 09 with explainer text&lt;/li&gt; 
 &lt;li&gt;11 Aug 2022 - finish skeleton code for notebook 09, course finishes deploying 2x models, one for FoodVision Mini &amp;amp; one for (secret)&lt;/li&gt; 
 &lt;li&gt;10 Aug 2022 - add section for PyTorch Extra Resources (places to learn more about PyTorch/deep learning): &lt;a href=&quot;https://www.learnpytorch.io/pytorch_extra_resources/&quot;&gt;https://www.learnpytorch.io/pytorch_extra_resources/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;09 Aug 2022 - add more skeleton code to notebook 09&lt;/li&gt; 
 &lt;li&gt;08 Aug 2022 - create draft notebook for 09, end goal to deploy FoodVision Mini model and make it publically accessible&lt;/li&gt; 
 &lt;li&gt;05 Aug 2022 - recorded 11 videos for 08, total videos: 263, section 08 videos finished!... the biggest section so far&lt;/li&gt; 
 &lt;li&gt;04 Aug 2022 - recorded 13 videos for 08, total videos: 252&lt;/li&gt; 
 &lt;li&gt;03 Aug 2022 - recorded 3 videos for 08, total videos: 239&lt;/li&gt; 
 &lt;li&gt;02 Aug 2022 - recorded 12 videos for 08, total videos: 236&lt;/li&gt; 
 &lt;li&gt;30 July 2022 - recorded 11 videos for 08, total videos: 224&lt;/li&gt; 
 &lt;li&gt;29 July 2022 - add exercises + solutions for 08, see live walkthrough on YouTube: &lt;a href=&quot;https://youtu.be/tjpW_BY8y3g&quot;&gt;https://youtu.be/tjpW_BY8y3g&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;28 July 2022 - add slides for 08&lt;/li&gt; 
 &lt;li&gt;27 July 2022 - cleanup much of 08, start on slides for 08, exercises and extra-curriculum next&lt;/li&gt; 
 &lt;li&gt;26 July 2022 - add annotations and images for 08&lt;/li&gt; 
 &lt;li&gt;25 July 2022 - add annotations for 08&lt;/li&gt; 
 &lt;li&gt;24 July 2022 - launched first half of course (notebooks 00-04) in a single video (25+ hours!!!) on YouTube: &lt;a href=&quot;https://youtu.be/Z_ikDlimN6A&quot;&gt;https://youtu.be/Z_ikDlimN6A&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;21 July 2022 - add annotations and images for 08&lt;/li&gt; 
 &lt;li&gt;20 July 2022 - add annotations and images for 08, getting so close! this is an epic section&lt;/li&gt; 
 &lt;li&gt;19 July 2022 - add annotations and images for 08&lt;/li&gt; 
 &lt;li&gt;15 July 2022 - add annotations and images for 08&lt;/li&gt; 
 &lt;li&gt;14 July 2022 - add annotations for 08&lt;/li&gt; 
 &lt;li&gt;12 July 2022 - add annotations for 08, woo woo this is bigggg section!&lt;/li&gt; 
 &lt;li&gt;11 July 2022 - add annotations for 08&lt;/li&gt; 
 &lt;li&gt;9 July 2022 - add annotations for 08&lt;/li&gt; 
 &lt;li&gt;8 July 2022 - add a bunch of annotations to 08&lt;/li&gt; 
 &lt;li&gt;6 July 2022 - course launched on ZTM Academy with videos for sections 00-07! ðŸš€ - &lt;a href=&quot;https://dbourke.link/ZTMPyTorch&quot;&gt;https://dbourke.link/ZTMPyTorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;1 July 2022 - add annotations and images for 08&lt;/li&gt; 
 &lt;li&gt;30 June 2022 - add annotations for 08&lt;/li&gt; 
 &lt;li&gt;28 June 2022 - recorded 11 videos for section 07, total video count 213, all videos for section 07 complete!&lt;/li&gt; 
 &lt;li&gt;27 June 2022 - recorded 11 videos for section 07, total video count 202&lt;/li&gt; 
 &lt;li&gt;25 June 2022 - recreated 7 videos for section 06 to include updated APIs, total video count 191&lt;/li&gt; 
 &lt;li&gt;24 June 2022 - recreated 12 videos for section 06 to include updated APIs&lt;/li&gt; 
 &lt;li&gt;23 June 2022 - finish annotations for 07, add exercise template and solutions for 07 + video walkthrough on YouTube: &lt;a href=&quot;https://youtu.be/cO_r2FYcAjU&quot;&gt;https://youtu.be/cO_r2FYcAjU&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;21 June 2022 - make 08 runnable end-to-end, add images and annotations for 07&lt;/li&gt; 
 &lt;li&gt;17 June 2022 - fix up 06, 07 v2 for upcoming torchvision version upgrade, add plenty of annotations to 08&lt;/li&gt; 
 &lt;li&gt;13 June 2022 - add notebook 08 first version, starting to replicate the Vision Transformer paper&lt;/li&gt; 
 &lt;li&gt;10 June 2022 - add annotations for 07 v2&lt;/li&gt; 
 &lt;li&gt;09 June 2022 - create 07 v2 for &lt;code&gt;torchvision&lt;/code&gt; v0.13 (this will replace 07 v1 when &lt;code&gt;torchvision=0.13&lt;/code&gt; is released)&lt;/li&gt; 
 &lt;li&gt;08 June 2022 - adapt 06 v2 for &lt;code&gt;torchvision&lt;/code&gt; v0.13 (this will replace 06 v1 when &lt;code&gt;torchvision=0.13&lt;/code&gt; is released)&lt;/li&gt; 
 &lt;li&gt;07 June 2022 - create notebook 06 v2 for upcoming &lt;code&gt;torchvision&lt;/code&gt; v0.13 update (new transfer learning methods)&lt;/li&gt; 
 &lt;li&gt;04 June 2022 - add annotations for 07&lt;/li&gt; 
 &lt;li&gt;03 June 2022 - huuuuuuge amount of annotations added to 07&lt;/li&gt; 
 &lt;li&gt;31 May 2022 - add a bunch of annotations for 07, make code runnable end-to-end&lt;/li&gt; 
 &lt;li&gt;30 May 2022 - record 4 videos for 06, finished section 06, onto section 07, total videos 186&lt;/li&gt; 
 &lt;li&gt;28 May 2022 - record 10 videos for 06, total videos 182&lt;/li&gt; 
 &lt;li&gt;24 May 2022 - add solutions and exercises for 06&lt;/li&gt; 
 &lt;li&gt;23 May 2022 - finished annotations and images for 06, time to do exercises and solutions&lt;/li&gt; 
 &lt;li&gt;22 May 2202 - add plenty of images to 06&lt;/li&gt; 
 &lt;li&gt;18 May 2022 - add plenty of annotations to 06&lt;/li&gt; 
 &lt;li&gt;17 May 2022 - added a bunch of annotations for section 06&lt;/li&gt; 
 &lt;li&gt;16 May 2022 - recorded 10 videos for section 05, finish videos for section 05 âœ…&lt;/li&gt; 
 &lt;li&gt;12 May 2022 - added exercises and solutions for 05&lt;/li&gt; 
 &lt;li&gt;11 May 2022 - clean up part 1 and part 2 notebooks for 05, make slides for 05, start on exercises and solutions for 05&lt;/li&gt; 
 &lt;li&gt;10 May 2022 - huuuuge updates to the 05 section, see the website, it looks pretty: &lt;a href=&quot;https://www.learnpytorch.io/05_pytorch_going_modular/&quot;&gt;https://www.learnpytorch.io/05_pytorch_going_modular/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;09 May 2022 - add a bunch of materials for 05, cleanup docs&lt;/li&gt; 
 &lt;li&gt;08 May 2022 - add a bunch of materials for 05&lt;/li&gt; 
 &lt;li&gt;06 May 2022 - continue making materials for 05&lt;/li&gt; 
 &lt;li&gt;05 May 2022 - update section 05 with headings/outline&lt;/li&gt; 
 &lt;li&gt;28 Apr 2022 - recorded 13 videos for 04, finished videos for 04, now to make materials for 05&lt;/li&gt; 
 &lt;li&gt;27 Apr 2022 - recorded 3 videos for 04&lt;/li&gt; 
 &lt;li&gt;26 Apr 2022 - recorded 10 videos for 04&lt;/li&gt; 
 &lt;li&gt;25 Apr 2022 - recorded 11 videos for 04&lt;/li&gt; 
 &lt;li&gt;24 Apr 2022 - prepared slides for 04&lt;/li&gt; 
 &lt;li&gt;23 Apr 2022 - recorded 6 videos for 03, finished videos for 03, now to 04&lt;/li&gt; 
 &lt;li&gt;22 Apr 2022 - recorded 5 videos for 03&lt;/li&gt; 
 &lt;li&gt;21 Apr 2022 - recorded 9 videos for 03&lt;/li&gt; 
 &lt;li&gt;20 Apr 2022 - recorded 3 videos for 03&lt;/li&gt; 
 &lt;li&gt;19 Apr 2022 - recorded 11 videos for 03&lt;/li&gt; 
 &lt;li&gt;18 Apr 2022 - finish exercises/solutions for 04, added live-coding walkthrough of 04 exercises/solutions on YouTube: &lt;a href=&quot;https://youtu.be/vsFMF9wqWx0&quot;&gt;https://youtu.be/vsFMF9wqWx0&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;16 Apr 2022 - finish exercises/solutions for 03, added live-coding walkthrough of 03 exercises/solutions on YouTube: &lt;a href=&quot;https://youtu.be/_PibmqpEyhA&quot;&gt;https://youtu.be/_PibmqpEyhA&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;14 Apr 2022 - add final images/annotations for 04, begin on exercises/solutions for 03 &amp;amp; 04&lt;/li&gt; 
 &lt;li&gt;13 Apr 2022 - add more images/annotations for 04&lt;/li&gt; 
 &lt;li&gt;3 Apr 2022 - add more annotations for 04&lt;/li&gt; 
 &lt;li&gt;2 Apr 2022 - add more annotations for 04&lt;/li&gt; 
 &lt;li&gt;1 Apr 2022 - add more annotations for 04&lt;/li&gt; 
 &lt;li&gt;31 Mar 2022 - add more annotations for 04&lt;/li&gt; 
 &lt;li&gt;29 Mar 2022 - add more annotations for 04&lt;/li&gt; 
 &lt;li&gt;27 Mar 2022 - starting to add annotations for 04&lt;/li&gt; 
 &lt;li&gt;26 Mar 2022 - making dataset for 04&lt;/li&gt; 
 &lt;li&gt;25 Mar 2022 - make slides for 03&lt;/li&gt; 
 &lt;li&gt;24 Mar 2022 - fix error for 03 not working in docs (finally)&lt;/li&gt; 
 &lt;li&gt;23 Mar 2022 - add more images for 03&lt;/li&gt; 
 &lt;li&gt;22 Mar 2022 - add images for 03&lt;/li&gt; 
 &lt;li&gt;20 Mar 2022 - add more annotations for 03&lt;/li&gt; 
 &lt;li&gt;18 Mar 2022 - add more annotations for 03&lt;/li&gt; 
 &lt;li&gt;17 Mar 2022 - add more annotations for 03&lt;/li&gt; 
 &lt;li&gt;16 Mar 2022 - add more annotations for 03&lt;/li&gt; 
 &lt;li&gt;15 Mar 2022 - add more annotations for 03&lt;/li&gt; 
 &lt;li&gt;14 Mar 2022 - start adding annotations for notebook 03, see the work in progress here: &lt;a href=&quot;https://www.learnpytorch.io/03_pytorch_computer_vision/&quot;&gt;https://www.learnpytorch.io/03_pytorch_computer_vision/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;12 Mar 2022 - recorded 12 videos for 02, finished section 02, now onto making materials for 03, 04, 05&lt;/li&gt; 
 &lt;li&gt;11 Mar 2022 - recorded 9 videos for 02&lt;/li&gt; 
 &lt;li&gt;10 Mar 2022 - recorded 10 videos for 02&lt;/li&gt; 
 &lt;li&gt;9 Mar 2022 - cleaning up slides/code for 02, getting ready for recording&lt;/li&gt; 
 &lt;li&gt;8 Mar 2022 - recorded 9 videos for section 01, finished section 01, now onto 02&lt;/li&gt; 
 &lt;li&gt;7 Mar 2022 - recorded 4 videos for section 01&lt;/li&gt; 
 &lt;li&gt;6 Mar 2022 - recorded 4 videos for section 01&lt;/li&gt; 
 &lt;li&gt;4 Mar 2022 - recorded 10 videos for section 01&lt;/li&gt; 
 &lt;li&gt;20 Feb 2022 - recorded 8 videos for section 00, finished section, now onto 01&lt;/li&gt; 
 &lt;li&gt;18 Feb 2022 - recorded 13 videos for section 00&lt;/li&gt; 
 &lt;li&gt;17 Feb 2022 - recorded 11 videos for section 00&lt;/li&gt; 
 &lt;li&gt;16 Feb 2022 - added setup guide&lt;/li&gt; 
 &lt;li&gt;12 Feb 2022 - tidy up README with table of course materials, finish images and slides for 01&lt;/li&gt; 
 &lt;li&gt;10 Feb 2022 - finished slides and images for 00, notebook is ready for publishing: &lt;a href=&quot;https://www.learnpytorch.io/00_pytorch_fundamentals/&quot;&gt;https://www.learnpytorch.io/00_pytorch_fundamentals/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;01-07 Feb 2022 - add annotations for 02, finished, still need images, going to work on exercises/solutions today&lt;/li&gt; 
 &lt;li&gt;31 Jan 2022 - start adding annotations for 02&lt;/li&gt; 
 &lt;li&gt;28 Jan 2022 - add exercies and solutions for 01&lt;/li&gt; 
 &lt;li&gt;26 Jan 2022 - lots more annotations to 01, should be finished tomorrow, will do exercises + solutions then too&lt;/li&gt; 
 &lt;li&gt;24 Jan 2022 - add a bunch of annotations to 01&lt;/li&gt; 
 &lt;li&gt;21 Jan 2022 - start adding annotations for 01&lt;/li&gt; 
 &lt;li&gt;20 Jan 2022 - finish annotations for 00 (still need to add images), add exercises and solutions for 00&lt;/li&gt; 
 &lt;li&gt;19 Jan 2022 - add more annotations for 00&lt;/li&gt; 
 &lt;li&gt;18 Jan 2022 - add more annotations for 00&lt;/li&gt; 
 &lt;li&gt;17 Jan 2022 - back from holidays, adding more annotations to 00&lt;/li&gt; 
 &lt;li&gt;10 Dec 2021 - start adding annotations for 00&lt;/li&gt; 
 &lt;li&gt;9 Dec 2021 - Created a website for the course (&lt;a href=&quot;https://learnpytorch.io&quot;&gt;learnpytorch.io&lt;/a&gt;) you&#39;ll see updates posted there as development continues&lt;/li&gt; 
 &lt;li&gt;8 Dec 2021 - Clean up notebook 07, starting to go back through code and add annotations&lt;/li&gt; 
 &lt;li&gt;26 Nov 2021 - Finish skeleton code for 07, added four different experiments, need to clean up and make more straightforward&lt;/li&gt; 
 &lt;li&gt;25 Nov 2021 - clean code for 06, add skeleton code for 07 (experiment tracking)&lt;/li&gt; 
 &lt;li&gt;24 Nov 2021 - Update 04, 05, 06 notebooks for easier digestion and learning, each section should cover a max of 3 big ideas, 05 is now dedicated to turning notebook code into modular code&lt;/li&gt; 
 &lt;li&gt;22 Nov 2021 - Update 04 train and test functions to make more straightforward&lt;/li&gt; 
 &lt;li&gt;19 Nov 2021 - Added 05 (transfer learning) notebook, update custom data loading code in 04&lt;/li&gt; 
 &lt;li&gt;18 Nov 2021 - Updated vision code for 03 and added custom dataset loading code in 04&lt;/li&gt; 
 &lt;li&gt;12 Nov 2021 - Added a bunch of skeleton code to notebook 04 for custom dataset loading, next is modelling with custom data&lt;/li&gt; 
 &lt;li&gt;10 Nov 2021 - researching best practice for custom datasets for 04&lt;/li&gt; 
 &lt;li&gt;9 Nov 2021 - Update 03 skeleton code to finish off building CNN model, onto 04 for loading custom datasets&lt;/li&gt; 
 &lt;li&gt;4 Nov 2021 - Add GPU code to 03 + train/test loops + &lt;code&gt;helper_functions.py&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;3 Nov 2021 - Add basic start for 03, going to finish by end of week&lt;/li&gt; 
 &lt;li&gt;29 Oct 2021 - Tidied up skeleton code for 02, still a few more things to clean/tidy, created 03&lt;/li&gt; 
 &lt;li&gt;28 Oct 2021 - Finished skeleton code for 02, going to clean/tidy tomorrow, 03 next week&lt;/li&gt; 
 &lt;li&gt;27 Oct 2021 - add a bunch of code for 02, going to finish tomorrow/by end of week&lt;/li&gt; 
 &lt;li&gt;26 Oct 2021 - update 00, 01, 02 with outline/code, skeleton code for 00 &amp;amp; 01 done, 02 next&lt;/li&gt; 
 &lt;li&gt;23, 24 Oct 2021 - update 00 and 01 notebooks with more outline/code&lt;/li&gt; 
 &lt;li&gt;20 Oct 2021 - add v0 outlines for 01 and 02, add rough outline of course to README, this course will focus on less but better&lt;/li&gt; 
 &lt;li&gt;19 Oct 2021 - Start repo ðŸ”¥, add fundamentals notebook draft v0&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>AI4Finance-Foundation/FinRobot</title>
      <link>https://github.com/AI4Finance-Foundation/FinRobot</link>
      <description>&lt;p&gt;FinRobot: An Open-Source AI Agent Platform for Financial Analysis using LLMs ðŸš€ ðŸš€ ðŸš€&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; width=&quot;30%&quot; alt=&quot;image&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinGPT/assets/31713746/e0371951-1ce1-488e-aa25-0992dafcc139&quot;&gt; 
&lt;/div&gt; 
&lt;h1&gt;FinRobot: An Open-Source AI Agent Platform for Financial Analysis using Large Language Models&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;%5Bhttps://pepy.tech/project/finrobot%5D(https://pepy.tech/project/finrobot)&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/finrobot&quot; alt=&quot;Downloads&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pepy.tech/project/finrobot&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/finrobot/week&quot; alt=&quot;Downloads&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.python.org/downloads/release/python-360/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.6-blue.svg?sanitize=true&quot; alt=&quot;Python 3.8&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/finrobot/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/finrobot.svg?sanitize=true&quot; alt=&quot;PyPI&quot;&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/github/license/AI4Finance-Foundation/finrobot.svg?color=brightgreen&quot; alt=&quot;License&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/issues-raw/AI4Finance-Foundation/finrobot?label=Issues&quot; alt=&quot;&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/issues-closed-raw/AI4Finance-Foundation/finrobot?label=Closed+Issues&quot; alt=&quot;&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/issues-pr-raw/AI4Finance-Foundation/finrobot?label=Open+PRs&quot; alt=&quot;&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/issues-pr-closed-raw/AI4Finance-Foundation/finrobot?label=Closed+PRs&quot; alt=&quot;&quot;&gt;&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRobot/master/figs/logo_white_background.jpg&quot; width=&quot;40%&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;FinRobot&lt;/strong&gt; is an AI Agent Platform that transcends the scope of FinGPT, representing a comprehensive solution meticulously designed for financial applications. It integrates &lt;strong&gt;a diverse array of AI technologies&lt;/strong&gt;, extending beyond mere language models. This expansive vision highlights the platform&#39;s versatility and adaptability, addressing the multifaceted needs of the financial industry.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Concept of AI Agent&lt;/strong&gt;: an AI Agent is an intelligent entity that uses large language models as its brain to perceive its environment, make decisions, and execute actions. Unlike traditional artificial intelligence, AI Agents possess the ability to independently think and utilize tools to progressively achieve given objectives.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14767&quot;&gt;Whitepaper of FinRobot&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/trsr8SXpW5&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/trsr8SXpW5&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://api.visitorbadge.io/api/VisitorHit?user=AI4Finance-Foundation&amp;amp;repo=FinRobot&amp;amp;countColor=%23B17A&quot; alt=&quot;Visitors&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;FinRobot Ecosystem&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/6b30d9c1-35e5-4d36-a138-7e2769718f62&quot; width=&quot;90%&quot;&gt; 
&lt;/div&gt; 
&lt;h3&gt;The overall framework of FinRobot is organized into four distinct layers, each designed to address specific aspects of financial AI processing and application:&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Financial AI Agents Layer&lt;/strong&gt;: The Financial AI Agents Layer now includes Financial Chain-of-Thought (CoT) prompting, enhancing complex analysis and decision-making capacity. Market Forecasting Agents, Document Analysis Agents, and Trading Strategies Agents utilize CoT to dissect financial challenges into logical steps, aligning their advanced algorithms and domain expertise with the evolving dynamics of financial markets for precise, actionable insights.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Financial LLMs Algorithms Layer&lt;/strong&gt;: The Financial LLMs Algorithms Layer configures and utilizes specially tuned models tailored to specific domains and global market analysis.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLMOps and DataOps Layers&lt;/strong&gt;: The LLMOps layer implements a multi-source integration strategy that selects the most suitable LLMs for specific financial tasks, utilizing a range of state-of-the-art models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-source LLM Foundation Models Layer&lt;/strong&gt;: This foundational layer supports the plug-and-play functionality of various general and specialized LLMs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;FinRobot: Agent Workflow&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/ff8033be-2326-424a-ac11-17e2c9c4983d&quot; width=&quot;60%&quot;&gt; 
&lt;/div&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Perception&lt;/strong&gt;: This module captures and interprets multimodal financial data from market feeds, news, and economic indicators, using sophisticated techniques to structure the data for thorough analysis.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Brain&lt;/strong&gt;: Acting as the core processing unit, this module perceives data from the Perception module with LLMs and utilizes Financial Chain-of-Thought (CoT) processes to generate structured instructions.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Action&lt;/strong&gt;: This module executes instructions from the Brain module, applying tools to translate analytical insights into actionable outcomes. Actions include trading, portfolio adjustments, generating reports, or sending alerts, thereby actively influencing the financial environment.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;FinRobot: Smart Scheduler&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/06fa0b78-ac53-48d3-8a6e-98d15386327e&quot; width=&quot;60%&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;The Smart Scheduler is central to ensuring model diversity and optimizing the integration and selection of the most appropriate LLM for each task.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Director Agent&lt;/strong&gt;: This component orchestrates the task assignment process, ensuring that tasks are allocated to agents based on their performance metrics and suitability for specific tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agent Registration&lt;/strong&gt;: Manages the registration and tracks the availability of agents within the system, facilitating an efficient task allocation process.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agent Adaptor&lt;/strong&gt;: Tailor agent functionalities to specific tasks, enhancing their performance and integration within the overall system.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task Manager&lt;/strong&gt;: Manages and stores different general and fine-tuned LLMs-based agents tailored for various financial tasks, updated periodically to ensure relevance and efficacy.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;File Structure&lt;/h2&gt; 
&lt;p&gt;The main folder &lt;strong&gt;finrobot&lt;/strong&gt; has three subfolders &lt;strong&gt;agents, data_source, functional&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;FinRobot
â”œâ”€â”€ finrobot (main folder)
â”‚   â”œâ”€â”€ agents
â”‚   	â”œâ”€â”€ agent_library.py
â”‚   	â””â”€â”€ workflow.py
â”‚   â”œâ”€â”€ data_source
â”‚   	â”œâ”€â”€ finnhub_utils.py
â”‚   	â”œâ”€â”€ finnlp_utils.py
â”‚   	â”œâ”€â”€ fmp_utils.py
â”‚   	â”œâ”€â”€ sec_utils.py
â”‚   	â””â”€â”€ yfinance_utils.py
â”‚   â”œâ”€â”€ functional
â”‚   	â”œâ”€â”€ analyzer.py
â”‚   	â”œâ”€â”€ charting.py
â”‚   	â”œâ”€â”€ coding.py
â”‚   	â”œâ”€â”€ quantitative.py
â”‚   	â”œâ”€â”€ reportlab.py
â”‚   	â””â”€â”€ text.py
â”‚   â”œâ”€â”€ toolkits.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ configs
â”œâ”€â”€ experiments
â”œâ”€â”€ tutorials_beginner (hands-on tutorial)
â”‚   â”œâ”€â”€ agent_fingpt_forecaster.ipynb
â”‚   â””â”€â”€ agent_annual_report.ipynb 
â”œâ”€â”€ tutorials_advanced (advanced tutorials for potential finrobot developers)
â”‚   â”œâ”€â”€ agent_trade_strategist.ipynb
â”‚   â”œâ”€â”€ agent_fingpt_forecaster.ipynb
â”‚   â”œâ”€â”€ agent_annual_report.ipynb 
â”‚   â”œâ”€â”€ lmm_agent_mplfinance.ipynb
â”‚   â””â”€â”€ lmm_agent_opt_smacross.ipynb
â”œâ”€â”€ setup.py
â”œâ”€â”€ OAI_CONFIG_LIST_sample
â”œâ”€â”€ config_api_keys_sample
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation:&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1. (Recommended) Create a new virtual environment&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;conda create --name finrobot python=3.10
conda activate finrobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;2. download the FinRobot repo use terminal or download it manually&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git clone https://github.com/AI4Finance-Foundation/FinRobot.git
cd FinRobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;3. install finrobot &amp;amp; dependencies from source or pypi&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;get our latest release from pypi&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U finrobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or install from this repo directly&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;4. modify OAI_CONFIG_LIST_sample file&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;1) rename OAI_CONFIG_LIST_sample to OAI_CONFIG_LIST
2) remove the four lines of comment within the OAI_CONFIG_LIST file
3) add your own openai api-key &amp;lt;your OpenAI API key here&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;5. modify config_api_keys_sample file&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;1) rename config_api_keys_sample to config_api_keys
2) remove the comment within the config_api_keys file
3) add your own finnhub-api &quot;YOUR_FINNHUB_API_KEY&quot;
4) add your own financialmodelingprep and sec-api keys &quot;YOUR_FMP_API_KEY&quot; and &quot;YOUR_SEC_API_KEY&quot; (for financial report generation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;6. start navigating the tutorials or the demos below:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# find these notebooks in tutorials
1) agent_annual_report.ipynb
2) agent_fingpt_forecaster.ipynb
3) agent_trade_strategist.ipynb
4) lmm_agent_mplfinance.ipynb
5) lmm_agent_opt_smacross.ipynb
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;h3&gt;1. Market Forecaster Agent (Predict Stock Movements Direction)&lt;/h3&gt; 
&lt;p&gt;Takes a company&#39;s ticker symbol, recent basic financials, and market news as input and predicts its stock movements.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Import&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import autogen
from finrobot.utils import get_current_date, register_keys_from_json
from finrobot.agents.workflow import SingleAssistant
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Config&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Read OpenAI API keys from a JSON file
llm_config = {
    &quot;config_list&quot;: autogen.config_list_from_json(
        &quot;../OAI_CONFIG_LIST&quot;,
        filter_dict={&quot;model&quot;: [&quot;gpt-4-0125-preview&quot;]},
    ),
    &quot;timeout&quot;: 120,
    &quot;temperature&quot;: 0,
}

# Register FINNHUB API keys
register_keys_from_json(&quot;../config_api_keys&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Run&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;company = &quot;NVDA&quot;

assitant = SingleAssistant(
    &quot;Market_Analyst&quot;,
    llm_config,
    # set to &quot;ALWAYS&quot; if you want to chat instead of simply receiving the prediciton
    human_input_mode=&quot;NEVER&quot;,
)
assitant.chat(
    f&quot;Use all the tools provided to retrieve information available for {company} upon {get_current_date()}. Analyze the positive developments and potential concerns of {company} &quot;
    &quot;with 2-4 most important factors respectively and keep them concise. Most factors should be inferred from company related news. &quot;
    f&quot;Then make a rough prediction (e.g. up/down by 2-3%) of the {company} stock price movement for next week. Provide a summary analysis to support your prediction.&quot;
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt;Result&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/812ec23a-9cb3-4fad-b716-78533ddcd9dc&quot; width=&quot;40%&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/9a2f9f48-b0e1-489c-8679-9a4c530f313c&quot; width=&quot;41%&quot;&gt; 
&lt;/div&gt; 
&lt;h3&gt;2. Financial Analyst Agent for Report Writing (Equity Research Report)&lt;/h3&gt; 
&lt;p&gt;Take a company&#39;s 10-k form, financial data, and market data as input and output an equity research report&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Import&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import os
import autogen
from textwrap import dedent
from finrobot.utils import register_keys_from_json
from finrobot.agents.workflow import SingleAssistantShadow
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Config&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;llm_config = {
    &quot;config_list&quot;: autogen.config_list_from_json(
        &quot;../OAI_CONFIG_LIST&quot;,
        filter_dict={
            &quot;model&quot;: [&quot;gpt-4-0125-preview&quot;],
        },
    ),
    &quot;timeout&quot;: 120,
    &quot;temperature&quot;: 0.5,
}
register_keys_from_json(&quot;../config_api_keys&quot;)

# Intermediate strategy modules will be saved in this directory
work_dir = &quot;../report&quot;
os.makedirs(work_dir, exist_ok=True)

assistant = SingleAssistantShadow(
    &quot;Expert_Investor&quot;,
    llm_config,
    max_consecutive_auto_reply=None,
    human_input_mode=&quot;TERMINATE&quot;,
)

&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Run&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;company = &quot;Microsoft&quot;
fyear = &quot;2023&quot;

message = dedent(
    f&quot;&quot;&quot;
    With the tools you&#39;ve been provided, write an annual report based on {company}&#39;s {fyear} 10-k report, format it into a pdf.
    Pay attention to the followings:
    - Explicitly explain your working plan before you kick off.
    - Use tools one by one for clarity, especially when asking for instructions. 
    - All your file operations should be done in &quot;{work_dir}&quot;. 
    - Display any image in the chat once generated.
    - All the paragraphs should combine between 400 and 450 words, don&#39;t generate the pdf until this is explicitly fulfilled.
&quot;&quot;&quot;
)

assistant.chat(message, use_cache=True, max_turns=50,
               summary_method=&quot;last_msg&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt;Result&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/d2d999e0-dc0e-4196-aca1-218f5fadcc5b&quot; width=&quot;60%&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/3a21873f-9498-4d73-896b-3740bf6d116d&quot; width=&quot;60%&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Financial CoT&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Gather Preliminary Data&lt;/strong&gt;: 10-K report, market data, financial ratios&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Analyze Financial Statements&lt;/strong&gt;: balance sheet, income statement, cash flow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Company Overview and Performance&lt;/strong&gt;: company description, business highlights, segment analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Risk Assessment&lt;/strong&gt;: assess risks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Financial Performance Visualization&lt;/strong&gt;: plot PE ratio and EPS&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Synthesize Findings into Paragraphs&lt;/strong&gt;: combine all parts into a coherent summary&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generate PDF Report&lt;/strong&gt;: use tools to generate PDF automatically&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quality Assurance&lt;/strong&gt;: check word counts&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;3. Trade Strategist Agent with multimodal capabilities&lt;/h3&gt; 
&lt;h2&gt;AI Agent Papers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Stanford University + Microsoft Research] &lt;a href=&quot;https://arxiv.org/abs/2401.03568&quot;&gt;Agent AI: Surveying the Horizons of Multimodal Interaction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Stanford University] &lt;a href=&quot;https://arxiv.org/abs/2304.03442&quot;&gt;Generative Agents: Interactive Simulacra of Human Behavior&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Fudan NLP Group] &lt;a href=&quot;https://arxiv.org/abs/2309.07864&quot;&gt;The Rise and Potential of Large Language Model Based Agents: A Survey&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Fudan NLP Group] &lt;a href=&quot;https://github.com/WooooDyy/LLM-Agent-Paper-List&quot;&gt;LLM-Agent-Paper-List&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Tsinghua University] &lt;a href=&quot;https://arxiv.org/abs/2312.11970&quot;&gt;Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Renmin University] &lt;a href=&quot;https://arxiv.org/pdf/2308.11432.pdf&quot;&gt;A Survey on Large Language Model-based Autonomous Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Nanyang Technological University] &lt;a href=&quot;https://arxiv.org/abs/2402.18485&quot;&gt;FinAgent: A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;AI Agent Blogs and Videos&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Medium] &lt;a href=&quot;https://medium.com/humansdotai/an-introduction-to-ai-agents-e8c4afd2ee8f&quot;&gt;An Introduction to AI Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Medium] &lt;a href=&quot;https://medium.com/@aitrendorbit/unmasking-the-best-character-ai-chatbots-2024-351de43792f4#the-best-character-ai-chatbots&quot;&gt;Unmasking the Best Character AI Chatbots | 2024&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[big-picture] &lt;a href=&quot;https://blog.big-picture.com/en/chatgpt-next-level-meet-10-autonomous-ai-agents-auto-gpt-babyagi-agentgpt-microsoft-jarvis-chaosgpt-friends/&quot;&gt;ChatGPT, Next Level: Meet 10 Autonomous AI Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[TowardsDataScience] &lt;a href=&quot;https://towardsdatascience.com/navigating-the-world-of-llm-agents-a-beginners-guide-3b8d499db7a9&quot;&gt;Navigating the World of LLM Agents: A Beginnerâ€™s Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[YouTube] &lt;a href=&quot;https://www.youtube.com/watch?v=iVbN95ica_k&quot;&gt;Introducing Devin - The &quot;First&quot; AI Agent Software Engineer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;AI Agent Open-Source Framework &amp;amp; Tool&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Significant-Gravitas/AutoGPT&quot;&gt;AutoGPT (163k stars)&lt;/a&gt; is a tool for everyone to use, aiming to democratize AI, making it accessible for everyone to use and build upon.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/langchain-ai/langchain&quot;&gt;LangChain (87.4k stars)&lt;/a&gt; is a framework for developing context-aware applications powered by language models, enabling them to connect to sources of context and rely on the model&#39;s reasoning capabilities for responses and actions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/geekan/MetaGPT&quot;&gt;MetaGPT (41k stars)&lt;/a&gt; is a multi-agent open-source framework that assigns different roles to GPTs, forming a collaborative software entity to execute complex tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/langgenius/dify&quot;&gt;dify (34.1.7k stars)&lt;/a&gt; is an LLM application development platform. It integrates the concepts of Backend as a Service and LLMOps, covering the core tech stack required for building generative AI-native applications, including a built-in RAG engine&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/autogen&quot;&gt;AutoGen (27.4k stars)&lt;/a&gt; is a framework for developing LLM applications with conversational agents that collaborate to solve tasks. These agents are customizable, support human interaction, and operate in modes combining LLMs, human inputs, and tools.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/OpenBMB/ChatDev&quot;&gt;ChatDev (24.1k stars)&lt;/a&gt; is a framework that focuses on developing conversational AI Agents capable of dialogue and question-answering. It provides a range of pre-trained models and interactive interfaces, facilitating the development of customized chat Agents for users.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yoheinakajima/babyagi&quot;&gt;BabyAGI (19.5k stars)&lt;/a&gt; is an AI-powered task management system, dedicated to building AI Agents with preliminary general intelligence.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/joaomdmoura/crewAI&quot;&gt;CrewAI (16k stars)&lt;/a&gt; is a framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/TransformerOptimus/SuperAGI&quot;&gt;SuperAGI (14.8k stars)&lt;/a&gt; is a dev-first open-source autonomous AI agent framework enabling developers to build, manage &amp;amp; run useful autonomous agents.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/labring/FastGPT&quot;&gt;FastGPT (14.6k stars)&lt;/a&gt; is a knowledge-based platform built on the LLM, offers out-of-the-box data processing and model invocation capabilities, allows for workflow orchestration through Flow visualization.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/OpenBMB/XAgent&quot;&gt;XAgent (7.8k stars)&lt;/a&gt; is an open-source experimental Large Language Model (LLM) driven autonomous agent that can automatically solve various tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dataelement/bisheng&quot;&gt;Bisheng (7.8k stars)&lt;/a&gt; is a leading open-source platform for developing LLM applications.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/OpenBMB/XAgent&quot;&gt;Voyager (5.3k stars)&lt;/a&gt; An Open-Ended Embodied Agent with Large Language Models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/camel-ai/camel&quot;&gt;CAMEL (4.7k stars)&lt;/a&gt; is a framework that offers a comprehensive set of tools and algorithms for building multimodal AI Agents, enabling them to handle various data forms such as text, images, and speech.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/langfuse/langfuse&quot;&gt;Langfuse (4.3k stars)&lt;/a&gt; is a language fusion framework that can integrate the language abilities of multiple AI Agents, enabling them to simultaneously possess multilingual understanding and generation capabilities.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citing FinRobot&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@inproceedings{
zhou2024finrobot,
title={FinRobot: {AI} Agent for Equity Research and Valuation with Large Language Models},
author={Tianyu Zhou and Pinqiao Wang and Yilin Wu and Hongyang Yang},
booktitle={ICAIF 2024: The 1st Workshop on Large Language Models and Generative AI for Finance},
year={2024}
}

@article{yang2024finrobot,
  title={FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models},
  author={Yang, Hongyang and Zhang, Boyu and Wang, Neng and Guo, Cheng and Zhang, Xiaoli and Lin, Likun and Wang, Junlin and Zhou, Tianyu and Guan, Mao and Zhang, Runjia and others},
  journal={arXiv preprint arXiv:2405.14767},
  year={2024}
}

@inproceedings{han2024enhancing,
  title={Enhancing Investment Analysis: Optimizing AI-Agent Collaboration in Financial Research},
  author={Han, Xuewen and Wang, Neng and Che, Shangkun and Yang, Hongyang and Zhang, Kunpeng and Xu, Sean Xin},
  booktitle={ICAIF 2024: Proceedings of the 5th ACM International Conference on AI in Finance},
  pages={538--546},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: The codes and documents provided herein are released under the Apache-2.0 license. They should not be construed as financial counsel or recommendations for live trading. It is imperative to exercise caution and consult with qualified financial professionals prior to any trading or investment actions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>evidentlyai/evidently</title>
      <link>https://github.com/evidentlyai/evidently</link>
      <description>&lt;p&gt;Evidently is â€‹â€‹an open-source ML and LLM observability framework. Evaluate, test, and monitor any AI-powered system or data pipeline. From tabular data to Gen AI. 100+ metrics.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&quot;center&quot;&gt;Evidently&lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;b&gt;An open-source framework to evaluate, test and monitor ML and LLM-powered systems.&lt;/b&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://pepy.tech/project/evidently&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://pepy.tech/badge/evidently&quot; alt=&quot;PyPi Downloads&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/evidentlyai/evidently/raw/main/LICENSE&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/evidentlyai/evidently&quot; alt=&quot;License&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/evidently/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/evidently&quot; alt=&quot;PyPi&quot;&gt;&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/evidentlyai/evidently/main/docs/images/gh_header.png&quot; alt=&quot;Evidently&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://docs.evidentlyai.com&quot;&gt;Documentation&lt;/a&gt; | &lt;a href=&quot;https://discord.gg/xZjKRaNp8b&quot;&gt;Discord Community&lt;/a&gt; | &lt;a href=&quot;https://evidentlyai.com/blog&quot;&gt;Blog&lt;/a&gt; | &lt;a href=&quot;https://twitter.com/EvidentlyAI&quot;&gt;Twitter&lt;/a&gt; | &lt;a href=&quot;https://www.evidentlyai.com/register&quot;&gt;Evidently Cloud&lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;ðŸ†•&lt;/span&gt; New release&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Evidently 0.4.25&lt;/strong&gt;. LLM evaluation -&amp;gt; &lt;a href=&quot;https://docs.evidentlyai.com/tutorials-and-examples/tutorial-llm&quot;&gt;Tutorial&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;ðŸ“Š&lt;/span&gt; What is Evidently?&lt;/h1&gt; 
&lt;p&gt;Evidently is an open-source Python library for ML and LLM evaluation and observability. It helps evaluate, test, and monitor AI-powered systems and data pipelines from experimentation to production.&amp;nbsp;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ðŸ”¡ Works with tabular, text data, and embeddings.&lt;/li&gt; 
 &lt;li&gt;âœ¨ Supports predictive and generative systems, from classification to RAG.&lt;/li&gt; 
 &lt;li&gt;ðŸ“š 100+ built-in metrics from data drift detection to LLM judges.&lt;/li&gt; 
 &lt;li&gt;ðŸ› ï¸ Python interface for custom metrics and tests.&amp;nbsp;&lt;/li&gt; 
 &lt;li&gt;ðŸš¦ Both offline evals and live monitoring.&lt;/li&gt; 
 &lt;li&gt;ðŸ’» Open architecture: easily export data and integrate with existing tools.&amp;nbsp;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Evidently is very modular. You can start with one-off evaluations using &lt;code&gt;Reports&lt;/code&gt; or &lt;code&gt;Test Suites&lt;/code&gt; in Python or get a real-time monitoring &lt;code&gt;Dashboard&lt;/code&gt; service.&lt;/p&gt; 
&lt;h2&gt;1. Reports&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Reports&lt;/strong&gt; compute various data, ML and LLM quality metrics. You can start with Presets or customize.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Out-of-the-box interactive visuals.&lt;/li&gt; 
 &lt;li&gt;Best for exploratory analysis and debugging.&lt;/li&gt; 
 &lt;li&gt;Get results in Python, export as JSON, Python dictionary, HTML, DataFrame, or view in monitoring UI.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Reports&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/evidentlyai/evidently/main/docs/book/.gitbook/assets/main/reports-min.png&quot; alt=&quot;Report example&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;2. Test Suites&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Test Suites&lt;/strong&gt; check for defined conditions on metric values and return a pass or fail result.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Best for regression testing, CI/CD checks, or data validation pipelines.&lt;/li&gt; 
 &lt;li&gt;Zero setup option: auto-generate test conditions from the reference dataset.&lt;/li&gt; 
 &lt;li&gt;Simple syntax to set custom test conditions as &lt;code&gt;gt&lt;/code&gt; (greater than), &lt;code&gt;lt&lt;/code&gt; (less than), etc.&lt;/li&gt; 
 &lt;li&gt;Get results in Python, export as JSON, Python dictionary, HTML, DataFrame, or view in monitoring UI.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Test Suite&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/evidentlyai/evidently/main/docs/book/.gitbook/assets/main/tests.gif&quot; alt=&quot;Test example&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;3. Monitoring Dashboard&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Monitoring UI&lt;/strong&gt; service helps visualize metrics and test results over time.&lt;/p&gt; 
&lt;p&gt;You can choose:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Self-host the open-source version. &lt;a href=&quot;https://demo.evidentlyai.com&quot;&gt;Live demo&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Sign up for &lt;a href=&quot;https://www.evidentlyai.com/register&quot;&gt;Evidently Cloud&lt;/a&gt; (Recommended).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Evidently Cloud offers a generous free tier and extra features like user management, alerting, and no-code evals.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dashboard&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/evidentlyai/evidently/main/docs/book/.gitbook/assets/main/dashboard.gif&quot; alt=&quot;Dashboard example&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;&lt;span&gt;ðŸ‘©ðŸ’»&lt;/span&gt; Install Evidently&lt;/h1&gt; 
&lt;p&gt;Evidently is available as a PyPI package. To install it using pip package manager, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;pip install evidently
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install Evidently using conda installer, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;conda install -c conda-forge evidently
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;&lt;span&gt;â–¶&lt;/span&gt; Getting started&lt;/h1&gt; 
&lt;h3&gt;Option 1: Test Suites&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This is a simple Hello World. Check the Tutorials for more: &lt;a href=&quot;https://docs.evidentlyai.com/tutorials-and-examples/tutorial_reports_tests&quot;&gt;Tabular data&lt;/a&gt; or &lt;a href=&quot;https://docs.evidentlyai.com/tutorials-and-examples/tutorial-llm&quot;&gt;LLM evaluation&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Import the &lt;strong&gt;Test Suite&lt;/strong&gt;, evaluation Preset and toy tabular dataset.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import pandas as pd

from sklearn import datasets

from evidently.test_suite import TestSuite
from evidently.test_preset import DataStabilityTestPreset

iris_data = datasets.load_iris(as_frame=True)
iris_frame = iris_data.frame
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Split the &lt;code&gt;DataFrame&lt;/code&gt; into reference and current. Run the &lt;strong&gt;Data Stability&lt;/strong&gt; Test Suite that will automatically generate checks on column value ranges, missing values, etc. from the reference. Get the output in Jupyter notebook:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data_stability= TestSuite(tests=[
    DataStabilityTestPreset(),
])
data_stability.run(current_data=iris_frame.iloc[:60], reference_data=iris_frame.iloc[60:], column_mapping=None)
data_stability
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also save an HTML file. You&#39;ll need to open it from the destination folder.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data_stability.save_html(&quot;file.html&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To get the output as JSON:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data_stability.json()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can choose other Presets, individual Tests and set conditions.&lt;/p&gt; 
&lt;h3&gt;Option 2: Reports&lt;/h3&gt; 
&lt;p&gt;Import the &lt;strong&gt;Report&lt;/strong&gt;, evaluation Preset and toy tabular dataset.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import pandas as pd

from sklearn import datasets

from evidently.report import Report
from evidently.metric_preset import DataDriftPreset

iris_data = datasets.load_iris(as_frame=True)
iris_frame = iris_data.frame
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the &lt;strong&gt;Data Drift&lt;/strong&gt; Report that will compare column distributions between &lt;code&gt;current&lt;/code&gt; and &lt;code&gt;reference&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data_drift_report = Report(metrics=[
    DataDriftPreset(),
])

data_drift_report.run(current_data=iris_frame.iloc[:60], reference_data=iris_frame.iloc[60:], column_mapping=None)
data_drift_report

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Save the report as HTML. You&#39;ll later need to open it from the destination folder.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data_drift_report.save_html(&quot;file.html&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To get the output as JSON:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;data_drift_report.json()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can choose other Presets and individual Metrics, including LLM evaluations for text data.&lt;/p&gt; 
&lt;h3&gt;Option 3: ML monitoring dashboard&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This launches a demo project in the Evidently UI. Check tutorials for &lt;a href=&quot;https://docs.evidentlyai.com/tutorials-and-examples/tutorial-monitoring&quot;&gt;Self-hosting&lt;/a&gt; or &lt;a href=&quot;https://docs.evidentlyai.com/tutorials-and-examples/tutorial-cloud&quot;&gt;Evidently Cloud&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Recommended step: create a virtual environment and activate it.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install virtualenv
virtualenv venv
source venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After installing Evidently (&lt;code&gt;pip install evidently&lt;/code&gt;), run the Evidently UI with the demo projects:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;evidently ui --demo-projects all
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Access Evidently UI service in your browser. Go to the &lt;strong&gt;localhost:8000&lt;/strong&gt;.&lt;/p&gt; 
&lt;h1&gt;ðŸš¦ What can you evaluate?&lt;/h1&gt; 
&lt;p&gt;Evidently has 100+ built-in evals. You can also add custom ones. Each metric has an optional visualization: you can use it in &lt;code&gt;Reports&lt;/code&gt;, &lt;code&gt;Test Suites&lt;/code&gt;, or plot on a &lt;code&gt;Dashboard&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Here are examples of things you can check:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;ðŸ”¡ Text descriptors&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;ðŸ“ LLM outputs&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Length, sentiment, toxicity, language, special symbols, regular expression matches, etc.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Semantic similarity, retrieval relevance, summarization quality, etc. with model- and LLM-based evals.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;ðŸ›¢ Data quality&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;ðŸ“Š Data distribution drift&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Missing values, duplicates, min-max ranges, new categorical values, correlations, etc.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;20+ statistical tests and distance metrics to compare shifts in data distribution.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;ðŸŽ¯ Classification&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;ðŸ“ˆ Regression&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Accuracy, precision, recall, ROC AUC, confusion matrix, bias, etc.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;MAE, ME, RMSE, error distribution, error normality, error bias, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;ðŸ—‚ Ranking (inc. RAG)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;ðŸ›’ Recommendations&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;NDCG, MAP, MRR, Hit Rate, etc.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Serendipity, novelty, diversity, popularity bias, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;&lt;span&gt;ðŸ’»&lt;/span&gt; Contributions&lt;/h1&gt; 
&lt;p&gt;We welcome contributions! Read the &lt;a href=&quot;https://raw.githubusercontent.com/evidentlyai/evidently/main/CONTRIBUTING.md&quot;&gt;Guide&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;ðŸ“š&lt;/span&gt; Documentation&lt;/h1&gt; 
&lt;p&gt;For more information, refer to a complete &lt;a href=&quot;https://docs.evidentlyai.com&quot;&gt;Documentation&lt;/a&gt;. You can start with the tutorials:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.evidentlyai.com/tutorials-and-examples/tutorial_reports_tests&quot;&gt;Get Started with Tabular and ML Evaluation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.evidentlyai.com/tutorials-and-examples/tutorial-llm&quot;&gt;Get Started with LLM Evaluation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.evidentlyai.com/tutorials-and-examples/tutorial-monitoring&quot;&gt;Self-hosting ML monitoring Dashboard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.evidentlyai.com/tutorials-and-examples/tutorial-cloud&quot;&gt;Cloud ML monitoring Dashboard&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See more examples in the &lt;a href=&quot;%5Bhttps://docs.evidentlyai.com/tutorials-and-examples%5D(https://docs.evidentlyai.com/tutorials-and-examples/examples)&quot;&gt;Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;How-to guides&lt;/h2&gt; 
&lt;p&gt;Explore the &lt;a href=&quot;https://github.com/evidentlyai/evidently/tree/main/examples/how_to_questions&quot;&gt;How-to guides&lt;/a&gt; to understand specific features in Evidently.&lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;âœ…&lt;/span&gt; Discord Community&lt;/h1&gt; 
&lt;p&gt;If you want to chat and connect, join our &lt;a href=&quot;https://discord.gg/xZjKRaNp8b&quot;&gt;Discord community&lt;/a&gt;!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>causify-ai/tutorials</title>
      <link>https://github.com/causify-ai/tutorials</link>
      <description>&lt;p&gt;Causify system tutorials&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tutorials&lt;/h1&gt;</description>
    </item>
    
    <item>
      <title>modelscope/facechain</title>
      <link>https://github.com/modelscope/facechain</link>
      <description>&lt;p&gt;FaceChain is a deep-learning toolchain for generating your Digital-Twin.&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; &lt;br&gt; &lt;img src=&quot;https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif&quot; width=&quot;400&quot;&gt; &lt;br&gt; &lt;/p&gt;
&lt;h1&gt;FaceChain&lt;/h1&gt; 
&lt;p&gt; &lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://trendshift.io/repositories/1185&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/1185&quot; alt=&quot;modelscope%2Ffacechain | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;More Technology Details of FaceChain-FACT train-free portrait generation can be seen in &lt;a href=&quot;https://arxiv.org/abs/2410.12312&quot;&gt;Paper&lt;/a&gt;. (October 17th, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;Our work &lt;a href=&quot;https://arxiv.org/abs/2410.10587&quot;&gt;TopoFR&lt;/a&gt; got accepted to NeurIPS 2024 ! (September 26th, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;We provide training scripts for new styles, offering an automatic training for new style LoRas as well as the corresponding style prompts, along with the one click call in Infinite Style Portrait generation tab! (July 3rd, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;ðŸš€ðŸš€ðŸš€ We are launching [FACT] into the main branch, offering a 10-second impressive speed and seamless integration with standard ready-to-use LoRas and ControlNets, along with improved instruction-following capabilities ! The original train-based FaceChain is moved to (&lt;a href=&quot;https://github.com/modelscope/facechain/tree/v3.0.0&quot;&gt;https://github.com/modelscope/facechain/tree/v3.0.0&lt;/a&gt; ). (May 28th, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;Our work &lt;a href=&quot;https://arxiv.org/abs/2403.01901&quot;&gt;FaceChain-ImagineID&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2403.06775&quot;&gt;FaceChain-SuDe&lt;/a&gt; got accepted to CVPR 2024 ! (February 27th, 2024 UTC)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;å¦‚æžœæ‚¨ç†Ÿæ‚‰ä¸­æ–‡ï¼Œå¯ä»¥é˜…è¯»&lt;a href=&quot;https://raw.githubusercontent.com/modelscope/facechain/main/README_ZH.md&quot;&gt;ä¸­æ–‡ç‰ˆæœ¬çš„README&lt;/a&gt;ã€‚&lt;/p&gt; 
&lt;p&gt;FaceChain is a novel framework for generating identity-preserved human portraits. In the newest FaceChain FACT (Face Adapter with deCoupled Training) version, with only 1 photo and 10 seconds, you can generate personal portraits in different settings (multiple styles now supported!). FaceChain has both high controllability and authenticity in portrait generation, including text-to-image and inpainting based pipelines, and is seamlessly compatible with ControlNet and LoRAs. You may generate portraits via FaceChain&#39;s Python scripts, or via the familiar Gradio interface, or via sd webui. FaceChain is powered by &lt;a href=&quot;https://github.com/modelscope/modelscope&quot;&gt;ModelScope&lt;/a&gt;.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; ModelScope Studio &lt;a href=&quot;https://modelscope.cn/studios/CVstudio/FaceChain-FACT&quot;&gt;ðŸ¤–&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&amp;nbsp; ï½œAPI &lt;a href=&quot;https://help.aliyun.com/zh/dashscope/developer-reference/facechain-quick-start&quot;&gt;ðŸ”¥&lt;/a&gt;&lt;a&gt;&lt;/a&gt;&amp;nbsp; | SD WebUI | HuggingFace Space &lt;a href=&quot;https://huggingface.co/spaces/modelscope/FaceChain-FACT&quot;&gt;ðŸ¤—&lt;/a&gt;&amp;nbsp; &lt;/p&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;a href=&quot;https://facechain-fact.github.io/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Project-Page-Green&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/DHqEl0qwi-M?si=y6VpInXdhIX0HpbI&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/modelscope/facechain/main/resources/git_cover.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;More Technology Details of FaceChain-FACT train-free portrait generation can be seen in &lt;a href=&quot;https://arxiv.org/abs/2410.12312&quot;&gt;Paper&lt;/a&gt;. (October 17th, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;Our work &lt;a href=&quot;https://arxiv.org/abs/2410.10587&quot;&gt;TopoFR&lt;/a&gt; got accepted to NeurIPS 2024 ! (September 26th, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;We provide training scripts for new styles, offering an automatic training for new style LoRas as well as the corresponding style prompts, along with the one click call in Infinite Style Portrait generation tab! (July 3rd, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;ðŸš€ðŸš€ðŸš€ We are launching [FACT], offering a 10-second impressive speed and seamless integration with standard ready-to-use LoRas and ControlNets, along with improved instruction-following capabilities ! (May 28th, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;Our work &lt;a href=&quot;https://arxiv.org/abs/2403.01901&quot;&gt;FaceChain-ImagineID&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2403.06775&quot;&gt;FaceChain-SuDe&lt;/a&gt; got accepted to CVPR 2024 ! (February 27th, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;ðŸ†ðŸ†ðŸ†Alibaba Annual Outstanding Open Source Project, Alibaba Annual Open Source Pioneer (Yang Liu, Baigui Sun). (January 20th, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;Our work &lt;a href=&quot;https://github.com/henryqin1997/InfoBatch&quot;&gt;InfoBatch&lt;/a&gt; co-authored with NUS team got accepted to ICLR 2024(Oral)! (January 16th, 2024 UTC)&lt;/li&gt; 
 &lt;li&gt;ðŸ†OpenAtom&#39;s 2023 Rapidly Growing Open Source Projects Award. (December 20th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Add SDXL pipelineðŸ”¥ðŸ”¥ðŸ”¥, image detail is improved obviously. (November 22th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Support super resolutionðŸ”¥ðŸ”¥ðŸ”¥, provide multiple resolution choice (512&lt;em&gt;512, 768&lt;/em&gt;768, 1024&lt;em&gt;1024, 2048&lt;/em&gt;2048). (November 13th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;ðŸ†FaceChain has been selected in the &lt;a href=&quot;https://www.benchcouncil.org/evaluation/opencs/annual.html#Institutions&quot;&gt;BenchCouncil Open100 (2022-2023)&lt;/a&gt; annual ranking. (November 8th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Add virtual try-on module. (October 27th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Add wanx version &lt;a href=&quot;https://tongyi.aliyun.com/wanxiang/app/portrait-gallery&quot;&gt;online free app&lt;/a&gt;. (October 26th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;ðŸ†1024 Programmer&#39;s Day AIGC Application Tool Most Valuable Business Award. (2023-10-24, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Support FaceChain in stable-diffusion-webuiðŸ”¥ðŸ”¥ðŸ”¥. (October 13th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;High performance inpainting for single &amp;amp; double person, Simplify User Interface. (September 09th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;More Technology Details can be seen in &lt;a href=&quot;https://arxiv.org/abs/2308.14256&quot;&gt;Paper&lt;/a&gt;. (August 30th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Add validate &amp;amp; ensemble for Lora training, and InpaintTab(hide in gradio for now). (August 28th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Add pose control module. (August 27th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Add robust face lora training module, enhance the performance of one pic training &amp;amp; style-lora blending. (August 27th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;HuggingFace Space is available now! You can experience FaceChain directly with &lt;a href=&quot;https://huggingface.co/spaces/modelscope/FaceChain&quot;&gt;ðŸ¤—&lt;/a&gt; (August 25th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Add awesome prompts! Refer to: &lt;a href=&quot;https://raw.githubusercontent.com/modelscope/facechain/main/resources/awesome-prompts-facechain.txt&quot;&gt;awesome-prompts-facechain&lt;/a&gt; (August 18th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Support a series of new style models in a plug-and-play fashion. (August 16th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Support customizable prompts. (August 16th, 2023 UTC)&lt;/li&gt; 
 &lt;li&gt;Colab notebook is available now! You can experience FaceChain directly with &lt;a href=&quot;https://colab.research.google.com/github/modelscope/facechain/blob/main/facechain_demo.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;. (August 15th, 2023 UTC)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;To-Do List&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;full-body digital humans&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;p&gt;Please cite FaceChain and FaceChain-FACT in your publications if it helps your research&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{liu2023facechain,
  title={FaceChain: A Playground for Identity-Preserving Portrait Generation},
  author={Liu, Yang and Yu, Cheng and Shang, Lei and Wu, Ziheng and 
          Wang, Xingjun and Zhao, Yuze and Zhu, Lin and Cheng, Chen and 
          Chen, Weitao and Xu, Chao and Xie, Haoyu and Yao, Yuan and 
          Zhou,  Wenmeng and Chen Yingda and Xie, Xuansong and Sun, Baigui},
  journal={arXiv preprint arXiv:2308.14256},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;@article{yu2024facechain,
  title={FaceChain-FACT: Face Adapter with Decoupled Training for Identity-preserved Personalization},
  author={Yu, Cheng and Xie, Haoyu and Shang, Lei and Liu, Yang and Dan, Jun and Sun, Baigui and Bo, Liefeng},
  journal={arXiv preprint arXiv:2410.12312},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;h2&gt;Compatibility Verification&lt;/h2&gt; 
&lt;p&gt;We have verified e2e execution on the following environment:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;python: py3.8, py3.10&lt;/li&gt; 
 &lt;li&gt;pytorch: torch2.0.0, torch2.0.1&lt;/li&gt; 
 &lt;li&gt;CUDA: 11.7&lt;/li&gt; 
 &lt;li&gt;CUDNN: 8+&lt;/li&gt; 
 &lt;li&gt;OS: Ubuntu 20.04, CentOS 7.9&lt;/li&gt; 
 &lt;li&gt;GPU: Nvidia-A10 24G&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Memory Optimization&lt;/h2&gt; 
&lt;p&gt;Jemalloc are recommanded to install for optimizing the memory from above 30G to below 20G. Here is an example for installing Jemalloc in Modelscope notebook.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;apt-get install -y libjemalloc-dev
export LD_PRELOAD=/lib/x86_64-linux-gnu/libjemalloc.so
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation Guide&lt;/h2&gt; 
&lt;p&gt;The following installation methods are supported:&lt;/p&gt; 
&lt;h3&gt;1. ModelScope notebookã€recommendedã€‘&lt;/h3&gt; 
&lt;p&gt;The ModelScope Notebook offers a free-tier that allows ModelScope user to run the FaceChain application with minimum setup, refer to &lt;a href=&quot;https://modelscope.cn/my/mynotebook/preset&quot;&gt;ModelScope Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# Step1: æˆ‘çš„notebook -&amp;gt; PAI-DSW -&amp;gt; GPUçŽ¯å¢ƒ
# Note: Please use: ubuntu20.04-py38-torch2.0.1-tf1.15.5-modelscope1.8.1

# Step2: Entry the Notebook cellï¼Œclone FaceChain from github:
!GIT_LFS_SKIP_SMUDGE=1 git clone https://github.com/modelscope/facechain.git --depth 1

# Step3: Change the working directory to facechain, and install the dependencies:
import os
os.chdir(&#39;/mnt/workspace/facechain&#39;)    # You may change to your own path
print(os.getcwd())

!pip3 install gradio==3.47.1
!pip3 install controlnet_aux==0.0.6
!pip3 install python-slugify
!pip3 install diffusers==0.29.0
!pip3 install peft==0.11.1
!pip3 install modelscope -U
!pip3 install datasets==2.16

# Step4: Start the app service, click &quot;public URL&quot; or &quot;local URL&quot;, upload your images to 
# train your own model and then generate your digital twin.
!python3 app.py

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you may also purchase a &lt;a href=&quot;https://www.aliyun.com/activity/bigdata/pai/dsw&quot;&gt;PAI-DSW&lt;/a&gt; instance (using A10 resource), with the option of ModelScope image to run FaceChain following similar steps.&lt;/p&gt; 
&lt;h3&gt;2. Docker&lt;/h3&gt; 
&lt;p&gt;If you are familiar with using docker, we recommend to use this way:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# Step1: Prepare the environment with GPU on local or cloud, we recommend to use Alibaba Cloud ECS, refer to: https://www.aliyun.com/product/ecs

# Step2: Download the docker image (for installing docker engine, refer to https://docs.docker.com/engine/install/ï¼‰
# For China Mainland users:
docker pull registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.1
# For users outside China Mainland:
docker pull registry.us-west-1.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.1

# Step3: run the docker container
docker run -it --name facechain -p 7860:7860 --gpus all registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.1 /bin/bash
# Note: you may need to install the nvidia-container-runtime, follow the instructions:
# 1. Install nvidia-container-runtimeï¼šhttps://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
# 2. sudo systemctl restart docker

# Step4: Install the gradio in the docker container:
pip3 install gradio==3.47.1
pip3 install controlnet_aux==0.0.6
pip3 install python-slugify
pip3 install diffusers==0.29.0
pip3 install peft==0.11.1
pip3 install modelscope -U
pip3 install datasets==2.16

# Step5 clone facechain from github
GIT_LFS_SKIP_SMUDGE=1 git clone https://github.com/modelscope/facechain.git --depth 1
cd facechain
python3 app.py
# Note: FaceChain currently assume single-GPU, if your environment has multiple GPU, please use the following instead:
# CUDA_VISIBLE_DEVICES=0 python3 app.py

# Step6
Run the app server: click &quot;public URL&quot; --&amp;gt; in the form of: https://xxx.gradio.live
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. stable-diffusion-webui&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Select the &lt;code&gt;Extensions Tab&lt;/code&gt;, then choose &lt;code&gt;Install From URL&lt;/code&gt; (official plugin integration is integrated, please install from URL currently). &lt;img src=&quot;https://raw.githubusercontent.com/modelscope/facechain/main/resources/sdwebui_install.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Switch to &lt;code&gt;Installed&lt;/code&gt;, check the FaceChain plugin, then click &lt;code&gt;Apply and restart UI&lt;/code&gt;. It may take a while for installing the dependencies and downloading the models. Make sure that the &quot;CUDA Toolkit&quot; is installed correctly, otherwise the &quot;mmcv&quot; package cannot be successfully installed. &lt;img src=&quot;https://raw.githubusercontent.com/modelscope/facechain/main/resources/sdwebui_restart.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;After the page refreshes, the appearance of the &lt;code&gt;FaceChain&lt;/code&gt; Tab indicates a successful installation. &lt;img src=&quot;https://raw.githubusercontent.com/modelscope/facechain/main/resources/sdwebui_success.jpg&quot; alt=&quot;image&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;Script Execution&lt;/h1&gt; 
&lt;p&gt;FaceChain supports direct inference in the python environment. When inferring for Infinite Style Portrait generation, please edit the code in run_inference.py:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Use pose control, default False
use_pose_model = False
# The path of the input image containing ID information for portrait generation
input_img_path = &#39;poses/man/pose2.png&#39;
# The path of the image for pose control, only effective when using pose control
pose_image = &#39;poses/man/pose1.png&#39;
# The number of images to generate in inference
num_generate = 5
# The weight for the style model, see styles for detail
multiplier_style = 0.25
# Specify a folder to save the generated images, this parameter can be modified as needed
output_dir = &#39;./generated&#39;
# The index of the chosen base model, see facechain/constants.py for detail
base_model_idx = 0
# The index of the style model, see styles for detail
style_idx = 0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then execute:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python run_inference.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can find the generated personal digital image photos in the &lt;code&gt;output_dir&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;When inferring for Fixed Templates Portrait generation, please edit the code in run_inference_inpaint.py.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Number of faces for the template image
num_faces = 1
# Index of face for inpainting, counting from left to right
selected_face = 1
# The strength for inpainting, you do not need to change the parameter
strength = 0.6
# The path of the template image
inpaint_img = &#39;poses/man/pose1.png&#39;
# The path of the input image containing ID information for portrait generation
input_img_path = &#39;poses/man/pose2.png&#39;
# The number of images to generate in inference
num_generate = 1
# Specify a folder to save the generated images, this parameter can be modified as needed
output_dir = &#39;./generated_inpaint&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then execute:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python run_inference_inpaint.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can find the generated personal digital image photos in the &lt;code&gt;output_dir&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Algorithm Introduction&lt;/h1&gt; 
&lt;p&gt;The capability of AI portraits generation comes from the large generative models like Stable Diffusion and its fine-tuning techniques. Due to the strong generalization capability of large models, it is possible to perform downstream tasks by fine-tuning on specific types of data and tasks, while preserving the model&#39;s overall ability of text following and image generation. The technical foundation of train-based and train-free AI portraits generation comes from applying different fine-tuning tasks to generative models. Currently, most existing AI portraits tools adopt a two-stage â€œtrain then generateâ€ pipeline, where the fine-tuning task is â€œto generate portrait photos of a fixed character IDâ€, and the corresponding training data are multiple images of the fixed character ID. The effectiveness of such train-based pipeline depends on the scale of the training data, thus requiring certain image data support and training time, which also increases the cost for users.&lt;/p&gt; 
&lt;p&gt;Different from train-based pipeline, train-free pipeline adjusts the fine-tuning task to â€œgenerate portrait photos of a specified character IDâ€, meaning that the character ID image (face photo) is used as an additional input, and the output is a portrait photo preserving the input ID. Such a pipeline completely separates offline training from online inference, allowing users to generate portraits directly based on the fine-tuned model with only one photo in just 10 seconds, avoiding the cost for extensive data and training time. The fine-tuning task of train-free AI portraits generation is based on the adapter module. Face photos are processed through an image encoder with fixed weights and a parameter-efficient feature projection layer to obtain aligned features, and are then fed into the U-Net model of Stable Diffusion through attention mechanism similar as text conditions. At this point, face information as an independent branch condition is fed into the model alongside text information for inference, thereby enabling the generated images to maintain ID fidelity.&lt;/p&gt; 
&lt;p&gt;The basic algorithm based on face adapter is capable of achieving train-free AI portraits, but still requires certain adjustments to further improve its effectiveness. Existing train-free portrait tools generally suffer from the following issues: poor image quality of portraits, inadequate text following and style retention abilities in portraits, poor controllability and richness of portrait faces, and poor compatibility with extensions like ControlNet and style Lora. To address these issues, FaceChain attribute them to the fact that the fine-tuning tasks for existing train-free AI portrait tools have coupled with too much information beyond character IDs, and propose FaceChain Face Adapter with Decoupled Training (FaceChain FACT) to solve these problems. By fine-tuning the Stable Diffusion model on millions of portrait data, FaceChain FACT can achieve high-quality portrait image generation for specified character IDs. The entire framework of FaceChain FACT is shown in the figure below.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/modelscope/facechain/main/resources/framework.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt; 
&lt;p&gt;The decoupled training of FaceChain FACT consists of two parts: decoupling face from image, and decoupling ID from face. Existing methods often treat denoising portrait images as the fine-tuning task, which makes the model hard to accurately focus on the face area, thereby affecting the text-to-image ability of the base Stable Diffusion model. FaceChain FACT draws on the sequential processing and regional control advantages of face-swapping algorithms and implements the fine-tuning method for decoupling faces from images from both structural and training strategy aspects. Structurally, unlike existing methods that use a parallel cross-attention mechanism to process face and text information, FaceChain FACT adopts a sequential processing approach as an independent adapter layer inserted into the original Stable Diffusion&#39;s blocks. This way, face adaptation acts as an independent step similar to face-swapping during the denoising process, avoiding interference between face and text conditions. In terms of training strategy, besides the original MSE loss function, FaceChain FACT introduces the Face Adapting Incremental Regularization (FAIR) loss function, which controls the feature increment of the face adaptation step in the adapter layer to focus on the face region. During inference, users can flexibly adjust the generated effects by modifying the weight of the face adapter, balancing fidelity and generalization of the face while maintaining the text-to-image ability of Stable Diffusion. The FAIR loss function is formulated as follows:&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/modelscope/facechain/main/resources/FAIR.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Furthermore, addressing the issue of poor controllability and richness of generated faces, FaceChain FACT proposes a training method for decoupling ID from face, so that the portrait process only preserves the character ID rather than the entire face. Firstly, to better extract the ID information from the face while maintaining certain key facial details, and to better adapt to the structure of Stable Diffusion, FaceChain FACT employs a face feature extractor named &lt;a href=&quot;https://github.com/DanJun6737/TransFace&quot;&gt;TransFace&lt;/a&gt; based on the Transformer architecture, which is pre-trained on a large-scale face dataset. All tokens from the penultimate layer are subsequently fed into a simple attention query model for feature projection, thereby ensuring that the extracted ID features meet the aforementioned requirements. Additionally, during the training process, FaceChain FACT uses the Classifier Free Guidance (CFG) method to perform random shuffle and drop for different portrait images of the same ID, thus ensuring that the input face images and the target images used for denoising may have different faces with the same ID, thus further preventing the model from overfitting to non-ID information of the face. As such, FaceChain FACT possesses high compatibility with the massive exquisite styles of FaceChain, which is shown as follows.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/modelscope/facechain/main/resources/generated_examples.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;Model List&lt;/h2&gt; 
&lt;p&gt;The models used in FaceChain:&lt;/p&gt; 
&lt;p&gt;[1] Face recognition model TransFaceï¼š&lt;a href=&quot;https://www.modelscope.cn/models/iic/cv_vit_face-recognition&quot;&gt;https://www.modelscope.cn/models/iic/cv_vit_face-recognition&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[2] Face detection model DamoFDï¼š&lt;a href=&quot;https://modelscope.cn/models/damo/cv_ddsar_face-detection_iclr23-damofd&quot;&gt;https://modelscope.cn/models/damo/cv_ddsar_face-detection_iclr23-damofd&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[3] Human parsing model M2FPï¼š&lt;a href=&quot;https://modelscope.cn/models/damo/cv_resnet101_image-multiple-human-parsing&quot;&gt;https://modelscope.cn/models/damo/cv_resnet101_image-multiple-human-parsing&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[4] Skin retouching model ABPNï¼š&lt;a href=&quot;https://www.modelscope.cn/models/damo/cv_unet_skin_retouching_torch&quot;&gt;https://www.modelscope.cn/models/damo/cv_unet_skin_retouching_torch&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[5] Face fusion modelï¼š&lt;a href=&quot;https://www.modelscope.cn/models/damo/cv_unet_face_fusion_torch&quot;&gt;https://www.modelscope.cn/models/damo/cv_unet_face_fusion_torch&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[6] FaceChain FACT model: &lt;a href=&quot;https://www.modelscope.cn/models/yucheng1996/FaceChain-FACT&quot;&gt;https://www.modelscope.cn/models/yucheng1996/FaceChain-FACT&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[7] Face attribute recognition model FairFace: &lt;a href=&quot;https://modelscope.cn/models/damo/cv_resnet34_face-attribute-recognition_fairface&quot;&gt;https://modelscope.cn/models/damo/cv_resnet34_face-attribute-recognition_fairface&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;More Information&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/modelscope/modelscope/&quot;&gt;ModelScope library&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;â€‹ ModelScope Library provides the foundation for building the model-ecosystem of ModelScope, including the interface and implementation to integrate various models into ModelScope.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88&quot;&gt;Contribute models to ModelScope&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;This project is licensed under the &lt;a href=&quot;https://github.com/modelscope/modelscope/raw/master/LICENSE&quot;&gt;Apache License (Version 2.0)&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>jackfrued/Python-100-Days</title>
      <link>https://github.com/jackfrued/Python-100-Days</link>
      <description>&lt;p&gt;Python - 100å¤©ä»Žæ–°æ‰‹åˆ°å¤§å¸ˆ&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Python - 100å¤©ä»Žæ–°æ‰‹åˆ°å¤§å¸ˆ&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ä½œè€…&lt;/strong&gt;ï¼šéª†æ˜Š&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;è¯´æ˜Ž&lt;/strong&gt;ï¼šå¦‚æžœè®¿é—® GitHub æ¯”è¾ƒæ…¢çš„è¯ï¼Œå¯ä»¥å…³æ³¨æˆ‘çš„çŸ¥ä¹Žå·ï¼ˆ&lt;a href=&quot;https://www.zhihu.com/people/jackfrued&quot;&gt;&lt;strong&gt;Python-Jack&lt;/strong&gt;&lt;/a&gt;ï¼‰ï¼Œä¸Šé¢çš„&lt;a href=&quot;https://zhuanlan.zhihu.com/c_1216656665569013760&quot;&gt;â€œ&lt;strong&gt;ä»Žé›¶å¼€å§‹å­¦Python&lt;/strong&gt;â€&lt;/a&gt;ä¸“æ ï¼ˆå¯¹åº”æœ¬é¡¹ç›®å‰ 20 å¤©çš„å†…å®¹ï¼‰æ¯”è¾ƒé€‚åˆåˆå­¦è€…ï¼Œå…¶ä»–çš„ä¸“æ å¦‚â€œ&lt;a href=&quot;https://www.zhihu.com/column/c_1620074540456964096&quot;&gt;&lt;strong&gt;æ•°æ®æ€ç»´å’Œç»Ÿè®¡æ€ç»´&lt;/strong&gt;&lt;/a&gt;â€ã€â€œ&lt;a href=&quot;https://www.zhihu.com/column/c_1217746527315496960&quot;&gt;&lt;strong&gt;åŸºäºŽPythonçš„æ•°æ®åˆ†æž&lt;/strong&gt;&lt;/a&gt;â€ã€â€œ&lt;a href=&quot;https://www.zhihu.com/column/c_1628900668109946880&quot;&gt;&lt;strong&gt;è¯´èµ°å°±èµ°çš„AIä¹‹æ—…&lt;/strong&gt;&lt;/a&gt;â€ç­‰ä¹Ÿåœ¨æŒç»­åˆ›ä½œå’Œæ›´æ–°ä¸­ï¼Œæ¬¢è¿Žå¤§å®¶å…³æ³¨ã€ç‚¹èµžå’Œè¯„è®ºã€‚å¦‚æžœå¸Œæœ›å…è´¹å­¦ä¹ æ‰“å¡æˆ–è€…å‚ä¸Žé—®é¢˜è®¨è®ºï¼Œå¯ä»¥åŠ å…¥ä¸‹é¢çš„ QQ äº¤æµç¾¤ï¼ˆä¸‰ä¸ªç¾¤åŠ ä¸€ä¸ªå³å¯ï¼‰ï¼Œè¯·ä¸è¦é‡å¤åŠ ç¾¤ï¼Œä¹Ÿä¸è¦åœ¨ç¾¤é‡Œå‘å¸ƒå¹¿å‘Šå’Œå…¶ä»–è‰²æƒ…ã€ä½Žä¿—æˆ–æ•æ„Ÿå†…å®¹ã€‚å¦‚æžœæœ‰ä»˜è´¹å­¦ä¹ æˆ–ä»˜è´¹å’¨è¯¢çš„éœ€æ±‚ï¼Œå¯ä»¥æ·»åŠ æˆ‘çš„ç§äººå¾®ä¿¡ï¼ˆå¾®ä¿¡å·ï¼š&lt;strong&gt;jackfrued&lt;/strong&gt;ï¼‰ï¼Œå¤‡æ³¨å¥½è‡ªå·±çš„ç§°å‘¼å’Œéœ€æ±‚ï¼Œæˆ‘ä¼šä¸ºå¤§å®¶æä¾›åŠ›æ‰€èƒ½åŠçš„å¸®åŠ©ã€‚&lt;/p&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/res/python_study_qq_group.png&quot; style=&quot;zoom:30%;&quot;&gt; 
 &lt;p&gt;æœ¬é¡¹ç›®å¯¹åº”çš„éƒ¨åˆ†è§†é¢‘å·²ç»åŒæ­¥åˆ° &lt;a href=&quot;https://space.bilibili.com/1177252794&quot;&gt;Bilibili&lt;/a&gt;ï¼Œæœ‰å…´è¶£çš„å°ä¼™ä¼´å¯ä»¥ç‚¹èµžã€æŠ•å¸ã€å…³æ³¨ï¼Œä¸€é”®ä¸‰è¿žæ”¯æŒä¸€ä¸‹ï¼&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Pythonåº”ç”¨é¢†åŸŸå’ŒèŒä¸šå‘å±•åˆ†æž&lt;/h3&gt; 
&lt;p&gt;ç®€å•çš„è¯´ï¼ŒPythonæ˜¯ä¸€ä¸ªâ€œä¼˜é›…â€ã€â€œæ˜Žç¡®â€ã€â€œç®€å•â€çš„ç¼–ç¨‹è¯­è¨€ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;å­¦ä¹ æ›²çº¿ä½Žï¼Œéžä¸“ä¸šäººå£«ä¹Ÿèƒ½ä¸Šæ‰‹&lt;/li&gt; 
 &lt;li&gt;å¼€æºç³»ç»Ÿï¼Œæ‹¥æœ‰å¼ºå¤§çš„ç”Ÿæ€åœˆ&lt;/li&gt; 
 &lt;li&gt;è§£é‡Šåž‹è¯­è¨€ï¼Œå®Œç¾Žçš„å¹³å°å¯ç§»æ¤æ€§&lt;/li&gt; 
 &lt;li&gt;åŠ¨æ€ç±»åž‹è¯­è¨€ï¼Œæ”¯æŒé¢å‘å¯¹è±¡å’Œå‡½æ•°å¼ç¼–ç¨‹&lt;/li&gt; 
 &lt;li&gt;ä»£ç è§„èŒƒç¨‹åº¦é«˜ï¼Œå¯è¯»æ€§å¼º&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Pythonåœ¨ä»¥ä¸‹é¢†åŸŸéƒ½æœ‰ç”¨æ­¦ä¹‹åœ°ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;åŽç«¯å¼€å‘ - Python / Java / Go / PHP&lt;/li&gt; 
 &lt;li&gt;DevOps - Python / Shell / Ruby&lt;/li&gt; 
 &lt;li&gt;æ•°æ®é‡‡é›† - Python / C++ / Java&lt;/li&gt; 
 &lt;li&gt;é‡åŒ–äº¤æ˜“ - Python / C++ / R&lt;/li&gt; 
 &lt;li&gt;æ•°æ®ç§‘å­¦ - Python / R / Julia / Matlab&lt;/li&gt; 
 &lt;li&gt;æœºå™¨å­¦ä¹  - Python / R / C++ / Julia&lt;/li&gt; 
 &lt;li&gt;è‡ªåŠ¨åŒ–æµ‹è¯• - Python / Shell&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ä½œä¸ºä¸€åPythonå¼€å‘è€…ï¼Œæ ¹æ®ä¸ªäººçš„å–œå¥½å’ŒèŒä¸šè§„åˆ’ï¼Œå¯ä»¥é€‰æ‹©çš„å°±ä¸šé¢†åŸŸä¹Ÿéžå¸¸å¤šã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PythonåŽç«¯å¼€å‘å·¥ç¨‹å¸ˆï¼ˆæœåŠ¡å™¨ã€äº‘å¹³å°ã€æ•°æ®æŽ¥å£ï¼‰&lt;/li&gt; 
 &lt;li&gt;Pythonè¿ç»´å·¥ç¨‹å¸ˆï¼ˆè‡ªåŠ¨åŒ–è¿ç»´ã€SREã€DevOpsï¼‰&lt;/li&gt; 
 &lt;li&gt;Pythonæ•°æ®åˆ†æžå¸ˆï¼ˆæ•°æ®åˆ†æžã€å•†ä¸šæ™ºèƒ½ã€æ•°å­—åŒ–è¿è¥ï¼‰&lt;/li&gt; 
 &lt;li&gt;Pythonæ•°æ®ç§‘å­¦å®¶ï¼ˆæœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€ç®—æ³•ä¸“å®¶ï¼‰&lt;/li&gt; 
 &lt;li&gt;Pythonçˆ¬è™«å·¥ç¨‹å¸ˆï¼ˆä¸æŽ¨èæ­¤èµ›é“ï¼ï¼ï¼ï¼‰&lt;/li&gt; 
 &lt;li&gt;Pythonæµ‹è¯•å·¥ç¨‹å¸ˆï¼ˆè‡ªåŠ¨åŒ–æµ‹è¯•ã€æµ‹è¯•å¼€å‘ï¼‰&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;è¯´æ˜Ž&lt;/strong&gt;ï¼šç›®å‰ï¼Œ&lt;strong&gt;æ•°æ®ç§‘å­¦èµ›é“æ˜¯éžå¸¸çƒ­é—¨çš„æ–¹å‘&lt;/strong&gt;ï¼Œå› ä¸ºä¸ç®¡æ˜¯äº’è”ç½‘è¡Œä¸šè¿˜æ˜¯ä¼ ç»Ÿè¡Œä¸šéƒ½å·²ç»ç§¯ç´¯äº†å¤§é‡çš„æ•°æ®ï¼Œå„è¡Œå„ä¸šéƒ½éœ€è¦æ•°æ®ç§‘å­¦å®¶ä»Žå·²æœ‰çš„æ•°æ®ä¸­å‘çŽ°æ›´å¤šçš„å•†ä¸šä»·å€¼ï¼Œä»Žè€Œä¸ºä¼ä¸šçš„å†³ç­–æä¾›æ•°æ®çš„æ”¯æ’‘ï¼Œè¿™å°±æ˜¯æ‰€è°“çš„æ•°æ®é©±åŠ¨å†³ç­–ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;ç»™åˆå­¦è€…çš„å‡ ä¸ªå»ºè®®ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Make English as your working language.&lt;/strong&gt; ï¼ˆè®©è‹±è¯­æˆä¸ºä½ çš„å·¥ä½œè¯­è¨€ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Practice makes perfect.&lt;/strong&gt; ï¼ˆç†Ÿèƒ½ç”Ÿå·§ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;All experience comes from the mistakes you&#39;ve made.&lt;/strong&gt; ï¼ˆæ‰€æœ‰çš„ç»éªŒéƒ½æºäºŽä½ çŠ¯è¿‡çš„é”™è¯¯ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Don&#39;t be a freeloader.&lt;/strong&gt; ï¼ˆä¸è¦å½“ä¼¸æ‰‹å…šï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Either outstanding or out.&lt;/strong&gt; ï¼ˆè¦ä¹ˆå‡ºä¼—ï¼Œè¦ä¹ˆå‡ºå±€ï¼‰&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Day01~20 - Pythonè¯­è¨€åŸºç¡€&lt;/h3&gt; 
&lt;h4&gt;Day01 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/01.%E5%88%9D%E8%AF%86Python.md&quot;&gt;åˆè¯†Python&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Pythonç®€ä»‹ 
  &lt;ul&gt; 
   &lt;li&gt;Pythonç¼–å¹´å²&lt;/li&gt; 
   &lt;li&gt;Pythonä¼˜ç¼ºç‚¹&lt;/li&gt; 
   &lt;li&gt;Pythonåº”ç”¨é¢†åŸŸ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;å®‰è£…PythonçŽ¯å¢ƒ 
  &lt;ul&gt; 
   &lt;li&gt;WindowsçŽ¯å¢ƒ&lt;/li&gt; 
   &lt;li&gt;macOSçŽ¯å¢ƒ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day02 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/02.%E7%AC%AC%E4%B8%80%E4%B8%AAPython%E7%A8%8B%E5%BA%8F.md&quot;&gt;ç¬¬ä¸€ä¸ªPythonç¨‹åº&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;ç¼–å†™ä»£ç çš„å·¥å…·&lt;/li&gt; 
 &lt;li&gt;ä½ å¥½ä¸–ç•Œ&lt;/li&gt; 
 &lt;li&gt;æ³¨é‡Šä½ çš„ä»£ç &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day03 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/03.Python%E8%AF%AD%E8%A8%80%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F.md&quot;&gt;Pythonè¯­è¨€ä¸­çš„å˜é‡&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;ä¸€äº›å¸¸è¯†&lt;/li&gt; 
 &lt;li&gt;å˜é‡å’Œç±»åž‹&lt;/li&gt; 
 &lt;li&gt;å˜é‡å‘½å&lt;/li&gt; 
 &lt;li&gt;å˜é‡çš„ä½¿ç”¨&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day04 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/04.Python%E8%AF%AD%E8%A8%80%E4%B8%AD%E7%9A%84%E8%BF%90%E7%AE%97%E7%AC%A6.md&quot;&gt;Pythonè¯­è¨€ä¸­çš„è¿ç®—ç¬¦&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;ç®—æœ¯è¿ç®—ç¬¦&lt;/li&gt; 
 &lt;li&gt;èµ‹å€¼è¿ç®—ç¬¦&lt;/li&gt; 
 &lt;li&gt;æ¯”è¾ƒè¿ç®—ç¬¦å’Œé€»è¾‘è¿ç®—ç¬¦&lt;/li&gt; 
 &lt;li&gt;è¿ç®—ç¬¦å’Œè¡¨è¾¾å¼åº”ç”¨ 
  &lt;ul&gt; 
   &lt;li&gt;åŽæ°å’Œæ‘„æ°æ¸©åº¦è½¬æ¢&lt;/li&gt; 
   &lt;li&gt;è®¡ç®—åœ†çš„å‘¨é•¿å’Œé¢ç§¯&lt;/li&gt; 
   &lt;li&gt;åˆ¤æ–­é—°å¹´&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day05 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/05.%E5%88%86%E6%94%AF%E7%BB%93%E6%9E%84.md&quot;&gt;åˆ†æ”¯ç»“æž„&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;ä½¿ç”¨ifå’Œelseæž„é€ åˆ†æ”¯ç»“æž„&lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨matchå’Œcaseæž„é€ åˆ†æ”¯ç»“æž„&lt;/li&gt; 
 &lt;li&gt;åˆ†æ”¯ç»“æž„çš„åº”ç”¨ 
  &lt;ul&gt; 
   &lt;li&gt;åˆ†æ®µå‡½æ•°æ±‚å€¼&lt;/li&gt; 
   &lt;li&gt;ç™¾åˆ†åˆ¶æˆç»©è½¬æ¢æˆç­‰çº§&lt;/li&gt; 
   &lt;li&gt;è®¡ç®—ä¸‰è§’å½¢çš„å‘¨é•¿å’Œé¢ç§¯&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day06 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/06.%E5%BE%AA%E7%8E%AF%E7%BB%93%E6%9E%84.md&quot;&gt;å¾ªçŽ¯ç»“æž„&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;for-inå¾ªçŽ¯&lt;/li&gt; 
 &lt;li&gt;whileå¾ªçŽ¯&lt;/li&gt; 
 &lt;li&gt;breakå’Œcontinue&lt;/li&gt; 
 &lt;li&gt;åµŒå¥—çš„å¾ªçŽ¯ç»“æž„&lt;/li&gt; 
 &lt;li&gt;å¾ªçŽ¯ç»“æž„çš„åº”ç”¨ 
  &lt;ul&gt; 
   &lt;li&gt;åˆ¤æ–­ç´ æ•°&lt;/li&gt; 
   &lt;li&gt;æœ€å¤§å…¬çº¦æ•°&lt;/li&gt; 
   &lt;li&gt;çŒœæ•°å­—æ¸¸æˆ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day07 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/07.%E5%88%86%E6%94%AF%E5%92%8C%E5%BE%AA%E7%8E%AF%E7%BB%93%E6%9E%84%E5%AE%9E%E6%88%98.md&quot;&gt;åˆ†æ”¯å’Œå¾ªçŽ¯ç»“æž„å®žæˆ˜&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;ä¾‹å­1ï¼š100ä»¥å†…çš„ç´ æ•°&lt;/li&gt; 
 &lt;li&gt;ä¾‹å­2ï¼šæ–æ³¢é‚£å¥‘æ•°åˆ—&lt;/li&gt; 
 &lt;li&gt;ä¾‹å­3ï¼šå¯»æ‰¾æ°´ä»™èŠ±æ•°&lt;/li&gt; 
 &lt;li&gt;ä¾‹å­4ï¼šç™¾é’±ç™¾é¸¡é—®é¢˜&lt;/li&gt; 
 &lt;li&gt;ä¾‹å­5ï¼šCRAPSèµŒåšæ¸¸æˆ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day08 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/08.%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%88%97%E8%A1%A8-1.md&quot;&gt;å¸¸ç”¨æ•°æ®ç»“æž„ä¹‹åˆ—è¡¨-1&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;åˆ›å»ºåˆ—è¡¨&lt;/li&gt; 
 &lt;li&gt;åˆ—è¡¨çš„è¿ç®—&lt;/li&gt; 
 &lt;li&gt;å…ƒç´ çš„éåŽ†&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day09 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/09.%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%88%97%E8%A1%A8-2.md&quot;&gt;å¸¸ç”¨æ•°æ®ç»“æž„ä¹‹åˆ—è¡¨-2&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;åˆ—è¡¨çš„æ–¹æ³• 
  &lt;ul&gt; 
   &lt;li&gt;æ·»åŠ å’Œåˆ é™¤å…ƒç´ &lt;/li&gt; 
   &lt;li&gt;å…ƒç´ ä½ç½®å’Œé¢‘æ¬¡&lt;/li&gt; 
   &lt;li&gt;å…ƒç´ æŽ’åºå’Œåè½¬&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;åˆ—è¡¨ç”Ÿæˆå¼&lt;/li&gt; 
 &lt;li&gt;åµŒå¥—åˆ—è¡¨&lt;/li&gt; 
 &lt;li&gt;åˆ—è¡¨çš„åº”ç”¨&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day10 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/10.%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%85%83%E7%BB%84.md&quot;&gt;å¸¸ç”¨æ•°æ®ç»“æž„ä¹‹å…ƒç»„&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;å…ƒç»„çš„å®šä¹‰å’Œè¿ç®—&lt;/li&gt; 
 &lt;li&gt;æ‰“åŒ…å’Œè§£åŒ…æ“ä½œ&lt;/li&gt; 
 &lt;li&gt;äº¤æ¢å˜é‡çš„å€¼&lt;/li&gt; 
 &lt;li&gt;å…ƒç»„å’Œåˆ—è¡¨çš„æ¯”è¾ƒ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day11 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/11.%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%AD%97%E7%AC%A6%E4%B8%B2.md&quot;&gt;å¸¸ç”¨æ•°æ®ç»“æž„ä¹‹å­—ç¬¦ä¸²&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;å­—ç¬¦ä¸²çš„å®šä¹‰ 
  &lt;ul&gt; 
   &lt;li&gt;è½¬ä¹‰å­—ç¬¦&lt;/li&gt; 
   &lt;li&gt;åŽŸå§‹å­—ç¬¦ä¸²&lt;/li&gt; 
   &lt;li&gt;å­—ç¬¦çš„ç‰¹æ®Šè¡¨ç¤º&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;å­—ç¬¦ä¸²çš„è¿ç®— 
  &lt;ul&gt; 
   &lt;li&gt;æ‹¼æŽ¥å’Œé‡å¤&lt;/li&gt; 
   &lt;li&gt;æ¯”è¾ƒè¿ç®—&lt;/li&gt; 
   &lt;li&gt;æˆå‘˜è¿ç®—&lt;/li&gt; 
   &lt;li&gt;èŽ·å–å­—ç¬¦ä¸²é•¿åº¦&lt;/li&gt; 
   &lt;li&gt;ç´¢å¼•å’Œåˆ‡ç‰‡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;å­—ç¬¦çš„éåŽ†&lt;/li&gt; 
 &lt;li&gt;å­—ç¬¦ä¸²çš„æ–¹æ³• 
  &lt;ul&gt; 
   &lt;li&gt;å¤§å°å†™ç›¸å…³æ“ä½œ&lt;/li&gt; 
   &lt;li&gt;æŸ¥æ‰¾æ“ä½œ&lt;/li&gt; 
   &lt;li&gt;æ€§è´¨åˆ¤æ–­&lt;/li&gt; 
   &lt;li&gt;æ ¼å¼åŒ–&lt;/li&gt; 
   &lt;li&gt;ä¿®å‰ªæ“ä½œ&lt;/li&gt; 
   &lt;li&gt;æ›¿æ¢æ“ä½œ&lt;/li&gt; 
   &lt;li&gt;æ‹†åˆ†ä¸Žåˆå¹¶&lt;/li&gt; 
   &lt;li&gt;ç¼–ç ä¸Žè§£ç &lt;/li&gt; 
   &lt;li&gt;å…¶ä»–æ–¹æ³•&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day12 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/12.%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%9B%86%E5%90%88.md&quot;&gt;å¸¸ç”¨æ•°æ®ç»“æž„ä¹‹é›†åˆ&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;åˆ›å»ºé›†åˆ&lt;/li&gt; 
 &lt;li&gt;å…ƒç´ çš„å˜é‡&lt;/li&gt; 
 &lt;li&gt;é›†åˆçš„è¿ç®— 
  &lt;ul&gt; 
   &lt;li&gt;æˆå‘˜è¿ç®—&lt;/li&gt; 
   &lt;li&gt;äºŒå…ƒè¿ç®—&lt;/li&gt; 
   &lt;li&gt;æ¯”è¾ƒè¿ç®—&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;é›†åˆçš„æ–¹æ³•&lt;/li&gt; 
 &lt;li&gt;ä¸å¯å˜é›†åˆ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day13 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/13.%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%AD%97%E5%85%B8.md&quot;&gt;å¸¸ç”¨æ•°æ®ç»“æž„ä¹‹å­—å…¸&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;åˆ›å»ºå’Œä½¿ç”¨å­—å…¸&lt;/li&gt; 
 &lt;li&gt;å­—å…¸çš„è¿ç®—&lt;/li&gt; 
 &lt;li&gt;å­—å…¸çš„æ–¹æ³•&lt;/li&gt; 
 &lt;li&gt;å­—å…¸çš„åº”ç”¨&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day14 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/14.%E5%87%BD%E6%95%B0%E5%92%8C%E6%A8%A1%E5%9D%97.md&quot;&gt;å‡½æ•°å’Œæ¨¡å—&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;å®šä¹‰å‡½æ•°&lt;/li&gt; 
 &lt;li&gt;å‡½æ•°çš„å‚æ•° 
  &lt;ul&gt; 
   &lt;li&gt;ä½ç½®å‚æ•°å’Œå…³é”®å­—å‚æ•°&lt;/li&gt; 
   &lt;li&gt;å‚æ•°çš„é»˜è®¤å€¼&lt;/li&gt; 
   &lt;li&gt;å¯å˜å‚æ•°&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;ç”¨æ¨¡å—ç®¡ç†å‡½æ•°&lt;/li&gt; 
 &lt;li&gt;æ ‡å‡†åº“ä¸­çš„æ¨¡å—å’Œå‡½æ•°&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day15 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/15.%E5%87%BD%E6%95%B0%E5%BA%94%E7%94%A8%E5%AE%9E%E6%88%98.md&quot;&gt;å‡½æ•°åº”ç”¨å®žæˆ˜&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;ä¾‹å­1ï¼šéšæœºéªŒè¯ç &lt;/li&gt; 
 &lt;li&gt;ä¾‹å­2ï¼šåˆ¤æ–­ç´ æ•°&lt;/li&gt; 
 &lt;li&gt;ä¾‹å­3ï¼šæœ€å¤§å…¬çº¦æ•°å’Œæœ€å°å…¬å€æ•°&lt;/li&gt; 
 &lt;li&gt;ä¾‹å­4ï¼šæ•°æ®ç»Ÿè®¡&lt;/li&gt; 
 &lt;li&gt;ä¾‹å­5ï¼šåŒè‰²çƒéšæœºé€‰å·&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day16 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/16.%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8%E8%BF%9B%E9%98%B6.md&quot;&gt;å‡½æ•°ä½¿ç”¨è¿›é˜¶&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;é«˜é˜¶å‡½æ•°&lt;/li&gt; 
 &lt;li&gt;Lambdaå‡½æ•°&lt;/li&gt; 
 &lt;li&gt;åå‡½æ•°&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day17 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/17.%E5%87%BD%E6%95%B0%E9%AB%98%E7%BA%A7%E5%BA%94%E7%94%A8.md&quot;&gt;å‡½æ•°é«˜çº§åº”ç”¨&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;è£…é¥°å™¨&lt;/li&gt; 
 &lt;li&gt;é€’å½’è°ƒç”¨&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day18 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/18.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8.md&quot;&gt;é¢å‘å¯¹è±¡ç¼–ç¨‹å…¥é—¨&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;ç±»å’Œå¯¹è±¡&lt;/li&gt; 
 &lt;li&gt;å®šä¹‰ç±»&lt;/li&gt; 
 &lt;li&gt;åˆ›å»ºå’Œä½¿ç”¨å¯¹è±¡&lt;/li&gt; 
 &lt;li&gt;åˆå§‹åŒ–æ–¹æ³•&lt;/li&gt; 
 &lt;li&gt;é¢å‘å¯¹è±¡çš„æ”¯æŸ±&lt;/li&gt; 
 &lt;li&gt;é¢å‘å¯¹è±¡æ¡ˆä¾‹ 
  &lt;ul&gt; 
   &lt;li&gt;ä¾‹å­1ï¼šæ•°å­—æ—¶é’Ÿ&lt;/li&gt; 
   &lt;li&gt;ä¾‹å­2ï¼šå¹³é¢ä¸Šçš„ç‚¹&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day19 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/19.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E8%BF%9B%E9%98%B6.md&quot;&gt;é¢å‘å¯¹è±¡ç¼–ç¨‹è¿›é˜¶&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;å¯è§æ€§å’Œå±žæ€§è£…é¥°å™¨&lt;/li&gt; 
 &lt;li&gt;åŠ¨æ€å±žæ€§&lt;/li&gt; 
 &lt;li&gt;é™æ€æ–¹æ³•å’Œç±»æ–¹æ³•&lt;/li&gt; 
 &lt;li&gt;ç»§æ‰¿å’Œå¤šæ€&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day20 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day01-20/20.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E5%BA%94%E7%94%A8.md&quot;&gt;é¢å‘å¯¹è±¡ç¼–ç¨‹åº”ç”¨&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;æ‰‘å…‹æ¸¸æˆ&lt;/li&gt; 
 &lt;li&gt;å·¥èµ„ç»“ç®—ç³»ç»Ÿ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Day21~30 - Pythonè¯­è¨€åº”ç”¨&lt;/h3&gt; 
&lt;h4&gt;Day21 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day21-30/21.%E6%96%87%E4%BB%B6%E8%AF%BB%E5%86%99%E5%92%8C%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86.md&quot;&gt;æ–‡ä»¶è¯»å†™å’Œå¼‚å¸¸å¤„ç†&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;æ‰“å¼€å’Œå…³é—­æ–‡ä»¶&lt;/li&gt; 
 &lt;li&gt;è¯»å†™æ–‡æœ¬æ–‡ä»¶&lt;/li&gt; 
 &lt;li&gt;å¼‚å¸¸å¤„ç†æœºåˆ¶&lt;/li&gt; 
 &lt;li&gt;ä¸Šä¸‹æ–‡ç®¡ç†å™¨è¯­æ³•&lt;/li&gt; 
 &lt;li&gt;è¯»å†™äºŒè¿›åˆ¶æ–‡ä»¶&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day22 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day21-30/22.%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E5%92%8C%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96.md&quot;&gt;å¯¹è±¡çš„åºåˆ—åŒ–å’Œååºåˆ—åŒ–&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;JSONæ¦‚è¿°&lt;/li&gt; 
 &lt;li&gt;è¯»å†™JSONæ ¼å¼çš„æ•°æ®&lt;/li&gt; 
 &lt;li&gt;åŒ…ç®¡ç†å·¥å…·pip&lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨ç½‘ç»œAPIèŽ·å–æ•°æ®&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day23 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/23.Python%E8%AF%BB%E5%86%99CSV%E6%96%87%E4%BB%B6.md&quot;&gt;Pythonè¯»å†™CSVæ–‡ä»¶&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;CSVæ–‡ä»¶ä»‹ç»&lt;/li&gt; 
 &lt;li&gt;å°†æ•°æ®å†™å…¥CSVæ–‡ä»¶&lt;/li&gt; 
 &lt;li&gt;ä»ŽCSVæ–‡ä»¶è¯»å–æ•°æ®&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day24 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day21-30/24.%E7%94%A8Python%E8%AF%BB%E5%86%99Excel%E6%96%87%E4%BB%B6-1.md&quot;&gt;Pythonè¯»å†™Excelæ–‡ä»¶-1&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Excelç®€ä»‹&lt;/li&gt; 
 &lt;li&gt;è¯»Excelæ–‡ä»¶&lt;/li&gt; 
 &lt;li&gt;å†™Excelæ–‡ä»¶&lt;/li&gt; 
 &lt;li&gt;è°ƒæ•´æ ·å¼&lt;/li&gt; 
 &lt;li&gt;å…¬å¼è®¡ç®—&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day25 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day21-30/25.Python%E8%AF%BB%E5%86%99Excel%E6%96%87%E4%BB%B6-2.md&quot;&gt;Pythonè¯»å†™Excelæ–‡ä»¶-2&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Excelç®€ä»‹&lt;/li&gt; 
 &lt;li&gt;è¯»Excelæ–‡ä»¶&lt;/li&gt; 
 &lt;li&gt;å†™Excelæ–‡ä»¶&lt;/li&gt; 
 &lt;li&gt;è°ƒæ•´æ ·å¼&lt;/li&gt; 
 &lt;li&gt;ç”Ÿæˆç»Ÿè®¡å›¾è¡¨&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day26 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day21-30/26.Python%E6%93%8D%E4%BD%9CWord%E5%92%8CPowerPoint%E6%96%87%E4%BB%B6.md&quot;&gt;Pythonæ“ä½œWordå’ŒPowerPointæ–‡ä»¶&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;æ“ä½œWordæ–‡æ¡£&lt;/li&gt; 
 &lt;li&gt;ç”ŸæˆPowerPoint&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day27 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day21-30/27.Python%E6%93%8D%E4%BD%9CPDF%E6%96%87%E4%BB%B6.md&quot;&gt;Pythonæ“ä½œPDFæ–‡ä»¶&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;ä»ŽPDFä¸­æå–æ–‡æœ¬&lt;/li&gt; 
 &lt;li&gt;æ—‹è½¬å’Œå åŠ é¡µé¢&lt;/li&gt; 
 &lt;li&gt;åŠ å¯†PDFæ–‡ä»¶&lt;/li&gt; 
 &lt;li&gt;æ‰¹é‡æ·»åŠ æ°´å°&lt;/li&gt; 
 &lt;li&gt;åˆ›å»ºPDFæ–‡ä»¶&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day28 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day21-30/28.Python%E5%A4%84%E7%90%86%E5%9B%BE%E5%83%8F.md&quot;&gt;Pythonå¤„ç†å›¾åƒ&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;å…¥é—¨çŸ¥è¯†&lt;/li&gt; 
 &lt;li&gt;ç”¨Pillowå¤„ç†å›¾åƒ&lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨Pillowç»˜å›¾&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day29 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day21-30/29.Python%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%E5%92%8C%E7%9F%AD%E4%BF%A1.md&quot;&gt;Pythonå‘é€é‚®ä»¶å’ŒçŸ­ä¿¡&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;å‘é€ç”µå­é‚®ä»¶&lt;/li&gt; 
 &lt;li&gt;å‘é€çŸ­ä¿¡&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day30 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day21-30/30.%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E7%9A%84%E5%BA%94%E7%94%A8.md&quot;&gt;æ­£åˆ™è¡¨è¾¾å¼çš„åº”ç”¨&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;æ­£åˆ™è¡¨è¾¾å¼ç›¸å…³çŸ¥è¯†&lt;/li&gt; 
 &lt;li&gt;Pythonå¯¹æ­£åˆ™è¡¨è¾¾å¼çš„æ”¯æŒ 
  &lt;ul&gt; 
   &lt;li&gt;ä¾‹å­1ï¼šè¾“å…¥éªŒè¯&lt;/li&gt; 
   &lt;li&gt;ä¾‹å­2ï¼šå†…å®¹æå–&lt;/li&gt; 
   &lt;li&gt;ä¾‹å­3ï¼šå†…å®¹æ›¿æ¢&lt;/li&gt; 
   &lt;li&gt;ä¾‹å­4ï¼šé•¿å¥æ‹†åˆ†&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Day31~35 - å…¶ä»–ç›¸å…³å†…å®¹&lt;/h3&gt; 
&lt;h4&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day31-35/31.Python%E8%AF%AD%E8%A8%80%E8%BF%9B%E9%98%B6.md&quot;&gt;Pythonè¯­è¨€è¿›é˜¶&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;é‡è¦çŸ¥è¯†ç‚¹&lt;/li&gt; 
 &lt;li&gt;æ•°æ®ç»“æž„å’Œç®—æ³•&lt;/li&gt; 
 &lt;li&gt;å‡½æ•°çš„ä½¿ç”¨æ–¹å¼&lt;/li&gt; 
 &lt;li&gt;é¢å‘å¯¹è±¡ç›¸å…³çŸ¥è¯†&lt;/li&gt; 
 &lt;li&gt;è¿­ä»£å™¨å’Œç”Ÿæˆå™¨&lt;/li&gt; 
 &lt;li&gt;å¹¶å‘ç¼–ç¨‹&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day31-35/32-33.Web%E5%89%8D%E7%AB%AF%E5%85%A5%E9%97%A8.md&quot;&gt;Webå‰ç«¯å…¥é—¨&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;ç”¨HTMLæ ‡ç­¾æ‰¿è½½é¡µé¢å†…å®¹&lt;/li&gt; 
 &lt;li&gt;ç”¨CSSæ¸²æŸ“é¡µé¢&lt;/li&gt; 
 &lt;li&gt;ç”¨JavaScriptå¤„ç†äº¤äº’å¼è¡Œä¸º&lt;/li&gt; 
 &lt;li&gt;Vue.jså…¥é—¨&lt;/li&gt; 
 &lt;li&gt;Elementçš„ä½¿ç”¨&lt;/li&gt; 
 &lt;li&gt;Bootstrapçš„ä½¿ç”¨&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day31-35/34-35.%E7%8E%A9%E8%BD%ACLinux%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.md&quot;&gt;çŽ©è½¬Linuxæ“ä½œç³»ç»Ÿ&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;æ“ä½œç³»ç»Ÿå‘å±•å²å’ŒLinuxæ¦‚è¿°&lt;/li&gt; 
 &lt;li&gt;LinuxåŸºç¡€å‘½ä»¤&lt;/li&gt; 
 &lt;li&gt;Linuxä¸­çš„å®žç”¨ç¨‹åº&lt;/li&gt; 
 &lt;li&gt;Linuxçš„æ–‡ä»¶ç³»ç»Ÿ&lt;/li&gt; 
 &lt;li&gt;Vimç¼–è¾‘å™¨çš„åº”ç”¨&lt;/li&gt; 
 &lt;li&gt;çŽ¯å¢ƒå˜é‡å’ŒShellç¼–ç¨‹&lt;/li&gt; 
 &lt;li&gt;è½¯ä»¶çš„å®‰è£…å’ŒæœåŠ¡çš„é…ç½®&lt;/li&gt; 
 &lt;li&gt;ç½‘ç»œè®¿é—®å’Œç®¡ç†&lt;/li&gt; 
 &lt;li&gt;å…¶ä»–ç›¸å…³å†…å®¹&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Day36~45 - æ•°æ®åº“åŸºç¡€å’Œè¿›é˜¶&lt;/h3&gt; 
&lt;h4&gt;Day36 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/36.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E5%92%8CMySQL%E6%A6%82%E8%BF%B0.md&quot;&gt;å…³ç³»åž‹æ•°æ®åº“å’ŒMySQLæ¦‚è¿°&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;å…³ç³»åž‹æ•°æ®åº“æ¦‚è¿°&lt;/li&gt; 
 &lt;li&gt;MySQLç®€ä»‹&lt;/li&gt; 
 &lt;li&gt;å®‰è£…MySQL&lt;/li&gt; 
 &lt;li&gt;MySQLåŸºæœ¬å‘½ä»¤&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day37 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/37.SQL%E8%AF%A6%E8%A7%A3%E4%B9%8BDDL.md&quot;&gt;SQLè¯¦è§£ä¹‹DDL&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;å»ºåº“å»ºè¡¨&lt;/li&gt; 
 &lt;li&gt;åˆ é™¤è¡¨å’Œä¿®æ”¹è¡¨&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day38 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/38.SQL%E8%AF%A6%E8%A7%A3%E4%B9%8BDML.md&quot;&gt;SQLè¯¦è§£ä¹‹DML&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;insertæ“ä½œ&lt;/li&gt; 
 &lt;li&gt;deleteæ“ä½œ&lt;/li&gt; 
 &lt;li&gt;updateæ“ä½œ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day39 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/39.SQL%E8%AF%A6%E8%A7%A3%E4%B9%8BDQL.md&quot;&gt;SQLè¯¦è§£ä¹‹DQL&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;æŠ•å½±å’Œåˆ«å&lt;/li&gt; 
 &lt;li&gt;ç­›é€‰æ•°æ®&lt;/li&gt; 
 &lt;li&gt;ç©ºå€¼å¤„ç†&lt;/li&gt; 
 &lt;li&gt;åŽ»é‡&lt;/li&gt; 
 &lt;li&gt;æŽ’åº&lt;/li&gt; 
 &lt;li&gt;èšåˆå‡½æ•°&lt;/li&gt; 
 &lt;li&gt;åµŒå¥—æŸ¥è¯¢&lt;/li&gt; 
 &lt;li&gt;åˆ†ç»„æ“ä½œ&lt;/li&gt; 
 &lt;li&gt;è¡¨è¿žæŽ¥ 
  &lt;ul&gt; 
   &lt;li&gt;ç¬›å¡å°”ç§¯&lt;/li&gt; 
   &lt;li&gt;å†…è¿žæŽ¥&lt;/li&gt; 
   &lt;li&gt;è‡ªç„¶è¿žæŽ¥&lt;/li&gt; 
   &lt;li&gt;å¤–è¿žæŽ¥&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;çª—å£å‡½æ•° 
  &lt;ul&gt; 
   &lt;li&gt;å®šä¹‰çª—å£&lt;/li&gt; 
   &lt;li&gt;æŽ’åå‡½æ•°&lt;/li&gt; 
   &lt;li&gt;å–æ•°å‡½æ•°&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day40 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/40.SQL%E8%AF%A6%E8%A7%A3%E4%B9%8BDCL.md&quot;&gt;SQLè¯¦è§£ä¹‹DCL&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;åˆ›å»ºç”¨æˆ·&lt;/li&gt; 
 &lt;li&gt;æŽˆäºˆæƒé™&lt;/li&gt; 
 &lt;li&gt;å¬å›žæƒé™&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day41 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/41.MySQL%E6%96%B0%E7%89%B9%E6%80%A7.md&quot;&gt;MySQLæ–°ç‰¹æ€§&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;JSONç±»åž‹&lt;/li&gt; 
 &lt;li&gt;çª—å£å‡½æ•°&lt;/li&gt; 
 &lt;li&gt;å…¬å…±è¡¨è¡¨è¾¾å¼&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Day42 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/42.%E8%A7%86%E5%9B%BE%E3%80%81%E5%87%BD%E6%95%B0%E5%92%8C%E8%BF%87%E7%A8%8B.md&quot;&gt;è§†å›¾ã€å‡½æ•°å’Œè¿‡ç¨‹&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;è§†å›¾ 
  &lt;ul&gt; 
   &lt;li&gt;ä½¿ç”¨åœºæ™¯&lt;/li&gt; 
   &lt;li&gt;åˆ›å»ºè§†å›¾&lt;/li&gt; 
   &lt;li&gt;ä½¿ç”¨é™åˆ¶&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;å‡½æ•° 
  &lt;ul&gt; 
   &lt;li&gt;å†…ç½®å‡½æ•°&lt;/li&gt; 
   &lt;li&gt;ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°ï¼ˆUDFï¼‰&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;è¿‡ç¨‹ 
  &lt;ul&gt; 
   &lt;li&gt;åˆ›å»ºè¿‡ç¨‹&lt;/li&gt; 
   &lt;li&gt;è°ƒç”¨è¿‡ç¨‹&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day43 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/43.%E7%B4%A2%E5%BC%95.md&quot;&gt;ç´¢å¼•&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;æ‰§è¡Œè®¡åˆ’&lt;/li&gt; 
 &lt;li&gt;ç´¢å¼•çš„åŽŸç†&lt;/li&gt; 
 &lt;li&gt;åˆ›å»ºç´¢å¼• 
  &lt;ul&gt; 
   &lt;li&gt;æ™®é€šç´¢å¼•&lt;/li&gt; 
   &lt;li&gt;å”¯ä¸€ç´¢å¼•&lt;/li&gt; 
   &lt;li&gt;å‰ç¼€ç´¢å¼•&lt;/li&gt; 
   &lt;li&gt;å¤åˆç´¢å¼•&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;æ³¨æ„äº‹é¡¹&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day44 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/44.Python%E6%8E%A5%E5%85%A5MySQL%E6%95%B0%E6%8D%AE%E5%BA%93.md&quot;&gt;PythonæŽ¥å…¥MySQLæ•°æ®åº“&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;å®‰è£…ä¸‰æ–¹åº“&lt;/li&gt; 
 &lt;li&gt;åˆ›å»ºè¿žæŽ¥&lt;/li&gt; 
 &lt;li&gt;èŽ·å–æ¸¸æ ‡&lt;/li&gt; 
 &lt;li&gt;æ‰§è¡ŒSQLè¯­å¥&lt;/li&gt; 
 &lt;li&gt;é€šè¿‡æ¸¸æ ‡æŠ“å–æ•°æ®&lt;/li&gt; 
 &lt;li&gt;äº‹åŠ¡æäº¤å’Œå›žæ»š&lt;/li&gt; 
 &lt;li&gt;é‡Šæ”¾è¿žæŽ¥&lt;/li&gt; 
 &lt;li&gt;ç¼–å†™ETLè„šæœ¬&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day45 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day36-45/45.%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E5%92%8CHiveSQL.md&quot;&gt;å¤§æ•°æ®å¹³å°å’ŒHiveSQL&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Hadoopç”Ÿæ€åœˆ&lt;/li&gt; 
 &lt;li&gt;Hiveæ¦‚è¿°&lt;/li&gt; 
 &lt;li&gt;å‡†å¤‡å·¥ä½œ&lt;/li&gt; 
 &lt;li&gt;æ•°æ®ç±»åž‹&lt;/li&gt; 
 &lt;li&gt;DDLæ“ä½œ&lt;/li&gt; 
 &lt;li&gt;DMLæ“ä½œ&lt;/li&gt; 
 &lt;li&gt;æ•°æ®æŸ¥è¯¢&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Day46~60 - å®žæˆ˜Django&lt;/h3&gt; 
&lt;h4&gt;Day46 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/46.Django%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B.md&quot;&gt;Djangoå¿«é€Ÿä¸Šæ‰‹&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Webåº”ç”¨å·¥ä½œæœºåˆ¶&lt;/li&gt; 
 &lt;li&gt;HTTPè¯·æ±‚å’Œå“åº”&lt;/li&gt; 
 &lt;li&gt;Djangoæ¡†æž¶æ¦‚è¿°&lt;/li&gt; 
 &lt;li&gt;5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day47 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/47.%E6%B7%B1%E5%85%A5%E6%A8%A1%E5%9E%8B.md&quot;&gt;æ·±å…¥æ¨¡åž‹&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;å…³ç³»åž‹æ•°æ®åº“é…ç½®&lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨ORMå®Œæˆå¯¹æ¨¡åž‹çš„CRUDæ“ä½œ&lt;/li&gt; 
 &lt;li&gt;ç®¡ç†åŽå°çš„ä½¿ç”¨&lt;/li&gt; 
 &lt;li&gt;Djangoæ¨¡åž‹æœ€ä½³å®žè·µ&lt;/li&gt; 
 &lt;li&gt;æ¨¡åž‹å®šä¹‰å‚è€ƒ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day48 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/48.%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%92%8CAjax%E8%AF%B7%E6%B1%82.md&quot;&gt;é™æ€èµ„æºå’ŒAjaxè¯·æ±‚&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;åŠ è½½é™æ€èµ„æº&lt;/li&gt; 
 &lt;li&gt;Ajaxæ¦‚è¿°&lt;/li&gt; 
 &lt;li&gt;ç”¨Ajaxå®žçŽ°æŠ•ç¥¨åŠŸèƒ½&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day49 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/49.Cookie%E5%92%8CSession.md&quot;&gt;Cookieå’ŒSession&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;å®žçŽ°ç”¨æˆ·è·Ÿè¸ª&lt;/li&gt; 
 &lt;li&gt;cookieå’Œsessionçš„å…³ç³»&lt;/li&gt; 
 &lt;li&gt;Djangoæ¡†æž¶å¯¹sessionçš„æ”¯æŒ&lt;/li&gt; 
 &lt;li&gt;è§†å›¾å‡½æ•°ä¸­çš„cookieè¯»å†™æ“ä½œ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day50 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/50.%E5%88%B6%E4%BD%9C%E6%8A%A5%E8%A1%A8.md&quot;&gt;æŠ¥è¡¨å’Œæ—¥å¿—&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;é€šè¿‡&lt;code&gt;HttpResponse&lt;/code&gt;ä¿®æ”¹å“åº”å¤´&lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨&lt;code&gt;StreamingHttpResponse&lt;/code&gt;å¤„ç†å¤§æ–‡ä»¶&lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨&lt;code&gt;xlwt&lt;/code&gt;ç”ŸæˆExcelæŠ¥è¡¨&lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨&lt;code&gt;reportlab&lt;/code&gt;ç”ŸæˆPDFæŠ¥è¡¨&lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨EChartsç”Ÿæˆå‰ç«¯å›¾è¡¨&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day51 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/51.%E6%97%A5%E5%BF%97%E5%92%8C%E8%B0%83%E8%AF%95%E5%B7%A5%E5%85%B7%E6%A0%8F.md&quot;&gt;æ—¥å¿—å’Œè°ƒè¯•å·¥å…·æ &lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;é…ç½®æ—¥å¿—&lt;/li&gt; 
 &lt;li&gt;é…ç½®Django-Debug-Toolbar&lt;/li&gt; 
 &lt;li&gt;ä¼˜åŒ–ORMä»£ç &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day52 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/52.%E4%B8%AD%E9%97%B4%E4%BB%B6%E7%9A%84%E5%BA%94%E7%94%A8.md&quot;&gt;ä¸­é—´ä»¶çš„åº”ç”¨&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;ä»€ä¹ˆæ˜¯ä¸­é—´ä»¶&lt;/li&gt; 
 &lt;li&gt;Djangoæ¡†æž¶å†…ç½®çš„ä¸­é—´ä»¶&lt;/li&gt; 
 &lt;li&gt;è‡ªå®šä¹‰ä¸­é—´ä»¶åŠå…¶åº”ç”¨åœºæ™¯&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day53 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/53.%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8.md&quot;&gt;å‰åŽç«¯åˆ†ç¦»å¼€å‘å…¥é—¨&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;è¿”å›žJSONæ ¼å¼çš„æ•°æ®&lt;/li&gt; 
 &lt;li&gt;ç”¨Vue.jsæ¸²æŸ“é¡µé¢&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day54 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/54.RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E5%85%A5%E9%97%A8.md&quot;&gt;RESTfulæž¶æž„å’ŒDRFå…¥é—¨&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;RESTæ¦‚è¿°&lt;/li&gt; 
 &lt;li&gt;DRFåº“ä½¿ç”¨å…¥é—¨&lt;/li&gt; 
 &lt;li&gt;å‰åŽç«¯åˆ†ç¦»å¼€å‘&lt;/li&gt; 
 &lt;li&gt;JWTçš„åº”ç”¨&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day55 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/55.RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E8%BF%9B%E9%98%B6.md&quot;&gt;RESTfulæž¶æž„å’ŒDRFè¿›é˜¶&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;ä½¿ç”¨CBV&lt;/li&gt; 
 &lt;li&gt;æ•°æ®åˆ†é¡µ&lt;/li&gt; 
 &lt;li&gt;æ•°æ®ç­›é€‰&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day56 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/56.%E4%BD%BF%E7%94%A8%E7%BC%93%E5%AD%98.md&quot;&gt;ä½¿ç”¨ç¼“å­˜&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;ç½‘ç«™ä¼˜åŒ–ç¬¬ä¸€å®šå¾‹&lt;/li&gt; 
 &lt;li&gt;åœ¨Djangoé¡¹ç›®ä¸­ä½¿ç”¨Redisæä¾›ç¼“å­˜æœåŠ¡&lt;/li&gt; 
 &lt;li&gt;åœ¨è§†å›¾å‡½æ•°ä¸­è¯»å†™ç¼“å­˜&lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨è£…é¥°å™¨å®žçŽ°é¡µé¢ç¼“å­˜&lt;/li&gt; 
 &lt;li&gt;ä¸ºæ•°æ®æŽ¥å£æä¾›ç¼“å­˜æœåŠ¡&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day57 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/57.%E6%8E%A5%E5%85%A5%E4%B8%89%E6%96%B9%E5%B9%B3%E5%8F%B0.md&quot;&gt;æŽ¥å…¥ä¸‰æ–¹å¹³å°&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;æ–‡ä»¶ä¸Šä¼ è¡¨å•æŽ§ä»¶å’Œå›¾ç‰‡æ–‡ä»¶é¢„è§ˆ&lt;/li&gt; 
 &lt;li&gt;æœåŠ¡å™¨ç«¯å¦‚ä½•å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day58 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/58.%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%E5%92%8C%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1.md&quot;&gt;å¼‚æ­¥ä»»åŠ¡å’Œå®šæ—¶ä»»åŠ¡&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;ç½‘ç«™ä¼˜åŒ–ç¬¬äºŒå®šå¾‹&lt;/li&gt; 
 &lt;li&gt;é…ç½®æ¶ˆæ¯é˜Ÿåˆ—æœåŠ¡&lt;/li&gt; 
 &lt;li&gt;åœ¨é¡¹ç›®ä¸­ä½¿ç”¨Celeryå®žçŽ°ä»»åŠ¡å¼‚æ­¥åŒ–&lt;/li&gt; 
 &lt;li&gt;åœ¨é¡¹ç›®ä¸­ä½¿ç”¨Celeryå®žçŽ°å®šæ—¶ä»»åŠ¡&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day59 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/59.%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95.md&quot;&gt;å•å…ƒæµ‹è¯•&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;Day60 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day46-60/60.%E9%A1%B9%E7%9B%AE%E4%B8%8A%E7%BA%BF.md&quot;&gt;é¡¹ç›®ä¸Šçº¿&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Pythonä¸­çš„å•å…ƒæµ‹è¯•&lt;/li&gt; 
 &lt;li&gt;Djangoæ¡†æž¶å¯¹å•å…ƒæµ‹è¯•çš„æ”¯æŒ&lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨ç‰ˆæœ¬æŽ§åˆ¶ç³»ç»Ÿ&lt;/li&gt; 
 &lt;li&gt;é…ç½®å’Œä½¿ç”¨uWSGI&lt;/li&gt; 
 &lt;li&gt;åŠ¨é™åˆ†ç¦»å’ŒNginxé…ç½®&lt;/li&gt; 
 &lt;li&gt;é…ç½®HTTPS&lt;/li&gt; 
 &lt;li&gt;é…ç½®åŸŸåè§£æž&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Day61~65 - ç½‘ç»œæ•°æ®é‡‡é›†&lt;/h3&gt; 
&lt;h4&gt;Day61 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day61-65/61.%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E6%A6%82%E8%BF%B0.md&quot;&gt;ç½‘ç»œæ•°æ®é‡‡é›†æ¦‚è¿°&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;ç½‘ç»œçˆ¬è™«çš„æ¦‚å¿µåŠå…¶åº”ç”¨é¢†åŸŸ&lt;/li&gt; 
 &lt;li&gt;ç½‘ç»œçˆ¬è™«çš„åˆæ³•æ€§æŽ¢è®¨&lt;/li&gt; 
 &lt;li&gt;å¼€å‘ç½‘ç»œçˆ¬è™«çš„ç›¸å…³å·¥å…·&lt;/li&gt; 
 &lt;li&gt;ä¸€ä¸ªçˆ¬è™«ç¨‹åºçš„æž„æˆ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day62 - æ•°æ®æŠ“å–å’Œè§£æž&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day61-65/62.%E7%94%A8Python%E8%8E%B7%E5%8F%96%E7%BD%91%E7%BB%9C%E8%B5%84%E6%BA%90-1.md&quot;&gt;ä½¿ç”¨&lt;code&gt;requests&lt;/code&gt;ä¸‰æ–¹åº“å®žçŽ°æ•°æ®æŠ“å–&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day61-65/62.%E7%94%A8Python%E8%A7%A3%E6%9E%90HTML%E9%A1%B5%E9%9D%A2-2.md&quot;&gt;é¡µé¢è§£æžçš„ä¸‰ç§æ–¹å¼&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;æ­£åˆ™è¡¨è¾¾å¼è§£æž&lt;/li&gt; 
   &lt;li&gt;XPathè§£æž&lt;/li&gt; 
   &lt;li&gt;CSSé€‰æ‹©å™¨è§£æž&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day63 - Pythonä¸­çš„å¹¶å‘ç¼–ç¨‹&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day61-65/63.Python%E4%B8%AD%E7%9A%84%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-1.md&quot;&gt;å¤šçº¿ç¨‹&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day61-65/63.Python%E4%B8%AD%E7%9A%84%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-2.md&quot;&gt;å¤šè¿›ç¨‹&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day61-65/63.Python%E4%B8%AD%E7%9A%84%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-3.md&quot;&gt;å¼‚æ­¥I/O&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day64 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day61-65/64.%E4%BD%BF%E7%94%A8Selenium%E6%8A%93%E5%8F%96%E7%BD%91%E9%A1%B5%E5%8A%A8%E6%80%81%E5%86%85%E5%AE%B9.md&quot;&gt;ä½¿ç”¨SeleniumæŠ“å–ç½‘é¡µåŠ¨æ€å†…å®¹&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;å®‰è£…Selenium&lt;/li&gt; 
 &lt;li&gt;åŠ è½½é¡µé¢&lt;/li&gt; 
 &lt;li&gt;æŸ¥æ‰¾å…ƒç´ å’Œæ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸º&lt;/li&gt; 
 &lt;li&gt;éšå¼ç­‰å¾…å’Œæ˜¾ç¤ºç­‰å¾…&lt;/li&gt; 
 &lt;li&gt;æ‰§è¡ŒJavaScriptä»£ç &lt;/li&gt; 
 &lt;li&gt;Seleniumåçˆ¬ç ´è§£&lt;/li&gt; 
 &lt;li&gt;è®¾ç½®æ— å¤´æµè§ˆå™¨&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day65 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day61-65/65.%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6Scrapy%E7%AE%80%E4%BB%8B.md&quot;&gt;çˆ¬è™«æ¡†æž¶Scrapyç®€ä»‹&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Scrapyæ ¸å¿ƒç»„ä»¶&lt;/li&gt; 
 &lt;li&gt;Scrapyå·¥ä½œæµç¨‹&lt;/li&gt; 
 &lt;li&gt;å®‰è£…Scrapyå’Œåˆ›å»ºé¡¹ç›®&lt;/li&gt; 
 &lt;li&gt;ç¼–å†™èœ˜è››ç¨‹åº&lt;/li&gt; 
 &lt;li&gt;ç¼–å†™ä¸­é—´ä»¶å’Œç®¡é“ç¨‹åº&lt;/li&gt; 
 &lt;li&gt;Scrapyé…ç½®æ–‡ä»¶&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Day66~80 - Pythonæ•°æ®åˆ†æž&lt;/h3&gt; 
&lt;h4&gt;Day66 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/66.%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%A6%82%E8%BF%B0.md&quot;&gt;æ•°æ®åˆ†æžæ¦‚è¿°&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;æ•°æ®åˆ†æžå¸ˆçš„èŒè´£&lt;/li&gt; 
 &lt;li&gt;æ•°æ®åˆ†æžå¸ˆçš„æŠ€èƒ½æ ˆ&lt;/li&gt; 
 &lt;li&gt;æ•°æ®åˆ†æžç›¸å…³åº“&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day67 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/67.%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87.md&quot;&gt;çŽ¯å¢ƒå‡†å¤‡&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;å®‰è£…å’Œä½¿ç”¨anaconda 
  &lt;ul&gt; 
   &lt;li&gt;condaç›¸å…³å‘½ä»¤&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;å®‰è£…å’Œä½¿ç”¨jupyter-lab 
  &lt;ul&gt; 
   &lt;li&gt;å®‰è£…å’Œå¯åŠ¨&lt;/li&gt; 
   &lt;li&gt;ä½¿ç”¨å°æŠ€å·§&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day68 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/68.NumPy%E7%9A%84%E5%BA%94%E7%94%A8-1.md&quot;&gt;NumPyçš„åº”ç”¨-1&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;åˆ›å»ºæ•°ç»„å¯¹è±¡&lt;/li&gt; 
 &lt;li&gt;æ•°ç»„å¯¹è±¡çš„å±žæ€§&lt;/li&gt; 
 &lt;li&gt;æ•°ç»„å¯¹è±¡çš„ç´¢å¼•è¿ç®— 
  &lt;ul&gt; 
   &lt;li&gt;æ™®é€šç´¢å¼•&lt;/li&gt; 
   &lt;li&gt;èŠ±å¼ç´¢å¼•&lt;/li&gt; 
   &lt;li&gt;å¸ƒå°”ç´¢å¼•&lt;/li&gt; 
   &lt;li&gt;åˆ‡ç‰‡ç´¢å¼•&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;æ¡ˆä¾‹ï¼šä½¿ç”¨æ•°ç»„å¤„ç†å›¾åƒ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day69 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/69.NumPy%E7%9A%84%E5%BA%94%E7%94%A8-2.md&quot;&gt;NumPyçš„åº”ç”¨-2&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;æ•°ç»„å¯¹è±¡çš„ç›¸å…³æ–¹æ³• 
  &lt;ul&gt; 
   &lt;li&gt;èŽ·å–æè¿°æ€§ç»Ÿè®¡ä¿¡æ¯&lt;/li&gt; 
   &lt;li&gt;å…¶ä»–ç›¸å…³æ–¹æ³•&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day70 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/70.NumPy%E7%9A%84%E5%BA%94%E7%94%A8-3.md&quot;&gt;NumPyçš„åº”ç”¨-3&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;æ•°ç»„çš„è¿ç®— 
  &lt;ul&gt; 
   &lt;li&gt;æ•°ç»„è·Ÿæ ‡é‡çš„è¿ç®—&lt;/li&gt; 
   &lt;li&gt;æ•°ç»„è·Ÿæ•°ç»„çš„è¿ç®—&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;é€šç”¨ä¸€å…ƒå‡½æ•°&lt;/li&gt; 
 &lt;li&gt;é€šç”¨äºŒå…ƒå‡½æ•°&lt;/li&gt; 
 &lt;li&gt;å¹¿æ’­æœºåˆ¶&lt;/li&gt; 
 &lt;li&gt;Numpyå¸¸ç”¨å‡½æ•°&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day71 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/71.NumPy%E7%9A%84%E5%BA%94%E7%94%A8-4.md&quot;&gt;NumPyçš„åº”ç”¨-4&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;å‘é‡&lt;/li&gt; 
 &lt;li&gt;è¡Œåˆ—å¼&lt;/li&gt; 
 &lt;li&gt;çŸ©é˜µ&lt;/li&gt; 
 &lt;li&gt;å¤šé¡¹å¼&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day72 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/72.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApandas-1.md&quot;&gt;æ·±å…¥æµ…å‡ºpandas-1&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;åˆ›å»ºSerieså¯¹è±¡&lt;/li&gt; 
 &lt;li&gt;Serieså¯¹è±¡çš„è¿ç®—&lt;/li&gt; 
 &lt;li&gt;Serieså¯¹è±¡çš„å±žæ€§å’Œæ–¹æ³•&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day73 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/73.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApandas-2.md&quot;&gt;æ·±å…¥æµ…å‡ºpandas-2&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;åˆ›å»ºDataFrameå¯¹è±¡&lt;/li&gt; 
 &lt;li&gt;DataFrameå¯¹è±¡çš„å±žæ€§å’Œæ–¹æ³•&lt;/li&gt; 
 &lt;li&gt;è¯»å†™DataFrameä¸­çš„æ•°æ®&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day74 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/74.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApandas-3.md&quot;&gt;æ·±å…¥æµ…å‡ºpandas-3&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;æ•°æ®é‡å¡‘ 
  &lt;ul&gt; 
   &lt;li&gt;æ•°æ®æ‹¼æŽ¥&lt;/li&gt; 
   &lt;li&gt;æ•°æ®åˆå¹¶&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;æ•°æ®æ¸…æ´— 
  &lt;ul&gt; 
   &lt;li&gt;ç¼ºå¤±å€¼&lt;/li&gt; 
   &lt;li&gt;é‡å¤å€¼&lt;/li&gt; 
   &lt;li&gt;å¼‚å¸¸å€¼&lt;/li&gt; 
   &lt;li&gt;é¢„å¤„ç†&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day75 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/75.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApandas-4.md&quot;&gt;æ·±å…¥æµ…å‡ºpandas-4&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;æ•°æ®é€è§† 
  &lt;ul&gt; 
   &lt;li&gt;èŽ·å–æè¿°æ€§ç»Ÿè®¡ä¿¡æ¯&lt;/li&gt; 
   &lt;li&gt;æŽ’åºå’Œå¤´éƒ¨å€¼&lt;/li&gt; 
   &lt;li&gt;åˆ†ç»„èšåˆ&lt;/li&gt; 
   &lt;li&gt;é€è§†è¡¨å’Œäº¤å‰è¡¨&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;æ•°æ®å‘ˆçŽ°&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day76 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/76.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApandas-5.md&quot;&gt;æ·±å…¥æµ…å‡ºpandas-5&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;è®¡ç®—åŒæ¯”çŽ¯æ¯”&lt;/li&gt; 
 &lt;li&gt;çª—å£è®¡ç®—&lt;/li&gt; 
 &lt;li&gt;ç›¸å…³æ€§åˆ¤å®š&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day77 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/77.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApandas-6.md&quot;&gt;æ·±å…¥æµ…å‡ºpandas-6&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;ç´¢å¼•çš„ä½¿ç”¨ 
  &lt;ul&gt; 
   &lt;li&gt;èŒƒå›´ç´¢å¼•&lt;/li&gt; 
   &lt;li&gt;åˆ†ç±»ç´¢å¼•&lt;/li&gt; 
   &lt;li&gt;å¤šçº§ç´¢å¼•&lt;/li&gt; 
   &lt;li&gt;é—´éš”ç´¢å¼•&lt;/li&gt; 
   &lt;li&gt;æ—¥æœŸæ—¶é—´ç´¢å¼•&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day78 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/78.%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96-1.md&quot;&gt;æ•°æ®å¯è§†åŒ–-1&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;å®‰è£…å’Œå¯¼å…¥matplotlib&lt;/li&gt; 
 &lt;li&gt;åˆ›å»ºç”»å¸ƒ&lt;/li&gt; 
 &lt;li&gt;åˆ›å»ºåæ ‡ç³»&lt;/li&gt; 
 &lt;li&gt;ç»˜åˆ¶å›¾è¡¨ 
  &lt;ul&gt; 
   &lt;li&gt;æŠ˜çº¿å›¾&lt;/li&gt; 
   &lt;li&gt;æ•£ç‚¹å›¾&lt;/li&gt; 
   &lt;li&gt;æŸ±çŠ¶å›¾&lt;/li&gt; 
   &lt;li&gt;é¥¼çŠ¶å›¾&lt;/li&gt; 
   &lt;li&gt;ç›´æ–¹å›¾&lt;/li&gt; 
   &lt;li&gt;ç®±çº¿å›¾&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;æ˜¾ç¤ºå’Œä¿å­˜å›¾è¡¨&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day79 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/79.%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96-2.md&quot;&gt;æ•°æ®å¯è§†åŒ–-2&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;é«˜é˜¶å›¾è¡¨ 
  &lt;ul&gt; 
   &lt;li&gt;æ°”æ³¡å›¾&lt;/li&gt; 
   &lt;li&gt;é¢ç§¯å›¾&lt;/li&gt; 
   &lt;li&gt;é›·è¾¾å›¾&lt;/li&gt; 
   &lt;li&gt;çŽ«ç‘°å›¾&lt;/li&gt; 
   &lt;li&gt;3Då›¾è¡¨&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day80 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day66-80/80.%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96-3.md&quot;&gt;æ•°æ®å¯è§†åŒ–-3&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Seaborn&lt;/li&gt; 
 &lt;li&gt;Pyecharts&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Day81~90 - æœºå™¨å­¦ä¹ &lt;/h3&gt; 
&lt;h4&gt;Day81 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/81.%E6%B5%85%E8%B0%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.md&quot;&gt;æµ…è°ˆæœºå™¨å­¦ä¹ &lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;äººå·¥æ™ºèƒ½å‘å±•å²&lt;/li&gt; 
 &lt;li&gt;ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ &lt;/li&gt; 
 &lt;li&gt;æœºå™¨å­¦ä¹ åº”ç”¨é¢†åŸŸ&lt;/li&gt; 
 &lt;li&gt;æœºå™¨å­¦ä¹ çš„åˆ†ç±»&lt;/li&gt; 
 &lt;li&gt;æœºå™¨å­¦ä¹ çš„æ­¥éª¤&lt;/li&gt; 
 &lt;li&gt;ç¬¬ä¸€æ¬¡æœºå™¨å­¦ä¹ &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day82 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/82.k%E6%9C%80%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95.md&quot;&gt;kæœ€è¿‘é‚»ç®—æ³•&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;è·ç¦»çš„åº¦é‡&lt;/li&gt; 
 &lt;li&gt;æ•°æ®é›†ä»‹ç»&lt;/li&gt; 
 &lt;li&gt;kNNåˆ†ç±»çš„å®žçŽ°&lt;/li&gt; 
 &lt;li&gt;æ¨¡åž‹è¯„ä¼°&lt;/li&gt; 
 &lt;li&gt;å‚æ•°è°ƒä¼˜&lt;/li&gt; 
 &lt;li&gt;kNNå›žå½’çš„å®žçŽ°&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day83 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/83.%E5%86%B3%E7%AD%96%E6%A0%91%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.md&quot;&gt;å†³ç­–æ ‘å’Œéšæœºæ£®æž—&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;å†³ç­–æ ‘çš„æž„å»º 
  &lt;ul&gt; 
   &lt;li&gt;ç‰¹å¾é€‰æ‹©&lt;/li&gt; 
   &lt;li&gt;æ•°æ®åˆ†è£‚&lt;/li&gt; 
   &lt;li&gt;æ ‘çš„å‰ªæž&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;å®žçŽ°å†³ç­–æ ‘æ¨¡åž‹&lt;/li&gt; 
 &lt;li&gt;éšæœºæ£®æž—æ¦‚è¿°&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day84 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/84.%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95.md&quot;&gt;æœ´ç´ è´å¶æ–¯ç®—æ³•&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;è´å¶æ–¯å®šç†&lt;/li&gt; 
 &lt;li&gt;æœ´ç´ è´å¶æ–¯&lt;/li&gt; 
 &lt;li&gt;ç®—æ³•åŽŸç† 
  &lt;ul&gt; 
   &lt;li&gt;è®­ç»ƒé˜¶æ®µ&lt;/li&gt; 
   &lt;li&gt;é¢„æµ‹é˜¶æ®µ&lt;/li&gt; 
   &lt;li&gt;ä»£ç å®žçŽ°&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;ç®—æ³•ä¼˜ç¼ºç‚¹&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day85 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/85.%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B.md&quot;&gt;å›žå½’æ¨¡åž‹&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;å›žå½’æ¨¡åž‹çš„åˆ†ç±»&lt;/li&gt; 
 &lt;li&gt;å›žå½’ç³»æ•°çš„è®¡ç®—&lt;/li&gt; 
 &lt;li&gt;æ–°æ•°æ®é›†ä»‹ç»&lt;/li&gt; 
 &lt;li&gt;çº¿æ€§å›žå½’ä»£ç å®žçŽ°&lt;/li&gt; 
 &lt;li&gt;å›žå½’æ¨¡åž‹çš„è¯„ä¼°&lt;/li&gt; 
 &lt;li&gt;å¼•å…¥æ­£åˆ™åŒ–é¡¹&lt;/li&gt; 
 &lt;li&gt;çº¿æ€§å›žå½’å¦ä¸€ç§å®žçŽ°&lt;/li&gt; 
 &lt;li&gt;å¤šé¡¹å¼å›žå½’&lt;/li&gt; 
 &lt;li&gt;é€»è¾‘å›žå½’&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day86 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/86.K-Means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95.md&quot;&gt;K-Meansèšç±»ç®—æ³•&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;ç®—æ³•åŽŸç†&lt;/li&gt; 
 &lt;li&gt;æ•°å­¦æè¿°&lt;/li&gt; 
 &lt;li&gt;ä»£ç å®žçŽ°&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day87 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/87.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95.md&quot;&gt;é›†æˆå­¦ä¹ ç®—æ³•&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;ç®—æ³•åˆ†ç±»&lt;/li&gt; 
 &lt;li&gt;AdaBoost&lt;/li&gt; 
 &lt;li&gt;GBDT&lt;/li&gt; 
 &lt;li&gt;XGBoost&lt;/li&gt; 
 &lt;li&gt;LightGBM&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day88 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/88.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.md&quot;&gt;ç¥žç»ç½‘ç»œæ¨¡åž‹&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;åŸºæœ¬æž„æˆ&lt;/li&gt; 
 &lt;li&gt;å·¥ä½œåŽŸç†&lt;/li&gt; 
 &lt;li&gt;ä»£ç å®žçŽ°&lt;/li&gt; 
 &lt;li&gt;æ¨¡åž‹ä¼˜ç¼ºç‚¹&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day89 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/89.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8.md&quot;&gt;è‡ªç„¶è¯­è¨€å¤„ç†å…¥é—¨&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;è¯è¢‹æ¨¡åž‹&lt;/li&gt; 
 &lt;li&gt;è¯å‘é‡&lt;/li&gt; 
 &lt;li&gt;NPLMå’ŒRNN&lt;/li&gt; 
 &lt;li&gt;Seq2Seq&lt;/li&gt; 
 &lt;li&gt;Transformer&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Day90 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day81-90/90.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98.md&quot;&gt;æœºå™¨å­¦ä¹ å®žæˆ˜&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;æ•°æ®æŽ¢ç´¢&lt;/li&gt; 
 &lt;li&gt;ç‰¹å¾å·¥ç¨‹&lt;/li&gt; 
 &lt;li&gt;æ¨¡åž‹è®­ç»ƒ&lt;/li&gt; 
 &lt;li&gt;æ¨¡åž‹è¯„ä¼°&lt;/li&gt; 
 &lt;li&gt;æ¨¡åž‹éƒ¨ç½²&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Day91~99 - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100&quot;&gt;å›¢é˜Ÿé¡¹ç›®å¼€å‘&lt;/a&gt;&lt;/h3&gt; 
&lt;h4&gt;ç¬¬91å¤©ï¼š&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/91.%E5%9B%A2%E9%98%9F%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.md&quot;&gt;å›¢é˜Ÿé¡¹ç›®å¼€å‘çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;è½¯ä»¶è¿‡ç¨‹æ¨¡åž‹&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;ç»å…¸è¿‡ç¨‹æ¨¡åž‹ï¼ˆç€‘å¸ƒæ¨¡åž‹ï¼‰&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;å¯è¡Œæ€§åˆ†æžï¼ˆç ”ç©¶åšè¿˜æ˜¯ä¸åšï¼‰ï¼Œè¾“å‡ºã€Šå¯è¡Œæ€§åˆ†æžæŠ¥å‘Šã€‹ã€‚&lt;/li&gt; 
     &lt;li&gt;éœ€æ±‚åˆ†æžï¼ˆç ”ç©¶åšä»€ä¹ˆï¼‰ï¼Œè¾“å‡ºã€Šéœ€æ±‚è§„æ ¼è¯´æ˜Žä¹¦ã€‹å’Œäº§å“ç•Œé¢åŽŸåž‹å›¾ã€‚&lt;/li&gt; 
     &lt;li&gt;æ¦‚è¦è®¾è®¡å’Œè¯¦ç»†è®¾è®¡ï¼Œè¾“å‡ºæ¦‚å¿µæ¨¡åž‹å›¾ï¼ˆERå›¾ï¼‰ã€ç‰©ç†æ¨¡åž‹å›¾ã€ç±»å›¾ã€æ—¶åºå›¾ç­‰ã€‚&lt;/li&gt; 
     &lt;li&gt;ç¼–ç  / æµ‹è¯•ã€‚&lt;/li&gt; 
     &lt;li&gt;ä¸Šçº¿ / ç»´æŠ¤ã€‚&lt;/li&gt; 
    &lt;/ul&gt; &lt;p&gt;ç€‘å¸ƒæ¨¡åž‹æœ€å¤§çš„ç¼ºç‚¹æ˜¯æ— æ³•æ‹¥æŠ±éœ€æ±‚å˜åŒ–ï¼Œæ•´å¥—æµç¨‹ç»“æŸåŽæ‰èƒ½çœ‹åˆ°äº§å“ï¼Œå›¢é˜Ÿå£«æ°”ä½Žè½ã€‚&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;æ•æ·å¼€å‘ï¼ˆScrumï¼‰- äº§å“æ‰€æœ‰è€…ã€Scrum Masterã€ç ”å‘äººå‘˜ - Sprint&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;äº§å“çš„Backlogï¼ˆç”¨æˆ·æ•…äº‹ã€äº§å“åŽŸåž‹ï¼‰ã€‚&lt;/li&gt; 
     &lt;li&gt;è®¡åˆ’ä¼šè®®ï¼ˆè¯„ä¼°å’Œé¢„ç®—ï¼‰ã€‚&lt;/li&gt; 
     &lt;li&gt;æ—¥å¸¸å¼€å‘ï¼ˆç«™ç«‹ä¼šè®®ã€ç•ªèŒ„å·¥ä½œæ³•ã€ç»“å¯¹ç¼–ç¨‹ã€æµ‹è¯•å…ˆè¡Œã€ä»£ç é‡æž„â€¦â€¦ï¼‰ã€‚&lt;/li&gt; 
     &lt;li&gt;ä¿®å¤bugï¼ˆé—®é¢˜æè¿°ã€é‡çŽ°æ­¥éª¤ã€æµ‹è¯•äººå‘˜ã€è¢«æŒ‡æ´¾äººï¼‰ã€‚&lt;/li&gt; 
     &lt;li&gt;å‘å¸ƒç‰ˆæœ¬ã€‚&lt;/li&gt; 
     &lt;li&gt;è¯„å®¡ä¼šè®®ï¼ˆShowcaseï¼Œç”¨æˆ·éœ€è¦å‚ä¸Žï¼‰ã€‚&lt;/li&gt; 
     &lt;li&gt;å›žé¡¾ä¼šè®®ï¼ˆå¯¹å½“å‰è¿­ä»£å‘¨æœŸåšä¸€ä¸ªæ€»ç»“ï¼‰ã€‚&lt;/li&gt; 
    &lt;/ul&gt; 
    &lt;blockquote&gt; 
     &lt;p&gt;è¡¥å……ï¼šæ•æ·è½¯ä»¶å¼€å‘å®£è¨€&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;strong&gt;ä¸ªä½“å’Œäº’åŠ¨&lt;/strong&gt; é«˜äºŽ æµç¨‹å’Œå·¥å…·&lt;/li&gt; 
      &lt;li&gt;&lt;strong&gt;å·¥ä½œçš„è½¯ä»¶&lt;/strong&gt; é«˜äºŽ è¯¦å°½çš„æ–‡æ¡£&lt;/li&gt; 
      &lt;li&gt;&lt;strong&gt;å®¢æˆ·åˆä½œ&lt;/strong&gt; é«˜äºŽ åˆåŒè°ˆåˆ¤&lt;/li&gt; 
      &lt;li&gt;&lt;strong&gt;å“åº”å˜åŒ–&lt;/strong&gt; é«˜äºŽ éµå¾ªè®¡åˆ’&lt;/li&gt; 
     &lt;/ul&gt; 
    &lt;/blockquote&gt; &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/res/agile-scrum-sprint-cycle.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt; 
    &lt;blockquote&gt; 
     &lt;p&gt;è§’è‰²ï¼šäº§å“æ‰€æœ‰è€…ï¼ˆå†³å®šåšä»€ä¹ˆï¼Œèƒ½å¯¹éœ€æ±‚æ‹æ¿çš„äººï¼‰ã€å›¢é˜Ÿè´Ÿè´£äººï¼ˆè§£å†³å„ç§é—®é¢˜ï¼Œä¸“æ³¨å¦‚ä½•æ›´å¥½çš„å·¥ä½œï¼Œå±è”½å¤–éƒ¨å¯¹å¼€å‘å›¢é˜Ÿçš„å½±å“ï¼‰ã€å¼€å‘å›¢é˜Ÿï¼ˆé¡¹ç›®æ‰§è¡Œäººå‘˜ï¼Œå…·ä½“æŒ‡å¼€å‘äººå‘˜å’Œæµ‹è¯•äººå‘˜ï¼‰ã€‚&lt;/p&gt; 
    &lt;/blockquote&gt; 
    &lt;blockquote&gt; 
     &lt;p&gt;å‡†å¤‡å·¥ä½œï¼šå•†ä¸šæ¡ˆä¾‹å’Œèµ„é‡‘ã€åˆåŒã€æ†§æ†¬ã€åˆå§‹äº§å“éœ€æ±‚ã€åˆå§‹å‘å¸ƒè®¡åˆ’ã€å…¥è‚¡ã€ç»„å»ºå›¢é˜Ÿã€‚&lt;/p&gt; 
    &lt;/blockquote&gt; 
    &lt;blockquote&gt; 
     &lt;p&gt;æ•æ·å›¢é˜Ÿé€šå¸¸äººæ•°ä¸º8-10äººã€‚&lt;/p&gt; 
    &lt;/blockquote&gt; 
    &lt;blockquote&gt; 
     &lt;p&gt;å·¥ä½œé‡ä¼°ç®—ï¼šå°†å¼€å‘ä»»åŠ¡é‡åŒ–ï¼ŒåŒ…æ‹¬åŽŸåž‹ã€Logoè®¾è®¡ã€UIè®¾è®¡ã€å‰ç«¯å¼€å‘ç­‰ï¼Œå°½é‡æŠŠæ¯ä¸ªå·¥ä½œåˆ†è§£åˆ°æœ€å°ä»»åŠ¡é‡ï¼Œæœ€å°ä»»åŠ¡é‡æ ‡å‡†ä¸ºå·¥ä½œæ—¶é—´ä¸èƒ½è¶…è¿‡ä¸¤å¤©ï¼Œç„¶åŽä¼°ç®—æ€»ä½“é¡¹ç›®æ—¶é—´ã€‚æŠŠæ¯ä¸ªä»»åŠ¡éƒ½è´´åœ¨çœ‹æ¿ä¸Šé¢ï¼Œçœ‹æ¿ä¸Šåˆ†ä¸‰éƒ¨åˆ†ï¼što doï¼ˆå¾…å®Œæˆï¼‰ã€in progressï¼ˆè¿›è¡Œä¸­ï¼‰å’Œdoneï¼ˆå·²å®Œæˆï¼‰ã€‚&lt;/p&gt; 
    &lt;/blockquote&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;é¡¹ç›®å›¢é˜Ÿç»„å»º&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;å›¢é˜Ÿçš„æž„æˆå’Œè§’è‰²&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/res/company_architecture.png&quot; alt=&quot;company_architecture&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;ç¼–ç¨‹è§„èŒƒå’Œä»£ç å®¡æŸ¥ï¼ˆ&lt;code&gt;flake8&lt;/code&gt;ã€&lt;code&gt;pylint&lt;/code&gt;ï¼‰&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/res/pylint.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Pythonä¸­çš„ä¸€äº›â€œæƒ¯ä¾‹â€ï¼ˆè¯·å‚è€ƒ&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/%E7%95%AA%E5%A4%96%E7%AF%87/Python%E7%BC%96%E7%A8%8B%E6%83%AF%E4%BE%8B.md&quot;&gt;ã€ŠPythonæƒ¯ä¾‹-å¦‚ä½•ç¼–å†™Pythonicçš„ä»£ç ã€‹&lt;/a&gt;ï¼‰&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;å½±å“ä»£ç å¯è¯»æ€§çš„åŽŸå› ï¼š&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;ä»£ç æ³¨é‡Šå¤ªå°‘æˆ–è€…æ²¡æœ‰æ³¨é‡Š&lt;/li&gt; 
     &lt;li&gt;ä»£ç ç ´åäº†è¯­è¨€çš„æœ€ä½³å®žè·µ&lt;/li&gt; 
     &lt;li&gt;åæ¨¡å¼ç¼–ç¨‹ï¼ˆæ„å¤§åˆ©é¢ä»£ç ã€å¤åˆ¶-é»è´´ç¼–ç¨‹ã€è‡ªè´Ÿç¼–ç¨‹ã€â€¦â€¦ï¼‰&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;å›¢é˜Ÿå¼€å‘å·¥å…·ä»‹ç»&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ç‰ˆæœ¬æŽ§åˆ¶ï¼šGitã€Mercury&lt;/li&gt; 
   &lt;li&gt;ç¼ºé™·ç®¡ç†ï¼š&lt;a href=&quot;https://about.gitlab.com/&quot;&gt;Gitlab&lt;/a&gt;ã€&lt;a href=&quot;http://www.redmine.org.cn/&quot;&gt;Redmine&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;æ•æ·é—­çŽ¯å·¥å…·ï¼š&lt;a href=&quot;https://www.zentao.net/&quot;&gt;ç¦…é“&lt;/a&gt;ã€&lt;a href=&quot;https://www.atlassian.com/software/jira/features&quot;&gt;JIRA&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;æŒç»­é›†æˆï¼š&lt;a href=&quot;https://jenkins.io/&quot;&gt;Jenkins&lt;/a&gt;ã€&lt;a href=&quot;https://travis-ci.org/&quot;&gt;Travis-CI&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;è¯·å‚è€ƒ&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/91.%E5%9B%A2%E9%98%9F%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.md&quot;&gt;ã€Šå›¢é˜Ÿé¡¹ç›®å¼€å‘çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆã€‹&lt;/a&gt;ã€‚&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h5&gt;é¡¹ç›®é€‰é¢˜å’Œç†è§£ä¸šåŠ¡&lt;/h5&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;é€‰é¢˜èŒƒå›´è®¾å®š&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;CMSï¼ˆç”¨æˆ·ç«¯ï¼‰ï¼šæ–°é—»èšåˆç½‘ç«™ã€é—®ç­”/åˆ†äº«ç¤¾åŒºã€å½±è¯„/ä¹¦è¯„ç½‘ç«™ç­‰ã€‚&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;MISï¼ˆç”¨æˆ·ç«¯+ç®¡ç†ç«¯ï¼‰ï¼šKMSã€KPIè€ƒæ ¸ç³»ç»Ÿã€HRSã€CRMç³»ç»Ÿã€ä¾›åº”é“¾ç³»ç»Ÿã€ä»“å‚¨ç®¡ç†ç³»ç»Ÿç­‰ã€‚&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;AppåŽå°ï¼ˆç®¡ç†ç«¯+æ•°æ®æŽ¥å£ï¼‰ï¼šäºŒæ‰‹äº¤æ˜“ç±»ã€æŠ¥åˆŠæ‚å¿—ç±»ã€å°ä¼—ç”µå•†ç±»ã€æ–°é—»èµ„è®¯ç±»ã€æ—…æ¸¸ç±»ã€ç¤¾äº¤ç±»ã€é˜…è¯»ç±»ç­‰ã€‚&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;å…¶ä»–ç±»åž‹ï¼šè‡ªèº«è¡Œä¸šèƒŒæ™¯å’Œå·¥ä½œç»éªŒã€ä¸šåŠ¡å®¹æ˜“ç†è§£å’ŒæŠŠæŽ§ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;éœ€æ±‚ç†è§£ã€æ¨¡å—åˆ’åˆ†å’Œä»»åŠ¡åˆ†é…&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;éœ€æ±‚ç†è§£ï¼šå¤´è„‘é£Žæš´å’Œç«žå“åˆ†æžã€‚&lt;/li&gt; 
   &lt;li&gt;æ¨¡å—åˆ’åˆ†ï¼šç”»æ€ç»´å¯¼å›¾ï¼ˆXMindï¼‰ï¼Œæ¯ä¸ªæ¨¡å—æ˜¯ä¸€ä¸ªæžèŠ‚ç‚¹ï¼Œæ¯ä¸ªå…·ä½“çš„åŠŸèƒ½æ˜¯ä¸€ä¸ªå¶èŠ‚ç‚¹ï¼ˆç”¨åŠ¨è¯è¡¨è¿°ï¼‰ï¼Œéœ€è¦ç¡®ä¿æ¯ä¸ªå¶èŠ‚ç‚¹æ— æ³•å†ç”Ÿå‡ºæ–°èŠ‚ç‚¹ï¼Œç¡®å®šæ¯ä¸ªå¶å­èŠ‚ç‚¹çš„é‡è¦æ€§ã€ä¼˜å…ˆçº§å’Œå·¥ä½œé‡ã€‚&lt;/li&gt; 
   &lt;li&gt;ä»»åŠ¡åˆ†é…ï¼šç”±é¡¹ç›®è´Ÿè´£äººæ ¹æ®ä¸Šé¢çš„æŒ‡æ ‡ä¸ºæ¯ä¸ªå›¢é˜Ÿæˆå‘˜åˆ†é…ä»»åŠ¡ã€‚&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/res/requirements_by_xmind.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;åˆ¶å®šé¡¹ç›®è¿›åº¦è¡¨ï¼ˆæ¯æ—¥æ›´æ–°ï¼‰&lt;/p&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th&gt;æ¨¡å—&lt;/th&gt; 
     &lt;th&gt;åŠŸèƒ½&lt;/th&gt; 
     &lt;th&gt;äººå‘˜&lt;/th&gt; 
     &lt;th&gt;çŠ¶æ€&lt;/th&gt; 
     &lt;th&gt;å®Œæˆ&lt;/th&gt; 
     &lt;th&gt;å·¥æ—¶&lt;/th&gt; 
     &lt;th&gt;è®¡åˆ’å¼€å§‹&lt;/th&gt; 
     &lt;th&gt;å®žé™…å¼€å§‹&lt;/th&gt; 
     &lt;th&gt;è®¡åˆ’ç»“æŸ&lt;/th&gt; 
     &lt;th&gt;å®žé™…ç»“æŸ&lt;/th&gt; 
     &lt;th&gt;å¤‡æ³¨&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td&gt;è¯„è®º&lt;/td&gt; 
     &lt;td&gt;æ·»åŠ è¯„è®º&lt;/td&gt; 
     &lt;td&gt;çŽ‹å¤§é”¤&lt;/td&gt; 
     &lt;td&gt;æ­£åœ¨è¿›è¡Œ&lt;/td&gt; 
     &lt;td&gt;50%&lt;/td&gt; 
     &lt;td&gt;4&lt;/td&gt; 
     &lt;td&gt;2018/8/7&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;2018/8/7&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;åˆ é™¤è¯„è®º&lt;/td&gt; 
     &lt;td&gt;çŽ‹å¤§é”¤&lt;/td&gt; 
     &lt;td&gt;ç­‰å¾…&lt;/td&gt; 
     &lt;td&gt;0%&lt;/td&gt; 
     &lt;td&gt;2&lt;/td&gt; 
     &lt;td&gt;2018/8/7&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;2018/8/7&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;æŸ¥çœ‹è¯„è®º&lt;/td&gt; 
     &lt;td&gt;ç™½å…ƒèŠ³&lt;/td&gt; 
     &lt;td&gt;æ­£åœ¨è¿›è¡Œ&lt;/td&gt; 
     &lt;td&gt;20%&lt;/td&gt; 
     &lt;td&gt;4&lt;/td&gt; 
     &lt;td&gt;2018/8/7&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;2018/8/7&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;éœ€è¦è¿›è¡Œä»£ç å®¡æŸ¥&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;è¯„è®ºæŠ•ç¥¨&lt;/td&gt; 
     &lt;td&gt;ç™½å…ƒèŠ³&lt;/td&gt; 
     &lt;td&gt;ç­‰å¾…&lt;/td&gt; 
     &lt;td&gt;0%&lt;/td&gt; 
     &lt;td&gt;4&lt;/td&gt; 
     &lt;td&gt;2018/8/8&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;2018/8/8&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;OOADå’Œæ•°æ®åº“è®¾è®¡&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;UMLï¼ˆç»Ÿä¸€å»ºæ¨¡è¯­è¨€ï¼‰çš„ç±»å›¾&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/res/uml-class-diagram.png&quot; alt=&quot;uml&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;é€šè¿‡æ¨¡åž‹åˆ›å»ºè¡¨ï¼ˆæ­£å‘å·¥ç¨‹ï¼‰ï¼Œä¾‹å¦‚åœ¨Djangoé¡¹ç›®ä¸­å¯ä»¥é€šè¿‡ä¸‹é¢çš„å‘½ä»¤åˆ›å»ºäºŒç»´è¡¨ã€‚&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-Shell&quot;&gt;python manage.py makemigrations app
python manage.py migrate
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ä½¿ç”¨PowerDesignerç»˜åˆ¶ç‰©ç†æ¨¡åž‹å›¾ã€‚&lt;/p&gt; &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/res/power-designer-pdm.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;é€šè¿‡æ•°æ®è¡¨åˆ›å»ºæ¨¡åž‹ï¼ˆåå‘å·¥ç¨‹ï¼‰ï¼Œä¾‹å¦‚åœ¨Djangoé¡¹ç›®ä¸­å¯ä»¥é€šè¿‡ä¸‹é¢çš„å‘½ä»¤ç”Ÿæˆæ¨¡åž‹ã€‚&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-Shell&quot;&gt;python manage.py inspectdb &amp;gt; app/models.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;ç¬¬92å¤©ï¼š&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/92.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%E8%AF%A6%E8%A7%A3.md&quot;&gt;Dockerå®¹å™¨æŠ€æœ¯è¯¦è§£&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Dockerç®€ä»‹&lt;/li&gt; 
 &lt;li&gt;å®‰è£…Docker&lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨Dockeråˆ›å»ºå®¹å™¨ï¼ˆNginxã€MySQLã€Redisã€Gitlabã€Jenkinsï¼‰&lt;/li&gt; 
 &lt;li&gt;æž„å»ºDockeré•œåƒï¼ˆDockerfileçš„ç¼–å†™å’Œç›¸å…³æŒ‡ä»¤ï¼‰&lt;/li&gt; 
 &lt;li&gt;å®¹å™¨ç¼–æŽ’ï¼ˆDocker-composeï¼‰&lt;/li&gt; 
 &lt;li&gt;é›†ç¾¤ç®¡ç†ï¼ˆKubernetesï¼‰&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;ç¬¬93å¤©ï¼š&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/93.MySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96.md&quot;&gt;MySQLæ€§èƒ½ä¼˜åŒ–&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;åŸºæœ¬åŽŸåˆ™&lt;/li&gt; 
 &lt;li&gt;InnoDBå¼•æ“Ž&lt;/li&gt; 
 &lt;li&gt;ç´¢å¼•çš„ä½¿ç”¨å’Œæ³¨æ„äº‹é¡¹&lt;/li&gt; 
 &lt;li&gt;æ•°æ®åˆ†åŒº&lt;/li&gt; 
 &lt;li&gt;SQLä¼˜åŒ–&lt;/li&gt; 
 &lt;li&gt;é…ç½®ä¼˜åŒ–&lt;/li&gt; 
 &lt;li&gt;æž¶æž„ä¼˜åŒ–&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;ç¬¬94å¤©ï¼š&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/94.%E7%BD%91%E7%BB%9CAPI%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1.md&quot;&gt;ç½‘ç»œAPIæŽ¥å£è®¾è®¡&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;è®¾è®¡åŽŸåˆ™ 
  &lt;ul&gt; 
   &lt;li&gt;å…³é”®é—®é¢˜&lt;/li&gt; 
   &lt;li&gt;å…¶ä»–é—®é¢˜&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;æ–‡æ¡£æ’°å†™&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;ç¬¬95å¤©ï¼š[ä½¿ç”¨Djangoå¼€å‘å•†ä¸šé¡¹ç›®](./Day91-100/95.ä½¿ç”¨Djangoå¼€å‘å•†ä¸šé¡¹ ç›®.md)&lt;/h4&gt; 
&lt;h5&gt;é¡¹ç›®å¼€å‘ä¸­çš„å…¬å…±é—®é¢˜&lt;/h5&gt; 
&lt;ol&gt; 
 &lt;li&gt;æ•°æ®åº“çš„é…ç½®ï¼ˆå¤šæ•°æ®åº“ã€ä¸»ä»Žå¤åˆ¶ã€æ•°æ®åº“è·¯ç”±ï¼‰&lt;/li&gt; 
 &lt;li&gt;ç¼“å­˜çš„é…ç½®ï¼ˆåˆ†åŒºç¼“å­˜ã€é”®è®¾ç½®ã€è¶…æ—¶è®¾ç½®ã€ä¸»ä»Žå¤åˆ¶ã€æ•…éšœæ¢å¤ï¼ˆå“¨å…µï¼‰ï¼‰&lt;/li&gt; 
 &lt;li&gt;æ—¥å¿—çš„é…ç½®&lt;/li&gt; 
 &lt;li&gt;åˆ†æžå’Œè°ƒè¯•ï¼ˆDjango-Debug-ToolBarï¼‰&lt;/li&gt; 
 &lt;li&gt;å¥½ç”¨çš„Pythonæ¨¡å—ï¼ˆæ—¥æœŸè®¡ç®—ã€å›¾åƒå¤„ç†ã€æ•°æ®åŠ å¯†ã€ä¸‰æ–¹APIï¼‰&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h5&gt;REST APIè®¾è®¡&lt;/h5&gt; 
&lt;ol&gt; 
 &lt;li&gt;RESTfulæž¶æž„ 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;http://www.ruanyifeng.com/blog/2011/09/restful.html&quot;&gt;ç†è§£RESTfulæž¶æž„&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://www.ruanyifeng.com/blog/2014/05/restful_api.html&quot;&gt;RESTful APIè®¾è®¡æŒ‡å—&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://www.ruanyifeng.com/blog/2018/10/restful-api-best-practices.html&quot;&gt;RESTful APIæœ€ä½³å®žè·µ&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;APIæŽ¥å£æ–‡æ¡£çš„æ’°å†™ 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;http://rap2.taobao.org/&quot;&gt;RAP2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://yapi.demo.qunar.com/&quot;&gt;YAPI&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.django-rest-framework.org/&quot;&gt;django-REST-framework&lt;/a&gt;çš„åº”ç”¨&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h5&gt;é¡¹ç›®ä¸­çš„é‡ç‚¹éš¾ç‚¹å‰–æž&lt;/h5&gt; 
&lt;ol&gt; 
 &lt;li&gt;ä½¿ç”¨ç¼“å­˜ç¼“è§£æ•°æ®åº“åŽ‹åŠ› - Redis&lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨æ¶ˆæ¯é˜Ÿåˆ—åšè§£è€¦åˆå’Œå‰Šå³° - Celery + RabbitMQ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;ç¬¬96å¤©ï¼š&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/96.%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E5%92%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95.md&quot;&gt;è½¯ä»¶æµ‹è¯•å’Œè‡ªåŠ¨åŒ–æµ‹è¯•&lt;/a&gt;&lt;/h4&gt; 
&lt;h5&gt;å•å…ƒæµ‹è¯•&lt;/h5&gt; 
&lt;ol&gt; 
 &lt;li&gt;æµ‹è¯•çš„ç§ç±»&lt;/li&gt; 
 &lt;li&gt;ç¼–å†™å•å…ƒæµ‹è¯•ï¼ˆ&lt;code&gt;unittest&lt;/code&gt;ã€&lt;code&gt;pytest&lt;/code&gt;ã€&lt;code&gt;nose2&lt;/code&gt;ã€&lt;code&gt;tox&lt;/code&gt;ã€&lt;code&gt;ddt&lt;/code&gt;ã€â€¦â€¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;æµ‹è¯•è¦†ç›–çŽ‡ï¼ˆ&lt;code&gt;coverage&lt;/code&gt;ï¼‰&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h5&gt;Djangoé¡¹ç›®éƒ¨ç½²&lt;/h5&gt; 
&lt;ol&gt; 
 &lt;li&gt;éƒ¨ç½²å‰çš„å‡†å¤‡å·¥ä½œ 
  &lt;ul&gt; 
   &lt;li&gt;å…³é”®è®¾ç½®ï¼ˆSECRET_KEY / DEBUG / ALLOWED_HOSTS / ç¼“å­˜ / æ•°æ®åº“ï¼‰&lt;/li&gt; 
   &lt;li&gt;HTTPS / CSRF_COOKIE_SECUR / SESSION_COOKIE_SECURE&lt;/li&gt; 
   &lt;li&gt;æ—¥å¿—ç›¸å…³é…ç½®&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Linuxå¸¸ç”¨å‘½ä»¤å›žé¡¾&lt;/li&gt; 
 &lt;li&gt;Linuxå¸¸ç”¨æœåŠ¡çš„å®‰è£…å’Œé…ç½®&lt;/li&gt; 
 &lt;li&gt;uWSGI/Gunicornå’ŒNginxçš„ä½¿ç”¨ 
  &lt;ul&gt; 
   &lt;li&gt;Gunicornå’ŒuWSGIçš„æ¯”è¾ƒ 
    &lt;ul&gt; 
     &lt;li&gt;å¯¹äºŽä¸éœ€è¦å¤§é‡å®šåˆ¶åŒ–çš„ç®€å•åº”ç”¨ç¨‹åºï¼ŒGunicornæ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼ŒuWSGIçš„å­¦ä¹ æ›²çº¿æ¯”Gunicornè¦é™¡å³­å¾—å¤šï¼ŒGunicornçš„é»˜è®¤å‚æ•°å°±å·²ç»èƒ½å¤Ÿé€‚åº”å¤§å¤šæ•°åº”ç”¨ç¨‹åºã€‚&lt;/li&gt; 
     &lt;li&gt;uWSGIæ”¯æŒå¼‚æž„éƒ¨ç½²ã€‚&lt;/li&gt; 
     &lt;li&gt;ç”±äºŽNginxæœ¬èº«æ”¯æŒuWSGIï¼Œåœ¨çº¿ä¸Šä¸€èˆ¬éƒ½å°†Nginxå’ŒuWSGIæ†ç»‘åœ¨ä¸€èµ·éƒ¨ç½²ï¼Œè€Œä¸”uWSGIå±žäºŽåŠŸèƒ½é½å…¨ä¸”é«˜åº¦å®šåˆ¶çš„WSGIä¸­é—´ä»¶ã€‚&lt;/li&gt; 
     &lt;li&gt;åœ¨æ€§èƒ½ä¸Šï¼ŒGunicornå’ŒuWSGIå…¶å®žè¡¨çŽ°ç›¸å½“ã€‚&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨è™šæ‹ŸåŒ–æŠ€æœ¯ï¼ˆDockerï¼‰éƒ¨ç½²æµ‹è¯•çŽ¯å¢ƒå’Œç”Ÿäº§çŽ¯å¢ƒ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h5&gt;æ€§èƒ½æµ‹è¯•&lt;/h5&gt; 
&lt;ol&gt; 
 &lt;li&gt;ABçš„ä½¿ç”¨&lt;/li&gt; 
 &lt;li&gt;SQLslapçš„ä½¿ç”¨&lt;/li&gt; 
 &lt;li&gt;sysbenchçš„ä½¿ç”¨&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h5&gt;è‡ªåŠ¨åŒ–æµ‹è¯•&lt;/h5&gt; 
&lt;ol&gt; 
 &lt;li&gt;ä½¿ç”¨Shellå’ŒPythonè¿›è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•&lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨Seleniumå®žçŽ°è‡ªåŠ¨åŒ–æµ‹è¯• 
  &lt;ul&gt; 
   &lt;li&gt;Selenium IDE&lt;/li&gt; 
   &lt;li&gt;Selenium WebDriver&lt;/li&gt; 
   &lt;li&gt;Selenium Remote Control&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;æµ‹è¯•å·¥å…·Robot Frameworkä»‹ç»&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;ç¬¬97å¤©ï¼š&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/97.%E7%94%B5%E5%95%86%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E8%A6%81%E7%82%B9%E5%89%96%E6%9E%90.md&quot;&gt;ç”µå•†ç½‘ç«™æŠ€æœ¯è¦ç‚¹å‰–æž&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;å•†ä¸šæ¨¡å¼å’Œéœ€æ±‚è¦ç‚¹&lt;/li&gt; 
 &lt;li&gt;ç‰©ç†æ¨¡åž‹è®¾è®¡&lt;/li&gt; 
 &lt;li&gt;ç¬¬ä¸‰æ–¹ç™»å½•&lt;/li&gt; 
 &lt;li&gt;ç¼“å­˜é¢„çƒ­å’ŒæŸ¥è¯¢ç¼“å­˜&lt;/li&gt; 
 &lt;li&gt;è´­ç‰©è½¦çš„å®žçŽ°&lt;/li&gt; 
 &lt;li&gt;æ”¯ä»˜åŠŸèƒ½é›†æˆ&lt;/li&gt; 
 &lt;li&gt;ç§’æ€å’Œè¶…å–é—®é¢˜&lt;/li&gt; 
 &lt;li&gt;é™æ€èµ„æºç®¡ç†&lt;/li&gt; 
 &lt;li&gt;å…¨æ–‡æ£€ç´¢æ–¹æ¡ˆ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;ç¬¬98å¤©ï¼š&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/98.%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E4%B8%8A%E7%BA%BF%E5%92%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98.md&quot;&gt;é¡¹ç›®éƒ¨ç½²ä¸Šçº¿å’Œæ€§èƒ½è°ƒä¼˜&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;MySQLæ•°æ®åº“è°ƒä¼˜&lt;/li&gt; 
 &lt;li&gt;WebæœåŠ¡å™¨æ€§èƒ½ä¼˜åŒ– 
  &lt;ul&gt; 
   &lt;li&gt;Nginxè´Ÿè½½å‡è¡¡é…ç½®&lt;/li&gt; 
   &lt;li&gt;Keepalivedå®žçŽ°é«˜å¯ç”¨&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;ä»£ç æ€§èƒ½è°ƒä¼˜ 
  &lt;ul&gt; 
   &lt;li&gt;å¤šçº¿ç¨‹&lt;/li&gt; 
   &lt;li&gt;å¼‚æ­¥åŒ–&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;é™æ€èµ„æºè®¿é—®ä¼˜åŒ– 
  &lt;ul&gt; 
   &lt;li&gt;äº‘å­˜å‚¨&lt;/li&gt; 
   &lt;li&gt;CDN&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;ç¬¬99å¤©ï¼š&lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/99.%E9%9D%A2%E8%AF%95%E4%B8%AD%E7%9A%84%E5%85%AC%E5%85%B1%E9%97%AE%E9%A2%98.md&quot;&gt;é¢è¯•ä¸­çš„å…¬å…±é—®é¢˜&lt;/a&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;è®¡ç®—æœºåŸºç¡€&lt;/li&gt; 
 &lt;li&gt;PythonåŸºç¡€&lt;/li&gt; 
 &lt;li&gt;Webæ¡†æž¶ç›¸å…³&lt;/li&gt; 
 &lt;li&gt;çˆ¬è™«ç›¸å…³é—®é¢˜&lt;/li&gt; 
 &lt;li&gt;æ•°æ®åˆ†æž&lt;/li&gt; 
 &lt;li&gt;é¡¹ç›®ç›¸å…³&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ç¬¬100å¤© - &lt;a href=&quot;https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/Day91-100/100.%E8%A1%A5%E5%85%85%E5%86%85%E5%AE%B9.md&quot;&gt;è¡¥å……å†…å®¹&lt;/a&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;é¢è¯•å®å…¸&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Python é¢è¯•å®å…¸&lt;/li&gt; 
   &lt;li&gt;SQL é¢è¯•å®å…¸ï¼ˆæ•°æ®åˆ†æžå¸ˆï¼‰&lt;/li&gt; 
   &lt;li&gt;å•†ä¸šåˆ†æžé¢è¯•å®å…¸&lt;/li&gt; 
   &lt;li&gt;æœºå™¨å­¦ä¹ é¢è¯•å®å…¸&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;æœºå™¨å­¦ä¹ æ•°å­¦åŸºç¡€&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;æ·±åº¦å­¦ä¹ &lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;è®¡ç®—æœºè§†è§‰&lt;/li&gt; 
   &lt;li&gt;å¤§è¯­è¨€æ¨¡åž‹&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>aws-samples/amazon-bedrock-workshop</title>
      <link>https://github.com/aws-samples/amazon-bedrock-workshop</link>
      <description>&lt;p&gt;This is a workshop designed for Amazon Bedrock a foundational model service.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Amazon Bedrock Workshop &lt;a href=&quot;https://github.com/dwyl/esta/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat&quot; alt=&quot;contributions welcome&quot;&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;This hands-on workshop, aimed at developers and solution builders, introduces how to leverage foundation models (FMs) through &lt;a href=&quot;https://aws.amazon.com/bedrock/&quot;&gt;Amazon Bedrock&lt;/a&gt;. This code goes alongside the self-paced or instructor lead workshop here - &lt;a href=&quot;https://catalog.us-east-1.prod.workshops.aws/amazon-bedrock/en-US&quot;&gt;https://catalog.us-east-1.prod.workshops.aws/amazon-bedrock/en-US&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Please follow the prerequisites listed in the link above or ask your AWS workshop instructor how to get started.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Amazon Bedrock is a fully managed service that provides access to FMs from third-party providers and Amazon; available via an API. With Bedrock, you can choose from a variety of models to find the one thatâ€™s best suited for your use case.&lt;/p&gt; 
&lt;p&gt;Within this series of labs, you&#39;ll explore some of the most common usage patterns we are seeing with our customers for Generative AI. We will show techniques for generating text and images, creating value for organizations by improving productivity. This is achieved by leveraging foundation models to help in composing emails, summarizing text, answering questions, building chatbots, and creating images. While the focus of this workshop is for you to gain hands-on experience implementing these patterns via Bedrock APIs and SDKs, you will also have the option of exploring integrations with open-source packages like &lt;a href=&quot;https://python.langchain.com/docs/get_started/introduction&quot;&gt;LangChain&lt;/a&gt; and &lt;a href=&quot;https://faiss.ai/index.html&quot;&gt;FAISS&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Labs include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;01 - Text Generation&lt;/strong&gt; [Estimated time to complete - 15 mins] &lt;a href=&quot;https://&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Test-pass-2ea44f&quot; alt=&quot;Test - pass&quot;&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Text, code generation with Bedrock&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;02 - Knowledge bases and RAG&lt;/strong&gt; [Estimated time to complete - 35 mins] &lt;a href=&quot;https://&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Test-pass-2ea44f&quot; alt=&quot;Test - pass&quot;&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Managed RAG retrieve and generate example&lt;/li&gt; 
   &lt;li&gt;Langchain RAG retrieve and generate example&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;03 - Model customization&lt;/strong&gt; [Estimated time to complete - 30 mins] &lt;a href=&quot;https://&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Test-pass-2ea44f&quot; alt=&quot;Test - pass&quot;&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fine tuning Titan lite, Llama2&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt; - &lt;em&gt;You must run this on your own AWS account, and this will not work on AWS Workshop Studio!&lt;/em&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;04 - Image and Multimodal&lt;/strong&gt; [Estimated time to complete - 30 mins] &lt;a href=&quot;https://&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Test-pass-2ea44f&quot; alt=&quot;Test - pass&quot;&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Bedrock Titan image generator&lt;/li&gt; 
   &lt;li&gt;Bedrock Stable Diffusion XL&lt;/li&gt; 
   &lt;li&gt;Bedrock Titan Multimodal embeddings&lt;/li&gt; 
   &lt;li&gt;Nova Reel and Canvas notebooks&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;05 - Agents&lt;/strong&gt; [Estimated time to complete - 30 mins] &lt;a href=&quot;https://&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Test-pass-2ea44f&quot; alt=&quot;Test - pass&quot;&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Customer service agent&lt;/li&gt; 
   &lt;li&gt;Insurance claims agent&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;06 - Open source examples (optional)&lt;/strong&gt; [Estimated time to complete - 30 mins] &lt;a href=&quot;https://&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Test-fail-red&quot; alt=&quot;Test - fail&quot;&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Langchain Text Generation examples&lt;/li&gt; 
   &lt;li&gt;Langchain KB RAG examples&lt;/li&gt; 
   &lt;li&gt;Langchain Chatbot examples&lt;/li&gt; 
   &lt;li&gt;NVIDIA NeMo Guardrails examples&lt;/li&gt; 
   &lt;li&gt;NodeJS Bedrock examples&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/main/imgs/11-overview.png&quot; alt=&quot;imgs/11-overview&quot; title=&quot;Overview of the different labs in the workshop&quot;&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;You can also refer to these &lt;a href=&quot;https://catalog.us-east-1.prod.workshops.aws/workshops/a4bdb007-5600-4368-81c5-ff5b4154f518/en-US&quot;&gt;Step-by-step guided instructions on the workshop website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;h3&gt;Choose a notebook environment&lt;/h3&gt; 
&lt;p&gt;This workshop is presented as a series of &lt;strong&gt;Python notebooks&lt;/strong&gt;, which you can run from the environment of your choice:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For a fully-managed environment with rich AI/ML features, we&#39;d recommend using &lt;a href=&quot;https://aws.amazon.com/sagemaker/studio/&quot;&gt;SageMaker Studio&lt;/a&gt;. To get started quickly, you can refer to the &lt;a href=&quot;https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-quick-start.html&quot;&gt;instructions for domain quick setup&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For a fully-managed but more basic experience, you could instead &lt;a href=&quot;https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html&quot;&gt;create a SageMaker Notebook Instance&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;If you prefer to use your existing (local or other) notebook environment, make sure it has &lt;a href=&quot;https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html&quot;&gt;credentials for calling AWS&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Enable AWS IAM permissions for Bedrock&lt;/h3&gt; 
&lt;p&gt;The AWS identity you assume from your notebook environment (which is the &lt;a href=&quot;https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html&quot;&gt;&lt;em&gt;Studio/notebook Execution Role&lt;/em&gt;&lt;/a&gt; from SageMaker, or could be a role or IAM User for self-managed notebooks), must have sufficient &lt;a href=&quot;https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html&quot;&gt;AWS IAM permissions&lt;/a&gt; to call the Amazon Bedrock service.&lt;/p&gt; 
&lt;p&gt;To grant Bedrock access to your identity, you can:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open the &lt;a href=&quot;https://us-east-1.console.aws.amazon.com/iam/home?#&quot;&gt;AWS IAM Console&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Find your &lt;a href=&quot;https://us-east-1.console.aws.amazon.com/iamv2/home?#/roles&quot;&gt;Role&lt;/a&gt; (if using SageMaker or otherwise assuming an IAM Role), or else &lt;a href=&quot;https://us-east-1.console.aws.amazon.com/iamv2/home?#/users&quot;&gt;User&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;em&gt;Add Permissions &amp;gt; Create Inline Policy&lt;/em&gt; to attach new inline permissions, open the &lt;em&gt;JSON&lt;/em&gt; editor and paste in the below example policy:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Sid&quot;: &quot;BedrockFullAccess&quot;,
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: [&quot;bedrock:*&quot;],
            &quot;Resource&quot;: &quot;*&quot;
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;âš ï¸ &lt;strong&gt;Note:&lt;/strong&gt; With Amazon SageMaker, your notebook execution role will typically be &lt;em&gt;separate&lt;/em&gt; from the user or role that you log in to the AWS Console with. If you&#39;d like to explore the AWS Console for Amazon Bedrock, you&#39;ll need to grant permissions to your Console user/role too. You can run the notebooks anywhere as long as you have access to the AWS Bedrock service and have appropriate credentials&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For more information on the fine-grained action and resource permissions in Bedrock, check out the Bedrock Developer Guide.&lt;/p&gt; 
&lt;h3&gt;Clone and use the notebooks&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;â„¹ï¸ &lt;strong&gt;Note:&lt;/strong&gt; In SageMaker Studio, you can open a &quot;System Terminal&quot; to run these commands by clicking &lt;em&gt;File &amp;gt; New &amp;gt; Terminal&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Once your notebook environment is set up, clone this workshop repository into it.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;sudo yum install -y unzip
git clone https://github.com/aws-samples/amazon-bedrock-workshop.git
cd amazon-bedrock-workshop
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href=&quot;http://hits.dwyl.com/aws-samples/amazon-bedrock-workshop&quot;&gt;&lt;img src=&quot;https://hits.dwyl.com/aws-samples/amazon-bedrock-workshop.svg?style=flat-square&amp;amp;show=unique&quot; alt=&quot;HitCount&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You&#39;re now ready to explore the lab notebooks! Start with &lt;a href=&quot;https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/main/00_Prerequisites/bedrock_basics.ipynb&quot;&gt;00_Prerequisites/bedrock_basics.ipynb&lt;/a&gt; for details on how to install the Bedrock SDKs, create a client, and start calling the APIs from Python. Here is the directory structure at a high level:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Directory structure:
â””â”€â”€ aws-samples-amazon-bedrock-workshop/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ CODE_OF_CONDUCT.md
    â”œâ”€â”€ CONTRIBUTING.md
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ RELEASE_NOTES.md
    â”œâ”€â”€ 00_Prerequisites/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ Getting_started_with_Converse_API.ipynb
    â”‚   â””â”€â”€ bedrock_basics.ipynb
    â”œâ”€â”€ 01_Text_generation/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ 01_text_and_code_generation_w_bedrock.ipynb
    â”‚   â”œâ”€â”€ emails/
    â”‚   â”‚   â”œâ”€â”€ 00_treasure_island.txt
    â”‚   â”‚   â””â”€â”€ 01_return.txt
    â”‚   â””â”€â”€ images/
    â”‚       â””â”€â”€ nova/
    â”œâ”€â”€ 02_KnowledgeBases_and_RAG/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ 0_create_ingest_documents_test_kb.ipynb
    â”‚   â”œâ”€â”€ 1_managed-rag-kb-retrieve-generate-api.ipynb
    â”‚   â”œâ”€â”€ 2_Langchain-rag-retrieve-api-mistral-and-claude-3-haiku.ipynb
    â”‚   â”œâ”€â”€ 3_Langchain-rag-retrieve-api-claude-3.ipynb
    â”‚   â”œâ”€â”€ 4_CLEAN_UP.ipynb
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”œâ”€â”€ utility.py
    â”‚   â””â”€â”€ images/
    â”œâ”€â”€ 03_Model_customization/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ 00_setup.ipynb
    â”‚   â”œâ”€â”€ 01_fine-tuning-titan-lite.ipynb
    â”‚   â”œâ”€â”€ 02_fine-tuning_llama2.ipynb
    â”‚   â”œâ”€â”€ 03_continued_pretraining_titan_text.ipynb
    â”‚   â””â”€â”€ 04_cleanup.ipynb
    â”œâ”€â”€ 04_Image_and_Multimodal/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ bedrock-titan-multimodal-embeddings.ipynb
    â”‚   â”œâ”€â”€ nova-canvas-notebook.ipynb
    â”‚   â”œâ”€â”€ nova-reel-notebook.ipynb
    â”‚   â”œâ”€â”€ AmazonNova/
    â”‚   â”‚   â”œâ”€â”€ NovaCanvas/
    â”‚   â”‚   â”‚   â”œâ”€â”€ 00-NovaCanvas-prerequisites.ipynb
    â”‚   â”‚   â”‚   â”œâ”€â”€ 01-text-to-image.ipynb
    â”‚   â”‚   â”‚   â”œâ”€â”€ 02-image-inpainting.ipynb
    â”‚   â”‚   â”‚   â”œâ”€â”€ 03-image-outpainting.ipynb
    â”‚   â”‚   â”‚   â”œâ”€â”€ 04-background-removal.ipynb
    â”‚   â”‚   â”‚   â”œâ”€â”€ 05-image-variation.ipynb
    â”‚   â”‚   â”‚   â”œâ”€â”€ 06-image-conditioning.ipynb
    â”‚   â”‚   â”‚   â”œâ”€â”€ 07-color-conditioning.ipynb
    â”‚   â”‚   â”‚   â”œâ”€â”€ utils.py
    â”‚   â”‚   â”‚   â””â”€â”€ images/
    â”‚   â”‚   â””â”€â”€ NovaReel/
    â”‚   â”‚       â”œâ”€â”€ 00-NovaReel-prerequisites.ipynb
    â”‚   â”‚       â”œâ”€â”€ 01-text-to-video.ipynb
    â”‚   â”‚       â”œâ”€â”€ 02-image-to-video.ipynb
    â”‚   â”‚       â”œâ”€â”€ video_gen_util.py
    â”‚   â”‚       â””â”€â”€ images/
    â”‚   â””â”€â”€ images/
    â”‚       â””â”€â”€ octank_color_palette.JPG
    â”œâ”€â”€ 05_Agents/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ 00_inline_agents.ipynb
    â”‚   â”œâ”€â”€ 01_create_agent.ipynb
    â”‚   â”œâ”€â”€ 02_associate_knowledge_base_to_agent.ipynb
    â”‚   â”œâ”€â”€ 03_invoke_agent.ipynb
    â”‚   â”œâ”€â”€ 04_clean_up_agent_resources.ipynb
    â”‚   â”œâ”€â”€ agent.py
    â”‚   â”œâ”€â”€ knowledge_base.py
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”œâ”€â”€ images/
    â”‚   â””â”€â”€ kb_documents/
    â”œâ”€â”€ 06_OpenSource_examples/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ advance-langgraph-multi-agent-setup.ipynb
    â”‚   â”œâ”€â”€ find-relevant-information-using-RAG.ipynb
    â”‚   â”œâ”€â”€ intermediate-langgraph-agent-setup-w-tools.ipynb
    â”‚   â”œâ”€â”€ ragas-agent-evaluation.ipynb
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”œâ”€â”€ simple-crewai-agent-setup.ipynb
    â”‚   â”œâ”€â”€ simple-langgraph-agent-setup.ipynb
    â”‚   â”œâ”€â”€ utils.py
    â”‚   â”œâ”€â”€ data/
    â”‚   â”‚   â”œâ”€â”€ section_doc_store.pkl
    â”‚   â”‚   â”œâ”€â”€ section_vector_store.pkl
    â”‚   â”‚   â”œâ”€â”€ synthetic_travel_data.csv
    â”‚   â”‚   â””â”€â”€ travel_guides/
    â”‚   â”œâ”€â”€ images/
    â”‚   â””â”€â”€ text-generation-with-langchain/
    â”‚       â”œâ”€â”€ 01_zero_shot_generation.ipynb
    â”‚       â”œâ”€â”€ 02_code_interpret_w_langchain.ipynb
    â”‚       â”œâ”€â”€ 03_code_translate_w_langchain.ipynb
    â”‚       â”œâ”€â”€ 04_long text summarization using LCEL chains on Langchain.ipynb
    â”‚       â”œâ”€â”€ images/
    â”‚       â””â”€â”€ letters/
    â”‚           â””â”€â”€ 2022-letter.txt
    â”œâ”€â”€ 07_Cross_Region_Inference/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â””â”€â”€ Getting_started_with_Cross-region_Inference.ipynb
    â”œâ”€â”€ imgs/
    â””â”€â”€ .github/
        â””â”€â”€ ISSUE_TEMPLATE/
            â””â”€â”€ bug_report.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://star-history.com/#aws-samples/amazon-bedrock-workshop&amp;amp;Date&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=aws-samples/amazon-bedrock-workshop&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;ðŸ‘¥ Contributors&lt;/h1&gt; 
&lt;p&gt;Thanks to our awesome contributors! ðŸš€ðŸš€ðŸš€&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/aws-samples/amazon-bedrock-workshop/graphs/contributors&quot;&gt;&lt;img src=&quot;https://contrib.rocks/image?repo=aws-samples/amazon-bedrock-workshop&amp;amp;max=2000&quot; alt=&quot;contributors&quot;&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NirDiamant/GenAI_Agents</title>
      <link>https://github.com/NirDiamant/GenAI_Agents</link>
      <description>&lt;p&gt;This repository provides tutorials and implementations for various Generative AI Agent techniques, from basic to advanced. It serves as a comprehensive guide for building intelligent, interactive AI systems.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;http://makeapullrequest.com&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.linkedin.com/in/nir-diamant-759323134/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/LinkedIn-Connect-blue&quot; alt=&quot;LinkedIn&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/NirDiamantAI&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/NirDiamantAI?label=Follow%20@NirDiamantAI&amp;amp;style=social&quot; alt=&quot;Twitter&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/cA6Aa4uyDX&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20our%20community-7289da?style=flat-square&amp;amp;logo=discord&amp;amp;logoColor=white&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ðŸŒŸ &lt;strong&gt;Support This Project:&lt;/strong&gt; Your sponsorship fuels innovation in GenAI agent development. &lt;strong&gt;&lt;a href=&quot;https://github.com/sponsors/NirDiamant&quot;&gt;Become a sponsor&lt;/a&gt;&lt;/strong&gt; to help maintain and expand this valuable resource!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;GenAI Agents: Comprehensive Repository for Development and Implementation ðŸš€&lt;/h1&gt; 
&lt;p&gt;Welcome to one of the most extensive and dynamic collections of Generative AI (GenAI) agent tutorials and implementations available today. This repository serves as a comprehensive resource for learning, building, and sharing GenAI agents, ranging from simple conversational bots to complex, multi-agent systems.&lt;/p&gt; 
&lt;h2&gt;ðŸ“« Stay Updated!&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;ðŸš€&lt;br&gt;&lt;b&gt;Cutting-edge&lt;br&gt;Updates&lt;/b&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;ðŸ’¡&lt;br&gt;&lt;b&gt;Expert&lt;br&gt;Insights&lt;/b&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;ðŸŽ¯&lt;br&gt;&lt;b&gt;Top 0.1%&lt;br&gt;Content&lt;/b&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;p&gt;&lt;a href=&quot;https://diamantai.substack.com/?r=336pe4&amp;amp;utm_campaign=pub-share-checklist&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/NirDiamant/GenAI_Agents/main/images/subscribe-button.svg?sanitize=true&quot; alt=&quot;Subscribe to DiamantAI Newsletter&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Join over 15,000 of AI enthusiasts getting unique cutting-edge insights and free tutorials!&lt;/em&gt; &lt;em&gt;&lt;strong&gt;Plus, subscribers get exclusive early access and special 33% discounts to my book and the upcoming RAG Techniques course!&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href=&quot;https://diamantai.substack.com/?r=336pe4&amp;amp;utm_campaign=pub-share-checklist&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/NirDiamant/GenAI_Agents/main/images/substack_image.png&quot; alt=&quot;DiamantAI&#39;s newsletter&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Generative AI agents are at the forefront of artificial intelligence, revolutionizing the way we interact with and leverage AI technologies. This repository is designed to guide you through the development journey, from basic agent implementations to advanced, cutting-edge systems.&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td&gt; &lt;h3&gt;ðŸ“š Learn to Build Your First AI Agent&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://diamantai.substack.com/p/your-first-ai-agent-simpler-than&quot;&gt;Your First AI Agent: Simpler Than You Think&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This detailed blog post complements the repository by providing a complete A-Z walkthrough with in-depth explanations of core concepts, step-by-step implementation, and the theory behind AI agents. It&#39;s designed to be incredibly simple to follow while covering everything you need to know to build your first working agent from scratch.&lt;/p&gt; &lt;p&gt;&lt;em&gt;ðŸ’¡ Plus: Subscribe to the newsletter for exclusive early access to tutorials and special discounts on upcoming courses and books!&lt;/em&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;Our goal is to provide a valuable resource for everyone - from beginners taking their first steps in AI to seasoned practitioners pushing the boundaries of what&#39;s possible. By offering a range of examples from foundational to complex, we aim to facilitate learning, experimentation, and innovation in the rapidly evolving field of GenAI agents.&lt;/p&gt; 
&lt;p&gt;Furthermore, this repository serves as a platform for showcasing innovative agent creations. Whether you&#39;ve developed a novel agent architecture or found an innovative application for existing techniques, we encourage you to share your work with the community.&lt;/p&gt; 
&lt;h2&gt;Related Projects&lt;/h2&gt; 
&lt;p&gt;ðŸ“š Dive into my &lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_Techniques&quot;&gt;comprehensive guide on RAG techniques&lt;/a&gt;&lt;/strong&gt; to learn about integrating external knowledge into AI systems, enhancing their capabilities with up-to-date and relevant information retrieval.&lt;/p&gt; 
&lt;p&gt;ðŸ–‹ï¸ Explore my &lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/Prompt_Engineering&quot;&gt;Prompt Engineering Techniques guide&lt;/a&gt;&lt;/strong&gt; for an extensive collection of prompting strategies, from fundamental concepts to advanced methods, improving your ability to communicate effectively with AI language models.&lt;/p&gt; 
&lt;h2&gt;A Community-Driven Knowledge Hub&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;This repository grows stronger with your contributions!&lt;/strong&gt; Join our vibrant Discord community â€” the central hub for shaping and advancing this project together ðŸ¤&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://discord.gg/cA6Aa4uyDX&quot;&gt;GenAI Agents Discord Community&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Whether you&#39;re a novice eager to learn or an expert ready to share your knowledge, your insights can shape the future of GenAI agents. Join us to propose ideas, get feedback, and collaborate on innovative implementations. For contribution guidelines, please refer to our &lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/strong&gt; file. Let&#39;s advance GenAI agent technology together!&lt;/p&gt; 
&lt;p&gt;ðŸ”— For discussions on GenAI, agents, or to explore knowledge-sharing opportunities, feel free to &lt;strong&gt;&lt;a href=&quot;https://www.linkedin.com/in/nir-diamant-759323134/&quot;&gt;connect on LinkedIn&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ðŸŽ“ Learn to build GenAI agents from beginner to advanced levels&lt;/li&gt; 
 &lt;li&gt;ðŸ§  Explore a wide range of agent architectures and applications&lt;/li&gt; 
 &lt;li&gt;ðŸ“š Step-by-step tutorials and comprehensive documentation&lt;/li&gt; 
 &lt;li&gt;ðŸ› ï¸ Practical, ready-to-use agent implementations&lt;/li&gt; 
 &lt;li&gt;ðŸŒŸ Regular updates with the latest advancements in GenAI&lt;/li&gt; 
 &lt;li&gt;ðŸ¤ Share your own agent creations with the community&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;GenAI Agent Implementations&lt;/h2&gt; 
&lt;p&gt;Explore our extensive list of GenAI agent implementations, sorted by categories:&lt;/p&gt; 
&lt;h3&gt;ðŸŒ± Beginner-Friendly Agents&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Simple Conversational Agent&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/simple_conversational_agent.ipynb&quot;&gt;LangChain&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/simple_conversational_agent-pydanticai.ipynb&quot;&gt;PydanticAI&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A context-aware conversational AI maintains information across interactions, enabling more natural dialogues.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Integrates a language model, prompt template, and history manager to generate contextual responses and track conversation sessions.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/simple_question_answering_agent.ipynb&quot;&gt;Simple Question Answering Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Answering (QA) agent using LangChain and OpenAI&#39;s language model understands user queries and provides relevant, concise answers.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Combines OpenAI&#39;s GPT model, a prompt template, and an LLMChain to process user questions and generate AI-driven responses in a streamlined manner.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Simple Data Analysis Agent&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/simple_data_analysis_agent_notebook.ipynb&quot;&gt;LangChain&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/simple_data_analysis_agent_notebook-pydanticai.ipynb&quot;&gt;PydanticAI&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;An AI-powered data analysis agent interprets and answers questions about datasets using natural language, combining language models with data manipulation tools for intuitive data exploration.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Integrates a language model, data manipulation framework, and agent framework to process natural language queries and perform data analysis on a synthetic dataset, enabling accessible insights for non-technical users.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ðŸ”§ Framework Tutorial: LangGraph&lt;/h3&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/langgraph-tutorial.ipynb&quot;&gt;Introduction to LangGraph: Building Modular AI Workflows&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;This tutorial introduces LangGraph, a powerful framework for creating modular, graph-based AI workflows. Learn how to leverage LangGraph to build more complex and flexible AI agents that can handle multi-step processes efficiently.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Step-by-step guide on using LangGraph to create a StateGraph workflow. The tutorial covers key concepts such as state management, node creation, and graph compilation. It demonstrates these principles by constructing a simple text analysis pipeline, serving as a foundation for more advanced agent architectures.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/your-first-ai-agent-simpler-than?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false&quot;&gt;Blog Post&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ðŸŽ“ Educational and Research Agents&lt;/h3&gt; 
&lt;ol start=&quot;5&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/Academic_Task_Learning_Agent_LangGraph.ipynb&quot;&gt;ATLAS: Academic Task and Learning Agent System&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;ATLAS demonstrates how to build an intelligent multi-agent system that transforms academic support through AI-powered assistance. The system leverages LangGraph&#39;s workflow framework to coordinate multiple specialized agents that provide personalized academic planning, note-taking, and advisory support.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Implements a state-managed multi-agent architecture using four specialized agents (Coordinator, Planner, Notewriter, and Advisor) working in concert through LangGraph&#39;s workflow framework. The system features sophisticated workflows for profile analysis and academic support, with continuous adaptation based on student performance and feedback.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=yxowMLL2dDI&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/atlas-when-artificial-intelligence?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false&quot;&gt;Blog Post&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/scientific_paper_agent_langgraph.ipynb&quot;&gt;Scientific Paper Agent - Literature Review&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;An intelligent research assistant that helps users navigate, understand, and analyze scientific literature through an orchestrated workflow. The system combines academic APIs with sophisticated paper processing techniques to automate literature review tasks, enabling researchers to efficiently extract insights from academic papers while maintaining research rigor and quality control.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Leverages LangGraph to create a five-node workflow system including decision making, planning, tool execution, and quality validation nodes. The system integrates the CORE API for paper access, PDFplumber for document processing, and advanced language models for analysis. Key features include a retry mechanism for robust paper downloads, structured data handling through Pydantic models, and quality-focused improvement cycles with human-in-the-loop validation options.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://youtu.be/Bc4YtpHY6Ws&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/nexus-ai-the-revolutionary-research?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false&quot;&gt;Blog Post&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/chiron_learning_agent_langgraph.ipynb&quot;&gt;Chiron - A Feynman-Enhanced Learning Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;An adaptive learning agent that guides users through educational content using a structured checkpoint system and Feynman-style teaching. The system processes learning materials (either user-provided or web-retrieved), verifies understanding through interactive checkpoints, and provides simplified explanations when needed, creating a personalized learning experience that mimics one-on-one tutoring.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Uses LangGraph to orchestrate a learning workflow that includes checkpoint definition, context building, understanding verification, and Feynman teaching nodes. The system integrates web search for dynamic content retrieval, employs semantic chunking for context processing, and manages embeddings for relevant information retrieval. Key features include a 70% understanding threshold for progression, interactive human-in-the-loop validation, and structured output through Pydantic models for consistent data handling.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=qsdiTGkB8mk&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ðŸ’¼ Business and Professional Agents&lt;/h3&gt; 
&lt;ol start=&quot;8&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/customer_support_agent_langgraph.ipynb&quot;&gt;Customer Support Agent (LangGraph)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;An intelligent customer support agent using LangGraph categorizes queries, analyzes sentiment, and provides appropriate responses or escalates issues.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Utilizes LangGraph to create a workflow combining state management, query categorization, sentiment analysis, and response generation.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/essay_grading_system_langgraph.ipynb&quot;&gt;Essay Grading Agent (LangGraph)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;An automated essay grading system using LangGraph and an LLM model evaluates essays based on relevance, grammar, structure, and depth of analysis.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Utilizes a state graph to define the grading workflow, incorporating separate grading functions for each criterion.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/simple_travel_planner_langgraph.ipynb&quot;&gt;Travel Planning Agent (LangGraph)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A Travel Planner using LangGraph demonstrates how to build a stateful, multi-step conversational AI application that collects user input and generates personalized travel itineraries.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Utilizes StateGraph to define the application flow, incorporates custom PlannerState for process management.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/agent_hackathon_genAI_career_assistant.ipynb&quot;&gt;GenAI Career Assistant Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;The GenAI Career Assistant demonstrates how to create a multi-agent system that provides personalized guidance for careers in Generative AI. Using LangGraph and Gemini LLM, the system delivers customized learning paths, resume assistance, interview preparation, and job search support.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Leverages a multi-agent architecture using LangGraph to coordinate specialized agents (Learning, Resume, Interview, Job Search) through TypedDict-based state management. The system employs sophisticated query categorization and routing while integrating with external tools like DuckDuckGo for job searches and dynamic content generation.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=IcKh0ltXO_8&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/project_manager_assistant_agent.ipynb&quot;&gt;Project Manager Assistant Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;An AI agent designed to assist in project management tasks by automating the process of creating actionable tasks from project descriptions, identifying dependencies, scheduling work, and assigning tasks to team members based on expertise. The system includes risk assessment and self-reflection capabilities to optimize project plans through multiple iterations, aiming to minimize overall project risk.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Leverages LangGraph to orchestrate a workflow of specialized nodes including task generation, dependency mapping, scheduling, allocation, and risk assessment. Each node uses GPT-4o-mini for structured outputs following Pydantic models. The system implements a feedback loop for self-improvement, where risk scores trigger reflection cycles that generate insights to optimize the project plan. Visualization tools display Gantt charts of the generated schedules across iterations.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=R7YWjzg3LpI&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/ClauseAI.ipynb&quot;&gt;Contract Analysis Assistant (ClauseAI)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;ClauseAI demonstrates how to build an AI-powered contract analysis system using a multi-agent approach. The system employs specialized AI agents for different aspects of contract review, from clause analysis to compliance checking, and leverages LangGraph for workflow orchestration and Pinecone for efficient clause retrieval and comparison.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Implements a sophisticated state-based workflow using LangGraph to coordinate multiple AI agents through contract analysis stages. The system features Pydantic models for data validation, vector storage with Pinecone for clause comparison, and LLM-based analysis for generating comprehensive contract reports. The implementation includes parallel processing capabilities and customizable report generation based on user requirements.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=rP8uv_tXuSI&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/e2e_testing_agent.ipynb&quot;&gt;E2E Testing Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;The E2E Testing Agent demonstrates how to build an AI-powered system that converts natural language test instructions into executable end-to-end web tests. Using LangGraph for workflow orchestration and Playwright for browser automation, the system enables users to specify test cases in plain English while handling the complexity of test generation and execution.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Implements a structured workflow using LangGraph to coordinate test generation, validation, and execution. The system features TypedDict state management, integration with Playwright for browser automation, and LLM-based code generation for converting natural language instructions into executable test scripts. The implementation includes DOM state analysis, error handling, and comprehensive test reporting.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jPXtpzcCtyA&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ðŸŽ¨ Creative and Content Generation Agents&lt;/h3&gt; 
&lt;ol start=&quot;15&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/gif_animation_generator_langgraph.ipynb&quot;&gt;GIF Animation Generator Agent (LangGraph)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A GIF animation generator that integrates LangGraph for workflow management, GPT-4 for text generation, and DALL-E for image creation, producing custom animations from user prompts.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Utilizes LangGraph to orchestrate a workflow that generates character descriptions, plots, and image prompts using GPT-4, creates images with DALL-E 3, and assembles them into GIFs using PIL. Employs asynchronous programming for efficient parallel processing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/tts_poem_generator_agent_langgraph.ipynb&quot;&gt;TTS Poem Generator Agent (LangGraph)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;An advanced text-to-speech (TTS) agent using LangGraph and OpenAI&#39;s APIs classifies input text, processes it based on content type, and generates corresponding speech output.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Utilizes LangGraph to orchestrate a workflow that classifies input text using GPT models, applies content-specific processing, and converts the processed text to speech using OpenAI&#39;s TTS API. The system adapts its output based on the identified content type (general, poem, news, or joke).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/music_compositor_agent_langgraph.ipynb&quot;&gt;Music Compositor Agent (LangGraph)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;An AI Music Compositor using LangGraph and OpenAI&#39;s language models generates custom musical compositions based on user input. The system processes the input through specialized components, each contributing to the final musical piece, which is then converted to a playable MIDI file.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;LangGraph orchestrates a workflow that transforms user input into a musical composition, using ChatOpenAI (GPT-4) to generate melody, harmony, and rhythm, which are then style-adapted. The final AI-generated composition is converted to a MIDI file using music21 and can be played back using pygame.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/ContentIntelligence.ipynb&quot;&gt;Content Intelligence: Multi-Platform Content Generation Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Content Intelligence demonstrates how to build an advanced content generation system that transforms input text into platform-optimized content across multiple social media channels. The system employs LangGraph for workflow orchestration to analyze content, conduct research, and generate tailored content while maintaining brand consistency across different platforms.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Implements a sophisticated workflow using LangGraph to coordinate multiple specialized nodes (Summary, Research, Platform-Specific) through the content generation process. The system features TypedDict and Pydantic models for state management, integration with Tavily Search for research enhancement, and platform-specific content generation using GPT-4. The implementation includes parallel processing for multiple platforms and customizable content templates.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=DPMtPbKmWnU&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/business_meme_generator.ipynb&quot;&gt;Business Meme Generator Using LangGraph and Memegen.link&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;The Business Meme Generator demonstrates how to create an AI-powered system that generates contextually relevant memes based on company website analysis. Using LangGraph for workflow orchestration, the system combines Groq&#39;s Llama model for text analysis and the Memegen.link API to automatically produce brand-aligned memes for digital marketing.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Implements a state-managed workflow using LangGraph to coordinate website content analysis, meme concept generation, and image creation. The system features Pydantic models for data validation, asynchronous processing with aiohttp, and integration with external APIs (Groq, Memegen.link) to create a complete meme generation pipeline with customizable templates.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://youtu.be/lsdDaGmkSCw?si=oF3CGfhbRqz1_Vm8&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/murder_mystery_agent_langgraph.ipynb&quot;&gt;Murder Mystery Game with LLM Agents&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A text-based detective game that utilizes autonomous LLM agents as interactive characters in a procedurally generated murder mystery. Drawing inspiration from the UNBOUNDED paper, the system creates unique scenarios each time, with players taking on the role of Sherlock Holmes to solve the case through character interviews and deductive reasoning.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Leverages two LangGraph workflows - a main game loop for story/character generation and game progression, and a conversation sub-graph for character interactions. The system uses a combination of LLM-powered narrative generation, character AI, and structured game mechanics to create an immersive investigative experience with replayable storylines.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=_3cJYlk2EmA&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ðŸ“Š Analysis and Information Processing Agents&lt;/h3&gt; 
&lt;ol start=&quot;21&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/memory_enhanced_conversational_agent.ipynb&quot;&gt;Memory-Enhanced Conversational Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A memory-enhanced conversational AI agent incorporates short-term and long-term memory systems to maintain context within conversations and across multiple sessions, improving interaction quality and personalization.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Integrates a language model with separate short-term and long-term memory stores, utilizes a prompt template incorporating both memory types, and employs a memory manager for storage and retrieval. The system includes an interaction loop that updates and utilizes memories for each response.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/multi_agent_collaboration_system.ipynb&quot;&gt;Multi-Agent Collaboration System&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A multi-agent collaboration system combining historical research with data analysis, leveraging large language models to simulate specialized agents working together to answer complex historical questions.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Utilizes a base Agent class to create specialized HistoryResearchAgent and DataAnalysisAgent, orchestrated by a HistoryDataCollaborationSystem. The system follows a five-step process: historical context provision, data needs identification, historical data provision, data analysis, and final synthesis.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/self_improving_agent.ipynb&quot;&gt;Self-Improving Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A Self-Improving Agent using LangChain engages in conversations, learns from interactions, and continuously improves its performance over time through reflection and adaptation.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Integrates a language model with chat history management, response generation, and a reflection mechanism. The system employs a learning system that incorporates insights from reflection to enhance future performance, creating a continuous improvement loop.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/task_oriented_agent.ipynb&quot;&gt;Task-Oriented Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A language model application using LangChain that summarizes text and translates the summary to Spanish, combining custom functions, structured tools, and an agent for efficient text processing.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Utilizes custom functions for summarization and translation, wrapped as structured tools. Employs a prompt template to guide the agent, which orchestrates the use of tools. An agent executor manages the process, taking input text and producing both an English summary and its Spanish translation.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/search_the_internet_and_summarize.ipynb&quot;&gt;Internet Search and Summarize Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;An intelligent web research assistant that combines web search capabilities with AI-powered summarization, automating the process of gathering information from the internet and distilling it into concise, relevant summaries.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Integrates a web search module using DuckDuckGo&#39;s API, a result parser, and a text summarization engine leveraging OpenAI&#39;s language models. The system performs site-specific or general searches, extracts relevant content, generates concise summaries, and compiles attributed results for efficient information retrieval and synthesis.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/research_team_autogen.ipynb&quot;&gt;Multi agent research team - Autogen&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;This technique explores a multi-agent system for collaborative research using the AutoGen library. It employs agents to solve tasks collaboratively, focusing on efficient execution and quality assurance. The system enhances research by distributing tasks among specialized agents.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Agents are configured with specific roles using the GPT-4 model, including admin, developer, planner, executor, and quality assurance. Interaction management ensures orderly communication with defined transitions. Task execution involves collaborative planning, coding, execution, and quality checking, demonstrating a scalable framework for various domains.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/yanivvak/dream-team&quot;&gt;comprehensive solution with UI&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/build-your-dream-team-with-autogen/ba-p/4157961&quot;&gt;Blogpost&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/sales_call_analyzer_agent.ipynb&quot;&gt;Sales Call Analyzer&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;An intelligent system that automates the analysis of sales call recordings by combining audio transcription with advanced natural language processing. The analyzer transcribes audio using OpenAI&#39;s Whisper, processes the text using NLP techniques, and generates comprehensive reports including sentiment analysis, key phrases, pain points, and actionable recommendations to improve sales performance.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Utilizes multiple components in a structured workflow: OpenAI Whisper for audio transcription, CrewAI for task automation and agent management, and LangChain for orchestrating the analysis pipeline. The system processes audio through a series of steps from transcription to detailed analysis, leveraging custom agents and tasks to generate structured JSON reports containing insights about customer sentiment, sales opportunities, and recommended improvements.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=SKAt_PvznDw&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/Weather_Disaster_Management_AI_AGENT.ipynb&quot;&gt;Weather Emergency &amp;amp; Response System&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A comprehensive system demonstrating two agent graph implementations for weather emergency response: a real-time graph processing live weather data, and a hybrid graph combining real and simulated data for testing high-severity scenarios. The system handles complete workflow from data gathering through emergency plan generation, with automated notifications and human verification steps.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Utilizes LangGraph for orchestrating complex workflows with state management, integrating OpenWeatherMap API for real-time data, and Gemini for analysis and response generation. The system incorporates email notifications, social media monitoring simulation, and severity-based routing with configurable human verification for low/medium severity events.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=AgiOAJl_apw&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/self_healing_code.ipynb&quot;&gt;Self-Healing Codebase System&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;An intelligent system that automatically detects, diagnoses, and fixes runtime code errors using LangGraph workflow orchestration and ChromaDB vector storage. The system maintains a memory of encountered bugs and their fixes through vector embeddings, enabling pattern recognition for similar errors across the codebase.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Utilizes a state-based graph workflow that processes function definitions and runtime arguments through specialized nodes for error detection, code analysis, and fix generation. Incorporates ChromaDB for vector-based storage of bug patterns and fixes, with automated search and retrieval capabilities for similar error patterns, while maintaining code execution safety through structured validation steps.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ga7ShvIXOvE&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/database_discovery_fleet.ipynb&quot;&gt;DataScribe: AI-Powered Schema Explorer&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;An intelligent agent system that enables intuitive exploration and querying of relational databases through natural language interactions. The system utilizes a fleet of specialized agents, coordinated by a stateful Supervisor, to handle schema discovery, query planning, and data analysis tasks while maintaining contextual understanding through vector-based relationship graphs.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Leverages LangGraph for orchestrating a multi-agent workflow including discovery, inference, and planning agents, with NetworkX for relationship graph visualization and management. The system incorporates dynamic state management through TypedDict classes, maintains database context between sessions using a db_graph attribute, and includes safety measures to prevent unauthorized database modifications.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/memory-agent-tutorial.ipynb&quot;&gt;Memory-Enhanced Email Agent (LangGraph &amp;amp; LangMem)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;An intelligent email assistant that combines three types of memory (semantic, episodic, and procedural) to create a system that improves over time. The agent can triage incoming emails, draft contextually appropriate responses using stored knowledge, and enhance its performance based on user feedback.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Leverages LangGraph for workflow orchestration and LangMem for sophisticated memory management across multiple memory types. The system implements a triage workflow with memory-enhanced decision making, specialized tools for email composition and calendar management, and a self-improvement mechanism that updates its own prompts based on feedback and past performance.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;**&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/building-an-ai-agent-with-memory?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false&quot;&gt;Blog Post&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ðŸ“° News and Information Agents&lt;/h3&gt; 
&lt;ol start=&quot;32&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/news_tldr_langgraph.ipynb&quot;&gt;News TL;DR using LangGraph&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A news summarization system that generates concise TL;DR summaries of current events based on user queries. The system leverages large language models for decision making and summarization while integrating with news APIs to access up-to-date content, allowing users to quickly catch up on topics of interest through generated bullet-point summaries.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Utilizes LangGraph to orchestrate a workflow combining multiple components: GPT-4o-mini for generating search terms and article summaries, NewsAPI for retrieving article metadata, BeautifulSoup for web scraping article content, and Asyncio for concurrent processing. The system follows a structured pipeline from query processing through article selection and summarization, managing the flow between components to produce relevant TL;DRs of current news articles.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=0fRxW6miybI&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/stop-reading-start-understanding?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false&quot;&gt;Blog Post&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/ainsight_langgraph.ipynb&quot;&gt;AInsight: AI/ML Weekly News Reporter&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;AInsight demonstrates how to build an intelligent news aggregation and summarization system using a multi-agent architecture. The system employs three specialized agents (NewsSearcher, Summarizer, Publisher) to automatically collect, process and summarize AI/ML news for general audiences through LangGraph-based workflow orchestration.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Implements a state-managed multi-agent system using LangGraph to coordinate the news collection (Tavily API), technical content summarization (GPT-4), and report generation processes. The system features modular architecture with TypedDict-based state management, external API integration, and markdown report generation with customizable templates.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=kH5S1is2D_0&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/journalism_focused_ai_assistant_langgraph.ipynb&quot;&gt;Journalism-Focused AI Assistant&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A specialized AI assistant that helps journalists tackle modern journalistic challenges like misinformation, bias, and information overload. The system integrates fact-checking, tone analysis, summarization, and grammar review tools to enhance the accuracy and efficiency of journalistic work while maintaining ethical reporting standards.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Leverages LangGraph to orchestrate a workflow of specialized components including language models for analysis and generation, web search integration via DuckDuckGo&#39;s API, document parsing tools like PyMuPDFLoader and WebBaseLoader, text splitting with RecursiveCharacterTextSplitter, and structured JSON outputs. Each component works together through a unified workflow to analyze content, verify facts, detect bias, extract quotes, and generate comprehensive reports.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/blog_writer_swarm.ipynb&quot;&gt;Blog Writer (Open AI Swarm)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A multi-agent system for collaborative blog post creation using OpenAI&#39;s Swarm package. It leverages specialized agents to perform research, planning, writing, and editing tasks efficiently.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Utilizes OpenAI&#39;s Swarm Package to manage agent interactions. Includes an admin, researcher, planner, writer, and editor, each with specific roles. The system follows a structured workflow: topic setting, outlining, research, drafting, and editing. This approach enhances content creation through task distribution, specialization, and collaborative problem-solving.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/openai/swarm&quot;&gt;Swarm Repo&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/generate_podcast_agent_langgraph.ipynb&quot;&gt;Podcast Internet Search and Generate Agent ðŸŽ™ï¸&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A two step agent that first searches the internet for a given topic and then generates a podcast on the topic found. The search step uses a search agent and search function to find the most relevant information. The second step uses a podcast generation agent and generation function to create a podcast on the topic found.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Utilizes LangGraph to orchestrate a two-step workflow. The first step involves a search agent and function to gather information from the internet. The second step uses a podcast generation agent and function to create a podcast based on the gathered information.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ðŸ›ï¸ Shopping and Product Analysis Agents&lt;/h3&gt; 
&lt;ol start=&quot;37&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/ShopGenie.ipynb&quot;&gt;ShopGenie - Redefining Online Shopping Customer Experience&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;An AI-powered shopping assistant that helps customers make informed purchasing decisions even without domain expertise. The system analyzes product information from multiple sources, compares specifications and reviews, identifies the best option based on user needs, and delivers recommendations through email with supporting video reviews, creating a comprehensive shopping experience.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Uses LangGraph to orchestrate a workflow combining Tavily for web search, Llama-3.1-70B for structured data analysis and product comparison, and YouTube API for review video retrieval. The system processes search results through multiple nodes including schema mapping, product comparison, review identification, and email generation. Key features include structured Pydantic models for consistent data handling, retry mechanisms for robust API interactions, and email delivery through SMTP for sharing recommendations.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Js0sK0u53dQ&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/car_buyer_agent_langgraph.ipynb&quot;&gt;Car Buyer AI Agent&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;The Smart Product Buyer AI Agent demonstrates how to build an intelligent system that assists users in making informed purchasing decisions. Using LangGraph and LLM-based intelligence, the system processes user requirements, scrapes product listings from websites like AutoTrader, and provides detailed analysis and recommendations for car purchases.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Implements a state-based workflow using LangGraph to coordinate user interaction, web scraping, and decision support. The system features TypedDict state management, async web scraping with Playwright, and integrates with external APIs for comprehensive product analysis. The implementation includes a Gradio interface for real-time chat interaction and modular scraper architecture for easy extension to additional product categories.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=I61I1fp0qys&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ðŸŽ¯ Task Management and Productivity Agents&lt;/h3&gt; 
&lt;ol start=&quot;39&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/taskifier.ipynb&quot;&gt;Taskifier - Intelligent Task Allocation &amp;amp; Management&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;An intelligent task management system that analyzes user work styles and creates personalized task breakdown strategies, born from the observation that procrastination often stems from task ambiguity among students and early-career professionals. The system evaluates historical work patterns, gathers relevant task information through web search, and generates customized step-by-step approaches to optimize productivity and reduce workflow paralysis.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Leverages LangGraph for orchestrating a multi-step workflow including work style analysis, information gathering via Tavily API, and customized plan generation. The system maintains state through the process, integrating historical work pattern data with fresh task research to output detailed, personalized task execution plans aligned with the user&#39;s natural working style.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=1W_p_RVi9KE&amp;amp;t=25s&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/grocery_management_agents_system.ipynb&quot;&gt;Grocery Management Agents System&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A multi-agent system built with CrewAI that automates grocery management tasks including receipt interpretation, expiration date tracking, inventory management, and recipe recommendations. The system uses specialized agents to extract data from receipts, estimate product shelf life, track consumption, and suggest recipes to minimize food waste.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Implements four specialized agents using CrewAI - a Receipt Interpreter that extracts item details from receipts, an Expiration Date Estimator that determines shelf life using online sources, a Grocery Tracker that maintains inventory based on consumption, and a Recipe Recommender that suggests meals using available ingredients. Each agent has specific tools and tasks orchestrated through a crew workflow.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=FlMu5pKSaHI&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ðŸ” Quality Assurance and Testing Agents&lt;/h3&gt; 
&lt;ol start=&quot;41&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/graph_inspector_system_langgraph.ipynb&quot;&gt;LangGraph-Based Systems Inspector&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A comprehensive testing and validation tool for LangGraph-based applications that automatically analyzes system architecture, generates test cases, and identifies potential vulnerabilities through multi-agent inspection. The inspector employs specialized AI testers to evaluate different aspects of the system, from basic functionality to security concerns and edge cases.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Integrates LangGraph for workflow orchestration, multiple LLM-powered testing agents, and a structured evaluation pipeline that includes static analysis, test case generation, and results verification. The system uses Pydantic for data validation, NetworkX for graph representation, and implements a modular architecture that allows for parallel test execution and comprehensive result analysis.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=fQd6lXc-Y9A&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/langgraph-systems-inspector-an-ai?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false&quot;&gt;Blog Post&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/EU_Green_Compliance_FAQ_Bot.ipynb&quot;&gt;EU Green Deal FAQ Bot&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;The EU Green Deal FAQ Bot demonstrates how to build a RAG-based AI agent that helps businesses understand EU green deal policies. The system processes complex regulatory documents into manageable chunks and provides instant, accurate answers to common questions about environmental compliance, emissions reporting, and waste management requirements.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Implements a sophisticated RAG pipeline using FAISS vectorstore for document storage, semantic chunking for preprocessing, and multiple specialized agents (Retriever, Summarizer, Evaluator) for query processing. The system features query rephrasing for improved accuracy, cross-reference with gold Q&amp;amp;A datasets for answer validation, and comprehensive evaluation metrics to ensure response quality and relevance.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Av0kBQjwU-Y&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/raw/main/all_agents_tutorials/systematic_review_of_scientific_articles.ipynb&quot;&gt;Systematic Review Automation System + Paper Draft Creation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A comprehensive system for automating academic systematic reviews using a directed graph architecture and LangChain components. The system generates complete, publication-ready systematic review papers, automatically processing everything from literature search through final draft generation with multiple revision cycles.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Utilizes a state-based graph workflow that handles paper search and selection (up to 3 papers), PDF processing, and generates a complete academic paper with all standard sections (abstract, introduction, methods, results, conclusions, references). The system incorporates multiple revision cycles with automated critique and improvement phases, all orchestrated through LangGraph state management.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=qi35mGGkCtg&quot;&gt;YouTube Explanation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ðŸŒŸ Special Advanced Technique ðŸŒŸ&lt;/h3&gt; 
&lt;ol start=&quot;44&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/Controllable-RAG-Agent&quot;&gt;Sophisticated Controllable Agent for Complex RAG Tasks ðŸ¤–&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;An advanced RAG solution designed to tackle complex questions that simple semantic similarity-based retrieval cannot solve. This approach uses a sophisticated deterministic graph as the &quot;brain&quot; ðŸ§  of a highly controllable autonomous agent, capable of answering non-trivial questions from your own data.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;â€¢ Implement a multi-step process involving question anonymization, high-level planning, task breakdown, adaptive information retrieval and question answering, continuous re-planning, and rigorous answer verification to ensure grounded and accurate responses.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;To begin exploring and building GenAI agents:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone this repository: &lt;pre&gt;&lt;code&gt;git clone https://github.com/NirDiamant/GenAI_Agents.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Navigate to the technique you&#39;re interested in: &lt;pre&gt;&lt;code&gt;cd all_agents_tutorials/technique-name
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Follow the detailed implementation guide in each technique&#39;s notebook.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! If you have a new technique or improvement to suggest:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create your feature branch: &lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Commit your changes: &lt;code&gt;git commit -m &#39;Add some AmazingFeature&#39;&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Push to the branch: &lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Open a pull request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents/graphs/contributors&quot;&gt;&lt;img src=&quot;https://contrib.rocks/image?repo=NirDiamant/GenAI_Agents&quot; alt=&quot;Contributors&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under a custom non-commercial license - see the &lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/GenAI_Agents/main/LICENSE&quot;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;â­ï¸ If you find this repository helpful, please consider giving it a star!&lt;/p&gt; 
&lt;p&gt;Keywords: GenAI, Generative AI, Agents, NLP, AI, Machine Learning, Natural Language Processing, LLM, Conversational AI, Task-Oriented AI&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ed-donner/llm_engineering</title>
      <link>https://github.com/ed-donner/llm_engineering</link>
      <description>&lt;p&gt;Repo to accompany my mastering LLM engineering course&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLM Engineering - Master AI and LLMs&lt;/h1&gt; 
&lt;h2&gt;Your 8 week journey to proficiency starts today&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/ed-donner/llm_engineering/main/voyage.jpg&quot; alt=&quot;Voyage&quot;&gt;&lt;/p&gt; 
&lt;p&gt;I&#39;m so happy you&#39;re joining me on this path. We&#39;ll be building immensely satisfying projects in the coming weeks. Some will be easy, some will be challenging, many will ASTOUND you! The projects build on each other so you develop deeper and deeper expertise each week. One thing&#39;s for sure: you&#39;re going to have a lot of fun along the way.&lt;/p&gt; 
&lt;h3&gt;Before you begin&lt;/h3&gt; 
&lt;p&gt;I&#39;m here to help you be most successful with your learning! If you hit any snafus, or if you have any ideas on how I can improve the course, please do reach out in the platform or by emailing me direct (&lt;a href=&quot;mailto:ed@edwarddonner.com&quot;&gt;ed@edwarddonner.com&lt;/a&gt;). It&#39;s always great to connect with people on LinkedIn to build up the community - you&#39;ll find me here:&lt;br&gt; &lt;a href=&quot;https://www.linkedin.com/in/eddonner/&quot;&gt;https://www.linkedin.com/in/eddonner/&lt;/a&gt;&lt;br&gt; And this is new to me, but I&#39;m also trying out X/Twitter at &lt;a href=&quot;https://x.com/edwarddonner&quot;&gt;@edwarddonner&lt;/a&gt; - if you&#39;re on X, please show me how it&#39;s done ðŸ˜‚&lt;/p&gt; 
&lt;p&gt;Resources to accompany the course, including the slides and useful links, are here:&lt;br&gt; &lt;a href=&quot;https://edwarddonner.com/2024/11/13/llm-engineering-resources/&quot;&gt;https://edwarddonner.com/2024/11/13/llm-engineering-resources/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Instant Gratification instructions for Week 1, Day 1&lt;/h2&gt; 
&lt;h3&gt;Important note: see my warning about Llama3.3 below - it&#39;s too large for home computers! Stick with llama3.2! Several students have missed this warning...&lt;/h3&gt; 
&lt;p&gt;We will start the course by installing Ollama so you can see results immediately!&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download and install Ollama from &lt;a href=&quot;https://ollama.com&quot;&gt;https://ollama.com&lt;/a&gt; noting that on a PC you might need to have administrator permissions for the install to work properly&lt;/li&gt; 
 &lt;li&gt;On a PC, start a Command prompt / Powershell (Press Win + R, type &lt;code&gt;cmd&lt;/code&gt;, and press Enter). On a Mac, start a Terminal (Applications &amp;gt; Utilities &amp;gt; Terminal).&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;ollama run llama3.2&lt;/code&gt; or for smaller machines try &lt;code&gt;ollama run llama3.2:1b&lt;/code&gt; - &lt;strong&gt;please note&lt;/strong&gt; steer clear of Meta&#39;s latest model llama3.3 because at 70B parameters that&#39;s way too large for most home computers!&lt;/li&gt; 
 &lt;li&gt;If this doesn&#39;t work: you may need to run &lt;code&gt;ollama serve&lt;/code&gt; in another Powershell (Windows) or Terminal (Mac), and try step 3 again. On a PC, you may need to be running in an Admin instance of Powershell.&lt;/li&gt; 
 &lt;li&gt;And if that doesn&#39;t work on your box, I&#39;ve set up this on the cloud. This is on Google Colab, which will need you to have a Google account to sign in, but is free: &lt;a href=&quot;https://colab.research.google.com/drive/1-_f5XZPsChvfU1sJ0QqCePtIuc55LSdu?usp=sharing&quot;&gt;https://colab.research.google.com/drive/1-_f5XZPsChvfU1sJ0QqCePtIuc55LSdu?usp=sharing&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Any problems, please contact me!&lt;/p&gt; 
&lt;h2&gt;Then, Setup instructions&lt;/h2&gt; 
&lt;p&gt;After we do the Ollama quick project, and after I introduce myself and the course, we get to work with the full environment setup.&lt;/p&gt; 
&lt;p&gt;Hopefully I&#39;ve done a decent job of making these guides bulletproof - but please contact me right away if you hit roadblocks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PC people please follow the instructions in &lt;a href=&quot;https://raw.githubusercontent.com/ed-donner/llm_engineering/main/SETUP-PC.md&quot;&gt;SETUP-PC.md&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Mac people please follow the instructions in &lt;a href=&quot;https://raw.githubusercontent.com/ed-donner/llm_engineering/main/SETUP-mac.md&quot;&gt;SETUP-mac.md&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Linux people please follow the instructions in &lt;a href=&quot;https://raw.githubusercontent.com/ed-donner/llm_engineering/main/SETUP-linux.md&quot;&gt;SETUP-linux.md&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The are also PDF versions of the setup instructions in this folder if you&#39;d prefer.&lt;/p&gt; 
&lt;h3&gt;An important point on API costs (which are optional! No need to spend if you don&#39;t wish)&lt;/h3&gt; 
&lt;p&gt;During the course, I&#39;ll suggest you try out the leading models at the forefront of progress, known as the Frontier models. I&#39;ll also suggest you run open-source models using Google Colab. These services have some charges, but I&#39;ll keep cost minimal - like, a few cents at a time. And I&#39;ll provide alternatives if you&#39;d prefer not to use them.&lt;/p&gt; 
&lt;p&gt;Please do monitor your API usage to ensure you&#39;re comfortable with spend; I&#39;ve included links below. There&#39;s no need to spend anything more than a couple of dollars for the entire course. Some AI providers such as OpenAI require a minimum credit like $5 or local equivalent; we should only spend a fraction of it, and you&#39;ll have plenty of opportunity to put it to good use in your own projects. During Week 7 you have an option to spend a bit more if you&#39;re enjoying the process - I spend about $10 myself and the results make me very happy indeed! But it&#39;s not necessary in the least; the important part is that you focus on learning.&lt;/p&gt; 
&lt;h3&gt;Free alternative to Paid APIs&lt;/h3&gt; 
&lt;p&gt;Early in the course, I show you an alternative if you&#39;d rather not spend anything on APIs:&lt;br&gt; Any time that we have code like:&lt;br&gt; &lt;code&gt;openai = OpenAI()&lt;/code&gt;&lt;br&gt; You can use this as a direct replacement:&lt;br&gt; &lt;code&gt;openai = OpenAI(base_url=&#39;http://localhost:11434/v1&#39;, api_key=&#39;ollama&#39;)&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Below is a full example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# You need to do this one time on your computer
!ollama pull llama3.2

from openai import OpenAI
MODEL = &quot;llama3.2&quot;
openai = OpenAI(base_url=&quot;http://localhost:11434/v1&quot;, api_key=&quot;ollama&quot;)

response = openai.chat.completions.create(
 model=MODEL,
 messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is 2 + 2?&quot;}]
)

print(response.choices[0].message.content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How this Repo is organized&lt;/h3&gt; 
&lt;p&gt;There are folders for each of the &quot;weeks&quot;, representing modules of the class, culminating in a powerful autonomous Agentic AI solution in Week 8 that draws on many of the prior weeks.&lt;br&gt; Follow the setup instructions above, then open the Week 1 folder and prepare for joy.&lt;/p&gt; 
&lt;h3&gt;The most important part&lt;/h3&gt; 
&lt;p&gt;The mantra of the course is: the best way to learn is by &lt;strong&gt;DOING&lt;/strong&gt;. I don&#39;t type all the code during the course; I execute it for you to see the results. You should work along with me or after each lecture, running each cell, inspecting the objects to get a detailed understanding of what&#39;s happening. Then tweak the code and make it your own. There are juicy challenges for you throughout the course. I&#39;d love it if you wanted to submit a Pull Request for your code (instructions &lt;a href=&quot;https://chatgpt.com/share/677a9cb5-c64c-8012-99e0-e06e88afd293&quot;&gt;here&lt;/a&gt;) and I can make your solutions available to others so we share in your progress; as an added benefit, you&#39;ll be recognized in GitHub for your contribution to the repo. While the projects are enjoyable, they are first and foremost designed to be &lt;em&gt;educational&lt;/em&gt;, teaching you business skills that can be put into practice in your work.&lt;/p&gt; 
&lt;h2&gt;Starting in Week 3, we&#39;ll also be using Google Colab for running with GPUs&lt;/h2&gt; 
&lt;p&gt;You should be able to use the free tier or minimal spend to complete all the projects in the class. I personally signed up for Colab Pro+ and I&#39;m loving it - but it&#39;s not required.&lt;/p&gt; 
&lt;p&gt;Learn about Google Colab and set up a Google account (if you don&#39;t already have one) &lt;a href=&quot;https://colab.research.google.com/&quot;&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The colab links are in the Week folders and also here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For week 3 day 1, this Google Colab shows what &lt;a href=&quot;https://colab.research.google.com/drive/1DjcrYDZldAXKJ08x1uYIVCtItoLPk1Wr?usp=sharing&quot;&gt;colab can do&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For week 3 day 2, here is a colab for the HuggingFace &lt;a href=&quot;https://colab.research.google.com/drive/1aMaEw8A56xs0bRM4lu8z7ou18jqyybGm?usp=sharing&quot;&gt;pipelines API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For week 3 day 3, here&#39;s the colab on &lt;a href=&quot;https://colab.research.google.com/drive/1WD6Y2N7ctQi1X9wa6rpkg8UfyA4iSVuz?usp=sharing&quot;&gt;Tokenizers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For week 3 day 4, we go to a colab with HuggingFace &lt;a href=&quot;https://colab.research.google.com/drive/1hhR9Z-yiqjUe7pJjVQw4c74z_V3VchLy?usp=sharing&quot;&gt;models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For week 3 day 5, we return to colab to make our &lt;a href=&quot;https://colab.research.google.com/drive/1KSMxOCprsl1QRpt_Rq0UqCAyMtPqDQYx?usp=sharing&quot;&gt;Meeting Minutes product&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For week 7, we will use these Colab books: &lt;a href=&quot;https://colab.research.google.com/drive/15rqdMTJwK76icPBxNoqhI7Ww8UM-Y7ni?usp=sharing&quot;&gt;Day 1&lt;/a&gt; | &lt;a href=&quot;https://colab.research.google.com/drive/1T72pbfZw32fq-clQEp-p8YQ4_qFKv4TP?usp=sharing&quot;&gt;Day 2&lt;/a&gt; | &lt;a href=&quot;https://colab.research.google.com/drive/1csEdaECRtjV_1p9zMkaKKjCpYnltlN3M?usp=sharing&quot;&gt;Days 3 and 4&lt;/a&gt; | &lt;a href=&quot;https://colab.research.google.com/drive/1igA0HF0gvQqbdBD4GkcK3GpHtuDLijYn?usp=sharing&quot;&gt;Day 5&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Monitoring API charges&lt;/h3&gt; 
&lt;p&gt;You can keep your API spend very low throughout this course; you can monitor spend at the dashboards: &lt;a href=&quot;https://platform.openai.com/usage&quot;&gt;here&lt;/a&gt; for OpenAI, &lt;a href=&quot;https://console.anthropic.com/settings/cost&quot;&gt;here&lt;/a&gt; for Anthropic and &lt;a href=&quot;https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com/cost&quot;&gt;here&lt;/a&gt; for Google Gemini.&lt;/p&gt; 
&lt;p&gt;The charges for the exercsies in this course should always be quite low, but if you&#39;d prefer to keep them minimal, then be sure to always choose the cheapest versions of models:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;For OpenAI: Always use model &lt;code&gt;gpt-4o-mini&lt;/code&gt; in the code instead of &lt;code&gt;gpt-4o&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;For Anthropic: Always use model &lt;code&gt;claude-3-haiku-20240307&lt;/code&gt; in the code instead of the other Claude models&lt;/li&gt; 
 &lt;li&gt;During week 7, look out for my instructions for using the cheaper dataset&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Please do message me or email me at &lt;a href=&quot;mailto:ed@edwarddonner.com&quot;&gt;ed@edwarddonner.com&lt;/a&gt; if this doesn&#39;t work or if I can help with anything. I can&#39;t wait to hear how you get on.&lt;/p&gt; 
&lt;table style=&quot;margin: 0; text-align: left;&quot;&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td style=&quot;width: 150px; height: 150px; vertical-align: middle;&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/ed-donner/llm_engineering/main/resources.jpg&quot; width=&quot;150&quot; height=&quot;150&quot; style=&quot;display: block;&quot;&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;h2 style=&quot;color:#f71;&quot;&gt;Other resources&lt;/h2&gt; &lt;span style=&quot;color:#f71;&quot;&gt;I&#39;ve put together this webpage with useful resources for the course. This includes links to all the slides.&lt;br&gt; &lt;a href=&quot;https://edwarddonner.com/2024/11/13/llm-engineering-resources/&quot;&gt;https://edwarddonner.com/2024/11/13/llm-engineering-resources/&lt;/a&gt;&lt;br&gt; Please keep this bookmarked, and I&#39;ll continue to add more useful links there over time. &lt;/span&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>greyhatguy007/Machine-Learning-Specialization-Coursera</title>
      <link>https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera</link>
      <description>&lt;p&gt;Contains Solutions and Notes for the Machine Learning Specialization By Stanford University and Deeplearning.ai - Coursera (2022) by Prof. Andrew NG&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Specialization Coursera&lt;/h1&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/greyhatguy007/Machine-Learning-Specialization-Coursera/main/resources/title-head.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Contains Solutions and Notes for the &lt;a href=&quot;https://www.coursera.org/specializations/machine-learning-introduction/?utm_medium=coursera&amp;amp;utm_source=home-page&amp;amp;utm_campaign=mlslaunch2022IN&quot;&gt;Machine Learning Specialization&lt;/a&gt; by Andrew NG on Coursera&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note : If you would like to have a deeper understanding of the concepts by understanding all the math required, have a look at &lt;a href=&quot;https://github.com/greyhatguy007/Mathematics-for-Machine-Learning-and-Data-Science-Specialization-Coursera&quot;&gt;Mathematics for Machine Learning and Data Science&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;Course 1 : &lt;a href=&quot;https://www.coursera.org/learn/machine-learning?specialization=machine-learning-introduction&quot;&gt;Supervised Machine Learning: Regression and Classification &lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1&quot;&gt;Week 1&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Practice%20quiz%20-%20Regression&quot;&gt;Practice quiz: Regression&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Practice%20quiz%20-%20Supervised%20vs%20unsupervised%20learning&quot;&gt;Practice quiz: Supervised vs unsupervised learning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Practice%20quiz%20-%20Train%20the%20model%20with%20gradient%20descent&quot;&gt;Practice quiz: Train the model with gradient descent&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Optional%20Labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Optional%20Labs/C1_W1_Lab03_Model_Representation_Soln.ipynb&quot;&gt;Model Representation&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Optional%20Labs/C1_W1_Lab04_Cost_function_Soln.ipynb&quot;&gt;Cost Function&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Optional%20Labs/C1_W1_Lab05_Gradient_Descent_Soln.ipynb&quot;&gt;Gradient Descent&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2&quot;&gt;Week 2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Practice%20quiz%20-%20Gradient%20descent%20in%20practice&quot;&gt;Practice quiz: Gradient descent in practice&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Practice%20quiz%20-%20Multiple%20linear%20regression&quot;&gt;Practice quiz: Multiple linear regression&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb&quot;&gt;Numpy Vectorization&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab02_Multiple_Variable_Soln.ipynb&quot;&gt;Multi Variate Regression&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln.ipynb&quot;&gt;Feature Scaling&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab04_FeatEng_PolyReg_Soln.ipynb&quot;&gt;Feature Engineering&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab05_Sklearn_GD_Soln.ipynb&quot;&gt;Sklearn Gradient Descent&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab05_Sklearn_GD_Soln.ipynb&quot;&gt;Sklearn Normal Method&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/C1W2A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/C1W2A1/C1_W2_Linear_Regression.ipynb&quot;&gt;Linear Regression&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3&quot;&gt;Week 3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Practice%20quiz%20-%20Cost%20function%20for%20logistic%20regression&quot;&gt;Practice quiz: Cost function for logistic regression&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Practice%20quiz%20-%20Gradient%20descent%20for%20logistic%20regression&quot;&gt;Practice quiz: Gradient descent for logistic regression&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab01_Classification_Soln.ipynb&quot;&gt;Classification&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab02_Sigmoid_function_Soln.ipynb&quot;&gt;Sigmoid Function&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab03_Decision_Boundary_Soln.ipynb&quot;&gt;Decision Boundary&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab04_LogisticLoss_Soln.ipynb&quot;&gt;Logistic Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab05_Cost_Function_Soln.ipynb&quot;&gt;Cost Function&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab06_Gradient_Descent_Soln.ipynb&quot;&gt;Gradient Descent&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab07_Scikit_Learn_Soln.ipynb&quot;&gt;Scikit Learn - Logistic Regression&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab08_Overfitting_Soln.ipynb&quot;&gt;Overfitting&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab09_Regularization_Soln.ipynb&quot;&gt;Regularization&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/C1W3A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/C1W3A1/C1_W3_Logistic_Regression.ipynb&quot;&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href=&quot;https://coursera.org/share/195768f3c1a83e42298d3f61dae99d01&quot;&gt;Certificate Of Completion&lt;/a&gt;&lt;/h4&gt; 
&lt;br&gt; 
&lt;h2&gt;Course 2 : &lt;a href=&quot;https://www.coursera.org/learn/advanced-learning-algorithms?specialization=machine-learning-introduction&quot;&gt;Advanced Learning Algorithms&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1&quot;&gt;Week 1&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/Practice%20quiz%20-%20Neural%20networks%20intuition&quot;&gt;Practice quiz: Neural networks intuition&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/Practice%20quiz%20-%20Neural%20network%20model&quot;&gt;Practice quiz: Neural network model&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/Practice%20quiz%20-%20TensorFlow%20implementation&quot;&gt;Practice quiz: TensorFlow implementation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/Practice-Quiz-Neural-Networks-Implementation-in-python&quot;&gt;Practice quiz : Neural Networks Implementation in Numpy&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/optional-labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/optional-labs/C2_W1_Lab01_Neurons_and_Layers.ipynb&quot;&gt;Neurons and Layers&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/optional-labs/C2_W1_Lab02_CoffeeRoasting_TF.ipynb&quot;&gt;Coffee Roasting&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/optional-labs/C2_W1_Lab02_CoffeeRoasting_TF.ipynb&quot;&gt;Coffee Roasting Using Numpy&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/C2W1A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/C2W1A1/C2_W1_Assignment.ipynb&quot;&gt;Neural Networks for Binary Classification&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;br&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2&quot;&gt;Week 2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/Practice-Quiz-Neural-Network-Training&quot;&gt;Practice quiz : Neural Networks Training&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/Practice-Quiz-Activation-Functions&quot;&gt;Practice quiz : Activation Functions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/Practice-quiz-Multiclass-Classification&quot;&gt;Practice quiz : Multiclass Classification&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/Practice-Quiz-Additional-Neural-Network-Concepts&quot;&gt;Practice quiz : Additional Neural Networks Concepts&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/optional-labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/optional-labs/C2_W2_Relu.ipynb&quot;&gt;RElu&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/optional-labs/C2_W2_SoftMax.ipynb&quot;&gt;Softmax&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/optional-labs/C2_W2_Multiclass_TF.ipynb&quot;&gt;Multiclass Classification&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/C2W2A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/C2W2A1/C2_W2_Assignment.ipynb&quot;&gt;Neural Networks For Handwritten Digit Recognition - Multiclass&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3&quot;&gt;Week 3&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/Practice-Quiz-Advice-for-applying-machine-learning&quot;&gt;Practice quiz : Advice for Applying Machine Learning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/practice-quiz-bias-and-variance&quot;&gt;Practice quiz : Bias and Variance&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/practice-quiz-machine-learning-development-process&quot;&gt;Practice quiz : Machine Learning Development Process&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/C2W3A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/C2W3A1/C2_W3_Assignment.ipynb&quot;&gt;Advice for Applied Machine Learning&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4&quot;&gt;Week 4&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/practice-quiz-decision-trees&quot;&gt;Practice quiz : Decision Trees&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/practice-quiz-decision-tree-learning&quot;&gt;Practice quiz : Decision Trees Learning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/practice-quiz-tree-ensembles&quot;&gt;Practice quiz : Decision Trees Ensembles&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/C2W4A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/C2W4A1/C2_W4_Decision_Tree_with_Markdown.ipynb&quot;&gt;Decision Trees&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href=&quot;https://coursera.org/share/c9a7766b0c6eab27db2e955376d29bf7&quot;&gt;Certificate of Completion&lt;/a&gt;&lt;/h4&gt; 
&lt;br&gt; 
&lt;h2&gt;Course 3 : &lt;a href=&quot;https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning?specialization=machine-learning-introduction&quot;&gt;Unsupervised Learning, Recommenders, Reinforcement Learning&lt;/a&gt;&lt;/h2&gt; 
&lt;br&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1&quot;&gt;Week 1&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/Practice%20Quiz%20-%20Clustering&quot;&gt;Practice quiz : Clustering&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/Practice%20Quiz%20-%20Anomaly%20Detection&quot;&gt;Practice quiz : Anomaly Detection&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/C3W1A&quot;&gt;Programming Assignments&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/C3W1A/C3W1A1/C3_W1_KMeans_Assignment.ipynb&quot;&gt;K means&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/C3W1A/C3W1A2/C3_W1_Anomaly_Detection.ipynb&quot;&gt;Anomaly Detection&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2&quot;&gt;Week 2&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/Practice%20Quiz%20-%20Collaborative%20Filtering&quot;&gt;Practice quiz : Collaborative Filtering&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/Practice%20Quiz%20-%20Recommender%20systems%20implementation&quot;&gt;Practice quiz : Recommender systems implementation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/Practice%20Quiz%20-%20Content-based%20filtering&quot;&gt;Practice quiz : Content-based filtering&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/C3W2&quot;&gt;Programming Assignments&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/C3W2/C3W2A1/C3_W2_Collaborative_RecSys_Assignment.ipynb&quot;&gt;Collaborative Filtering RecSys&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/C3W2/C3W2A2/C3_W2_RecSysNN_Assignment.ipynb&quot;&gt;RecSys using Neural Networks&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3&quot;&gt;Week 3&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/Practice%20Quiz%20-%20Reinforcement%20learning%20introduction&quot;&gt;Practice quiz : Reinforcement learning introduction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/Practice%20Quiz%20-%20State-action%20value%20function&quot;&gt;Practice Quiz : State-action value function&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/Practice%20Quiz%20-%20Continuous%20state%20spaces&quot;&gt;Practice Quiz : Continuous state spaces&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/C3W3A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/C3W3A1/C3_W3_A1_Assignment.ipynb&quot;&gt;Deep Q-Learning - Lunar Lander&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href=&quot;https://coursera.org/share/5bf5ee456b0c806df9b8622067b47ca6&quot;&gt;Certificate of Completion&lt;/a&gt;&lt;/h4&gt; 
&lt;h3&gt;&lt;a href=&quot;https://coursera.org/share/a15ac6426f90924491a542850700a759&quot;&gt;Specialization Certificate&lt;/a&gt;&lt;/h3&gt; 
&lt;br&gt; 
&lt;br&gt; 
&lt;hr&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;h3&gt;Stargazers over time&lt;/h3&gt; 
 &lt;p&gt;&lt;a href=&quot;https://starchart.cc/greyhatguy007/Machine-Learning-Specialization-Coursera&quot;&gt;&lt;img src=&quot;https://starchart.cc/greyhatguy007/Machine-Learning-Specialization-Coursera.svg?variant=adaptive&quot; alt=&quot;Stargazers over time&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://hits.seeyoufarm.com&quot;&gt;&lt;img src=&quot;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fgreyhatguy007%2FMachine-Learning-Specialization-Coursera&amp;amp;count_bg=%2379C83D&amp;amp;title_bg=%23555555&amp;amp;icon=&amp;amp;icon_color=%23E7E7E7&amp;amp;title=hits&amp;amp;edge_flat=false&quot; alt=&quot;Hits&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h3&gt;Course Review :&lt;/h3&gt; 
&lt;p&gt;This Course is a best place towards becoming a Machine Learning Engineer. Even if you&#39;re an expert, many algorithms are covered in depth such as decision trees which may help in further improvement of skills.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Special thanks to &lt;a href=&quot;https://www.andrewng.org/&quot;&gt;Professor Andrew Ng&lt;/a&gt; for structuring and tailoring this Course.&lt;/strong&gt;&lt;/p&gt; 
&lt;br&gt; 
&lt;hr&gt; 
&lt;h4&gt;An insight of what you might be able to accomplish at the end of this specialization :&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;i&gt;Write an unsupervised learning algorithm to &lt;strong&gt;Land the Lunar Lander&lt;/strong&gt; Using Deep Q-Learning&lt;/i&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The Rover was trained to land correctly on the surface, correctly between the flags as indicators after many unsuccessful attempts in learning how to do it.&lt;/li&gt; 
   &lt;li&gt;The final landing after training the agent using appropriate parameters :&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://user-images.githubusercontent.com/77543865/182395635-703ae199-ba79-4940-86eb-23dd90093ab3.mp4&quot;&gt;https://user-images.githubusercontent.com/77543865/182395635-703ae199-ba79-4940-86eb-23dd90093ab3.mp4&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;i&gt;Write an algorithm for a &lt;strong&gt;Movie Recommender System&lt;/strong&gt;&lt;/i&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;A movie database is collected based on its genre.&lt;/li&gt; 
   &lt;li&gt;A content based filtering and collaborative filtering algorithm is trained and the movie recommender system is implemented.&lt;/li&gt; 
   &lt;li&gt;It gives movie recommendentations based on the movie genre.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/77543865/182398093-c7387754-34a9-4044-b842-0085060c3525.png&quot; alt=&quot;movie_recommendation&quot;&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;i&gt; And Much More !! &lt;/i&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Concluding, this is a course which I would recommend everyone to take. Not just because you learn many new stuffs, but also the assignments are real life examples which are &lt;em&gt;exciting to complete&lt;/em&gt;.&lt;/p&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;strong&gt;Happy Learning :))&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google-gemini/gemma-cookbook</title>
      <link>https://github.com/google-gemini/gemma-cookbook</link>
      <description>&lt;p&gt;A collection of guides and examples for the Gemma open models from Google.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to the Gemma Cookbook&lt;/h1&gt; 
&lt;p&gt;This is a collection of guides and examples for &lt;a href=&quot;https://ai.google.dev/gemma/&quot;&gt;Google Gemma&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Get started with the Gemma models&lt;/h2&gt; 
&lt;p&gt;Gemma is a family of lightweight, generative artificial intelligence (AI) open models, built from the same research and technology used to create the Gemini models. The Gemma model family includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Gemma&lt;br&gt; The core models of the Gemma family. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/core/model_card&quot;&gt;Gemma&lt;/a&gt;&lt;br&gt; For a variety of text generation tasks and can be further tuned for specific use cases&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/core/model_card_2&quot;&gt;Gemma 2&lt;/a&gt;&lt;br&gt; Higher-performing and more efficient, available in 2B, 9B, 27B parameter sizes&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/core/model_card_3&quot;&gt;Gemma 3&lt;/a&gt;&lt;br&gt; Longer context window and handling text and image input, available in 1B, 4B, 12B and 27B parameter sizes&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Gemma variants 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/codegemma&quot;&gt;CodeGemma&lt;/a&gt;&lt;br&gt; Fine-tuned for a variety of coding tasks&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/paligemma/model-card&quot;&gt;PaliGemma&lt;/a&gt;&lt;br&gt; Vision Language Model&lt;br&gt; For a deeper analysis of images and provide useful insights&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/paligemma/model-card-2&quot;&gt;PaliGemma 2&lt;/a&gt;&lt;br&gt; VLM which incorporates the capabilities of the Gemma 2 models&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/recurrentgemma&quot;&gt;RecurrentGemma&lt;/a&gt;&lt;br&gt; Based on &lt;a href=&quot;https://arxiv.org/abs/2402.19427&quot;&gt;Griffin&lt;/a&gt; architecture&lt;br&gt; For a variety of text generation tasks&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/shieldgemma/model_card&quot;&gt;ShieldGemma&lt;/a&gt;&lt;br&gt; Fine-tuned for evaluating the safety of text prompt input and text output responses against a set of defined safety policies&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/shieldgemma/model_card_2&quot;&gt;ShieldGemma 2&lt;/a&gt;&lt;br&gt; Fine-tuned on Gemma 3&#39;s 4B IT checkpoint for image safety classification&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.google.dev/gemma/docs/datagemma&quot;&gt;DataGemma&lt;/a&gt;&lt;br&gt; Fine-tuned for using Data Commons to address AI hallucinations&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can find the Gemma models on the Hugging Face Hub, Kaggle, Google Cloud Vertex AI Model Garden, and &lt;a href=&quot;https://ai.nvidia.com&quot;&gt;ai.nvidia.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Table of Notebooks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/gemma-cookbook/main/Gemma/README.md&quot;&gt;Gemma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/gemma-cookbook/main/CodeGemma/README.md&quot;&gt;CodeGemma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/gemma-cookbook/main/PaliGemma/README.md&quot;&gt;PaliGemma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/gemma-cookbook/main/Workshops/README.md&quot;&gt;Workshops and technical talks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/gemma-cookbook/main/Demos/README.md&quot;&gt;Showcase complex end-to-end use cases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/GoogleCloudPlatform/generative-ai/tree/main/open-models&quot;&gt;Gemma on Google Cloud&lt;/a&gt; : GCP open models has additional notebooks for using Gemma&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Get help&lt;/h2&gt; 
&lt;p&gt;Ask a Gemma cookbook-related question on the &lt;a href=&quot;https://discuss.ai.google.dev/c/gemma/10&quot;&gt;developer forum&lt;/a&gt;, or open an &lt;a href=&quot;https://github.com/google-gemini/gemma-cookbook/issues&quot;&gt;issue&lt;/a&gt; on GitHub.&lt;/p&gt; 
&lt;h2&gt;Wish list&lt;/h2&gt; 
&lt;p&gt;If you want to see additional cookbooks implemented for specific features/integrations, please open a new issue with &lt;a href=&quot;https://github.com/google-gemini/gemma-cookbook/issues/new?template=feature_request.yml&quot;&gt;â€œFeature Requestâ€ template&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to make contributions to the Gemma Cookbook project, you are welcome to pick any idea in the &lt;a href=&quot;https://github.com/google-gemini/gemma-cookbook/labels/wishlist&quot;&gt;â€œWish Listâ€&lt;/a&gt; and implement it.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are always welcome. Please read &lt;a href=&quot;https://github.com/google-gemini/gemma-cookbook/raw/main/CONTRIBUTING.md&quot;&gt;contributing&lt;/a&gt; before implementation.&lt;/p&gt; 
&lt;p&gt;Thank you for developing with Gemma! Weâ€™re excited to see what you create.&lt;/p&gt; 
&lt;h2&gt;Translation of this repository&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/doggy8088/gemma-cookbook&quot;&gt;Traditional Chinese&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xiaoxiong1006/gemma-cookbook&quot;&gt;Simplified Chinese&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Azure-Samples/AI-Gateway</title>
      <link>https://github.com/Azure-Samples/AI-Gateway</link>
      <description>&lt;p&gt;APIM â¤ï¸ OpenAI - this repo contains a set of experiments on using GenAI capabilities of Azure API Management with Azure OpenAI and other services&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;APIM â¤ï¸ OpenAI - ðŸ§ª Labs for the &lt;a href=&quot;https://techcommunity.microsoft.com/t5/azure-integration-services-blog/introducing-genai-gateway-capabilities-in-azure-api-management/ba-p/4146525&quot;&gt;GenAI Gateway capabilities&lt;/a&gt; of &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/api-management-key-concepts&quot;&gt;Azure API Management&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/firstcontributions/open-source-badges&quot;&gt;&lt;img src=&quot;https://firstcontributions.github.io/open-source-badges/badges/open-source-v1/open-source.svg?sanitize=true&quot; alt=&quot;Open Source Love&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What&#39;s new âœ¨&lt;/h2&gt; 
&lt;p&gt;âž• the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/finops-framework/finops-framework.ipynb&quot;&gt;&lt;strong&gt;FinOps Framework&lt;/strong&gt;&lt;/a&gt; lab to manage AI budgets effectively ðŸ’°&lt;br&gt; âž• &lt;strong&gt;Agentic âœ¨&lt;/strong&gt; experiments with &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-context-protocol/model-context-protocol.ipynb&quot;&gt;&lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;&lt;/a&gt;.&lt;br&gt; âž• &lt;strong&gt;Agentic âœ¨&lt;/strong&gt; experiments with &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/openai-agents/openai-agents.ipynb&quot;&gt;&lt;strong&gt;OpenAI Agents SDK&lt;/strong&gt;&lt;/a&gt;.&lt;br&gt; âž• &lt;strong&gt;Agentic âœ¨&lt;/strong&gt; experiments with &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-agent-service/ai-agent-service.ipynb&quot;&gt;&lt;strong&gt;AI Agent Service&lt;/strong&gt;&lt;/a&gt; from &lt;a href=&quot;https://azure.microsoft.com/en-us/products/ai-foundry&quot;&gt;Azure AI Foundry&lt;/a&gt;.&lt;br&gt; âž• the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-foundry-deepseek/ai-foundry-deepseek.ipynb&quot;&gt;&lt;strong&gt;AI Foundry Deepseek&lt;/strong&gt;&lt;/a&gt; lab with Deepseek R1 model from &lt;a href=&quot;https://azure.microsoft.com/en-us/products/ai-foundry&quot;&gt;Azure AI Foundry&lt;/a&gt;.&lt;br&gt; âž• the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/zero-to-production/zero-to-production.ipynb&quot;&gt;&lt;strong&gt;Zero-to-Production&lt;/strong&gt;&lt;/a&gt; lab with an iterative policy exploration to fine-tune the optimal production configuration.&lt;br&gt; âž• the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing-tf/backend-pool-load-balancing-tf.ipynb&quot;&gt;&lt;strong&gt;Terraform flavor of backend pool load balancing&lt;/strong&gt;&lt;/a&gt; lab.&lt;br&gt; âž• the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-foundry-sdk/ai-foundry-sdk.ipynb&quot;&gt;&lt;strong&gt;AI Foundry SDK&lt;/strong&gt;&lt;/a&gt; lab.&lt;br&gt; âž• the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/content-filtering.ipynb&quot;&gt;&lt;strong&gt;Content filtering&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/prompt-shielding.ipynb&quot;&gt;&lt;strong&gt;Prompt shielding&lt;/strong&gt;&lt;/a&gt; labs.&lt;br&gt; âž• the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-routing/model-routing.ipynb&quot;&gt;&lt;strong&gt;Model routing&lt;/strong&gt;&lt;/a&gt; lab with OpenAI model based routing.&lt;br&gt; âž• the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/prompt-flow/prompt-flow.ipynb&quot;&gt;&lt;strong&gt;Prompt flow&lt;/strong&gt;&lt;/a&gt; lab to try the &lt;a href=&quot;https://learn.microsoft.com/azure/ai-studio/how-to/prompt-flow&quot;&gt;Azure AI Studio Prompt Flow&lt;/a&gt; with Azure API Management.&lt;br&gt; âž• &lt;code&gt;priority&lt;/code&gt; and &lt;code&gt;weight&lt;/code&gt; parameters to the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb&quot;&gt;&lt;strong&gt;Backend pool load balancing&lt;/strong&gt;&lt;/a&gt; lab.&lt;br&gt; âž• the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/streaming.ipynb&quot;&gt;&lt;strong&gt;Streaming&lt;/strong&gt;&lt;/a&gt; tool to test OpenAI streaming with Azure API Management.&lt;br&gt; âž• the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/tools/tracing.ipynb&quot;&gt;&lt;strong&gt;Tracing&lt;/strong&gt;&lt;/a&gt; tool to debug and troubleshoot OpenAI APIs using &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/api-management-howto-api-inspector&quot;&gt;Azure API Management tracing capability&lt;/a&gt;.&lt;br&gt; âž• image processing to the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/GPT-4o-inferencing/GPT-4o-inferencing.ipynb&quot;&gt;&lt;strong&gt;GPT-4o inferencing&lt;/strong&gt;&lt;/a&gt; lab.&lt;br&gt; âž• the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/function-calling/function-calling.ipynb&quot;&gt;&lt;strong&gt;Function calling&lt;/strong&gt;&lt;/a&gt; lab with a sample API on Azure Functions.&lt;/p&gt; 
&lt;h2&gt;Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-genai-gateway&quot;&gt;ðŸ§  GenAI Gateway&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-labs-with-ai-agents&quot;&gt;ðŸ§ª Labs with AI Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-labs-with-the-inference-api&quot;&gt;ðŸ§ª Labs with the Inference API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-labs-based-on-azure-openai&quot;&gt;ðŸ§ª Labs based on Azure OpenAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-getting-started&quot;&gt;ðŸš€ Getting started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-roll-out-to-production&quot;&gt;â›µ Roll-out to production&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-supporting-tools&quot;&gt;ðŸ”¨ Supporting tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-well-architected-framework&quot;&gt;ðŸ›ï¸ Well-Architected Framework&lt;/a&gt; 
  &lt;!-- markdownlint-disable-line MD051 --&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-show-and-tell&quot;&gt;ðŸŽ’ Show and tell&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#-other-resources&quot;&gt;ðŸ¥‡ Other Resources&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The rapid pace of AI advances demands experimentation-driven approaches for organizations to remain at the forefront of the industry. With AI steadily becoming a game-changer for an array of sectors, maintaining a fast-paced innovation trajectory is crucial for businesses aiming to leverage its full potential.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AI services&lt;/strong&gt; are predominantly accessed via &lt;strong&gt;APIs&lt;/strong&gt;, underscoring the essential need for a robust and efficient API management strategy. This strategy is instrumental for maintaining control and governance over the consumption of &lt;strong&gt;AI services&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;With the expanding horizons of &lt;strong&gt;AI services&lt;/strong&gt; and their seamless integration with &lt;strong&gt;APIs&lt;/strong&gt;, there is a considerable demand for a comprehensive &lt;strong&gt;AI Gateway&lt;/strong&gt; pattern, which broadens the core principles of API management. Aiming to accelerate the experimentation of advanced use cases and pave the road for further innovation in this rapidly evolving field. The well-architected principles of the &lt;strong&gt;AI Gateway&lt;/strong&gt; provides a framework for the confident deployment of &lt;strong&gt;Intelligent Apps&lt;/strong&gt; into production.&lt;/p&gt; 
&lt;h2&gt;ðŸ§  GenAI Gateway&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/ai-gateway.gif&quot; alt=&quot;AI-Gateway flow&quot;&gt;&lt;/p&gt; 
&lt;p&gt;This repo explores the &lt;strong&gt;AI Gateway&lt;/strong&gt; pattern through a series of experimental labs. The &lt;a href=&quot;https://techcommunity.microsoft.com/t5/azure-integration-services-blog/introducing-genai-gateway-capabilities-in-azure-api-management/ba-p/4146525&quot;&gt;GenAI Gateway capabilities&lt;/a&gt; of &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/api-management-key-concepts&quot;&gt;Azure API Management&lt;/a&gt; plays a crucial role within these labs, handling AI services APIs, with security, reliability, performance, overall operational efficiency and cost controls. The primary focus is on &lt;a href=&quot;https://learn.microsoft.com/azure/ai-services/openai/overview&quot;&gt;Azure OpenAI&lt;/a&gt;, which sets the standard reference for Large Language Models (LLM). However, the same principles and design patterns could potentially be applied to any LLM.&lt;/p&gt; 
&lt;p&gt;Acknowledging the rising dominance of Python, particularly in the realm of AI, along with the powerful experimental capabilities of Jupyter notebooks, the following labs are structured around Jupyter notebooks, with step-by-step instructions with Python scripts, &lt;a href=&quot;https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep&quot;&gt;Bicep&lt;/a&gt; files and &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/api-management-howto-policies&quot;&gt;Azure API Management policies&lt;/a&gt;:&lt;/p&gt; 
&lt;h2&gt;ðŸ§ª Labs with AI Agents&lt;/h2&gt; 
&lt;!-- Model Context Protocol (MCP) --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-context-protocol/model-context-protocol.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª Model Context Protocol (MCP)&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to experiment the &lt;a href=&quot;https://modelcontextprotocol.io/&quot;&gt;Model Context Protocol&lt;/a&gt; with Azure API Management to enable plug &amp;amp; play of tools to LLMs. Leverages the &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/api-management/credentials-overview&quot;&gt;credential manager&lt;/a&gt; for managing OAuth 2.0 tokens to backend tools and &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/api-management/validate-jwt-policy&quot;&gt;client token validation&lt;/a&gt; to ensure end-to-end authentication and authorization.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-context-protocol/model-context-protocol.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/model-context-protocol-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-context-protocol/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-context-protocol/inference-policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-context-protocol/model-context-protocol.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- OpenAI Agents --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/openai-agents/openai-agents.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª OpenAI Agents&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the &lt;a href=&quot;https://openai.github.io/openai-agents-python/&quot;&gt;OpenAI Agents&lt;/a&gt; with Azure OpenAI models and API based tools controlled by Azure API Management.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/openai-agents/openai-agents.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/openai-agents-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/openai-agents/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/openai-agents/inference-policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/openai-agents/openai-agents.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- AI Agent Service --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-agent-service/ai-agent-service.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª AI Agent Service&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Use this playground to explore the &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/ai-services/agents/overview&quot;&gt;Azure AI Agent Service&lt;/a&gt;, leveraging Azure API Management to control multiple services, including Azure OpenAI models, Logic Apps Workflows, and OpenAPI-based APIs.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-agent-service/ai-agent-service.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/ai-agent-service-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-agent-service/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-agent-service/policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-agent-service/ai-agent-service.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Function calling --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/function-calling/function-calling.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª Function calling&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the OpenAI &lt;a href=&quot;https://learn.microsoft.com/azure/ai-services/openai/how-to/function-calling?tabs=non-streaming%2Cpython&quot;&gt;function calling&lt;/a&gt; feature with an Azure Functions API that is also managed by Azure API Management.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/function-calling/function-calling.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/function-calling-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/function-calling/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/function-calling/policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/function-calling/function-calling.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ðŸ§ª Labs with the Inference API&lt;/h2&gt; 
&lt;!-- AI Foundry Deepseek --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-foundry-deepseek/ai-foundry-deepseek.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª AI Foundry Deepseek&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/deepseek-r1-is-now-available-on-azure-ai-foundry-and-github/&quot;&gt;Deepseek R1 model&lt;/a&gt; via the AI Model Inference from &lt;a href=&quot;https://azure.microsoft.com/en-us/products/ai-foundry&quot;&gt;Azure AI Foundry&lt;/a&gt;. This lab uses the &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/ai-foundry/model-inference/how-to/inference?tabs=python&quot;&gt;Azure AI Model Inference API&lt;/a&gt; and two APIM LLM policies: &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/api-management/llm-token-limit-policy&quot;&gt;llm-token-limit&lt;/a&gt; and &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/api-management/llm-emit-token-metric-policy&quot;&gt;llm-emit-token-metric&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-foundry-deepseek/ai-foundry-deepseek.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/ai-foundry-deepseek-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-foundry-deepseek/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-foundry-deepseek/policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/ai-foundry-deepseek/ai-foundry-deepseek.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- SLM self-hosting --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/slm-self-hosting/slm-self-hosting.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª SLM self-hosting&lt;/strong&gt;&lt;/a&gt; (phy-3)&lt;/h3&gt; 
&lt;p&gt;Playground to try the self-hosted &lt;a href=&quot;https://azure.microsoft.com/blog/introducing-phi-3-redefining-whats-possible-with-slms/&quot;&gt;phy-3 Small Language Model (SLM)&lt;/a&gt; through the &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/self-hosted-gateway-overview&quot;&gt;Azure API Management self-hosted gateway&lt;/a&gt; with OpenAI API compatibility.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/slm-self-hosting/slm-self-hosting.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/slm-self-hosting-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/slm-self-hosting/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/slm-self-hosting/policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/slm-self-hosting/slm-self-hosting.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ðŸ§ª Labs based on Azure OpenAI&lt;/h2&gt; 
&lt;!--FinOps framework --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/finops-framework/finops-framework.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª FinOps Framework&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;This playground leverages the &lt;a href=&quot;https://www.finops.org/framework/&quot;&gt;FinOps Framework&lt;/a&gt; and Azure API Management to control AI costs. It uses the &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/api-management/azure-openai-token-limit-policy&quot;&gt;token limit&lt;/a&gt; policy for each &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-add-products?tabs=azure-portal&amp;amp;pivots=interactive&quot;&gt;product&lt;/a&gt; and integrates &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-overview&quot;&gt;Azure Monitor alerts&lt;/a&gt; with &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-logic-apps?tabs=send-email&quot;&gt;Logic Apps&lt;/a&gt; to automatically disable APIM &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/api-management/api-management-subscriptions&quot;&gt;subscriptions&lt;/a&gt; that exceed cost quotas.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/finops-framework/finops-framework.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/finops-framework-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/finops-framework/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/finops-framework/openai-policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/finops-framework/finops-framework.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Backend pool load balancing --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª Backend pool load balancing&lt;/strong&gt;&lt;/a&gt; - Available with &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb&quot;&gt;Bicep&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing-tf/backend-pool-load-balancing-tf.ipynb&quot;&gt;Terraform&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the built-in load balancing &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/backends?tabs=bicep&quot;&gt;backend pool functionality of Azure API Management&lt;/a&gt; to either a list of Azure OpenAI endpoints or mock servers.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/backend-pool-load-balancing-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing/policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Token rate limiting --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-rate-limiting/token-rate-limiting.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª Token rate limiting&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/azure-openai-token-limit-policy&quot;&gt;token rate limiting policy&lt;/a&gt; to one or more Azure OpenAI endpoints. When the token usage is exceeded, the caller receives a 429.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-rate-limiting/token-rate-limiting.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/token-rate-limiting-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-rate-limiting/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-rate-limiting/policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-rate-limiting/token-rate-limiting.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Token metrics emitting --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-metrics-emitting/token-metrics-emitting.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª Token metrics emitting&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/azure-openai-emit-token-metric-policy&quot;&gt;emit token metric policy&lt;/a&gt;. The policy sends metrics to Application Insights about consumption of large language model tokens through Azure OpenAI Service APIs.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-metrics-emitting/token-metrics-emitting.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/token-metrics-emitting-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-metrics-emitting/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-metrics-emitting/policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-metrics-emitting/token-metrics-emitting.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Semantic caching --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/semantic-caching/semantic-caching.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª Semantic caching&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/azure-openai-semantic-cache-lookup-policy&quot;&gt;semantic caching policy&lt;/a&gt;. Uses vector proximity of the prompt to previous requests and a specified similarity score threshold.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/semantic-caching/semantic-caching.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/semantic-caching-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/semantic-caching/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/semantic-caching/policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/semantic-caching/semantic-caching.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Access controlling --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/access-controlling/access-controlling.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª Access controlling&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/api-management-authenticate-authorize-azure-openai#oauth-20-authorization-using-identity-provider&quot;&gt;OAuth 2.0 authorization feature&lt;/a&gt; using identity provider to enable more fine-grained access to OpenAPI APIs by particular users or client.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/access-controlling/access-controlling.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/access-controlling-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/access-controlling/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/access-controlling/policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/access-controlling/access-controlling.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- zero-to-production --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/zero-to-production/zero-to-production.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª Zero-to-Production&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to create a combination of several policies in an iterative approach. We start with load balancing, then progressively add token emitting, rate limiting, and, eventually, semantic caching. Each of these sets of policies is derived from other labs in this repo.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/zero-to-production/zero-to-production.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/zero-to-production-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/zero-to-production/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/zero-to-production/policy-3.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/zero-to-production/zero-to-production.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- GPT-4o inferencing --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/GPT-4o-inferencing/GPT-4o-inferencing.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª GPT-4o inferencing&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the new GPT-4o model. GPT-4o (&quot;o&quot; for &quot;omni&quot;) is designed to handle a combination of text, audio, and video inputs, and can generate outputs in text, audio, and image formats.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/GPT-4o-inferencing/GPT-4o-inferencing.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/GPT-4o-inferencing-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/GPT-4o-inferencing/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/GPT-4o-inferencing/policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/GPT-4o-inferencing/GPT-4o-inferencing.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Model Routing --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-routing/model-routing.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª Model Routing&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try routing to a backend based on Azure OpenAI model and version.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-routing/model-routing.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/model-routing-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-routing/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-routing/policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/model-routing/model-routing.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Vector searching --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/vector-searching/vector-searching.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª Vector searching&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the &lt;a href=&quot;https://learn.microsoft.com/azure/search/retrieval-augmented-generation-overview&quot;&gt;Retrieval Augmented Generation (RAG) pattern&lt;/a&gt; with Azure AI Search, Azure OpenAI embeddings and Azure OpenAI completions.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/vector-searching/vector-searching.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/vector-searching-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/vector-searching/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/vector-searching/policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/vector-searching/vector-searching.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Built-in logging --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/built-in-logging/built-in-logging.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª Built-in logging&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/observability&quot;&gt;buil-in logging capabilities of Azure API Management&lt;/a&gt;. Logs requests into App Insights to track details and token usage.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/built-in-logging/built-in-logging.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/built-in-logging-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/built-in-logging/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/built-in-logging/policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/built-in-logging/built-in-logging.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Message storing --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/message-storing/message-storing.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª Message storing&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to test storing message details into Cosmos DB through the &lt;a href=&quot;https://learn.microsoft.com/azure/api-management/log-to-eventhub-policy&quot;&gt;Log to event hub&lt;/a&gt; policy. With the policy we can control which data will be stored in the DB (prompt, completion, model, region, tokens etc.).&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/message-storing/message-storing.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/message-storing-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/message-storing/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/message-storing/policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/message-storing/message-storing.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Prompt flow --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/prompt-flow/prompt-flow.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª Prompt flow&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try the &lt;a href=&quot;https://learn.microsoft.com/azure/ai-studio/how-to/prompt-flow&quot;&gt;Azure AI Studio Prompt Flow&lt;/a&gt; with Azure API Management.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/prompt-flow/prompt-flow.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/prompt-flow-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/prompt-flow/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/prompt-flow/policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/prompt-flow/prompt-flow.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Content Filtering --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/content-filtering.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª Content Filtering&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try integrating Azure API Management with &lt;a href=&quot;https://learn.microsoft.com/azure/ai-services/content-safety/overview&quot;&gt;Azure AI Content Safety&lt;/a&gt; to filter potentially offensive, risky, or undesirable content.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/content-filtering.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/content-filtering-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/content-filtering-policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/content-filtering.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Prompt Shielding --&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/prompt-shielding.ipynb&quot;&gt;&lt;strong&gt;ðŸ§ª Prompt Shielding&lt;/strong&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Playground to try Prompt Shields from Azure AI Content Safety service that analyzes LLM inputs and detects User Prompt attacks and Document attacks, which are two common types of adversarial inputs.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/prompt-shielding.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/content-filtering-small.gif&quot; alt=&quot;flow&quot; style=&quot;width: 437px; display: inline-block;&quot; data-target=&quot;animated-image.originalImage&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/main.bicep&quot;&gt;ðŸ¦¾ Bicep&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/prompt-shield-policy.xml&quot;&gt;âš™ï¸ Policy&lt;/a&gt; âž• &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/content-filtering/prompt-shielding.ipynb&quot;&gt;ðŸ§¾ Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Backlog of Labs&lt;/h2&gt; 
&lt;p&gt;This is a list of potential future labs to be developed.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Real Time API&lt;/li&gt; 
 &lt;li&gt;Semantic Kernel with Agents&lt;/li&gt; 
 &lt;li&gt;Logic Apps RAG&lt;/li&gt; 
 &lt;li&gt;PII handling&lt;/li&gt; 
 &lt;li&gt;Gemini&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Kindly use &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/discussions/9&quot;&gt;the feedback discussion&lt;/a&gt; so that we can continuously improve with your experiences, suggestions, ideas or lab requests.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ðŸš€ Getting Started&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.python.org/&quot;&gt;Python 3.12 or later version&lt;/a&gt; installed&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://code.visualstudio.com/&quot;&gt;VS Code&lt;/a&gt; installed with the &lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter&quot;&gt;Jupyter notebook extension&lt;/a&gt; enabled&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://code.visualstudio.com/docs/python/environments#_creating-environments&quot;&gt;Python environment&lt;/a&gt; with the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/requirements.txt&quot;&gt;requirements.txt&lt;/a&gt; or run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; in your terminal&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://azure.microsoft.com/free/&quot;&gt;An Azure Subscription&lt;/a&gt; with &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#contributor&quot;&gt;Contributor&lt;/a&gt; + &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#role-based-access-control-administrator&quot;&gt;RBAC Administrator&lt;/a&gt; or &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#owner&quot;&gt;Owner&lt;/a&gt; roles&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://learn.microsoft.com/cli/azure/install-azure-cli&quot;&gt;Azure CLI&lt;/a&gt; installed and &lt;a href=&quot;https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively&quot;&gt;Signed into your Azure subscription&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quickstart&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone this repo and configure your local machine with the prerequisites. Or just create a &lt;a href=&quot;https://codespaces.new/Azure-Samples/AI-Gateway/tree/main&quot;&gt;GitHub Codespace&lt;/a&gt; and run it on the browser or in VS Code.&lt;/li&gt; 
 &lt;li&gt;Navigate through the available labs and select one that best suits your needs. For starters we recommend the &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/token-rate-limiting/token-rate-limiting.ipynb&quot;&gt;token rate limiting&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Open the notebook and run the provided steps.&lt;/li&gt; 
 &lt;li&gt;Tailor the experiment according to your requirements. If you wish to contribute to our collective work, we would appreciate your &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/CONTRIBUTING.MD&quot;&gt;submission of a pull request&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] ðŸª² Please feel free to open a new &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/issues/new&quot;&gt;issue&lt;/a&gt; if you find something that should be fixed or enhanced.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;â›µ Roll-out to production&lt;/h2&gt; 
&lt;p&gt;We recommend the guidelines and best practices from the &lt;a href=&quot;https://github.com/Azure-Samples/ai-hub-gateway-solution-accelerator&quot;&gt;AI Hub Gateway Landing Zone&lt;/a&gt; to implement a central AI API gateway to empower various line-of-business units in an organization to leverage Azure AI services.&lt;/p&gt; 
&lt;h2&gt;ðŸ”¨ Supporting Tools&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/tools/mock-server/mock-server.ipynb&quot;&gt;AI-Gateway Mock server&lt;/a&gt; is designed to mimic the behavior and responses of the OpenAI API, thereby creating an efficient simulation environment suitable for testing and development purposes on the integration with Azure API Management and other use cases. The &lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/tools/mock-server/app.py&quot;&gt;app.py&lt;/a&gt; can be customized to tailor the Mock server to specific use cases.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/tools/tracing.ipynb&quot;&gt;Tracing&lt;/a&gt; - Invoke OpenAI API with trace enabled and returns the tracing information.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/streaming.ipynb&quot;&gt;Streaming&lt;/a&gt; - Invoke OpenAI API with stream enabled and returns response in chunks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ðŸ›ï¸ Well-Architected Framework&lt;/h2&gt; 
&lt;p&gt;The &lt;a href=&quot;https://learn.microsoft.com/azure/well-architected/what-is-well-architected-framework&quot;&gt;Azure Well-Architected Framework&lt;/a&gt; is a design framework that can improve the quality of a workload. The following table maps labs with the Well-Architected Framework pillars to set you up for success through architectural experimentation.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Lab&lt;/th&gt; 
   &lt;th&gt;Security&lt;/th&gt; 
   &lt;th&gt;Reliability&lt;/th&gt; 
   &lt;th&gt;Performance&lt;/th&gt; 
   &lt;th&gt;Operations&lt;/th&gt; 
   &lt;th&gt;Costs&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/request-forwarding/request-forwarding.ipynb&quot;&gt;Request forwarding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Zero trust, keyless approach with manage identities and Azure API Management security features&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-circuit-breaking/backend-circuit-breaking.ipynb&quot;&gt;Backend circuit breaking&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Zero trust, keyless approach with manage identities and Azure API Management security features&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Controls the availability of the OpenAI endpoint with the circuit breaker feature&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb&quot;&gt;Backend pool load balancing&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Zero trust, keyless approach with manage identities and Azure API Management security features&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;To ensure resilience, the request is distributed to two or more endpoints with the built-in feature&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Load balances the requests to increase performance with the built-in feature&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/advanced-load-balancing/advanced-load-balancing.ipynb&quot;&gt;Advanced load balancing&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Zero trust, keyless approach with manage identities and Azure API Management security features&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;To ensure resilience, the request is distributed to two or more endpoints with a custom policy&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Load balances the requests to increase performance with a custom policy&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/response-streaming/response-streaming.ipynb&quot;&gt;Response streaming&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Zero trust, keyless approach with manage identities and Azure API Management security features&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;To get responses sooner, you can &#39;stream&#39; the completion as it&#39;s being generated&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/vector-searching/vector-searching.ipynb&quot;&gt;Vector searching&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Zero trust, keyless approach with manage identities and Azure API Management security features&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;To ensure resilience, the request is distributed to two or more endpoints with the built-in feature&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Load balances the requests to increase performance with the built-in feature&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/built-in-logging/built-in-logging.ipynb&quot;&gt;Built-in logging&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Zero trust, keyless approach with manage identities and Azure API Management security features&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;To ensure resilience, the request is distributed to two or more endpoints with the built-in feature&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Load balances the requests to increase performance with the built-in feature&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Requests are logged to enable monitoring, alerting and automatic remediation&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Relation between Azure API Management subscription and token consumption allows cost control&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/labs/slm-self-hosting/slm-self-hosting.ipynb&quot;&gt;SLM self-hosting&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Self hosting the model might improve the security posture with network restrictions&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/#%EF%B8%8F-well-architected-framework&quot; title=&quot;Performance might be improved with full control to the self-hosted model&quot;&gt;â­&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Check the &lt;a href=&quot;https://learn.microsoft.com/azure/well-architected/service-guides/azure-openai&quot;&gt;Azure Well-Architected Framework perspective on Azure OpenAI Service&lt;/a&gt; for aditional guidance.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ðŸŽ’ Show and tell&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Install the &lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=evilz.vscode-reveal&quot;&gt;VS Code Reveal extension&lt;/a&gt;, open AI-GATEWAY.md and click on &#39;slides&#39; at the botton to present the AI Gateway without leaving VS Code. Or just open the &lt;a href=&quot;https://view.officeapps.live.com/op/view.aspx?src=https%3A%2F%2Fraw.githubusercontent.com%2FAzure-Samples%2FAI-Gateway%2Fmain%2FAI-GATEWAY.pptx&amp;amp;wdOrigin=BROWSELINK&quot;&gt;AI-GATEWAY.pptx&lt;/a&gt; for a plain old PowerPoint experience.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ðŸ¥‡ Other resources&lt;/h2&gt; 
&lt;p&gt;Numerous reference architectures, best practices and starter kits are available on this topic. Please refer to the resources provided if you need comprehensive solutions or a landing zone to initiate your project. We suggest leveraging the AI-Gateway labs to discover additional capabilities that can be integrated into the reference architectures.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genai-gateway&quot;&gt;GenAI Gateway Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/apim/genai/sample-app&quot;&gt;Azure OpenAI&amp;nbsp;+&amp;nbsp;APIM Sample&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://techcommunity.microsoft.com/t5/apps-on-azure-blog/ai-api-better-together-benefits-amp-best-practices-using-apis/ba-p/4157120&quot;&gt;AI+API better together: Benefits &amp;amp; Best Practices using APIs for AI workloads&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genai-gateway&quot;&gt;Designing and implementing a gateway solution with Azure OpenAI resources&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Azure/aoai-apim&quot;&gt;Azure OpenAI Using PTUs/TPMs With API Management - Using the Scaling Special Sauce&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/AzureOpenAI-with-APIM&quot;&gt;Manage Azure OpenAI using APIM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Azure/enterprise-azureai&quot;&gt;Setting up Azure OpenAI as a central capability with Azure API Management&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Azure/intro-to-intelligent-apps&quot;&gt;Introduction to Building AI Apps&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We believe that there may be valuable content that we are currently unaware of. We would greatly appreciate any suggestions or recommendations to enhance this list.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ðŸŒ WW GBB initiative&lt;/h3&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Azure-Samples/AI-Gateway/main/images/gbb.png&quot; alt=&quot;GBB&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;Disclaimer&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] This software is provided for demonstration purposes only. It is not intended to be relied upon for any purpose. The creators of this software make no representations or warranties of any kind, express or implied, about the completeness, accuracy, reliability, suitability or availability with respect to the software or the information, products, services, or related graphics contained in the software for any purpose. Any reliance you place on such information is therefore strictly at your own risk.&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>panaversity/learn-agentic-ai</title>
      <link>https://github.com/panaversity/learn-agentic-ai</link>
      <description>&lt;p&gt;Learn Agentic AI using OpenAI Agents SDK, Memory, MCP, Knowledge Graphs, LangGraph, and Autogen&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Learn Agentic AI&lt;/h1&gt; 
&lt;p&gt;This repo is part of the &lt;a href=&quot;https://docs.google.com/document/d/15usu1hkrrRLRjcq_3nCTT-0ljEcgiC44iSdvdqrCprk/edit?usp=sharing&quot;&gt;Panaversity Certified Agentic &amp;amp; Robotic AI Engineer&lt;/a&gt; program. It covers AI-201 and AI-202 courses.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/panaversity/learn-agentic-ai/main/toptrend.webp&quot; alt=&quot;Agentic AI Top Trend&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;Watch The NVIDIA CEO Jensen Huang Keynote at CES 2025&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=k82RwXqZHY8&quot; title=&quot;NVIDIA CEO Jensen Huang Keynote at CES 2025&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/panaversity/learn-agentic-ai/main/hr.jpeg&quot; alt=&quot;HR for Agents&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Reference:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/alexwang2911_aiagents-robotics-technology-activity-7282829390445453314-QLeS&quot;&gt;https://www.linkedin.com/posts/alexwang2911_aiagents-robotics-technology-activity-7282829390445453314-QLeS&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;AI-201: Fundamentals of Agentic AI - From Foundations to Autonomous Agents&lt;/h3&gt; 
&lt;p&gt;AI 201 Fundamentals of Agentic AI we cover chapters: 00-06&lt;/p&gt; 
&lt;p&gt;Kickstart your journey into Agentic AI! This course provides a rapid yet comprehensive introduction to Conversational, Generative, and Agentic AI. You&#39;ll master the foundational concepts using &lt;strong&gt;OpenAI Agents SDK&lt;/strong&gt;, then immediately build practical Conversational AI applications to understand human-AI interaction firsthand. The focus quickly shifts to Agentic Design Patterns, which you&#39;ll implement using OpenAI Agents SDK to create truly autonomous AI agents. You&#39;ll become proficient with OpenAI Agents SDK, developing agents ready for real-world tasks. Furthermore, you&#39;ll gain the unique skills to construct Model Context Protocol (MCP) servers and agents, enabling you to build next-generation augmented LLMs. Finally, we&#39;ll explore the groundbreaking potential of Agentic Payments, envisioning the future of AI in finance.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL0vKVrkG4hWovpr0FX6Gs-06hfsPDEUe6&quot;&gt;AI-201 Video Playlist&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Note: These videos are for additional learning, and do not cover all the material taught in the onsite classes.&lt;/p&gt; 
&lt;h3&gt;AI-202: Advanced Agentic AI Engineering - Master Enterprise-Scale AI Agent Development&lt;/h3&gt; 
&lt;p&gt;AI 202 Advanced Agentic AI we cover chapters: 06, 7a, 8, 8a, 9, 9a, 10, 10a, 11, and 12&lt;/p&gt; 
&lt;p&gt;Ready to engineer truly sophisticated AI agent systems? AI-202 builds upon your AI-201 foundation to propel you into advanced Agentic AI engineering. You&#39;ll master powerful frameworks like Microsoft AutoGen to construct complex agents for intricate tasks and advanced decision-making. Focusing on Agent-to-Agent communication and orchestration, you&#39;ll develop enterprise-ready multi-agent solutions. You&#39;ll build robust Model Context Protocol (MCP) servers, and then craft dynamic, user-centric agentic frontends with Next.js and TypeScript. The course culminates in a professional project where you&#39;ll design and deploy a complete enterprise-grade agentic solution, showcasing your mastery of cutting-edge AI technologies and your readiness for the forefront of the field.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>salesforce/LAVIS</title>
      <link>https://github.com/salesforce/LAVIS</link>
      <description>&lt;p&gt;LAVIS - A One-stop Library for Language-Vision Intelligence&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; &lt;br&gt; &lt;img src=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/docs/_static/logo_final.png&quot; width=&quot;400&quot;&gt; &lt;br&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://github.com/salesforce/LAVIS/releases&quot;&gt;&lt;img alt=&quot;Latest Release&quot; src=&quot;https://img.shields.io/github/release/salesforce/LAVIS.svg?sanitize=true&quot;&gt;&lt;/a&gt; 
 &lt;a href=&quot;https://opensource.salesforce.com/LAVIS/index.html&quot;&gt; &lt;img alt=&quot;docs&quot; src=&quot;https://github.com/salesforce/LAVIS/actions/workflows/docs.yaml/badge.svg?sanitize=true&quot;&gt; &lt;/a&gt;
 &lt;a href=&quot;https://opensource.org/licenses/BSD-3-Clause&quot;&gt; &lt;img alt=&quot;license&quot; src=&quot;https://img.shields.io/badge/License-BSD_3--Clause-blue.svg?sanitize=true&quot;&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://pepy.tech/project/salesforce-lavis&quot;&gt; &lt;img alt=&quot;Downloads&quot; src=&quot;https://pepy.tech/badge/salesforce-lavis&quot;&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://opensource.salesforce.com/LAVIS//latest/benchmark.html&quot;&gt;Benchmark&lt;/a&gt;, 
 &lt;a href=&quot;https://arxiv.org/abs/2209.09019&quot;&gt;Technical Report&lt;/a&gt;, 
 &lt;a href=&quot;https://opensource.salesforce.com/LAVIS//latest/index.html&quot;&gt;Documentation&lt;/a&gt;, 
 &lt;a href=&quot;https://github.com/salesforce/LAVIS/tree/main/examples&quot;&gt;Jupyter Notebook Examples&lt;/a&gt;, 
 &lt;a href=&quot;https://blog.salesforceairesearch.com/lavis-language-vision-library/&quot;&gt;Blog&lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;LAVIS - A Library for Language-Vision Intelligence&lt;/h1&gt; 
&lt;h2&gt;What&#39;s New: ðŸŽ‰&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Model Release] November 2023, released implementation of &lt;strong&gt;X-InstructBLIP&lt;/strong&gt; &lt;br&gt; &lt;a href=&quot;https://arxiv.org/pdf/2311.18799.pdf&quot;&gt;Paper&lt;/a&gt;, &lt;a href=&quot;https://github.com/salesforce/LAVIS/tree/main/projects/xinstructblip&quot;&gt;Project Page&lt;/a&gt;, &lt;a href=&quot;https://artemisp.github.io/X-InstructBLIP-page/&quot;&gt;Website&lt;/a&gt;, &lt;a href=&quot;https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/xinstructblip/demo/run_demo.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A simple, yet effective, cross-modality framework built atop frozen LLMs that allows the integration of various modalities (image, video, audio, 3D) without extensive modality-specific customization.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Model Release] July 2023, released implementation of &lt;strong&gt;BLIP-Diffusion&lt;/strong&gt; &lt;br&gt; &lt;a href=&quot;https://arxiv.org/abs/2305.06500&quot;&gt;Paper&lt;/a&gt;, &lt;a href=&quot;https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion&quot;&gt;Project Page&lt;/a&gt;, &lt;a href=&quot;https://dxli94.github.io/BLIP-Diffusion-website/&quot;&gt;Website&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A text-to-image generation model that trains 20x than DreamBooth. Also facilitates zero-shot subject-driven generation and editing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Model Release] May 2023, released implementation of &lt;strong&gt;InstructBLIP&lt;/strong&gt; &lt;br&gt; &lt;a href=&quot;https://arxiv.org/abs/2305.06500&quot;&gt;Paper&lt;/a&gt;, &lt;a href=&quot;https://github.com/salesforce/LAVIS/tree/main/projects/instructblip&quot;&gt;Project Page&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A new vision-language instruction-tuning framework using BLIP-2 models, achieving state-of-the-art zero-shot generalization performance on a wide range of vision-language tasks.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Model Release] Jan 2023, released implementation of &lt;strong&gt;BLIP-2&lt;/strong&gt; &lt;br&gt; &lt;a href=&quot;https://arxiv.org/abs/2301.12597&quot;&gt;Paper&lt;/a&gt;, &lt;a href=&quot;https://github.com/salesforce/LAVIS/tree/main/projects/blip2&quot;&gt;Project Page&lt;/a&gt;, &lt;a href=&quot;https://colab.research.google.com/github/salesforce/LAVIS/blob/main/examples/blip2_instructed_generation.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A generic and efficient pre-training strategy that easily harvests development of pretrained vision models and large language models (LLMs) for vision-language pretraining. BLIP-2 beats Flamingo on zero-shot VQAv2 (&lt;strong&gt;65.0&lt;/strong&gt; vs &lt;strong&gt;56.3&lt;/strong&gt;), establishing new state-of-the-art on zero-shot captioning (on NoCaps &lt;strong&gt;121.6&lt;/strong&gt; CIDEr score vs previous best &lt;strong&gt;113.2&lt;/strong&gt;). In addition, equipped with powerful LLMs (e.g. OPT, FlanT5), BLIP-2 also unlocks the new &lt;strong&gt;zero-shot instructed vision-to-language generation&lt;/strong&gt; capabilities for various interesting applications!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;Jan 2023, LAVIS is now available on &lt;a href=&quot;https://pypi.org/project/salesforce-lavis/&quot;&gt;PyPI&lt;/a&gt; for installation!&lt;/li&gt; 
 &lt;li&gt;[Model Release] Dec 2022, released implementation of &lt;strong&gt;Img2LLM-VQA&lt;/strong&gt; (&lt;strong&gt;CVPR 2023&lt;/strong&gt;, &lt;em&gt;&quot;From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models&quot;&lt;/em&gt;, by Jiaxian Guo et al) &lt;br&gt; &lt;a href=&quot;https://arxiv.org/pdf/2212.10846.pdf&quot;&gt;Paper&lt;/a&gt;, &lt;a href=&quot;https://github.com/salesforce/LAVIS/tree/main/projects/img2llm-vqa&quot;&gt;Project Page&lt;/a&gt;, &lt;a href=&quot;https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/img2llm-vqa/img2llm_vqa.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A plug-and-play module that enables off-the-shelf use of Large Language Models (LLMs) for visual question answering (VQA). Img2LLM-VQA surpasses Flamingo on zero-shot VQA on VQAv2 (61.9 vs 56.3), while in contrast requiring no end-to-end training!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Model Release] Oct 2022, released implementation of &lt;strong&gt;PNP-VQA&lt;/strong&gt; (&lt;strong&gt;EMNLP Findings 2022&lt;/strong&gt;, &lt;em&gt;&quot;Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training&quot;&lt;/em&gt;, by Anthony T.M.H. et al), &lt;br&gt; &lt;a href=&quot;https://arxiv.org/abs/2210.08773&quot;&gt;Paper&lt;/a&gt;, &lt;a href=&quot;https://github.com/salesforce/LAVIS/tree/main/projects/pnp-vqa&quot;&gt;Project Page&lt;/a&gt;, &lt;a href=&quot;https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/pnp-vqa/pnp_vqa.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt;&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A modular zero-shot VQA framework that requires no PLMs training, achieving SoTA zero-shot VQA performance.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Technical Report and Citing LAVIS&lt;/h2&gt; 
&lt;p&gt;You can find more details in our &lt;a href=&quot;https://arxiv.org/abs/2209.09019&quot;&gt;technical report&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;If you&#39;re using LAVIS in your research or applications, please cite it using this BibTeX&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inproceedings{li-etal-2023-lavis,
    title = &quot;{LAVIS}: A One-stop Library for Language-Vision Intelligence&quot;,
    author = &quot;Li, Dongxu  and
      Li, Junnan  and
      Le, Hung  and
      Wang, Guangsen  and
      Savarese, Silvio  and
      Hoi, Steven C.H.&quot;,
    booktitle = &quot;Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)&quot;,
    month = jul,
    year = &quot;2023&quot;,
    address = &quot;Toronto, Canada&quot;,
    publisher = &quot;Association for Computational Linguistics&quot;,
    url = &quot;https://aclanthology.org/2023.acl-demo.3&quot;,
    pages = &quot;31--41&quot;,
    abstract = &quot;We introduce LAVIS, an open-source deep learning library for LAnguage-VISion research and applications. LAVIS aims to serve as a one-stop comprehensive library that brings recent advancements in the language-vision field accessible for researchers and practitioners, as well as fertilizing future research and development. It features a unified interface to easily access state-of-the-art image-language, video-language models and common datasets. LAVIS supports training, evaluation and benchmarking on a rich variety of tasks, including multimodal classification, retrieval, captioning, visual question answering, dialogue and pre-training. In the meantime, the library is also highly extensible and configurable, facilitating future development and customization. In this technical report, we describe design principles, key components and functionalities of the library, and also present benchmarking results across common language-vision tasks.&quot;,
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#installation&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#getting-started&quot;&gt;Getting Started&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#model-zoo&quot;&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#image-captioning&quot;&gt;Image Captioning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#visual-question-answering-vqa&quot;&gt;Visual question answering (VQA)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#unified-feature-extraction-interface&quot;&gt;Unified Feature Extraction Interface&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#load-datasets&quot;&gt;Load Datasets&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#jupyter-notebook-examples&quot;&gt;Jupyter Notebook Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#resources-and-tools&quot;&gt;Resources and Tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#documentations&quot;&gt;Documentations&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#ethical-and-responsible-use&quot;&gt;Ethical and Responsible Use&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#technical-report-and-citing-lavis&quot;&gt;Technical Report and Citing LAVIS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/#license&quot;&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;LAVIS is a Python deep learning library for LAnguage-and-VISion intelligence research and applications. This library aims to provide engineers and researchers with a one-stop solution to rapidly develop models for their specific multimodal scenarios, and benchmark them across standard and customized datasets. It features a unified interface design to access&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;10+&lt;/strong&gt; tasks (retrieval, captioning, visual question answering, multimodal classification etc.);&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;20+&lt;/strong&gt; datasets (COCO, Flickr, Nocaps, Conceptual Commons, SBU, etc.);&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;30+&lt;/strong&gt; pretrained weights of state-of-the-art foundation language-vision models and their task-specific adaptations, including &lt;a href=&quot;https://arxiv.org/pdf/2107.07651.pdf&quot;&gt;ALBEF&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2201.12086.pdf&quot;&gt;BLIP&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2112.09583.pdf&quot;&gt;ALPRO&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2103.00020.pdf&quot;&gt;CLIP&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;br&gt; &lt;img src=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/assets/demo-6.png&quot;&gt; &lt;br&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Key features of LAVIS include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified and Modular Interface&lt;/strong&gt;: facilitating to easily leverage and repurpose existing modules (datasets, models, preprocessors), also to add new modules.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy Off-the-shelf Inference and Feature Extraction&lt;/strong&gt;: readily available pre-trained models let you take advantage of state-of-the-art multimodal understanding and generation capabilities on your own data.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reproducible Model Zoo and Training Recipes&lt;/strong&gt;: easily replicate and extend state-of-the-art models on existing and new tasks.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dataset Zoo and Automatic Downloading Tools&lt;/strong&gt;: it can be a hassle to prepare the many language-vision datasets. LAVIS provides automatic downloading scripts to help prepare a large variety of datasets and their annotations.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The following table shows the supported tasks, datasets and models in our library. This is a continuing effort and we are working on further growing the list.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;Tasks&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Supported Models&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Supported Datasets&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Image-text Pre-training&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;ALBEF, BLIP&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;COCO, VisualGenome, SBU ConceptualCaptions&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Image-text Retrieval&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;ALBEF, BLIP, CLIP&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;COCO, Flickr30k&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Text-image Retrieval&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;ALBEF, BLIP, CLIP&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;COCO, Flickr30k&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Visual Question Answering&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;ALBEF, BLIP&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;VQAv2, OKVQA, A-OKVQA&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Image Captioning&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;BLIP&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;COCO, NoCaps&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Image Classification&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;CLIP&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;ImageNet&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Natural Language Visual Reasoning (NLVR)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;ALBEF, BLIP&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;NLVR2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Visual Entailment (VE)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;ALBEF&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;SNLI-VE&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Visual Dialogue&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;BLIP&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;VisDial&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Video-text Retrieval&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;BLIP, ALPRO&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;MSRVTT, DiDeMo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Text-video Retrieval&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;BLIP, ALPRO&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;MSRVTT, DiDeMo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Video Question Answering (VideoQA)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;BLIP, ALPRO&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;MSRVTT, MSVD&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Video Dialogue&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;VGD-GPT&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;AVSD&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Multimodal Feature Extraction&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;ALBEF, CLIP, BLIP, ALPRO&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;customized&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Text-to-image Generation&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;[COMING SOON]&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;(Optional) Creating conda environment&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;conda create -n lavis python=3.8
conda activate lavis
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;install from &lt;a href=&quot;https://pypi.org/project/salesforce-lavis/&quot;&gt;PyPI&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install salesforce-lavis
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Or, for development, you may build from source&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/salesforce/LAVIS.git
cd LAVIS
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Model Zoo&lt;/h3&gt; 
&lt;p&gt;Model zoo summarizes supported models in LAVIS, to view:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from lavis.models import model_zoo
print(model_zoo)
# ==================================================
# Architectures                  Types
# ==================================================
# albef_classification           ve
# albef_feature_extractor        base
# albef_nlvr                     nlvr
# albef_pretrain                 base
# albef_retrieval                coco, flickr
# albef_vqa                      vqav2
# alpro_qa                       msrvtt, msvd
# alpro_retrieval                msrvtt, didemo
# blip_caption                   base_coco, large_coco
# blip_classification            base
# blip_feature_extractor         base
# blip_nlvr                      nlvr
# blip_pretrain                  base
# blip_retrieval                 coco, flickr
# blip_vqa                       vqav2, okvqa, aokvqa
# clip_feature_extractor         ViT-B-32, ViT-B-16, ViT-L-14, ViT-L-14-336, RN50
# clip                           ViT-B-32, ViT-B-16, ViT-L-14, ViT-L-14-336, RN50
# gpt_dialogue                   base
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Letâ€™s see how to use models in LAVIS to perform inference on example data. We first load a sample image from local.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch
from PIL import Image
# setup device to use
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
# load sample image
raw_image = Image.open(&quot;docs/_static/merlion.png&quot;).convert(&quot;RGB&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This example image shows &lt;a href=&quot;https://en.wikipedia.org/wiki/Merlion&quot;&gt;Merlion park&lt;/a&gt; (&lt;a href=&quot;https://theculturetrip.com/asia/singapore/articles/what-exactly-is-singapores-merlion-anyway/&quot;&gt;source&lt;/a&gt;), a landmark in Singapore.&lt;/p&gt; 
&lt;h3&gt;Image Captioning&lt;/h3&gt; 
&lt;p&gt;In this example, we use the BLIP model to generate a caption for the image. To make inference even easier, we also associate each pre-trained model with its preprocessors (transforms), accessed via &lt;code&gt;load_model_and_preprocess()&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch
from lavis.models import load_model_and_preprocess
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
# loads BLIP caption base model, with finetuned checkpoints on MSCOCO captioning dataset.
# this also loads the associated image processors
model, vis_processors, _ = load_model_and_preprocess(name=&quot;blip_caption&quot;, model_type=&quot;base_coco&quot;, is_eval=True, device=device)
# preprocess the image
# vis_processors stores image transforms for &quot;train&quot; and &quot;eval&quot; (validation / testing / inference)
image = vis_processors[&quot;eval&quot;](raw_image).unsqueeze(0).to(device)
# generate caption
model.generate({&quot;image&quot;: image})
# [&#39;a large fountain spewing water into the air&#39;]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Visual question answering (VQA)&lt;/h3&gt; 
&lt;p&gt;BLIP model is able to answer free-form questions about images in natural language. To access the VQA model, simply replace the &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;model_type&lt;/code&gt; arguments passed to &lt;code&gt;load_model_and_preprocess()&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from lavis.models import load_model_and_preprocess
model, vis_processors, txt_processors = load_model_and_preprocess(name=&quot;blip_vqa&quot;, model_type=&quot;vqav2&quot;, is_eval=True, device=device)
# ask a random question.
question = &quot;Which city is this photo taken?&quot;
image = vis_processors[&quot;eval&quot;](raw_image).unsqueeze(0).to(device)
question = txt_processors[&quot;eval&quot;](question)
model.predict_answers(samples={&quot;image&quot;: image, &quot;text_input&quot;: question}, inference_method=&quot;generate&quot;)
# [&#39;singapore&#39;]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Unified Feature Extraction Interface&lt;/h3&gt; 
&lt;p&gt;LAVIS provides a unified interface to extract features from each architecture. To extract features, we load the feature extractor variants of each model. The multimodal feature can be used for multimodal classification. The low-dimensional unimodal features can be used to compute cross-modal similarity.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from lavis.models import load_model_and_preprocess
model, vis_processors, txt_processors = load_model_and_preprocess(name=&quot;blip_feature_extractor&quot;, model_type=&quot;base&quot;, is_eval=True, device=device)
caption = &quot;a large fountain spewing water into the air&quot;
image = vis_processors[&quot;eval&quot;](raw_image).unsqueeze(0).to(device)
text_input = txt_processors[&quot;eval&quot;](caption)
sample = {&quot;image&quot;: image, &quot;text_input&quot;: [text_input]}

features_multimodal = model.extract_features(sample)
print(features_multimodal.multimodal_embeds.shape)
# torch.Size([1, 12, 768]), use features_multimodal[:,0,:] for multimodal classification tasks

features_image = model.extract_features(sample, mode=&quot;image&quot;)
features_text = model.extract_features(sample, mode=&quot;text&quot;)
print(features_image.image_embeds.shape)
# torch.Size([1, 197, 768])
print(features_text.text_embeds.shape)
# torch.Size([1, 12, 768])

# low-dimensional projected features
print(features_image.image_embeds_proj.shape)
# torch.Size([1, 197, 256])
print(features_text.text_embeds_proj.shape)
# torch.Size([1, 12, 256])
similarity = features_image.image_embeds_proj[:,0,:] @ features_text.text_embeds_proj[:,0,:].t()
print(similarity)
# tensor([[0.2622]])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Load Datasets&lt;/h3&gt; 
&lt;p&gt;LAVIS inherently supports a wide variety of common language-vision datasets by providing &lt;a href=&quot;https://opensource.salesforce.com/LAVIS//latest/benchmark&quot;&gt;automatic download tools&lt;/a&gt; to help download and organize these datasets. After downloading, to load the datasets, use the following code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from lavis.datasets.builders import dataset_zoo
dataset_names = dataset_zoo.get_names()
print(dataset_names)
# [&#39;aok_vqa&#39;, &#39;coco_caption&#39;, &#39;coco_retrieval&#39;, &#39;coco_vqa&#39;, &#39;conceptual_caption_12m&#39;,
#  &#39;conceptual_caption_3m&#39;, &#39;didemo_retrieval&#39;, &#39;flickr30k&#39;, &#39;imagenet&#39;, &#39;laion2B_multi&#39;,
#  &#39;msrvtt_caption&#39;, &#39;msrvtt_qa&#39;, &#39;msrvtt_retrieval&#39;, &#39;msvd_caption&#39;, &#39;msvd_qa&#39;, &#39;nlvr&#39;,
#  &#39;nocaps&#39;, &#39;ok_vqa&#39;, &#39;sbu_caption&#39;, &#39;snli_ve&#39;, &#39;vatex_caption&#39;, &#39;vg_caption&#39;, &#39;vg_vqa&#39;]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After downloading the images, we can use &lt;code&gt;load_dataset()&lt;/code&gt; to obtain the dataset.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from lavis.datasets.builders import load_dataset
coco_dataset = load_dataset(&quot;coco_caption&quot;)
print(coco_dataset.keys())
# dict_keys([&#39;train&#39;, &#39;val&#39;, &#39;test&#39;])
print(len(coco_dataset[&quot;train&quot;]))
# 566747
print(coco_dataset[&quot;train&quot;][0])
# {&#39;image&#39;: &amp;lt;PIL.Image.Image image mode=RGB size=640x480&amp;gt;,
#  &#39;text_input&#39;: &#39;A woman wearing a net on her head cutting a cake. &#39;,
#  &#39;image_id&#39;: 0}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you already host a local copy of the dataset, you can pass in the &lt;code&gt;vis_path&lt;/code&gt; argument to change the default location to load images.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;coco_dataset = load_dataset(&quot;coco_caption&quot;, vis_path=YOUR_LOCAL_PATH)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Jupyter Notebook Examples&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://github.com/salesforce/LAVIS/tree/main/examples&quot;&gt;examples&lt;/a&gt; for more inference examples, e.g. captioning, feature extraction, VQA, GradCam, zeros-shot classification.&lt;/p&gt; 
&lt;h2&gt;Resources and Tools&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmarks&lt;/strong&gt;: see &lt;a href=&quot;https://opensource.salesforce.com/LAVIS//latest/benchmark&quot;&gt;Benchmark&lt;/a&gt; for instructions to evaluate and train supported models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dataset Download and Browsing&lt;/strong&gt;: see &lt;a href=&quot;https://opensource.salesforce.com/LAVIS//latest/benchmark&quot;&gt;Dataset Download&lt;/a&gt; for instructions and automatic tools on download common language-vision datasets.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GUI Demo&lt;/strong&gt;: to run the demo locally, run &lt;code&gt;bash run_scripts/run_demo.sh&lt;/code&gt; and then follow the instruction on the prompts to view in browser. A web demo is coming soon.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Documentations&lt;/h2&gt; 
&lt;p&gt;For more details and advanced usages, please refer to &lt;a href=&quot;https://opensource.salesforce.com/LAVIS//latest/index.html#&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Ethical and Responsible Use&lt;/h2&gt; 
&lt;p&gt;We note that models in LAVIS provide no guarantees on their multimodal abilities; incorrect or biased predictions may be observed. In particular, the datasets and pretrained models utilized in LAVIS may contain socioeconomic biases which could result in misclassification and other unwanted behaviors such as offensive or inappropriate speech. We strongly recommend that users review the pre-trained models and overall system in LAVIS before practical adoption. We plan to improve the library by investigating and mitigating these potential biases and inappropriate behaviors in the future.&lt;/p&gt; 
&lt;h2&gt;Contact us&lt;/h2&gt; 
&lt;p&gt;If you have any questions, comments or suggestions, please do not hesitate to contact us at &lt;a href=&quot;mailto:lavis@salesforce.com&quot;&gt;lavis@salesforce.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/salesforce/LAVIS/main/LICENSE.txt&quot;&gt;BSD 3-Clause License&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MLEveryday/practicalAI-cn</title>
      <link>https://github.com/MLEveryday/practicalAI-cn</link>
      <description>&lt;p&gt;AIå®žæˆ˜-practicalAI ä¸­æ–‡ç‰ˆ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AIå®žæˆ˜-&lt;a href=&quot;https://github.com/LisonEvf/practicalAI-cn&quot;&gt;practicalAI&lt;/a&gt; ä¸­æ–‡ç‰ˆ&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/launch-Google%20Colab-orange.svg?sanitize=true&quot; alt=&quot;Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/LisonEvf/practicalAI-cn/raw/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-MIT-brightgreen.svg?sanitize=true&quot; alt=&quot;MIT&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/GokuMohandas&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Author-GokuMohandas-blue.svg?sanitize=true&quot; alt=&quot;Author&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/MLEveryday/practicalAI-cn&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Fork-MLEveryday/practicalAI--cn-yellow.svg?sanitize=true&quot; alt=&quot;Fork&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;è®©ä½ æœ‰èƒ½åŠ›ä½¿ç”¨æœºå™¨å­¦ä¹ ä»Žæ•°æ®ä¸­èŽ·å–æœ‰ä»·å€¼çš„è§è§£ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ðŸ”¥ ä½¿ç”¨ &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt; å®žçŽ°åŸºæœ¬çš„æœºå™¨å­¦ä¹ ç®—æ³•å’Œæ·±åº¦ç¥žç»ç½‘ç»œã€‚&lt;/li&gt; 
 &lt;li&gt;ðŸ–¥ï¸ ä¸éœ€è¦ä»»ä½•è®¾ç½®ï¼Œåœ¨æµè§ˆå™¨ä¸­ä½¿ç”¨ &lt;a href=&quot;https://colab.research.google.com/&quot;&gt;Google Colab&lt;/a&gt; è¿è¡Œæ‰€æœ‰ç¨‹åºã€‚&lt;/li&gt; 
 &lt;li&gt;ðŸ“¦ ä¸ä»…ä»…æ˜¯æ•™ç¨‹ï¼Œè€Œæ˜¯å­¦ä¹ äº§å“çº§çš„é¢å‘å¯¹è±¡æœºå™¨å­¦ä¹ ç¼–ç¨‹ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Notebooks&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;åŸºç¡€&lt;/th&gt; 
   &lt;th&gt;æ·±åº¦å­¦ä¹ &lt;/th&gt; 
   &lt;th&gt;è¿›é˜¶&lt;/th&gt; 
   &lt;th&gt;ä¸»é¢˜&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ðŸ““ &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/00_Notebooks.ipynb&quot;&gt;Notebooks&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ðŸ”¥ &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/07_PyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ðŸ“š &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/14_Advanced_RNNs.ipynb&quot;&gt;é«˜çº§å¾ªçŽ¯ç¥žç»ç½‘ç»œ Advanced RNNs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ðŸ“¸ &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/15_Computer_Vision.ipynb&quot;&gt;è®¡ç®—æœºè§†è§‰ Computer Vision&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ðŸ &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/01_Python.ipynb&quot;&gt;Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ðŸŽ›ï¸ &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/08_Multilayer_Perceptron.ipynb&quot;&gt;å¤šå±‚æ„ŸçŸ¥ Multilayer Perceptrons&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ðŸŽï¸ Highway and Residual Networks&lt;/td&gt; 
   &lt;td&gt;â° æ—¶é—´åºåˆ—åˆ†æž Time Series Analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ðŸ”¢ &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/02_NumPy.ipynb&quot;&gt;NumPy&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ðŸ”Ž &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/09_Data_and_Models.ipynb&quot;&gt;æ•°æ®å’Œæ¨¡åž‹ Data &amp;amp; Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ðŸ”® è‡ªç¼–ç å™¨ Autoencoders&lt;/td&gt; 
   &lt;td&gt;ðŸ˜ï¸ Topic Modeling&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ðŸ¼ &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/03_Pandas.ipynb&quot;&gt;Pandas&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ðŸ“¦ &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/10_Object_Oriented_ML.ipynb&quot;&gt;é¢å‘å¯¹è±¡çš„æœºå™¨å­¦ä¹  Object-Oriented ML&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ðŸŽ­ ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ Generative Adversarial Networks&lt;/td&gt; 
   &lt;td&gt;ðŸ›’ æŽ¨èç³»ç»Ÿ Recommendation Systems&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ðŸ“ˆ &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/04_Linear_Regression.ipynb&quot;&gt;çº¿æ€§å›žå½’ Linear Regression&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ðŸ–¼ï¸ &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/11_Convolutional_Neural_Networks.ipynb&quot;&gt;å·ç§¯ç¥žç»ç½‘ç»œ Convolutional Neural Networks&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ðŸ ç©ºé—´å˜æ¢æ¨¡åž‹ Spatial Transformer Networks&lt;/td&gt; 
   &lt;td&gt;ðŸ—£ï¸ é¢„è®­ç»ƒè¯­è¨€æ¨¡åž‹ Pretrained Language Modeling&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ðŸ“Š &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/05_Logistic_Regression.ipynb&quot;&gt;é€»è¾‘å›žå½’ Logistic Regression&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ðŸ“ &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/12_Embeddings.ipynb&quot;&gt;åµŒå…¥å±‚ Embeddings&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;ðŸ¤· å¤šä»»åŠ¡å­¦ä¹  Multitask Learning&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ðŸŒ³ &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/06_Random_Forests.ipynb&quot;&gt;éšæœºæ£®æž— Random Forests&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ðŸ“— &lt;a href=&quot;https://nbviewer.jupyter.org/github/LisonEvf/practicalAI-cn/blob/master/notebooks/13_Recurrent_Neural_Networks.ipynb&quot;&gt;é€’å½’ç¥žç»ç½‘ç»œ Recurrent Neural Networks&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;ðŸŽ¯ Low Shot Learning&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ðŸ’¥ k-å‡å€¼èšç±» KMeans Clustering&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;ðŸ’ å¼ºåŒ–å­¦ä¹  Reinforcement Learning&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;æŸ¥çœ‹ notebooks&lt;/h2&gt; 
&lt;p&gt;å¦‚æžœä¸éœ€è¦è¿è¡Œ notebooksï¼Œä½¿ç”¨ Jupyter nbviewer å°±å¯ä»¥æ–¹ä¾¿åœ°æŸ¥çœ‹å®ƒä»¬ã€‚&lt;/p&gt; 
&lt;p&gt;å°† &lt;code&gt;https://github.com/&lt;/code&gt; æ›¿æ¢ä¸º &lt;code&gt;https://nbviewer.jupyter.org/github/&lt;/code&gt; ï¼Œæˆ–è€…æ‰“å¼€ &lt;code&gt;https://nbviewer.jupyter.org&lt;/code&gt; å¹¶è¾“å…¥ notebook çš„ URLã€‚&lt;/p&gt; 
&lt;h2&gt;è¿è¡Œ notebooks&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;åœ¨æœ¬é¡¹ç›®çš„ &lt;a href=&quot;https://raw.githubusercontent.com/MLEveryday/practicalAI-cn/master/notebooks/&quot;&gt;&lt;code&gt;notebooks&lt;/code&gt;&lt;/a&gt; æ–‡ä»¶å¤¹èŽ·å– notebookï¼›&lt;/li&gt; 
 &lt;li&gt;ä½ å¯ä»¥åœ¨ Google Colabï¼ˆæŽ¨èï¼‰æˆ–æœ¬åœ°ç”µè„‘è¿è¡Œè¿™äº› notebookï¼›&lt;/li&gt; 
 &lt;li&gt;ç‚¹å‡»ä¸€ä¸ª notebookï¼Œç„¶åŽæ›¿æ¢URLåœ°å€ä¸­ &lt;code&gt;https://github.com/&lt;/code&gt; ä¸º &lt;code&gt;https://colab.research.google.com/github/&lt;/code&gt; ï¼Œæˆ–è€…ä½¿ç”¨è¿™ä¸ª &lt;a href=&quot;https://chrome.google.com/webstore/detail/open-in-colab/iogfkhleblhcpcekbiedikdehleodpjo&quot;&gt;Chromeæ‰©å±•&lt;/a&gt; ä¸€é”®å®Œæˆï¼›&lt;/li&gt; 
 &lt;li&gt;ç™»å½•ä½ è‡ªå·±çš„ Google è´¦æˆ·ï¼›&lt;/li&gt; 
 &lt;li&gt;ç‚¹å‡»å·¥å…·æ ä¸Šçš„ &lt;code&gt;å¤åˆ¶åˆ°äº‘ç«¯ç¡¬ç›˜&lt;/code&gt;ï¼Œä¼šåœ¨ä¸€ä¸ªæ–°çš„æ ‡ç­¾é¡µæ‰“å¼€ notebookï¼›&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/MLEveryday/practicalAI-cn/master/images/copy_to_drive.png&quot;&gt; 
&lt;ol start=&quot;5&quot;&gt; 
 &lt;li&gt;é€šè¿‡åŽ»æŽ‰æ ‡é¢˜ä¸­çš„&lt;code&gt;å‰¯æœ¬&lt;/code&gt;å®Œæˆ notebook é‡å‘½åï¼›&lt;/li&gt; 
 &lt;li&gt;è¿è¡Œä»£ç ã€ä¿®æ”¹ç­‰ï¼Œæ‰€æœ‰è¿™äº›éƒ½ä¼šè‡ªåŠ¨ä¿å­˜åˆ°ä½ çš„ä¸ªäºº Google Driveã€‚&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;è´¡çŒ® notebooks&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;ä¿®æ”¹åŽä¸‹è½½ Google Colab notebook ä¸º .ipynb æ–‡ä»¶ï¼›&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/MLEveryday/practicalAI-cn/master/images/download_ipynb.png&quot;&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;è½¬åˆ° &lt;a href=&quot;https://github.com/LisonEvf/practicalAI-cn/tree/master/notebooks&quot;&gt;https://github.com/LisonEvf/practicalAI-cn/tree/master/notebooks&lt;/a&gt; ï¼›&lt;/li&gt; 
 &lt;li&gt;ç‚¹å‡» &lt;code&gt;Upload files&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/MLEveryday/practicalAI-cn/master/images/upload.png&quot;&gt; 
&lt;ol start=&quot;5&quot;&gt; 
 &lt;li&gt;ä¸Šä¼ è¿™ä¸ª .ipynb æ–‡ä»¶ï¼›&lt;/li&gt; 
 &lt;li&gt;å†™ä¸€ä¸ªè¯¦ç»†è¯¦ç»†çš„æäº¤æ ‡é¢˜å’Œè¯´æ˜Žï¼›&lt;/li&gt; 
 &lt;li&gt;é€‚å½“å‘½åä½ çš„åˆ†æ”¯ï¼›&lt;/li&gt; 
 &lt;li&gt;ç‚¹å‡» &lt;code&gt;Propose changes&lt;/code&gt;ã€‚&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/MLEveryday/practicalAI-cn/master/images/commit.png&quot;&gt; 
&lt;h2&gt;è´¡çŒ®åˆ—è¡¨&lt;/h2&gt; 
&lt;p&gt;æ¬¢è¿Žä»»ä½•äººå‚ä¸Žå’Œå®Œå–„ã€‚&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Notebook&lt;/th&gt; 
   &lt;th&gt;è¯‘è€…&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;00_Notebooks.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/amusi&quot;&gt;@amusi&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;01_Python.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/amusi&quot;&gt;@amusi&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;02_NumPy.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/amusi&quot;&gt;@amusi&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;03_Pandas.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/amusi&quot;&gt;@amusi&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;04_Linear_Regression.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/jasonhhao&quot;&gt;@jasonhhao&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;05_Logistic_Regression.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/jasonhhao&quot;&gt;@jasonhhao&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;06_Random_Forests.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/jasonhhao&quot;&gt;@jasonhhao&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;07_PyTorch.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/amusi&quot;&gt;@amusi&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;08_Multilayer_Perceptron.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/zhyongquan&quot;&gt;@zhyongquan&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;09_Data_and_Models.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/zhyongquan&quot;&gt;@zhyongquan&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10_Object_Oriented_ML.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/zhyongquan&quot;&gt;@zhyongquan&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11_Convolutional_Neural_Networks.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12_Embeddings.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/wengJJ&quot;&gt;@wengJJ&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13_Recurrent_Neural_Networks.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14_Advanced_RNNs.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;15_Computer_Vision.ipynb&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>causify-ai/kaizenflow</title>
      <link>https://github.com/causify-ai/kaizenflow</link>
      <description>&lt;p&gt;KaizenFlow is a framework for Bayesian reasoning and AI/ML stream computing&lt;/p&gt;&lt;hr&gt;&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/causify-ai/kaizenflow/master/#kaizen-technologies-internal-documentation&quot;&gt;Kaizen Technologies internal documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- tocstop --&gt; 
&lt;h1&gt;Kaizen Technologies internal documentation&lt;/h1&gt; 
&lt;p&gt;Welcome to the internal documentation pages of Kaizen Technologies.&lt;/p&gt; 
&lt;p&gt;To start exploring the repository refer to &lt;a href=&quot;https://raw.githubusercontent.com/causify-ai/kaizenflow/master/all.workflow.explanation.md&quot;&gt;workflows reference&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/AI-For-Beginners</title>
      <link>https://github.com/microsoft/AI-For-Beginners</link>
      <description>&lt;p&gt;12 Weeks, 24 Lessons, AI for All!&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/microsoft/AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub license&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/AI-For-Beginners/graphs/contributors/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/microsoft/AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub contributors&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/AI-For-Beginners/issues/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/microsoft/AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub issues&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/AI-For-Beginners/pulls/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr/microsoft/AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub pull-requests&quot;&gt;&lt;/a&gt; &lt;a href=&quot;http://makeapullrequest.com&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://GitHub.com/microsoft/AI-For-Beginners/watchers/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/watchers/microsoft/AI-For-Beginners.svg?style=social&amp;amp;label=Watch&quot; alt=&quot;GitHub watchers&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/AI-For-Beginners/network/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/microsoft/AI-For-Beginners.svg?style=social&amp;amp;label=Fork&quot; alt=&quot;GitHub forks&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/AI-For-Beginners/stargazers/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/microsoft/AI-For-Beginners.svg?style=social&amp;amp;label=Star&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://mybinder.org/v2/gh/microsoft/ai-for-beginners/HEAD&quot;&gt;&lt;img src=&quot;https://mybinder.org/badge_logo.svg?sanitize=true&quot; alt=&quot;Binder&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://gitter.im/Microsoft/ai-for-beginners?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&quot;&gt;&lt;img src=&quot;https://badges.gitter.im/Microsoft/ai-for-beginners.svg?sanitize=true&quot; alt=&quot;Gitter&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/zxKYvhSnVp?WT.mc_id=academic-000002-leestott&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/ByRwuEEgH4&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Artificial Intelligence for Beginners - A Curriculum&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/sketchnotes/ai-overview.png&quot; alt=&quot; Sketchnote by (@girlie_mac) &quot;&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;AI For Beginners - &lt;em&gt;Sketchnote by &lt;a href=&quot;https://twitter.com/girlie_mac&quot;&gt;@girlie_mac&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Explore the world of &lt;strong&gt;Artificial Intelligence&lt;/strong&gt; (AI) with our 12-week, 24-lesson curriculum! It includes practical lessons, quizzes, and labs. The curriculum is beginner-friendly and covers tools like TensorFlow and PyTorch, as well as ethics in AI&lt;/p&gt; 
&lt;h2&gt;What you will learn&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://soshnikov.com/courses/ai-for-beginners/mindmap.html&quot;&gt;Mindmap of the Course&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;In this curriculum, you will learn:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Different approaches to Artificial Intelligence, including the &quot;good old&quot; symbolic approach with &lt;strong&gt;Knowledge Representation&lt;/strong&gt; and reasoning (&lt;a href=&quot;https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence&quot;&gt;GOFAI&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Neural Networks&lt;/strong&gt; and &lt;strong&gt;Deep Learning&lt;/strong&gt;, which are at the core of modern AI. We will illustrate the concepts behind these important topics using code in two of the most popular frameworks - &lt;a href=&quot;http://Tensorflow.org&quot;&gt;TensorFlow&lt;/a&gt; and &lt;a href=&quot;http://pytorch.org&quot;&gt;PyTorch&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Neural Architectures&lt;/strong&gt; for working with images and text. We will cover recent models but may be a bit lacking in the state-of-the-art.&lt;/li&gt; 
 &lt;li&gt;Less popular AI approaches, such as &lt;strong&gt;Genetic Algorithms&lt;/strong&gt; and &lt;strong&gt;Multi-Agent Systems&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;What we will not cover in this curriculum:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href=&quot;https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Find all additional resources for this course in our Microsoft Learn collection&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;Business cases of using &lt;strong&gt;AI in Business&lt;/strong&gt;. Consider taking &lt;a href=&quot;https://docs.microsoft.com/learn/paths/introduction-ai-for-business-users/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Introduction to AI for business users&lt;/a&gt; learning path on Microsoft Learn, or &lt;a href=&quot;https://www.microsoft.com/ai/ai-business-school/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;AI Business School&lt;/a&gt;, developed in cooperation with &lt;a href=&quot;https://www.insead.edu/&quot;&gt;INSEAD&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Classic Machine Learning&lt;/strong&gt;, which is well described in our &lt;a href=&quot;http://github.com/Microsoft/ML-for-Beginners&quot;&gt;Machine Learning for Beginners Curriculum&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Practical AI applications built using &lt;strong&gt;&lt;a href=&quot;https://azure.microsoft.com/services/cognitive-services/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Cognitive Services&lt;/a&gt;&lt;/strong&gt;. For this, we recommend that you start with modules Microsoft Learn for &lt;a href=&quot;https://docs.microsoft.com/learn/paths/create-computer-vision-solutions-azure-cognitive-services/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;vision&lt;/a&gt;, &lt;a href=&quot;https://docs.microsoft.com/learn/paths/explore-natural-language-processing/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;natural language processing&lt;/a&gt;, &lt;strong&gt;&lt;a href=&quot;https://learn.microsoft.com/en-us/training/paths/develop-ai-solutions-azure-openai/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Generative AI with Azure OpenAI Service&lt;/a&gt;&lt;/strong&gt; and others.&lt;/li&gt; 
 &lt;li&gt;Specific ML &lt;strong&gt;Cloud Frameworks&lt;/strong&gt;, such as &lt;a href=&quot;https://azure.microsoft.com/services/machine-learning/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Azure Machine Learning&lt;/a&gt;, &lt;a href=&quot;https://learn.microsoft.com/en-us/training/paths/get-started-fabric/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Microsoft Fabric&lt;/a&gt;, or &lt;a href=&quot;https://docs.microsoft.com/learn/paths/data-engineer-azure-databricks?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Azure Databricks&lt;/a&gt;. Consider using &lt;a href=&quot;https://docs.microsoft.com/learn/paths/build-ai-solutions-with-azure-ml-service/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Build and operate machine learning solutions with Azure Machine Learning&lt;/a&gt; and &lt;a href=&quot;https://docs.microsoft.com/learn/paths/build-operate-machine-learning-solutions-azure-databricks/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Build and Operate Machine Learning Solutions with Azure Databricks&lt;/a&gt; learning paths.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Conversational AI&lt;/strong&gt; and &lt;strong&gt;Chat Bots&lt;/strong&gt;. There is a separate &lt;a href=&quot;https://docs.microsoft.com/learn/paths/create-conversational-ai-solutions/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Create conversational AI solutions&lt;/a&gt; learning path, and you can also refer to &lt;a href=&quot;https://soshnikov.com/azure/hello-bot-conversational-ai-on-microsoft-platform/&quot;&gt;this blog post&lt;/a&gt; for more detail.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep Mathematics&lt;/strong&gt; behind deep learning. For this, we would recommend &lt;a href=&quot;https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618&quot;&gt;Deep Learning&lt;/a&gt; by Ian Goodfellow, Yoshua Bengio and Aaron Courville, which is also available online at &lt;a href=&quot;https://www.deeplearningbook.org/&quot;&gt;https://www.deeplearningbook.org/&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For a gentle introduction to &lt;em&gt;AI in the Cloud&lt;/em&gt; topics you may consider taking the &lt;a href=&quot;https://docs.microsoft.com/learn/paths/get-started-with-artificial-intelligence-on-azure/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Get started with artificial intelligence on Azure&lt;/a&gt; Learning Path.&lt;/p&gt; 
&lt;h1&gt;Content&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Lesson Link&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;PyTorch/Keras/TensorFlow&lt;/th&gt; 
   &lt;th&gt;Lab&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;0&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/0-course-setup/setup.md&quot;&gt;Course Setup&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/0-course-setup/how-to-run.md&quot;&gt;Setup Your Development Environment&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;I&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/1-Intro/README.md&quot;&gt;&lt;strong&gt;Introduction to AI&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;01&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/1-Intro/README.md&quot;&gt;Introduction and History of AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;II&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;Symbolic AI&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;02&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/2-Symbolic/README.md&quot;&gt;Knowledge Representation and Expert Systems&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/2-Symbolic/Animals.ipynb&quot;&gt;Expert Systems&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/2-Symbolic/FamilyOntology.ipynb&quot;&gt;Ontology&lt;/a&gt; /&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/2-Symbolic/MSConceptGraph.ipynb&quot;&gt;Concept Graph&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;III&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/README.md&quot;&gt;&lt;strong&gt;Introduction to Neural Networks&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;03&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/03-Perceptron/README.md&quot;&gt;Perceptron&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/3-NeuralNetworks/03-Perceptron/Perceptron.ipynb&quot;&gt;Notebook&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/03-Perceptron/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;04&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/04-OwnFramework/README.md&quot;&gt;Multi-Layered Perceptron and Creating our own Framework&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb&quot;&gt;Notebook&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/04-OwnFramework/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;05&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/README.md&quot;&gt;Intro to Frameworks (PyTorch/TensorFlow) and Overfitting&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb&quot;&gt;Keras&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;IV&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/README.md&quot;&gt;&lt;strong&gt;Computer Vision&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.microsoft.com/learn/modules/intro-computer-vision-pytorch/?WT.mc_id=academic-77998-cacaste&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://docs.microsoft.com/learn/modules/intro-computer-vision-TensorFlow/?WT.mc_id=academic-77998-cacaste&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Explore Computer Vision on Microsoft Azure&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;06&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/06-IntroCV/README.md&quot;&gt;Intro to Computer Vision. OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/06-IntroCV/OpenCV.ipynb&quot;&gt;Notebook&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/06-IntroCV/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;07&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/README.md&quot;&gt;Convolutional Neural Networks&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/CNN_Architectures.md&quot;&gt;CNN Architectures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/07-ConvNets/ConvNetsPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; /&lt;a href=&quot;https://microsoft.github.io/AI-For-Beginners/lessons/4-ComputerVision/07-ConvNets/ConvNetsTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;08&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/README.md&quot;&gt;Pre-trained Networks and Transfer Learning&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/TrainingTricks.md&quot;&gt;Training Tricks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/08-TransferLearning/TransferLearningPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;09&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/09-Autoencoders/README.md&quot;&gt;Autoencoders and VAEs&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;10&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/10-GANs/README.md&quot;&gt;Generative Adversarial Networks &amp;amp; Artistic Style Transfer&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/10-GANs/GANPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/10-GANs/GANTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;11&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/11-ObjectDetection/README.md&quot;&gt;Object Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/11-ObjectDetection/ObjectDetection.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/11-ObjectDetection/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;12&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/12-Segmentation/README.md&quot;&gt;Semantic Segmentation. U-Net&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/4-ComputerVision/12-Segmentation/SemanticSegmentationPytorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;(https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/4-ComputerVision/12-Segmentation/SemanticSegmentationTF.ipynb)&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;V&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/README.md&quot;&gt;&lt;strong&gt;Natural Language Processing&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste&quot;&gt;PyTorch&lt;/a&gt; /&lt;a href=&quot;https://docs.microsoft.com/learn/modules/intro-natural-language-processing-TensorFlow/?WT.mc_id=academic-77998-cacaste&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Explore Natural Language Processing on Microsoft Azure&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;13&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/13-TextRep/README.md&quot;&gt;Text Representation. Bow/TF-IDF&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;14&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/14-Embeddings/README.md&quot;&gt;Semantic word embeddings. Word2Vec and GloVe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;15&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/15-LanguageModeling/README.md&quot;&gt;Language Modeling. Training your own embeddings&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/15-LanguageModeling/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;16&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/16-RNN/README.md&quot;&gt;Recurrent Neural Networks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/16-RNN/RNNPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/16-RNN/RNNTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;17&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/17-GenerativeNetworks/README.md&quot;&gt;Generative Recurrent Networks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://microsoft.github.io/AI-For-Beginners/lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.md&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://microsoft.github.io/AI-For-Beginners/lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.md&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/17-GenerativeNetworks/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;18&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/18-Transformers/READMEtransformers.md&quot;&gt;Transformers. BERT.&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; /&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/18-Transformers/TransformersTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;19&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/19-NER/README.md&quot;&gt;Named Entity Recognition&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://microsoft.github.io/AI-For-Beginners/lessons/5-NLP/19-NER/NER-TF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/19-NER/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;20&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/20-LangModels/READMELargeLang.md&quot;&gt;Large Language Models, Prompt Programming and Few-Shot Tasks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://microsoft.github.io/AI-For-Beginners/lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;VI&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;Other AI Techniques&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;21&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/21-GeneticAlgorithms/README.md&quot;&gt;Genetic Algorithms&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/6-Other/21-GeneticAlgorithms/Genetic.ipynb&quot;&gt;Notebook&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;22&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/22-DeepRL/README.md&quot;&gt;Deep Reinforcement Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; /&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/22-DeepRL/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;23&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/23-MultiagentSystems/README.md&quot;&gt;Multi-Agent Systems&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;VII&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;AI Ethics&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;24&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/7-Ethics/README.md&quot;&gt;AI Ethics and Responsible AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.microsoft.com/learn/paths/responsible-ai-business-principles/?WT.mc_id=academic-77998-cacaste&quot;&gt;Microsoft Learn: Responsible AI Principles&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;IX&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;Extras&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;25&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/X-Extras/X1-MultiModal/README.md&quot;&gt;Multi-Modal Networks, CLIP and VQGAN&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/X-Extras/X1-MultiModal/Clip.ipynb&quot;&gt;Notebook&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Each lesson contains&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pre-reading material&lt;/li&gt; 
 &lt;li&gt;Executable Jupyter Notebooks, which are often specific to the framework (&lt;strong&gt;PyTorch&lt;/strong&gt; or &lt;strong&gt;TensorFlow&lt;/strong&gt;). The executable notebook also contains a lot of theoretical material, so to understand the topic you need to go through at least one version of the notebook (either PyTorch or TensorFlow).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Labs&lt;/strong&gt; available for some topics, which give you an opportunity to try applying the material you have learned to a specific problem.&lt;/li&gt; 
 &lt;li&gt;Some sections contain links to &lt;a href=&quot;https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum&quot;&gt;&lt;strong&gt;MS Learn&lt;/strong&gt;&lt;/a&gt; modules that cover related topics.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;We have created a &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/0-course-setup/setup.md&quot;&gt;setup lesson&lt;/a&gt; to help you with setting up your development environment. - For Educators, we have created a &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/0-course-setup/for-teachers.md&quot;&gt;curricula setup lesson&lt;/a&gt; for you too!&lt;/li&gt; 
 &lt;li&gt;How to &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/0-course-setup/how-to-run.md&quot;&gt;Run the code in a VSCode or a Codepace&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Follow these steps:&lt;/p&gt; 
&lt;p&gt;Fork the Repository: Click on the &quot;Fork&quot; button at the top-right corner of this page.&lt;/p&gt; 
&lt;p&gt;Clone the Repository: &lt;code&gt;git clone https://github.com/microsoft/AI-For-Beginners.git&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Don&#39;t forget to star (ðŸŒŸ) this repo to find it easier later.&lt;/p&gt; 
&lt;h2&gt;Meet other Learners&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href=&quot;https://aka.ms/genai-discord?WT.mc_id=academic-105485-bethanycheum&quot;&gt;official AI Discord server&lt;/a&gt; to meet and network with other learners taking this course and get support.&lt;/p&gt; 
&lt;h2&gt;Quizzes&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;A note about quizzes&lt;/strong&gt;: All quizzes are contained in the Quiz-app folder in etc\quiz-app, They are linked from within the lessons the quiz app can be run locally or deployed to Azure; follow the instruction in the &lt;code&gt;quiz-app&lt;/code&gt; folder. They are gradually being localized.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Help Wanted&lt;/h2&gt; 
&lt;p&gt;Do you have suggestions or found spelling or code errors? Raise an issue or create a pull request.&lt;/p&gt; 
&lt;h2&gt;Special Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;âœï¸ Primary Author:&lt;/strong&gt; &lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry Soshnikov&lt;/a&gt;, PhD&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ðŸ”¥ Editor:&lt;/strong&gt; &lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen Looper&lt;/a&gt;, PhD&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ðŸŽ¨ Sketchnote illustrator:&lt;/strong&gt; &lt;a href=&quot;https://twitter.com/girlie_mac&quot;&gt;Tomomi Imura&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;âœ… Quiz Creator:&lt;/strong&gt; &lt;a href=&quot;https://github.com/CinnamonXI&quot;&gt;Lateefah Bello&lt;/a&gt;, &lt;a href=&quot;https://studentambassadors.microsoft.com/&quot;&gt;MLSA&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ðŸ™ Core Contributors:&lt;/strong&gt; &lt;a href=&quot;https://github.com/Pe4enIks&quot;&gt;Evgenii Pishchik&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Other Curricula&lt;/h2&gt; 
&lt;p&gt;Our team produces other curricula! Check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genai-beginners&quot;&gt;Generative AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Generative-AI-for-beginners-dotnet&quot;&gt;Generative AI for Beginners .NET&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/generative-ai-with-javascript&quot;&gt;Generative AI with JavaScript&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-beginners&quot;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/datascience-beginners&quot;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ml-beginners&quot;&gt;ML for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Security-101&quot;&gt;Cybersecurity for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/webdev-beginners&quot;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/iot-beginners&quot;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/xr-development-for-beginners&quot;&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Mastering-GitHub-Copilot-for-Paired-Programming&quot;&gt;Mastering GitHub Copilot for Paired Programming&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers&quot;&gt;Mastering GitHub Copilot for C#/.NET Developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/CopilotAdventures&quot;&gt;Choose Your Own Copilot Adventure&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>
