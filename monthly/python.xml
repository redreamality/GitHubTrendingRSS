<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="data:text/xsl;base64,<?xml version="1.0" encoding="utf-8"?><xsl:stylesheet version="3.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform" xmlns:atom="http://www.w3.org/2005/Atom"><xsl:output method="html" version="1.0" encoding="UTF-8" indent="yes"/><xsl:template match="/"><xsl:variable name="title"><xsl:value-of select="/rss/channel/title"/></xsl:variable><xsl:variable name="description"><xsl:value-of select="/rss/channel/description"/></xsl:variable><xsl:variable name="link"><xsl:value-of select="/rss/channel/link"/></xsl:variable><html class="dark scroll-smooth"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="referrer" content="unsafe-url"/><title><xsl:value-of select="$title"/></title><style>*,:after,:before{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-gradient-from-position: ;--tw-gradient-via-position: ;--tw-gradient-to-position: ;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgba(59,130,246,.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: ;--tw-contain-size: ;--tw-contain-layout: ;--tw-contain-paint: ;--tw-contain-style: }::backdrop{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-gradient-from-position: ;--tw-gradient-via-position: ;--tw-gradient-to-position: ;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:rgba(59,130,246,.5);--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: ;--tw-contain-size: ;--tw-contain-layout: ;--tw-contain-paint: ;--tw-contain-style: }
        
        /*! tailwindcss v3.4.17 | MIT License | https://tailwindcss.com*/*,:after,:before{box-sizing:border-box;border:0 solid #e7e7f0}:after,:before{--tw-content:""}:host,html{line-height:1.5;-webkit-text-size-adjust:100%;-moz-tab-size:4;-o-tab-size:4;tab-size:4;font-family:ui-sans-serif,system-ui,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;font-feature-settings:normal;font-variation-settings:normal;-webkit-tap-highlight-color:transparent}body{margin:0;line-height:inherit}hr{height:0;color:inherit;border-top-width:1px}abbr:where([title]){-webkit-text-decoration:underline dotted;text-decoration:underline dotted}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}b,strong{font-weight:bolder}code,kbd,pre,samp{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-feature-settings:normal;font-variation-settings:normal;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}table{text-indent:0;border-color:inherit;border-collapse:collapse}button,input,optgroup,select,textarea{font-family:inherit;font-feature-settings:inherit;font-variation-settings:inherit;font-size:100%;font-weight:inherit;line-height:inherit;letter-spacing:inherit;color:inherit;margin:0;padding:0}button,select{text-transform:none}button,input:where([type=button]),input:where([type=reset]),input:where([type=submit]){-webkit-appearance:button;background-color:transparent;background-image:none}:-moz-focusring{outline:auto}:-moz-ui-invalid{box-shadow:none}progress{vertical-align:baseline}::-webkit-inner-spin-button,::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}summary{display:list-item}blockquote,dd,dl,figure,h1,h2,h3,h4,h5,h6,hr,p,pre{margin:0}fieldset{margin:0}fieldset,legend{padding:0}menu,ol,ul{list-style:none;margin:0;padding:0}dialog{padding:0}textarea{resize:vertical}input::-moz-placeholder,textarea::-moz-placeholder{opacity:1;color:#a8a8b8}input::placeholder,textarea::placeholder{opacity:1;color:#a8a8b8}[role=button],button{cursor:pointer}:disabled{cursor:default}audio,canvas,embed,iframe,img,object,svg,video{display:block;vertical-align:middle}img,video{max-width:100%;height:auto}[hidden]:where(:not([hidden=until-found])){display:none}:root{--card-radius:0.75rem;--btn-radius:var(--card-radius);--badge-radius:var(--btn-radius);--input-radius:var(--btn-radius);--avatar-radius:9999px;--annonce-radius:var(--avatar-radius);--ui-border-color:#1f1f31;--btn-border:#1f1f31;--badge-border:var(--btn-border);--input-border:var(--ui-border-color);--ui-disabled-border:#121220;--ui-error-border:#e11d48;--ui-success-border:#65a30d;--input-outline:#4f46e5;--ui-bg:rgb(18 18 32/var(--ui-bg-opacity));--ui-soft-bg:#1f1f31;--overlay-bg:rgba(2,2,13,.25);--input-bg:var(--ui-soft-bg);--ui-disabled-bg:#121220;--card-padding:1.5rem;--display-text-color:#fff;--title-text-color:var(--display-text-color);--body-text-color:#d6d6e1;--caption-text-color:#6e6e81;--placeholder-text-color:#4d4d5f;--ui-bg-opacity:1;color:var(--body-text-color)}*,.border{border-color:var(--ui-border-color)}button:disabled{border:none!important;background:var(--ui-disabled-bg)!important;background-image:none!important;box-shadow:none!important;color:var(--placeholder-text-color)!important;pointer-events:none!important}button:disabled:before{content:var(--tw-content);display:none}a:focus-visible,button:focus-visible{outline-width:2px;outline-offset:2px;outline-color:#4f46e5}a:focus-visible:focus-visible,button:focus-visible:focus-visible{outline-style:solid}input:user-invalid,select:user-invalid,textarea:user-invalid{--input-border:var(--ui-error-border);--ui-border-color:var(--ui-error-border);--input-outline:var(--ui-error-border);--title-text-color:#fb7185}[data-rounded=none]{--card-radius:0px;--avatar-radius:0px}[data-rounded=default]{--card-radius:0.25rem}[data-rounded=small]{--card-radius:0.125rem}[data-rounded=medium]{--card-radius:0.375rem}[data-rounded=large]{--card-radius:0.5rem}[data-rounded=xlarge]{--card-radius:0.75rem}[data-rounded="2xlarge"]{--card-radius:1rem;--input-radius:0.75rem}[data-rounded="3xlarge"]{--card-radius:1.5rem;--input-radius:0.75rem}[data-rounded=full]{--card-radius:1.5rem;--btn-radius:9999px;--input-radius:1rem}[data-shade=glassy]{--ui-bd-blur:40px;--ui-bg-opacity:0.75;--ui-bg:rgb(58 58 75/var(--ui-bg-opacity));--ui-border-color:rgba(250,250,254,.1);--ui-soft-bg:rgba(77,77,95,.5)}[data-shade="800"]{--ui-border-color:#3a3a4b;--ui-bg:#1f1f31;--ui-soft-bg:#121220}[data-shade="900"]{--ui-border-color:#1f1f31;--ui-bg:#121220;--ui-soft-bg:#1f1f31}[data-shade="950"]{--ui-border-color:#1f1f31;--ui-bg:#02020d;--ui-soft-bg:#1f1f31}.container{width:100%}@media (min-width:640px){.container{max-width:640px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:1024px){.container{max-width:1024px}}@media (min-width:1280px){.container{max-width:1280px}}@media (min-width:1536px){.container{max-width:1536px}}.icon-\[tabler--rss\]{display:inline-block;width:1em;height:1em;background-color:currentColor;-webkit-mask-image:var(--svg);mask-image:var(--svg);-webkit-mask-repeat:no-repeat;mask-repeat:no-repeat;-webkit-mask-size:100% 100%;mask-size:100% 100%;--svg:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' width='24' height='24'%3E%3Cpath fill='none' stroke='%23000' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M4 19a1 1 0 1 0 2 0 1 1 0 1 0-2 0M4 4a16 16 0 0 1 16 16M4 11a9 9 0 0 1 9 9'/%3E%3C/svg%3E")}.link{--tw-text-opacity:1;color:rgb(129 140 248/var(--tw-text-opacity,1));transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,-webkit-backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter,-webkit-backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1);transition-duration:.15s}.link.variant-ghost:hover,.link.variant-underlined{text-decoration-line:underline}.link.variant-animated{position:relative}.link.variant-animated:before{position:absolute;left:0;right:0;bottom:0;height:1px;transform-origin:right;--tw-scale-x:0;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y));transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,-webkit-backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter,-webkit-backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1);content:var(--tw-content);transition-duration:.2s}.link.variant-animated:hover:before{transform-origin:left;content:var(--tw-content);--tw-scale-x:1;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}.link.intent-info{--tw-text-opacity:1;color:rgb(96 165 250/var(--tw-text-opacity,1))}.link.intent-neutral{--tw-text-opacity:1;color:rgb(255 255 255/var(--tw-text-opacity,1))}.link.variant-animated.intent-neutral:before{content:var(--tw-content);background-color:hsla(0,0%,100%,.5)}.link.variant-animated.intent-info:before{content:var(--tw-content);--tw-bg-opacity:1;background-color:rgb(37 99 235/var(--tw-bg-opacity,1))}.link.variant-animated.intent-primary:before{content:var(--tw-content);--tw-bg-opacity:1;background-color:rgb(79 70 229/var(--tw-bg-opacity,1))}.link.variant-ghost.intent-neutral,.link.variant-underlined.intent-neutral{text-decoration-color:hsla(0,0%,100%,.5)}.mx-auto{margin-left:auto;margin-right:auto}.my-2{margin-top:.5rem;margin-bottom:.5rem}.my-6{margin-top:1.5rem;margin-bottom:1.5rem}.mb-2{margin-bottom:.5rem}.mb-6{margin-bottom:1.5rem}.mb-8{margin-bottom:2rem}.ml-1{margin-left:.25rem}.ml-4{margin-left:1rem}.mr-2{margin-right:.5rem}.mt-1{margin-top:.25rem}.mt-2{margin-top:.5rem}.mt-3{margin-top:.75rem}.block{display:block}.inline-block{display:inline-block}.inline{display:inline}.flex{display:flex}.grid{display:grid}.hidden{display:none}.h-4{height:1rem}.h-8{height:2rem}.min-h-screen{min-height:100vh}.min-h-svh{min-height:100svh}.w-4{width:1rem}.w-8{width:2rem}.max-w-full{max-width:100%}.max-w-screen-lg{max-width:1024px}.flex-1{flex:1 1 0%}.cursor-pointer{cursor:pointer}.list-disc{list-style-type:disc}.grid-cols-1{grid-template-columns:repeat(1,minmax(0,1fr))}.flex-col{flex-direction:column}.items-center{align-items:center}.justify-between{justify-content:space-between}.gap-4{gap:1rem}.gap-6{gap:1.5rem}.gap-8{gap:2rem}.space-y-2>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-top:calc(.5rem*(1 - var(--tw-space-y-reverse)));margin-bottom:calc(.5rem*var(--tw-space-y-reverse))}.space-y-3>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-top:calc(.75rem*(1 - var(--tw-space-y-reverse)));margin-bottom:calc(.75rem*var(--tw-space-y-reverse))}.space-y-4>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-top:calc(1rem*(1 - var(--tw-space-y-reverse)));margin-bottom:calc(1rem*var(--tw-space-y-reverse))}.space-y-6>:not([hidden])~:not([hidden]){--tw-space-y-reverse:0;margin-top:calc(1.5rem*(1 - var(--tw-space-y-reverse)));margin-bottom:calc(1.5rem*var(--tw-space-y-reverse))}.scroll-smooth{scroll-behavior:smooth}.truncate{overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.bg-gray-925{--tw-bg-opacity:1;background-color:rgb(9 9 21/var(--tw-bg-opacity,1))}.bg-gradient-to-r{background-image:linear-gradient(to right,var(--tw-gradient-stops))}.from-primary-600{--tw-gradient-from:#4f46e5 var(--tw-gradient-from-position);--tw-gradient-to:rgba(79,70,229,0) var(--tw-gradient-to-position);--tw-gradient-stops:var(--tw-gradient-from),var(--tw-gradient-to)}.to-accent-400{--tw-gradient-to:#e879f9 var(--tw-gradient-to-position)}.bg-clip-text{-webkit-background-clip:text;background-clip:text}.p-1{padding:.25rem}.px-4{padding-left:1rem;padding-right:1rem}.py-2{padding-top:.5rem;padding-bottom:.5rem}.py-4{padding-top:1rem;padding-bottom:1rem}.py-6{padding-top:1.5rem;padding-bottom:1.5rem}.pl-5{padding-left:1.25rem}.pt-2{padding-top:.5rem}.text-center{text-align:center}.font-sans{font-family:ui-sans-serif,system-ui,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}.text-2xl{font-size:1.5rem;line-height:2rem}.text-lg{font-size:1.125rem;line-height:1.75rem}.text-sm{font-size:.875rem;line-height:1.25rem}.text-xs{font-size:.75rem;line-height:1rem}.font-bold{font-weight:700}.font-medium{font-weight:500}.font-semibold{font-weight:600}.leading-normal{line-height:1.5}.text-gray-400{--tw-text-opacity:1;color:rgb(168 168 184/var(--tw-text-opacity,1))}.text-gray-500{--tw-text-opacity:1;color:rgb(110 110 129/var(--tw-text-opacity,1))}.text-transparent{color:transparent}.antialiased{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}.transition{transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,-webkit-backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter;transition-property:color,background-color,border-color,text-decoration-color,fill,stroke,opacity,box-shadow,transform,filter,backdrop-filter,-webkit-backdrop-filter;transition-timing-function:cubic-bezier(.4,0,.2,1);transition-duration:.15s}.text-title{color:var(--title-text-color)}.text-body{color:var(--body-text-color)}.\!text-caption{color:var(--caption-text-color)!important}.text-caption{color:var(--caption-text-color)}.dark{--display-text-color:#fff;--title-text-color:var(--display-text-color);--caption-text-color:#6e6e81;--body-text-color:#d6d6e1;--placeholder-text-color:#4d4d5f;--ui-border-color:#232323}[data-shade="900"]:where(.dark,.dark *),[data-shade="925"]:where(.dark,.dark *),[data-shade="950"]:where(.dark,.dark *){--ui-border-color:#383838}.hover\:text-gray-300:hover{--tw-text-opacity:1;color:rgb(214 214 225/var(--tw-text-opacity,1))}.group[open] .group-open\:rotate-180{--tw-rotate:180deg;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}@media (min-width:768px){.md\:grid-cols-2{grid-template-columns:repeat(2,minmax(0,1fr))}.md\:p-4{padding:1rem}.md\:px-6{padding-left:1.5rem;padding-right:1.5rem}.md\:pt-6{padding-top:1.5rem}}@media (min-width:1024px){.lg\:grid-cols-3{grid-template-columns:repeat(3,minmax(0,1fr))}.lg\:dark\:bg-gray-900:is(.dark *){--tw-bg-opacity:1;background-color:rgb(18 18 32/var(--tw-bg-opacity,1))}}</style></head><body class="bg-gray-925 min-h-screen min-h-svh font-sans leading-normal antialiased lg:dark:bg-gray-900"><main class="min-w-screen container mx-auto flex min-h-screen max-w-screen-lg flex-col px-4 py-6 md:px-6"><header class="space-y-2 pt-2 md:pt-6"><a title="{$title}" href="{$link}" target="_blank" rel="noopener noreferrer"><h1 class="flex text-2xl"><span class="icon-[tabler--rss] mr-2 h-8 w-8"/><span class="lg2:text-3xl from-primary-600 to-accent-400 inline-block bg-gradient-to-r bg-clip-text font-bold text-transparent"><xsl:value-of select="$title" disable-output-escaping="yes"/></span></h1></a><p class="text-body pt-2 text-lg py-4"><xsl:value-of select="$description" disable-output-escaping="yes"/></p><p class="text-caption text-sm">
              This RSS feed for the
              <a class="link intent-neutral variant-animated !text-caption font-bold" title="{$title}" href="{$link}" target="_blank" rel="noopener noreferrer"><xsl:value-of select="$title"/></a>
              website.
            </p><p class="text-body text-sm hidden" id="subscribe-links">
              You can subscribe this RSS feed by
              <a class="link intent-neutral variant-animated font-bold" title="Feedly" data-href="https://feedly.com/i/subscription/feed/" target="_blank" rel="noopener noreferrer">Feedly</a>,
              <a class="link intent-neutral variant-animated font-bold" title="Inoreader" data-href="https://www.inoreader.com/feed/" target="_blank" rel="noopener noreferrer">Inoreader</a>,
              <a class="link intent-neutral variant-animated font-bold" title="Newsblur" data-href="https://www.newsblur.com/?url=" target="_blank" rel="noopener noreferrer">Newsblur</a>,
              <a class="link intent-neutral variant-animated font-bold" title="Follow" data-href="follow://add?url=" rel="noopener noreferrer">Follow</a>,
              <a class="link intent-neutral variant-animated font-bold" title="RSS Reader" data-href="feed:" data-raw="true" rel="noopener noreferrer">RSS Reader</a>
              or
              <a class="link intent-neutral variant-animated font-bold" title="{$title} 's feed source" data-href="" data-raw="true" rel="noopener noreferrer">View Source</a>.
            </p><script>
              document.addEventListener('DOMContentLoaded', function () {
                document.querySelectorAll('a[data-href]').forEach(function (a) {
                  const url = new URL(location.href)
                  const feed = url.searchParams.get('url') || location.href
                  const raw = a.getAttribute('data-raw')
                  if (raw) {
                    a.href = a.getAttribute('data-href') + feed
                  } else {
                    a.href = a.getAttribute('data-href') + encodeURIComponent(feed)
                  }
                })
                document.getElementById('subscribe-links').classList.remove('hidden')
              })
            </script></header><hr class="my-6"/><section class="flex-1 space-y-6 p-1 md:p-4"><xsl:choose><xsl:when test="/rss/channel/item"><xsl:for-each select="/rss/channel/item"><article class="space-y-2"><details><summary class="max-w-full truncate"><xsl:if test="title"><h2 class="text-title inline cursor-pointer text-lg font-semibold"><xsl:value-of select="title" disable-output-escaping="yes"/></h2></xsl:if><xsl:if test="pubDate"><time class="text-caption ml-4 mt-1 block text-sm"><xsl:value-of select="pubDate"/></time></xsl:if></summary><div class="text-body px-4 py-2"><p class="my-2"><xsl:choose><xsl:when test="description"><xsl:value-of select="description" disable-output-escaping="yes"/></xsl:when></xsl:choose></p><xsl:if test="link"><a class="link variant-animated intent-neutral font-bold" href="{link}" target="_blank" rel="noopener noreferrer">
                            Read More
                          </a></xsl:if></div></details></article></xsl:for-each></xsl:when><xsl:when test="/atom:feed/atom:entry"><xsl:for-each select="/atom:feed/atom:entry"><article class="space-y-2"><details><summary class="max-w-full truncate"><xsl:if test="atom:title"><h2 class="text-title inline cursor-pointer text-lg font-semibold"><xsl:value-of select="atom:title" disable-output-escaping="yes"/></h2></xsl:if><xsl:if test="atom:updated"><time class="text-caption ml-4 mt-1 block text-sm"><xsl:value-of select="atom:updated"/></time></xsl:if></summary><div class="text-body px-4 py-2"><p class="my-2"><xsl:choose><xsl:when test="atom:summary"><xsl:value-of select="atom:summary" disable-output-escaping="yes"/></xsl:when><xsl:when test="atom:content"><xsl:value-of select="atom:content" disable-output-escaping="yes"/></xsl:when></xsl:choose></p><xsl:if test="atom:link/@href"><a class="link variant-animated intent-neutral font-bold" href="{atom:link/@href}" target="_blank" rel="noopener noreferrer">
                            Read More
                          </a></xsl:if></div></details></article></xsl:for-each></xsl:when></xsl:choose></section><hr class="my-6"/><footer class="text-gray-400"><div class="container mx-auto px-4"><div class="mb-8"><h3 class="text-lg font-semibold text-title mb-6">Popular Feed Collections</h3><div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6"><div class="space-y-3"><a href="https://redreamality.com/GitHubTrendingRSS/weekly/python.xml" class="block hover:text-gray-300"><div class="font-medium">🐍 Python TrendWatch</div><div class="text-xs text-gray-500">AI, ML &amp; Data Science Innovation Feed</div></a><a href="https://redreamality.com/GitHubTrendingRSS/weekly/cuda.xml" class="block hover:text-gray-300"><div class="font-medium">⚡ CUDA Accelerator</div><div class="text-xs text-gray-500">GPU Computing &amp; Deep Learning Updates</div></a><a href="https://redreamality.com/GitHubTrendingRSS/weekly/matlab.xml" class="block hover:text-gray-300"><div class="font-medium">🧠 MATLAB TrendPulse</div><div class="text-xs text-gray-500">MEG, EEG and iEEG Research Feed</div></a></div><div class="space-y-3"><a href="https://redreamality.com/GitHubTrendingRSS/weekly/rust.xml" class="block hover:text-gray-300"><div class="font-medium">🦀 Rust Systems Feed</div><div class="text-xs text-gray-500">High-Performance &amp; Safe Systems Programming</div></a><a href="https://redreamality.com/GitHubTrendingRSS/weekly/go.xml" class="block hover:text-gray-300"><div class="font-medium">🚀 Go Infrastructure</div><div class="text-xs text-gray-500">Cloud Native &amp; DevOps Excellence</div></a><a href="https://redreamality.com/GitHubTrendingRSS/weekly/typescript.xml" class="block hover:text-gray-300"><div class="font-medium">📱 TypeScript Ecosystem</div><div class="text-xs text-gray-500">Modern Web &amp; App Development</div></a></div><div class="space-y-3"><a href="https://redreamality.com/GitHubTrendingRSS/daily/adblock-filter-list.xml" class="block hover:text-gray-300"><div class="font-medium">🛡️ Privacy Shield</div><div class="text-xs text-gray-500">AdBlock &amp; Security Updates</div></a><a href="https://redreamality.com/GitHubTrendingRSS/daily/all.xml" class="block hover:text-gray-300"><div class="font-medium">🌟 Global TechRadar</div><div class="text-xs text-gray-500">Cross-Language Innovation Pulse, add it to your RSS reader rsshub://github/trending/monthly/any/zh</div></a></div></div></div><div class="grid grid-cols-1 md:grid-cols-2 gap-8 mb-8"><div class="space-y-4"><h3 class="text-lg font-semibold text-title">Getting Started</h3><div class="grid grid-cols-1 gap-4"><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">📖 Feed Integration Guide</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">1. Choose Your RSS Reader:</p><ul class="list-disc pl-5 space-y-2"><li>Feedly: Professional choice for cross-platform sync</li><li>Inoreader: Advanced filtering capabilities</li><li>NetNewsWire: Perfect for macOS/iOS users</li><li>FreshRSS: Self-hosted option with full control</li></ul><p class="mt-3 mb-2">2. Add Our Feeds:</p><ul class="list-disc pl-5 space-y-2"><li>Copy the feed URL (e.g., rsshub://github/trending/monthly/any/zh)</li><li>In your RSS reader, look for "Add Feed" or "Subscribe"</li><li>Paste the URL and customize update frequency</li></ul></div></details><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">🎯 Custom Feed Creation</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Create Your Perfect Feed:</p><ul class="list-disc pl-5 space-y-2"><li>Language-specific: /GitHubTrendingRSS/[frequency]/[language].xml</li><li>Topic-focused: Combine multiple language feeds</li><li>Custom time ranges: daily, weekly, or monthly updates</li><li>Regional feeds: Focus on specific developer communities</li></ul><p class="mt-3">Pro tip: Use tags in your RSS reader to organize feeds by topic, language, or priority.</p></div></details><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">⚡ Feed Management Tips</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Optimize Your Feed Reading:</p><ul class="list-disc pl-5 space-y-2"><li>Set update frequencies based on feed importance</li><li>Use folders to group related feeds (e.g., AI/ML, Web Dev)</li><li>Enable notifications only for high-priority feeds</li><li>Archive valuable resources for future reference</li></ul><p class="mt-3">Advanced Features:</p><ul class="list-disc pl-5 space-y-2"><li>Filter feeds using keywords to focus on specific topics</li><li>Set up IFTTT integrations for automated workflows</li><li>Export/backup your feed collection regularly</li></ul></div></details></div></div><div class="space-y-4"><h3 class="text-lg font-semibold text-title">Common Questions</h3><div class="grid grid-cols-1 gap-4"><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">🤔 About Github Radar</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Github Radar is your intelligent curator for:</p><ul class="list-disc pl-5 space-y-2"><li>Trending repositories across all programming languages</li><li>Language-specific innovation and updates</li><li>Regional development trends and patterns</li><li>Open source community movements</li></ul><p class="mt-3">Our mission is to help developers stay updated with minimal effort through smart feed curation and organization.</p></div></details><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">📊 Feed Frequency Options</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Choose Your Update Rhythm:</p><ul class="list-disc pl-5 space-y-2"><li>Daily: Perfect for fast-moving technologies and security updates</li><li>Weekly: Ideal for maintaining awareness without overwhelm</li><li>Monthly: Best for long-term trend analysis and strategic planning</li></ul><p class="mt-3">Customize by combining different frequencies for different topics based on your needs.</p></div></details><details class="group"><summary class="flex cursor-pointer items-center justify-between hover:text-gray-300"><h4 class="font-medium">🔧 Technical Support</h4><span class="transition group-open:rotate-180"><svg fill="none" height="24" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="24"><path d="M6 9l6 6 6-6"/></svg></span></summary><div class="text-sm text-gray-500 mt-3 group-open:animate-fadeIn"><p class="mb-2">Supported RSS Readers:</p><ul class="list-disc pl-5 space-y-2"><li>Desktop: NetNewsWire, Reeder, FeedReader</li><li>Mobile: Feedly, Inoreader, NewsBlur</li><li>Self-hosted: FreshRSS, Tiny Tiny RSS</li><li>Browser-based: Feedbro, RSS Feed Reader</li></ul><p class="mt-3">Common Issues:</p><ul class="list-disc pl-5 space-y-2"><li>Feed not updating? Check your reader's refresh settings</li><li>Missing content? Verify your internet connection</li><li>Format issues? Try re-subscribing to the feed</li></ul></div></details></div></div></div><div class="text-center text-sm"><p class="mt-2">Acknowledgement: Page decorated by <a href="https://github.com/ccbikai/RSS.Beauty"><svg class="inline-block w-4 h-4 ml-1" viewBox="0 0 16 16" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg></a> RSS.Beauty</p></div></div></footer></main></body></html></xsl:template></xsl:stylesheet>"?>
<rss version="2.0">
  <channel>
    <title>GitHub Python Monthly Trending</title>
    <description>Monthly Trending of Python in GitHub</description>
    <pubDate>Sat, 29 Mar 2025 02:48:31 GMT</pubDate>
    <link>http://redreamality.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>virattt/ai-hedge-fund</title>
      <link>https://github.com/virattt/ai-hedge-fund</link>
      <description>&lt;p&gt;An AI Hedge Fund Team&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Hedge Fund&lt;/h1&gt; 
&lt;p&gt;This is a proof of concept for an AI-powered hedge fund. The goal of this project is to explore the use of AI to make trading decisions. This project is for &lt;strong&gt;educational&lt;/strong&gt; purposes only and is not intended for real trading or investment.&lt;/p&gt; 
&lt;p&gt;This system employs several agents working together:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety&lt;/li&gt; 
 &lt;li&gt;Bill Ackman Agent - An activist investors, takes bold positions and pushes for change&lt;/li&gt; 
 &lt;li&gt;Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption&lt;/li&gt; 
 &lt;li&gt;Charlie Munger Agent - Warren Buffett&#39;s partner, only buys wonderful businesses at fair prices&lt;/li&gt; 
 &lt;li&gt;Phil Fisher Agent - Legendary growth investor who mastered scuttlebutt analysis&lt;/li&gt; 
 &lt;li&gt;Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential&lt;/li&gt; 
 &lt;li&gt;Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price&lt;/li&gt; 
 &lt;li&gt;Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Sentiment Agent - Analyzes market sentiment and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Fundamentals Agent - Analyzes fundamental data and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Technicals Agent - Analyzes technical indicators and generates trading signals&lt;/li&gt; 
 &lt;li&gt;Risk Manager - Calculates risk metrics and sets position limits&lt;/li&gt; 
 &lt;li&gt;Portfolio Manager - Makes final trading decisions and generates orders&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07 PM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot;&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: the system simulates trading decisions, it does not actually trade.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://twitter.com/virattt&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/virattt?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is for &lt;strong&gt;educational and research purposes only&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Not intended for real trading or investment&lt;/li&gt; 
 &lt;li&gt;No warranties or guarantees provided&lt;/li&gt; 
 &lt;li&gt;Past performance does not indicate future results&lt;/li&gt; 
 &lt;li&gt;Creator assumes no liability for financial losses&lt;/li&gt; 
 &lt;li&gt;Consult a financial advisor for investment decisions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to use it solely for learning purposes.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#setup&quot;&gt;Setup&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#usage&quot;&gt;Usage&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#running-the-hedge-fund&quot;&gt;Running the Hedge Fund&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#running-the-backtester&quot;&gt;Running the Backtester&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#project-structure&quot;&gt;Project Structure&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#feature-requests&quot;&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/virattt/ai-hedge-fund/main/#license&quot;&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;p&gt;Clone the repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Poetry (if not already installed):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;curl -sSL https://install.python-poetry.org | python3 -
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;poetry install
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Set up your environment variables:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Create .env file for your API keys
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt;Set your API keys:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
# Get your OpenAI API key from https://platform.openai.com/
OPENAI_API_KEY=your-openai-api-key

# For running LLMs hosted by groq (deepseek, llama3, etc.)
# Get your Groq API key from https://groq.com/
GROQ_API_KEY=your-groq-api-key

# For getting financial data to power the hedge fund
# Get your Financial Datasets API key from https://financialdatasets.ai/
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: You must set &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;GROQ_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;, or &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt; for the hedge fund to work. If you want to use LLMs from all providers, you will need to set all API keys.&lt;/p&gt; 
&lt;p&gt;Financial data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key.&lt;/p&gt; 
&lt;p&gt;For any other ticker, you will need to set the &lt;code&gt;FINANCIAL_DATASETS_API_KEY&lt;/code&gt; in the .env file.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Running the Hedge Fund&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt; &lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17 PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot;&gt;&lt;/p&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;--show-reasoning&lt;/code&gt; flag to print the reasoning of each agent to the console.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --show-reasoning
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can optionally specify the start and end dates to make decisions for a specific time period.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running the Backtester&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example Output:&lt;/strong&gt; &lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52 PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot;&gt;&lt;/p&gt; 
&lt;p&gt;You can optionally specify the start and end dates to backtest over a specific time period.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Project Structure&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;ai-hedge-fund/
├── src/
│   ├── agents/                   # Agent definitions and workflow
│   │   ├── bill_ackman.py        # Bill Ackman agent
│   │   ├── fundamentals.py       # Fundamental analysis agent
│   │   ├── portfolio_manager.py  # Portfolio management agent
│   │   ├── risk_manager.py       # Risk management agent
│   │   ├── sentiment.py          # Sentiment analysis agent
│   │   ├── technicals.py         # Technical analysis agent
│   │   ├── valuation.py          # Valuation analysis agent
│   │   ├── warren_buffett.py     # Warren Buffett agent
│   ├── tools/                    # Agent tools
│   │   ├── api.py                # API tools
│   ├── backtester.py             # Backtesting tools
│   ├── main.py # Main entry point
├── pyproject.toml
├── ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a feature branch&lt;/li&gt; 
 &lt;li&gt;Commit your changes&lt;/li&gt; 
 &lt;li&gt;Push to the branch&lt;/li&gt; 
 &lt;li&gt;Create a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Please keep your pull requests small and focused. This will make it easier to review and merge.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;If you have a feature request, please open an &lt;a href=&quot;https://github.com/virattt/ai-hedge-fund/issues&quot;&gt;issue&lt;/a&gt; and make sure it is tagged with &lt;code&gt;enhancement&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>agno-agi/agno</title>
      <link>https://github.com/agno-agi/agno</link>
      <description>&lt;p&gt;Agno is a lightweight library for building Multimodal Agents. Use it to give LLMs superpowers like memory, knowledge, tools and reasoning.&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot; id=&quot;top&quot;&gt; 
 &lt;a href=&quot;https://docs.agno.com&quot;&gt; 
  &lt;picture&gt; 
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-dark.svg&quot;&gt; 
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg&quot;&gt; 
   &lt;img src=&quot;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg?sanitize=true&quot; alt=&quot;Agno&quot;&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://docs.agno.com&quot;&gt;📚 Documentation&lt;/a&gt; &amp;nbsp;|&amp;nbsp; 
 &lt;a href=&quot;https://docs.agno.com/examples/introduction&quot;&gt;💡 Examples&lt;/a&gt; &amp;nbsp;|&amp;nbsp; 
 &lt;a href=&quot;https://github.com/agno-agi/agno/stargazers&quot;&gt;🌟 Star Us&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://docs.agno.com&quot;&gt;Agno&lt;/a&gt; is a lightweight library for building Multimodal Agents. It exposes LLMs as a unified API and gives them superpowers like memory, knowledge, tools and reasoning.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Build lightning-fast Agents that can generate text, image, audio and video.&lt;/li&gt; 
 &lt;li&gt;Add memory, knowledge, tools and reasoning as needed.&lt;/li&gt; 
 &lt;li&gt;Run anywhere, Agno is open-source.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Here&#39;s an Agent that can search the web:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    tools=[DuckDuckGoTools()],
    markdown=True
)
agent.print_response(&quot;What&#39;s happening in New York?&quot;, stream=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;🚨 Open Source AI Agent Hackathon! 🚨&lt;/h2&gt; 
&lt;p&gt;We&#39;re launching a Global AI Agent Hackathon in collaboration with AI Agent ecosystem partners — open to all developers, builders, and startups working on agents, RAG, tool use, or multi-agent systems.&lt;/p&gt; 
&lt;h3&gt;💰 Win up to $20,000 in cash by building Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;🏅 10 winners: $300 each&lt;/li&gt; 
 &lt;li&gt;🥉 10 winners: $500 each&lt;/li&gt; 
 &lt;li&gt;🥈 5 winners: $1,000 each&lt;/li&gt; 
 &lt;li&gt;🥇 1 winner: $2,000&lt;/li&gt; 
 &lt;li&gt;🏆 GRAND PRIZE: $5,000 🏆&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🎁 Bonus&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Top 5 projects will be featured in the top trending &lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps&quot;&gt;Awesome LLM Apps&lt;/a&gt; repo.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🤝 Partners&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.agno.com&quot;&gt;Agno&lt;/a&gt;, &lt;a href=&quot;https://www.theunwindai.com&quot;&gt;Unwind AI&lt;/a&gt; and more Agent ecosystem companies joining soon.&lt;/p&gt; 
&lt;h3&gt;📅 Here&#39;s the timeline:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;April 3rd - Final dates revealed&lt;/li&gt; 
 &lt;li&gt;April 10th - Prize and success criteria announced&lt;/li&gt; 
 &lt;li&gt;April 15th (tentative) - Hackathon starts&lt;/li&gt; 
 &lt;li&gt;May 30th (tentative) - Hackathon ends&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Join us for a month of building Agents!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Prizes will be distributed on an ongoing basis and continue till all prizes are awarded.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;⭐ Star this repo and follow along to stay updated.&lt;/p&gt; 
&lt;h3&gt;🤝 Want to join us as a partner or judge?&lt;/h3&gt; 
&lt;p&gt;If you&#39;re a company in the AI agent ecosystem or would like to judge the hackathon, reach out to &lt;a href=&quot;https://x.com/Saboo_Shubham_&quot;&gt;Shubham Saboo&lt;/a&gt; or &lt;a href=&quot;https://x.com/ashpreetbedi&quot;&gt;Ashpreet Bedi&lt;/a&gt; on X to partner. Let’s make this the biggest open source AI Agent hackathon.&lt;/p&gt; 
&lt;h2&gt;Key features&lt;/h2&gt; 
&lt;p&gt;Agno is simple, fast and model agnostic. Here are some key features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lightning Fast&lt;/strong&gt;: Agent creation is 10,000x faster than LangGraph (see &lt;a href=&quot;https://raw.githubusercontent.com/agno-agi/agno/main/#performance&quot;&gt;performance&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model Agnostic&lt;/strong&gt;: Use any model, any provider, no lock-in.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi Modal&lt;/strong&gt;: Native support for text, image, audio and video.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi Agent&lt;/strong&gt;: Build teams of specialized agents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Memory Management&lt;/strong&gt;: Store agent sessions and state in a database.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Knowledge Stores&lt;/strong&gt;: Use vector databases for RAG or dynamic few-shot learning.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Structured Outputs&lt;/strong&gt;: Make Agents respond in a structured format.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Monitoring&lt;/strong&gt;: Track agent sessions and performance in real-time on &lt;a href=&quot;https://app.agno.com&quot;&gt;agno.com&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start by &lt;a href=&quot;https://docs.agno.com/introduction/agents&quot;&gt;building your first Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Check out the &lt;a href=&quot;https://docs.agno.com/examples/introduction&quot;&gt;examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Read the &lt;a href=&quot;https://docs.agno.com&quot;&gt;documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install -U agno
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;What are Agents?&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Agents&lt;/strong&gt; are intelligent programs that solve problems autonomously.&lt;/p&gt; 
&lt;p&gt;Agents have memory, domain knowledge and the ability to use tools (like searching the web, querying a database, making API calls). Unlike traditional programs that follow a predefined execution path, Agents dynamically adapt their approach based on the context and tool results.&lt;/p&gt; 
&lt;p&gt;Instead of a rigid binary definition, let&#39;s think of Agents in terms of agency and autonomy.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Level 0&lt;/strong&gt;: Agents with no tools (basic inference tasks).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Level 1&lt;/strong&gt;: Agents with tools for autonomous task execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Level 2&lt;/strong&gt;: Agents with knowledge, combining memory and reasoning.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Level 3&lt;/strong&gt;: Teams of specialized agents collaborating on complex workflows.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Example - Basic Agent&lt;/h2&gt; 
&lt;p&gt;The simplest Agent is just an inference task, no tools, no memory, no knowledge.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    description=&quot;You are an enthusiastic news reporter with a flair for storytelling!&quot;,
    markdown=True
)
agent.print_response(&quot;Tell me about a breaking news story from New York.&quot;, stream=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run the agent, install dependencies and export your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install agno openai

export OPENAI_API_KEY=sk-xxxx

python basic_agent.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/agno-agi/agno/main/cookbook/getting_started/01_basic_agent.py&quot;&gt;View this example in the cookbook&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Example - Agent with tools&lt;/h2&gt; 
&lt;p&gt;This basic agent will obviously make up a story, lets give it a tool to search the web.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    description=&quot;You are an enthusiastic news reporter with a flair for storytelling!&quot;,
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True
)
agent.print_response(&quot;Tell me about a breaking news story from New York.&quot;, stream=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install dependencies and run the Agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install duckduckgo-search

python agent_with_tools.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now you should see a much more relevant result.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/agno-agi/agno/main/cookbook/getting_started/02_agent_with_tools.py&quot;&gt;View this example in the cookbook&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Example - Agent with knowledge&lt;/h2&gt; 
&lt;p&gt;Agents can store knowledge in a vector database and use it for RAG or dynamic few-shot learning.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Agno agents use Agentic RAG&lt;/strong&gt; by default, which means they will search their knowledge base for the specific information they need to achieve their task.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.embedder.openai import OpenAIEmbedder
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.lancedb import LanceDb, SearchType

agent = Agent(
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    description=&quot;You are a Thai cuisine expert!&quot;,
    instructions=[
        &quot;Search your knowledge base for Thai recipes.&quot;,
        &quot;If the question is better suited for the web, search the web to fill in gaps.&quot;,
        &quot;Prefer the information in your knowledge base over the web results.&quot;
    ],
    knowledge=PDFUrlKnowledgeBase(
        urls=[&quot;https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf&quot;],
        vector_db=LanceDb(
            uri=&quot;tmp/lancedb&quot;,
            table_name=&quot;recipes&quot;,
            search_type=SearchType.hybrid,
            embedder=OpenAIEmbedder(id=&quot;text-embedding-3-small&quot;),
        ),
    ),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True
)

# Comment out after the knowledge base is loaded
if agent.knowledge is not None:
    agent.knowledge.load()

agent.print_response(&quot;How do I make chicken and galangal in coconut milk soup&quot;, stream=True)
agent.print_response(&quot;What is the history of Thai curry?&quot;, stream=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install dependencies and run the Agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install lancedb tantivy pypdf duckduckgo-search

python agent_with_knowledge.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/agno-agi/agno/main/cookbook/getting_started/03_agent_with_knowledge.py&quot;&gt;View this example in the cookbook&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Example - Multi Agent Teams&lt;/h2&gt; 
&lt;p&gt;Agents work best when they have a singular purpose, a narrow scope and a small number of tools. When the number of tools grows beyond what the language model can handle or the tools belong to different categories, use a team of agents to spread the load.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools
from agno.team import Team

web_agent = Agent(
    name=&quot;Web Agent&quot;,
    role=&quot;Search the web for information&quot;,
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    tools=[DuckDuckGoTools()],
    instructions=&quot;Always include sources&quot;,
    show_tool_calls=True,
    markdown=True,
)

finance_agent = Agent(
    name=&quot;Finance Agent&quot;,
    role=&quot;Get financial data&quot;,
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    tools=[YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)],
    instructions=&quot;Use tables to display data&quot;,
    show_tool_calls=True,
    markdown=True,
)

agent_team = Team(
    mode=&quot;coordinate&quot;,
    members=[web_agent, finance_agent],
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    success_criteria=&quot;A comprehensive financial news report with clear sections and data-driven insights.&quot;,
    instructions=[&quot;Always include sources&quot;, &quot;Use tables to display data&quot;],
    show_tool_calls=True,
    markdown=True,
)

agent_team.print_response(&quot;What&#39;s the market outlook and financial performance of AI semiconductor companies?&quot;, stream=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install dependencies and run the Agent team:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install duckduckgo-search yfinance

python agent_team.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/agno-agi/agno/main/cookbook/getting_started/05_agent_team.py&quot;&gt;View this example in the cookbook&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;At Agno, we&#39;re obsessed with performance. Why? because even simple AI workflows can spawn thousands of Agents to achieve their goals. Scale that to a modest number of users and performance becomes a bottleneck. Agno is designed to power high performance agentic systems:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Agent instantiation: ~2μs on average (~10,000x faster than LangGraph).&lt;/li&gt; 
 &lt;li&gt;Memory footprint: ~3.75Kib on average (~50x less memory than LangGraph).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Tested on an Apple M4 Mackbook Pro.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;While an Agent&#39;s run-time is bottlenecked by inference, we must do everything possible to minimize execution time, reduce memory usage, and parallelize tool calls. These numbers may seem trivial at first, but our experience shows that they add up even at a reasonably small scale.&lt;/p&gt; 
&lt;h3&gt;Instantiation time&lt;/h3&gt; 
&lt;p&gt;Let&#39;s measure the time it takes for an Agent with 1 tool to start up. We&#39;ll run the evaluation 1000 times to get a baseline measurement.&lt;/p&gt; 
&lt;p&gt;You should run the evaluation yourself on your own machine, please, do not take these results at face value.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# Setup virtual environment
./scripts/perf_setup.sh
source .venvs/perfenv/bin/activate
# OR Install dependencies manually
# pip install openai agno langgraph langchain_openai

# Agno
python evals/performance/instantiation_with_tool.py

# LangGraph
python evals/performance/other/langgraph_instantiation.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The following evaluation is run on an Apple M4 Mackbook Pro. It also runs as a Github action on this repo.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;LangGraph is on the right, &lt;strong&gt;let&#39;s start it first and give it a head start&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;Agno is on the left, notice how it finishes before LangGraph gets 1/2 way through the runtime measurement, and hasn&#39;t even started the memory measurement. That&#39;s how fast Agno is.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/ba466d45-75dd-45ac-917b-0a56c5742e23&quot;&gt;https://github.com/user-attachments/assets/ba466d45-75dd-45ac-917b-0a56c5742e23&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Dividing the average time of a Langgraph Agent by the average time of an Agno Agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;0.020526s / 0.000002s ~ 10,263
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this particular run, &lt;strong&gt;Agno Agents startup is roughly 10,000 times faster than Langgraph Agents&lt;/strong&gt;. The numbers continue to favor Agno as the number of tools grow, and we add memory and knowledge stores.&lt;/p&gt; 
&lt;h3&gt;Memory usage&lt;/h3&gt; 
&lt;p&gt;To measure memory usage, we use the &lt;code&gt;tracemalloc&lt;/code&gt; library. We first calculate a baseline memory usage by running an empty function, then run the Agent 1000x times and calculate the difference. This gives a (reasonably) isolated measurement of the memory usage of the Agent.&lt;/p&gt; 
&lt;p&gt;We recommend running the evaluation yourself on your own machine, and digging into the code to see how it works. If we&#39;ve made a mistake, please let us know.&lt;/p&gt; 
&lt;p&gt;Dividing the average memory usage of a Langgraph Agent by the average memory usage of an Agno Agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;0.137273/0.002528 ~ 54.3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Langgraph Agents use ~50x more memory than Agno Agents&lt;/strong&gt;. In our opinion, memory usage is a much more important metric than instantiation time. As we start running thousands of Agents in production, these numbers directly start affecting the cost of running the Agents.&lt;/p&gt; 
&lt;h3&gt;Conclusion&lt;/h3&gt; 
&lt;p&gt;Agno agents are designed for performance and while we do share some benchmarks against other frameworks, we should be mindful that accuracy and reliability are more important than speed.&lt;/p&gt; 
&lt;p&gt;We&#39;ll be publishing accuracy and reliability benchmarks running on Github actions in the coming weeks. Given that each framework is different and we won&#39;t be able to tune their performance like we do with Agno, for future benchmarks we&#39;ll only be comparing against ourselves.&lt;/p&gt; 
&lt;h2&gt;Cursor Setup&lt;/h2&gt; 
&lt;p&gt;When building Agno agents, using Agno documentation as a source in Cursor is a great way to speed up your development.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;In Cursor, go to the settings or preferences section.&lt;/li&gt; 
 &lt;li&gt;Find the section to manage documentation sources.&lt;/li&gt; 
 &lt;li&gt;Add &lt;code&gt;https://docs.agno.com&lt;/code&gt; to the list of documentation URLs.&lt;/li&gt; 
 &lt;li&gt;Save the changes.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Now, Cursor will have access to the Agno documentation.&lt;/p&gt; 
&lt;h2&gt;Documentation, Community &amp;amp; More examples&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docs: &lt;a href=&quot;https://docs.agno.com&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;docs.agno.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Getting Started Examples: &lt;a href=&quot;https://github.com/agno-agi/agno/tree/main/cookbook/getting_started&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Getting Started Cookbook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;All Examples: &lt;a href=&quot;https://github.com/agno-agi/agno/tree/main/cookbook&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Cookbook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Community forum: &lt;a href=&quot;https://community.agno.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;community.agno.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Chat: &lt;a href=&quot;https://discord.gg/4MtYHHrgA8&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;discord&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;We welcome contributions, read our &lt;a href=&quot;https://github.com/agno-agi/agno/raw/main/CONTRIBUTING.md&quot;&gt;contributing guide&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;Agno logs which model an agent used so we can prioritize updates to the most popular providers. You can disable this by setting &lt;code&gt;AGNO_TELEMETRY=false&lt;/code&gt; in your environment.&lt;/p&gt; 
&lt;p align=&quot;left&quot;&gt; &lt;a href=&quot;https://raw.githubusercontent.com/agno-agi/agno/main/#top&quot;&gt;⬆️ Back to Top&lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Shubhamsaboo/awesome-llm-apps</title>
      <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
      <description>&lt;p&gt;Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;http://www.theunwindai.com&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt; &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;hr&gt; 
&lt;h1&gt;🌟 Awesome LLM Apps&lt;/h1&gt; 
&lt;p&gt;A curated collection of awesome LLM apps built with RAG and AI agents. This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot;&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;🤔 Why Awesome LLM Apps?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;💡 Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.&lt;/li&gt; 
 &lt;li&gt;🔥 Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with RAG and AI Agents.&lt;/li&gt; 
 &lt;li&gt;🎓 Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🚨 Open Source AI Agent Hackathon! 🚨&lt;/h2&gt; 
&lt;p&gt;We&#39;re launching a Global AI Agent Hackathon in collaboration with AI Agent ecosystem partners — open to all developers, builders, and startups working on agents, RAG, tool use, or multi-agent systems.&lt;/p&gt; 
&lt;h3&gt;💰 Win up to $20,000 in cash by building Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;🏅 10 winners: $300 each&lt;/li&gt; 
 &lt;li&gt;🥉 10 winners: $500 each&lt;/li&gt; 
 &lt;li&gt;🥈 5 winners: $1,000 each&lt;/li&gt; 
 &lt;li&gt;🥇 1 winner: $2,000&lt;/li&gt; 
 &lt;li&gt;🏆 GRAND PRIZE: $5,000 🏆&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🎁 Bonus&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Top 5 projects will be featured in the top trending &lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps&quot;&gt;Awesome LLM Apps&lt;/a&gt; repo.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🤝 Partners&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.theunwindai.com&quot;&gt;Unwind AI&lt;/a&gt;, &lt;a href=&quot;https://www.agno.com&quot;&gt;Agno&lt;/a&gt; and more Agent ecosystem companies joining soon.&lt;/p&gt; 
&lt;h3&gt;📅 Here&#39;s the timeline:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;April 3rd - Final dates revealed&lt;/li&gt; 
 &lt;li&gt;April 10th - Prize and success criteria announced&lt;/li&gt; 
 &lt;li&gt;April 15th (tentative) - Hackathon starts&lt;/li&gt; 
 &lt;li&gt;May 30th (tentative) - Hackathon ends&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Join us for a month of building Agents!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Prizes will be distributed on an ongoing basis and continue till all prizes are awarded.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;⭐ Star this repo and follow along to stay updated.&lt;/p&gt; 
&lt;h3&gt;🤝 Want to join us as a partner or judge?&lt;/h3&gt; 
&lt;p&gt;If you&#39;re a company in the AI agent ecosystem or would like to judge the hackathon, reach out to &lt;a href=&quot;https://x.com/Saboo_Shubham_&quot;&gt;Shubham Saboo&lt;/a&gt; or &lt;a href=&quot;https://x.com/ashpreetbedi&quot;&gt;Ashpreet Bedi&lt;/a&gt; on X to partner. Let’s make this the biggest open source AI Agent hackathon.&lt;/p&gt; 
&lt;h2&gt;📂 Featured AI Projects&lt;/h2&gt; 
&lt;h3&gt;AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_customer_support_agent&quot;&gt;💼 AI Customer Support Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_investment_agent&quot;&gt;📈 AI Investment Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_legal_agent_team&quot;&gt;👨‍⚖️ AI Legal Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_recruitment_agent_team&quot;&gt;💼 AI Recruitment Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_services_agency&quot;&gt;👨‍💼 AI Services Agency&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_competitor_intelligence_agent_team&quot;&gt;🧲 AI Competitor Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_health_fitness_agent&quot;&gt;🏋️‍♂️ AI Health &amp;amp; Fitness Planner Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_startup_trend_analysis_agent&quot;&gt;📈 AI Startup Trend Analysis Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_journalist_agent&quot;&gt;🗞️ AI Journalist Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_finance_agent_team&quot;&gt;💲 AI Finance Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_competitor_intelligence_agent_team&quot;&gt;🧲 AI Competitor Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_lead_generation_agent&quot;&gt;🎯 AI Lead Generation Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_personal_finance_agent&quot;&gt;💰 AI Personal Finance Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_medical_imaging_agent&quot;&gt;🩻 AI Medical Scan Diagnosis Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_teaching_agent_team&quot;&gt;👨‍🏫 AI Teaching Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_travel_agent&quot;&gt;🛫 AI Travel Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_movie_production_agent&quot;&gt;🎬 AI Movie Production Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/multi_agent_researcher&quot;&gt;📰 Multi-Agent AI Researcher&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_coding_agent_o3-mini&quot;&gt;💻 Multimodal AI Coding Agent Team with o3-mini and Gemini&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_meeting_agent&quot;&gt;📑 AI Meeting Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_chess_agent&quot;&gt;♜ AI Chess Agent Game&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_real_estate_agent&quot;&gt;🏠 AI Real Estate Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/local_news_agent_openai_swarm&quot;&gt;🌐 Local News Agent OpenAI Swarm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/xai_finance_agent&quot;&gt;📊 AI Finance Agent with xAI Grok&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_3dpygame_r1&quot;&gt;🎮 AI 3D PyGame Visualizer with DeepSeek R1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_reasoning_agent&quot;&gt;🧠 AI Reasoning Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/multimodal_ai_agent&quot;&gt;🧬 Multimodal AI Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;RAG (Retrieval Augmented Generation)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/autonomous_rag&quot;&gt;🔍 Autonomous RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/agentic_rag&quot;&gt;🔗 Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/gemini_agentic_rag&quot;&gt;🤔 Agentic RAG with Gemini Flash Thinking&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/deepseek_local_rag_agent&quot;&gt;🐋 Deepseek Local RAG Reasoning Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/llama3.1_local_rag&quot;&gt;🔄 Llama3.1 Local RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/rag-as-a-service&quot;&gt;🧩 RAG-as-a-Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/local_rag_agent&quot;&gt;🦙 Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/hybrid_search_rag&quot;&gt;👀 RAG App with Hybrid Search&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/local_hybrid_search_rag&quot;&gt;🖥️ Local RAG App with Hybrid Search&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/rag_database_routing&quot;&gt;📠 RAG Agent with Database Routing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/corrective_rag&quot;&gt;🔄 Corrective RAG Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;MCP AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/mcp_ai_agents/github_mcp_agent&quot;&gt;🐙 MCP GitHub Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;LLM Apps with Memory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory&quot;&gt;💾 AI Arxiv Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/llm_app_personalized_memory&quot;&gt;📝 LLM App with Personalized Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/ai_travel_agent_memory&quot;&gt;🛩️ AI Travel Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/local_chatgpt_with_memory&quot;&gt;🗄️ Local ChatGPT with Memory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Chat with X&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_github&quot;&gt;💬 Chat with GitHub Repo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_gmail&quot;&gt;📨 Chat with Gmail&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_pdf&quot;&gt;📄 Chat with PDF&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_research_papers&quot;&gt;📚 Chat with Research Papers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_substack&quot;&gt;📝 Chat with Substack Newsletter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_youtube_videos&quot;&gt;📽️ Chat with YouTube Videos&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;LLM Finetuning&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_finetuning_tutorials/llama3.2_finetuning&quot;&gt;🌐 Llama3.2 Finetuning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Tools and Frameworks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/gemini_multimodal_chatbot&quot;&gt;🧪 Gemini Multimodal Chatbot&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/mixture_of_agents&quot;&gt;🔄 Mixture of Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/multillm_chat_playground&quot;&gt;🌐 MultiLLM Chat Playground&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/llm_router_app&quot;&gt;🔗 LLM Router App&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/local_chatgpt_clone&quot;&gt;💬 Local ChatGPT Clone&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/web_scrapping_ai_agent&quot;&gt;🌍 Web Scraping AI Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/web_search_ai_assistant&quot;&gt;🔍 Web Search AI Assistant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/cursor_ai_experiments&quot;&gt;🧪 Cursor AI Experiments&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🚀 Getting Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to the desired project directory&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd awesome-llm-apps/chat_with_X_tutorials/chat_with_gmail
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install the required dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Follow the project-specific instructions&lt;/strong&gt; in each project&#39;s &lt;code&gt;README.md&lt;/code&gt; file to set up and run the app.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;🤝 Contributing to Open Source&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new &lt;a href=&quot;https://github.com/Shubhamsaboo/awesome-llm-apps/issues&quot;&gt;GitHub Issue&lt;/a&gt; or submit a pull request. Make sure to follow the existing project structure and include a detailed &lt;code&gt;README.md&lt;/code&gt; for each new app.&lt;/p&gt; 
&lt;h3&gt;Thank You, Community, for the Support! 🙏&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;amp;Date&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;🌟 &lt;strong&gt;Don’t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hiroi-sora/Umi-OCR</title>
      <link>https://github.com/hiroi-sora/Umi-OCR</link>
      <description>&lt;p&gt;OCR software, free and offline. 开源、免费的离线OCR软件。支持截屏/批量导入图片，PDF文档识别，排除水印/页眉页脚，扫描/生成二维码。内置多国语言库。&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;left&quot;&gt; &lt;span&gt; &lt;b&gt;中文&lt;/b&gt; &lt;/span&gt; &lt;span&gt; • &lt;/span&gt; &lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/README_en.md&quot;&gt; English &lt;/a&gt; &lt;span&gt; • &lt;/span&gt; &lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/README_ja.md&quot;&gt; 日本語 &lt;/a&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR&quot;&gt; &lt;img width=&quot;200&quot; height=&quot;128&quot; src=&quot;https://tupian.li/images/2022/10/27/icon---256.png&quot; alt=&quot;Umi-OCR&quot;&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1 align=&quot;center&quot;&gt;Umi-OCR 文字识别工具&lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/releases/latest&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/v/release/hiroi-sora/Umi-OCR?style=flat-square&quot; alt=&quot;Umi-OCR&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/raw/main/LICENSE&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/license/hiroi-sora/Umi-OCR?style=flat-square&quot; alt=&quot;LICENSE&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E4%B8%8B%E8%BD%BD%E5%8F%91%E8%A1%8C%E7%89%88&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/downloads/hiroi-sora/Umi-OCR/total?style=flat-square&quot; alt=&quot;forks&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://star-history.com/#hiroi-sora/Umi-OCR&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/stars/hiroi-sora/Umi-OCR?style=flat-square&quot; alt=&quot;stars&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/forks&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/forks/hiroi-sora/Umi-OCR?style=flat-square&quot; alt=&quot;forks&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://hosted.weblate.org/engage/umi-ocr/&quot;&gt; &lt;img src=&quot;https://hosted.weblate.org/widget/umi-ocr/svg-badge.svg?sanitize=true&quot; alt=&quot;翻译状态&quot;&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;h3&gt; &lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E7%9B%AE%E5%BD%95&quot;&gt; 使用说明 &lt;/a&gt; &lt;span&gt; • &lt;/span&gt; &lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E4%B8%8B%E8%BD%BD%E5%8F%91%E8%A1%8C%E7%89%88&quot;&gt; 下载地址 &lt;/a&gt; &lt;span&gt; • &lt;/span&gt; &lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/CHANGE_LOG.md&quot;&gt; 更新日志 &lt;/a&gt; &lt;span&gt; • &lt;/span&gt; &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/issues&quot;&gt; 提交Bug &lt;/a&gt; &lt;/h3&gt; 
&lt;/div&gt; 
&lt;br&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;strong&gt;免费，开源，可批量的离线OCR软件&lt;/strong&gt;
 &lt;br&gt; 
 &lt;sub&gt;适用于 Windows7 x64 、Linux x64 &lt;/sub&gt;
&lt;/div&gt;
&lt;br&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;免费&lt;/strong&gt;：本项目所有代码开源，完全免费。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;方便&lt;/strong&gt;：解压即用，离线运行，无需网络。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;高效&lt;/strong&gt;：自带高效率的离线OCR引擎，内置多种语言识别库。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;灵活&lt;/strong&gt;：支持命令行、HTTP接口等外部调用方式。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;功能&lt;/strong&gt;：截图OCR / 批量OCR / PDF识别 / 二维码 / 公式识别&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/65599097ab5f4.png&quot; alt=&quot;1-标题-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/6559909fdeeba.png&quot; alt=&quot;1-标题-2.png&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;目录&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%88%AA%E5%9B%BEOCR&quot;&gt;截图识别&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%96%87%E6%9C%AC%E5%90%8E%E5%A4%84%E7%90%86&quot;&gt;排版解析&lt;/a&gt; - 识别不同排版，按正确顺序输出文字&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%89%B9%E9%87%8FOCR&quot;&gt;批量识别&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E5%BF%BD%E7%95%A5%E5%8C%BA%E5%9F%9F&quot;&gt;忽略区域&lt;/a&gt; - 排除截图水印处的文字&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E4%BA%8C%E7%BB%B4%E7%A0%81&quot;&gt;二维码&lt;/a&gt; 支持扫码或生成二维码图片&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%96%87%E6%A1%A3%E8%AF%86%E5%88%AB&quot;&gt;文档识别&lt;/a&gt; 从PDF扫描件中提取文本，或转为双层可搜索PDF&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E5%85%A8%E5%B1%80%E8%AE%BE%E7%BD%AE&quot;&gt;全局设置&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/docs/README_CLI.md&quot;&gt;命令行调用&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/docs/http/README.md&quot;&gt;HTTP接口&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%9E%84%E5%BB%BA%E9%A1%B9%E7%9B%AE&quot;&gt;构建项目（Windows、Linux）&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;使用源码&lt;/h2&gt; 
&lt;p&gt;开发者请务必阅读 &lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%9E%84%E5%BB%BA%E9%A1%B9%E7%9B%AE&quot;&gt;构建项目&lt;/a&gt; 。&lt;/p&gt; 
&lt;h2&gt;下载发行版&lt;/h2&gt; 
&lt;p&gt;以下发布链接均长期维护，提供稳定版本的下载。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;蓝奏云&lt;/strong&gt; &lt;a href=&quot;https://hiroi-sora.lanzoul.com/s/umi-ocr&quot;&gt;https://hiroi-sora.lanzoul.com/s/umi-ocr&lt;/a&gt; （国内推荐，免注册/无限速）&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub&lt;/strong&gt; &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/releases/latest&quot;&gt;https://github.com/hiroi-sora/Umi-OCR/releases/latest&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source Forge&lt;/strong&gt; &lt;a href=&quot;https://sourceforge.net/projects/umi-ocr&quot;&gt;https://sourceforge.net/projects/umi-ocr&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;•&amp;nbsp;&amp;nbsp;Scoop Installer&lt;/b&gt;（点击展开）&lt;/summary&gt; 
 &lt;p&gt;&lt;a href=&quot;https://scoop.sh/&quot;&gt;Scoop&lt;/a&gt; 是一款Windows下的命令行安装程序，可方便地管理多个应用。您可以先安装 Scoop ，再使用以下指令安装 &lt;code&gt;Umi-OCR&lt;/code&gt; ：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;添加 &lt;code&gt;extras&lt;/code&gt; 桶：&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code&gt;scoop bucket add extras
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;（可选1）安装 Umi-OCR（自带 &lt;code&gt;Rapid-OCR&lt;/code&gt; 引擎，兼容性好）：&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code&gt;scoop install extras/umi-ocr
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;（可选2）安装 Umi-OCR（自带 &lt;code&gt;Paddle-OCR&lt;/code&gt; 引擎，速度稍快）：&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code&gt;scoop install extras/umi-ocr-paddle
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;不要同时安装二者，快捷方式可能会被覆盖。但您可以额外导入 &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR_plugins&quot;&gt;插件&lt;/a&gt; ，随时切换不同OCR引擎。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;br&gt; 
&lt;h2&gt;开始使用&lt;/h2&gt; 
&lt;p&gt;软件发布包下载为 &lt;code&gt;.7z&lt;/code&gt; 压缩包或 &lt;code&gt;.7z.exe&lt;/code&gt; 自解压包。自解压包可在没有安装压缩软件的电脑上，解压文件。&lt;/p&gt; 
&lt;p&gt;本软件无需安装。解压后，点击 &lt;code&gt;Umi-OCR.exe&lt;/code&gt; 即可启动程序。&lt;/p&gt; 
&lt;p&gt;遇到任何问题，请提 &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/issues&quot;&gt;Issue&lt;/a&gt; ，我会尽可能帮助你。&lt;/p&gt; 
&lt;h2&gt;界面语言&lt;/h2&gt; 
&lt;p&gt;Umi-OCR 支持的界面多国语言。在第一次打开软件时，将会按照你的电脑的系统设置，自动切换语言。&lt;/p&gt; 
&lt;p&gt;如果需要手动切换语言，请参考下图，&lt;code&gt;全局设置&lt;/code&gt;→&lt;code&gt;语言/Language&lt;/code&gt; 。&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/65599c3f9e600.png&quot; alt=&quot;1-标题-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;标签页&lt;/h2&gt; 
&lt;p&gt;Umi-OCR v2 由一系列灵活好用的&lt;strong&gt;标签页&lt;/strong&gt;组成。您可按照自己的喜好，打开需要的标签页。&lt;/p&gt; 
&lt;p&gt;标签栏左上角可以切换&lt;strong&gt;窗口置顶&lt;/strong&gt;。右上角能够&lt;strong&gt;锁定标签页&lt;/strong&gt;，以防止日常使用中误触关闭标签页。&lt;/p&gt; 
&lt;h3&gt;截图OCR&lt;/h3&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/65599097aba8e.png&quot; alt=&quot;2-截图-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;截图OCR&lt;/strong&gt;：打开这一页后，就可以用快捷键唤起截图，识别图中的文字。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;左侧的图片预览栏，可直接用鼠标划选复制。&lt;/li&gt; 
 &lt;li&gt;右侧的识别记录栏，可以编辑文字，允许划选多个记录复制。&lt;/li&gt; 
 &lt;li&gt;也支持在别处复制图片，粘贴到Umi-OCR进行识别。&lt;/li&gt; 
 &lt;li&gt;关于 &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/issues/254&quot;&gt;公式识别&lt;/a&gt; 功能&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;文本后处理&lt;/h4&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/6559909f3e378.png&quot; alt=&quot;2-截图-2.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt; 
&lt;p&gt;关于 &lt;strong&gt;OCR文本后处理 - 排版解析方案&lt;/strong&gt;： 可以整理OCR结果的排版和顺序，使文本更适合阅读和使用。预设方案：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;多栏-按自然段换行&lt;/code&gt;：适合大部分情景，自动识别多栏布局，按自然段规则进行换行。&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;多栏-总是换行&lt;/code&gt;：每段语句都进行换行。&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;多栏-无换行&lt;/code&gt;：强制将所有语句合并到同一行。&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;单栏-按自然段换行&lt;/code&gt;/&lt;code&gt;总是换行&lt;/code&gt;/&lt;code&gt;无换行&lt;/code&gt;：与上述类似，不过 不区分多栏布局。&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;单栏-保留缩进&lt;/code&gt;：适用于解析代码截图，保留行首缩进和行中空格。&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;不做处理&lt;/code&gt;：OCR引擎的原始输出，默认每段语句都进行换行。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;上述方案，均能自动处理横排和竖排（从右到左）的排版。（竖排文字还需要OCR引擎本身支持）&lt;/p&gt; 
&lt;hr&gt; 
&lt;h3&gt;批量OCR&lt;/h3&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/655990a2511e0.png&quot; alt=&quot;3-批量-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;批量OCR&lt;/strong&gt;：这一页用于批量导入本地图片进行识别。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;支持格式：&lt;code&gt;jpg, jpe, jpeg, jfif, png, webp, bmp, tif, tiff&lt;/code&gt;。&lt;/li&gt; 
 &lt;li&gt;保存识别结果的支持格式：&lt;code&gt;txt, jsonl, md, csv(Excel)&lt;/code&gt;。&lt;/li&gt; 
 &lt;li&gt;与截图OCR一样，支持&lt;code&gt;文本后处理&lt;/code&gt;功能，整理OCR文本的排版和顺序。&lt;/li&gt; 
 &lt;li&gt;没有数量上限，可一次性导入几百张图片进行任务。&lt;/li&gt; 
 &lt;li&gt;支持任务完成后自动关机/待机。&lt;/li&gt; 
 &lt;li&gt;如果要识别像素超大的长图或大图，请调整：&lt;strong&gt;页面的设置→文字识别→限制图像边长→【调高数值】&lt;/strong&gt;。&lt;/li&gt; 
 &lt;li&gt;拥有特殊功能 &lt;code&gt;忽略区域&lt;/code&gt; 。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;忽略区域&lt;/h4&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/6559911d28be7.png&quot; alt=&quot;3-批量-2.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt; 
&lt;p&gt;关于 &lt;strong&gt;OCR文本后处理 - 忽略区域&lt;/strong&gt;： 批量OCR中的一种特殊功能，适用于排除图片中的不想要的文字。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;在批量识别页的右栏设置中可进入忽略区域编辑器。&lt;/li&gt; 
 &lt;li&gt;如上方样例，图片顶部和右下角存在多个水印 / LOGO。如果批量识别这类图片，水印会对识别结果造成干扰。&lt;/li&gt; 
 &lt;li&gt;按住右键，绘制多个矩形框。这些区域内的文字将在任务中被忽略。&lt;/li&gt; 
 &lt;li&gt;请尽量将矩形框画得大一些，完全包裹住水印所有可能出现的位置。&lt;/li&gt; 
 &lt;li&gt;注意，只有处于忽略区域框内部的整个文本块（而不是单个字符）会被忽略。如下图所示，黄色边框的深色矩形是一个忽略区域。那么只有&lt;code&gt;key_mouse&lt;/code&gt;才会被忽略。&lt;code&gt;pubsub_connector.py&lt;/code&gt;、&lt;code&gt;pubsub_service.py&lt;/code&gt; 这两个文本块得以保留。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2024/05/30/66587bf03ae15.png&quot; alt=&quot;忽略区域范围示例.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;h3&gt;文档识别&lt;/h3&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/hiroi-sora/Umi-OCR/assets/56373419/fc2266ee-b9b7-4079-8b10-6610e6da6cf5&quot; alt=&quot;&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;文档识别&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;支持格式：&lt;code&gt;pdf, xps, epub, mobi, fb2, cbz&lt;/code&gt;。&lt;/li&gt; 
 &lt;li&gt;对扫描件进行OCR，或提取原有文本。可输出为 &lt;strong&gt;双层可搜索PDF&lt;/strong&gt; 。&lt;/li&gt; 
 &lt;li&gt;支持设定 &lt;strong&gt;忽略区域&lt;/strong&gt; ，可用于排除页眉页脚的文字。&lt;/li&gt; 
 &lt;li&gt;可设置任务完成后 &lt;strong&gt;自动关机/休眠&lt;/strong&gt; 。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h3&gt;二维码&lt;/h3&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/655991268d6b1.png&quot; alt=&quot;4-二维码-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;扫码&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;截图/粘贴/拖入本地图片，读取其中的二维码、条形码。&lt;/li&gt; 
 &lt;li&gt;支持一图多码。&lt;/li&gt; 
 &lt;li&gt;支持19种协议，如下：&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;Aztec&lt;/code&gt;,&lt;code&gt;Codabar&lt;/code&gt;,&lt;code&gt;Code128&lt;/code&gt;,&lt;code&gt;Code39&lt;/code&gt;,&lt;code&gt;Code93&lt;/code&gt;,&lt;code&gt;DataBar&lt;/code&gt;,&lt;code&gt;DataBarExpanded&lt;/code&gt;,&lt;code&gt;DataMatrix&lt;/code&gt;,&lt;code&gt;EAN13&lt;/code&gt;,&lt;code&gt;EAN8&lt;/code&gt;,&lt;code&gt;ITF&lt;/code&gt;,&lt;code&gt;LinearCodes&lt;/code&gt;,&lt;code&gt;MatrixCodes&lt;/code&gt;,&lt;code&gt;MaxiCode&lt;/code&gt;,&lt;code&gt;MicroQRCode&lt;/code&gt;,&lt;code&gt;PDF417&lt;/code&gt;,&lt;code&gt;QRCode&lt;/code&gt;,&lt;code&gt;UPCA&lt;/code&gt;,&lt;code&gt;UPCE&lt;/code&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/6559911cda737.png&quot; alt=&quot;4-二维码-2.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;生成码&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;输入文本，生成二维码图片。&lt;/li&gt; 
 &lt;li&gt;支持19种协议和&lt;strong&gt;纠错等级&lt;/strong&gt;等参数。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h3&gt;全局设置&lt;/h3&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/655991252e780.png&quot; alt=&quot;5-全局设置-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;全局设置&lt;/strong&gt;：在这里可以调整软件的全局参数。常用功能如下：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;一键添加快捷方式或设置开机自启。&lt;/li&gt; 
 &lt;li&gt;更改界面&lt;strong&gt;语言&lt;/strong&gt;。Umi支持繁中、英语、日语等语言。&lt;/li&gt; 
 &lt;li&gt;切换界面&lt;strong&gt;主题&lt;/strong&gt;。Umi拥有多个亮/暗主题。&lt;/li&gt; 
 &lt;li&gt;调整界面&lt;strong&gt;文字的大小&lt;/strong&gt;和&lt;strong&gt;字体&lt;/strong&gt;。&lt;/li&gt; 
 &lt;li&gt;切换OCR插件。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;渲染器&lt;/strong&gt;：软件界面默认支持显卡加速渲染。如果在你的机器上出现截屏闪烁、UI错位的情况，请调整&lt;code&gt;界面和外观&lt;/code&gt; → &lt;code&gt;渲染器&lt;/code&gt; ，尝试切换到不同渲染方案，或关闭硬件加速。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;调用接口：&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/docs/README_CLI.md&quot;&gt;命令行手册&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/docs/http/README.md&quot;&gt;HTTP接口手册&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h2&gt;关于项目结构&lt;/h2&gt; 
&lt;h3&gt;各仓库：&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR&quot;&gt;主仓库&lt;/a&gt; 👈&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR_plugins&quot;&gt;插件库&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR_runtime_windows&quot;&gt;Windows 运行库&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR_runtime_linux&quot;&gt;Linux 运行库&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;工程结构：&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;**&lt;/code&gt; 后缀表示本仓库(&lt;code&gt;主仓库&lt;/code&gt;)包含的内容。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Umi-OCR
├─ Umi-OCR.exe
├─ umi-ocr.sh
└─ UmiOCR-data
   ├─ main.py **
   ├─ version.py **
   ├─ qt_res **
   │  └─ 项目qt资源，包括图标和qml源码
   ├─ py_src **
   │  └─ 项目python源码
   ├─ plugins
   │  └─ 插件
   └─ i18n **
      └─ 翻译文件
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;支持的离线OCR引擎：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hiroi-sora/PaddleOCR-json&quot;&gt;PaddleOCR-json&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hiroi-sora/RapidOCR-json&quot;&gt;RapidOCR-json&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;运行环境框架：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/skywind3000/PyStand&quot;&gt;PyStand&lt;/a&gt; 定制版&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;构建项目&lt;/h2&gt; 
&lt;p&gt;请跳转下述仓库，完成对应平台的开发/运行环境部署。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR_runtime_windows&quot;&gt;Windows&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR_runtime_linux&quot;&gt;Linux&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;h2&gt;软件本地化翻译：&lt;/h2&gt; 
&lt;p&gt;本项目使用 Weblate 平台进行UI界面的本地化翻译协作。我们欢迎任何译者参与翻译工作，您可进入此链接 &lt;a href=&quot;https://hosted.weblate.org/engage/umi-ocr/&quot;&gt;Weblate: Umi-OCR&lt;/a&gt; ，在线校对、补充现有语言，或添加新语言。&lt;/p&gt; 
&lt;p&gt;感谢以下译者，为 Umi-OCR 贡献了本地化翻译工作：&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;译者&lt;/th&gt; 
   &lt;th&gt;贡献语言&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hosted.weblate.org/user/q021&quot;&gt;bob&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, 繁體中文, 日本語&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QZGao&quot;&gt;Qingzheng Gao&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, 繁體中文&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hosted.weblate.org/user/ChiaLingWeng&quot;&gt;Weng, Chia-Ling&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, 繁體中文&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hosted.weblate.org/user/linzow&quot;&gt;linzow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, 繁體中文&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hosted.weblate.org/user/ultramarkorj9&quot;&gt;Marcos i&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, Português&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hosted.weblate.org/user/qwedc001&quot;&gt;Eric Guo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hosted.weblate.org/user/steven0081&quot;&gt;steven0081&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hosted.weblate.org/user/random4t4x14&quot;&gt;Brandon Cagle&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hosted.weblate.org/user/plum7x&quot;&gt;plum7x&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;繁體中文&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hosted.weblate.org/user/hugoalh&quot;&gt;hugoalh&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;繁體中文&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hosted.weblate.org/user/Anarkiisto&quot;&gt;Anarkiisto&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;繁體中文&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hosted.weblate.org/user/umren190402&quot;&gt;ドコモ光&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;日本語&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hosted.weblate.org/user/ypf&quot;&gt;杨鹏&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Português&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hosted.weblate.org/user/1969&quot;&gt;Вячеслав Анатольевич Малышев&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Русский&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hosted.weblate.org/user/muhammadyusuf.kurbonov2002&quot;&gt;Muhammadyusuf Kurbonov&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Русский&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://hosted.weblate.org/user/TamilNeram/&quot;&gt;தமிழ்நேரம்&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;தமிழ்&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;如果有信息错误或人员缺漏，请在 &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/discussions/449&quot;&gt;这个讨论&lt;/a&gt; 中回复。&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;赞助&lt;/h2&gt; 
&lt;p&gt;Umi-OCR 项目主要由作者 &lt;a href=&quot;https://github.com/hiroi-sora&quot;&gt;hiroi-sora&lt;/a&gt; 用业余时间在开发和维护。如果您喜欢这款软件，欢迎赞助。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;国内用户可通过 &lt;a href=&quot;https://afdian.com/a/hiroi-sora&quot;&gt;爱发电&lt;/a&gt; 赞助作者。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://star-history.com/#hiroi-sora/Umi-OCR&amp;amp;Date&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=hiroi-sora/Umi-OCR&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/CHANGE_LOG.md&quot;&gt;更新日志&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;开发计划&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;已完成的工作&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;标签页框架。&lt;/li&gt; 
  &lt;li&gt;OCR API控制器。&lt;/li&gt; 
  &lt;li&gt;OCR 任务控制器。&lt;/li&gt; 
  &lt;li&gt;主题管理器，支持切换浅色/深色主题主题。&lt;/li&gt; 
  &lt;li&gt;实现 &lt;strong&gt;批量OCR&lt;/strong&gt;。&lt;/li&gt; 
  &lt;li&gt;实现 &lt;strong&gt;截图OCR&lt;/strong&gt;。&lt;/li&gt; 
  &lt;li&gt;快捷键机制。&lt;/li&gt; 
  &lt;li&gt;系统托盘菜单。&lt;/li&gt; 
  &lt;li&gt;文本块后处理（排版优化）。&lt;/li&gt; 
  &lt;li&gt;引擎内存清理。&lt;/li&gt; 
  &lt;li&gt;软件界面多国语言。&lt;/li&gt; 
  &lt;li&gt;命令行模式。&lt;/li&gt; 
  &lt;li&gt;Win7兼容。&lt;/li&gt; 
  &lt;li&gt;Excel（csv）输出格式。&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;Esc&lt;/code&gt;中断截图操作&lt;/li&gt; 
  &lt;li&gt;外置主题文件&lt;/li&gt; 
  &lt;li&gt;字体切换&lt;/li&gt; 
  &lt;li&gt;加载动画&lt;/li&gt; 
  &lt;li&gt;忽略区域。&lt;/li&gt; 
  &lt;li&gt;二维码识别。&lt;/li&gt; 
  &lt;li&gt;批量识别页面的图片预览窗口。&lt;/li&gt; 
  &lt;li&gt;PDF识别。&lt;/li&gt; 
  &lt;li&gt;调用本地图片浏览器打开图片。 &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/issues/335&quot;&gt;#335&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;重复上一次截图。 &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/issues/357&quot;&gt;#357&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;修Bug：文档识别在Windows7系统的兼容性问题。&lt;/li&gt; 
  &lt;li&gt;HTTP/命令行接口添加二维码识别/生成功能。 (#423)&lt;/li&gt; 
  &lt;li&gt;二维码接口的文档。&lt;/li&gt; 
  &lt;li&gt;Linux 平台移植。&lt;/li&gt; 
  &lt;li&gt;HTTP 文档识别接口。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;!-- ##### 正在进行的工作 --&gt; 
&lt;h5&gt;远期计划&lt;/h5&gt; 
&lt;details&gt; 
 &lt;summary&gt;展开&lt;/summary&gt; 
 &lt;p&gt;这些是预想中的功能，在开发初期已预留好接口，将在远期慢慢实现。&lt;/p&gt; 
 &lt;p&gt;但开发途中受限于实际情况，可能更改功能设计、新增及取消功能。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; &lt;p&gt;重构底层插件机制。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; &lt;p&gt;在线 OCR API 插件。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; &lt;p&gt;独立的数学公式识别插件。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; &lt;p&gt;“数学公式”标签页，提供独立的数学公式识别/Latex渲染。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; &lt;p&gt;检查更新机制。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; &lt;p&gt;排版解析之外的文本后处理模块（如保留数字、半全角字符转换、文本纠错）。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; &lt;p&gt;关键接口函数添加事件触发方式。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;基于GPU的离线OCR。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;图片翻译&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;离线翻译。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;固定区域识别。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;识别表格图片，输出为Excel。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;历史记录系统。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;兼容 MacOS / Ubuntu 等平台。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>bregman-arie/devops-exercises</title>
      <link>https://github.com/bregman-arie/devops-exercises</link>
      <description>&lt;p&gt;Linux, Jenkins, AWS, SRE, Prometheus, Docker, Python, Ansible, Git, Kubernetes, Terraform, OpenStack, SQL, NoSQL, Azure, GCP, DNS, Elastic, Network, Virtualization. DevOps Interview Questions&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/devops_exercises.png&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;ℹ&lt;/span&gt; &amp;nbsp;This repo contains questions and exercises on various technical topics, sometimes related to DevOps and SRE&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;📊&lt;/span&gt; &amp;nbsp;There are currently &lt;strong&gt;2624&lt;/strong&gt; exercises and questions&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; &amp;nbsp;You can use these for preparing for an interview but most of the questions and exercises don&#39;t represent an actual interview. Please read &lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/faq.md&quot;&gt;FAQ page&lt;/a&gt; for more details&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;🛑&lt;/span&gt; &amp;nbsp;If you are interested in pursuing a career as DevOps engineer, learning some of the concepts mentioned here would be useful, but you should know it&#39;s not about learning all the topics and technologies mentioned in this repository&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;📝&lt;/span&gt; &amp;nbsp;You can add more exercises by submitting pull requests :) Read about contribution guidelines &lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/CONTRIBUTING.md&quot;&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;!-- ALL-TOPICS-LIST:START --&gt; 
&lt;!-- prettier-ignore-start --&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;center&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/devops/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/devops.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;DevOps&quot;&gt;&lt;br&gt;&lt;b&gt;DevOps&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/git/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/git.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Git&quot;&gt;&lt;br&gt;&lt;b&gt;Git&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#network&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/network.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Network&quot;&gt;&lt;br&gt;&lt;b&gt;Network&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#hardware&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/hardware.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Hardware&quot;&gt;&lt;br&gt;&lt;b&gt;Hardware&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/kubernetes/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/kubernetes.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;kubernetes&quot;&gt;&lt;br&gt;&lt;b&gt;Kubernetes&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/software_development/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/programming.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;programming&quot;&gt;&lt;br&gt;&lt;b&gt;Software Development&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bregman-arie/python-exercises&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/python.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Python&quot;&gt;&lt;br&gt;&lt;b&gt;Python&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bregman-arie/go-exercises&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/Go.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;go&quot;&gt;&lt;br&gt;&lt;b&gt;Go&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/perl/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/perl.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;perl&quot;&gt;&lt;br&gt;&lt;b&gt;Perl&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#regex&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/regex.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;RegEx&quot;&gt;&lt;br&gt;&lt;b&gt;Regex&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/cloud/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/cloud.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Cloud&quot;&gt;&lt;br&gt;&lt;b&gt;Cloud&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/aws/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/aws.png&quot; width=&quot;100px;&quot; height=&quot;75px;&quot; alt=&quot;aws&quot;&gt;&lt;br&gt;&lt;b&gt;AWS&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/azure/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/azure.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;azure&quot;&gt;&lt;br&gt;&lt;b&gt;Azure&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/gcp/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/googlecloud.png&quot; width=&quot;70px;&quot; height=&quot;70px;&quot; alt=&quot;Google Cloud Platform&quot;&gt;&lt;br&gt;&lt;b&gt;Google Cloud Platform&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#openstack/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/openstack.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;openstack&quot;&gt;&lt;br&gt;&lt;b&gt;OpenStack&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#operating-system&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/os.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Operating System&quot;&gt;&lt;br&gt;&lt;b&gt;Operating System&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/linux/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/linux.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Linux&quot;&gt;&lt;br&gt;&lt;b&gt;Linux&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#virtualization&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/virtualization.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Virtualization&quot;&gt;&lt;br&gt;&lt;b&gt;Virtualization&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/dns/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/dns.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;DNS&quot;&gt;&lt;br&gt;&lt;b&gt;DNS&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/shell/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/bash.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Bash&quot;&gt;&lt;br&gt;&lt;b&gt;Shell Scripting&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/databases/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/databases.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Databases&quot;&gt;&lt;br&gt;&lt;b&gt;Databases&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#sql&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/sql.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;sql&quot;&gt;&lt;br&gt;&lt;b&gt;SQL&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#mongo&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/mongo.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Mongo&quot;&gt;&lt;br&gt;&lt;b&gt;Mongo&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#testing&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/testing.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Testing&quot;&gt;&lt;br&gt;&lt;b&gt;Testing&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#big-data&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/big-data.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Big Data&quot;&gt;&lt;br&gt;&lt;b&gt;Big Data&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/cicd/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/cicd.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;cicd&quot;&gt;&lt;br&gt;&lt;b&gt;CI/CD&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#certificates&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/certificates.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Certificates&quot;&gt;&lt;br&gt;&lt;b&gt;Certificates&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/containers/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/containers.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Containers&quot;&gt;&lt;br&gt;&lt;b&gt;Containers&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/openshift/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/openshift.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;OpenShift&quot;&gt;&lt;br&gt;&lt;b&gt;OpenShift&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#storage&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/storage.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Storage&quot;&gt;&lt;br&gt;&lt;b&gt;Storage&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/terraform/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/terraform.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Terraform&quot;&gt;&lt;br&gt;&lt;b&gt;Terraform&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#puppet&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/puppet.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;puppet&quot;&gt;&lt;br&gt;&lt;b&gt;Puppet&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#distributed&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/distributed.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Distributed&quot;&gt;&lt;br&gt;&lt;b&gt;Distributed&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#questions-you-ask&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/you.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;you&quot;&gt;&lt;br&gt;&lt;b&gt;Questions you can ask&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/ansible/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/ansible.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;ansible&quot;&gt;&lt;br&gt;&lt;b&gt;Ansible&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/observability/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/observability.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;observability&quot;&gt;&lt;br&gt;&lt;b&gt;Observability&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#prometheus&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/prometheus.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Prometheus&quot;&gt;&lt;br&gt;&lt;b&gt;Prometheus&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/circleci/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/circleci.png&quot; width=&quot;70px;&quot; height=&quot;70px;&quot; alt=&quot;Circle CI&quot;&gt;&lt;br&gt;&lt;b&gt;Circle CI&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/datadog/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/datadog.png&quot; width=&quot;80px;&quot; height=&quot;80px;&quot; alt=&quot;DataDog&quot;&gt;&lt;br&gt;&lt;b&gt;&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/grafana/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/grafana.png&quot; width=&quot;80px;&quot; height=&quot;80px;&quot; alt=&quot;Grafana&quot;&gt;&lt;br&gt;&lt;b&gt;Grafana&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/argo/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/argo.png&quot; width=&quot;80px;&quot; height=&quot;80px;&quot; alt=&quot;Argo&quot;&gt;&lt;br&gt;&lt;b&gt;Argo&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/soft_skills/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/HR.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;HR&quot;&gt;&lt;br&gt;&lt;b&gt;Soft Skills&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/security/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/security.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;security&quot;&gt;&lt;br&gt;&lt;b&gt;Security&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#system-design&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/design.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Design&quot;&gt;&lt;br&gt;&lt;b&gt;System Design&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/chaos_engineering/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/chaos_engineering.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Chaos Engineering&quot;&gt;&lt;br&gt;&lt;b&gt;Chaos Engineering&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#Misc&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/general.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Misc&quot;&gt;&lt;br&gt;&lt;b&gt;Misc&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/#elastic&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/elastic.png&quot; width=&quot;75px;&quot; height=&quot;75px;&quot; alt=&quot;Elastic&quot;&gt;&lt;br&gt;&lt;b&gt;Elastic&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/kafka/README.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/logos/kafka.png&quot; width=&quot;85px;&quot; height=&quot;80px;&quot; alt=&quot;Kafka&quot;&gt;&lt;br&gt;&lt;b&gt;Kafka&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/node/node_questions_basic.md&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/nodejs.png&quot; width=&quot;85px;&quot; height=&quot;80px;&quot; alt=&quot;NodeJs&quot;&gt;&lt;br&gt;&lt;b&gt;NodeJs&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/center&gt; 
&lt;!-- markdownlint-enable --&gt; 
&lt;!-- prettier-ignore-end --&gt; 
&lt;!-- ALL-TOPICS-LIST:END --&gt; 
&lt;h2&gt;DevOps Applications&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://play.google.com/store/apps/details?id=com.codingshell.kubeprep&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/apps/kubeprep.png&quot; width=&quot;200px;&quot; height=&quot;300px;&quot; alt=&quot;KubePrep&quot;&gt;&lt;br&gt;&lt;b&gt;KubePrep&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://play.google.com/store/apps/details?id=com.codingshell.linuxmaster&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/apps/linux_master.png&quot; width=&quot;200px;&quot; height=&quot;300px;&quot; alt=&quot;Linux Master&quot;&gt;&lt;br&gt;&lt;b&gt;Linux Master&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://play.google.com/store/apps/details?id=com.codingshell.system_design_hero&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/apps/system_design_hero.png&quot; width=&quot;200px;&quot; height=&quot;300px;&quot; alt=&quot;Sytem Design Hero&quot;&gt;&lt;br&gt;&lt;b&gt;System Design Hero&lt;/b&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Network&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;In general, what do you need in order to communicate?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; 
  &lt;ul&gt; 
   &lt;li&gt;A common language (for the two ends to understand)&lt;/li&gt; 
   &lt;li&gt;A way to address who you want to communicate with&lt;/li&gt; 
   &lt;li&gt;A Connection (so the content of the communication can reach the recipients)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is TCP/IP?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A set of protocols that define how two or more devices can communicate with each other.&lt;/p&gt; &lt;p&gt;To learn more about TCP/IP, read &lt;a href=&quot;http://www.penguintutor.com/linux/basic-network-reference&quot;&gt;here&lt;/a&gt;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Ethernet?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Ethernet simply refers to the most common type of Local Area Network (LAN) used today. A LAN—in contrast to a WAN (Wide Area Network), which spans a larger geographical area—is a connected network of computers in a small area, like your office, college campus, or even home.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a MAC address? What is it used for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A MAC address is a unique identification number or code used to identify individual devices on the network.&lt;/p&gt; &lt;p&gt;Packets that are sent on the ethernet are always coming from a MAC address and sent to a MAC address. If a network adapter is receiving a packet, it is comparing the packet’s destination MAC address to the adapter’s own MAC address.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;When is this MAC address used?: ff:ff:ff:ff:ff:ff&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;When a device sends a packet to the broadcast MAC address (FF:FF:FF:FF:FF:FF​), it is delivered to all stations on the local network. Ethernet broadcasts are used to resolve IP addresses to MAC addresses (by ARP) at the data link layer. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an IP address?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;An Internet Protocol address (IP address) is a numerical label assigned to each device connected to a computer network that uses the Internet Protocol for communication.An IP address serves two main functions: host or network interface identification and location addressing. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain the subnet mask and give an example&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A Subnet mask is a 32-bit number that masks an IP address and divides the IP addresses into network addresses and host addresses. Subnet Mask is made by setting network bits to all &quot;1&quot;s and setting host bits to all &quot;0&quot;s. Within a given network, out of the total usable host addresses, two are always reserved for specific purposes and cannot be allocated to any host. These are the first address, which is reserved as a network address (a.k.a network ID), and the last address used for network broadcast.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/philemonnwanne/projects/tree/main/exercises/exe-09&quot;&gt;Example&lt;/a&gt;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a private IP address? In which scenarios/system designs, one should use it?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; Private IP addresses are assigned to the hosts in the same network to communicate with one another. As the name &quot;private&quot; suggests, the devices having the private IP addresses assigned can&#39;t be reached by the devices from any external network. For example, if I am living in a hostel and I want my hostel mates to join the game server I have hosted, I will ask them to join via my server&#39;s private IP address, since the network is local to the hostel. &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a public IP address? In which scenarios/system designs, one should use it?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; A public IP address is a public-facing IP address. In the event that you were hosting a game server that you want your friends to join, you will give your friends your public IP address to allow their computers to identify and locate your network and server in order for the connection to take place. One time that you would not need to use a public-facing IP address is in the event that you were playing with friends who were connected to the same network as you, in that case, you would use a private IP address. In order for someone to be able to connect to your server that is located internally, you will have to set up a port forward to tell your router to allow traffic from the public domain into your network and vice versa. &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain the OSI model. What layers there are? What each layer is responsible for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Application: user end (HTTP is here)&lt;/li&gt; 
   &lt;li&gt;Presentation: establishes context between application-layer entities (Encryption is here)&lt;/li&gt; 
   &lt;li&gt;Session: establishes, manages, and terminates the connections&lt;/li&gt; 
   &lt;li&gt;Transport: transfers variable-length data sequences from a source to a destination host (TCP &amp;amp; UDP are here)&lt;/li&gt; 
   &lt;li&gt;Network: transfers datagrams from one network to another (IP is here)&lt;/li&gt; 
   &lt;li&gt;Data link: provides a link between two directly connected nodes (MAC is here)&lt;/li&gt; 
   &lt;li&gt;Physical: the electrical and physical spec of the data connection (Bits are here)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;You can read more about the OSI model in &lt;a href=&quot;http://www.penguintutor.com/linux/basic-network-reference&quot;&gt;penguintutor.com&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;For each of the following determines to which OSI layer it belongs: 
  &lt;ul&gt; 
   &lt;li&gt;Error correction&lt;/li&gt; 
   &lt;li&gt;Packets routing&lt;/li&gt; 
   &lt;li&gt;Cables and electrical signals&lt;/li&gt; 
   &lt;li&gt;MAC address&lt;/li&gt; 
   &lt;li&gt;IP address&lt;/li&gt; 
   &lt;li&gt;Terminate connections&lt;/li&gt; 
   &lt;li&gt;3 way handshake&lt;/li&gt;
  &lt;/ul&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;li&gt;Error correction - Data link&lt;/li&gt; &lt;li&gt;Packets routing - Network&lt;/li&gt; &lt;li&gt;Cables and electrical signals - Physical&lt;/li&gt; &lt;li&gt;MAC address - Data link&lt;/li&gt; &lt;li&gt;IP address - Network&lt;/li&gt; &lt;li&gt;Terminate connections - Session&lt;/li&gt; &lt;/b&gt;
 &lt;li&gt;&lt;b&gt;3-way handshake - Transport &lt;/b&gt;&lt;/li&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What delivery schemes are you familiar with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Unicast: One-to-one communication where there is one sender and one receiver.&lt;/p&gt; &lt;p&gt;Broadcast: Sending a message to everyone in the network. The address ff:ff:ff:ff:ff:ff is used for broadcasting. Two common protocols which use broadcast are ARP and DHCP.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Multicast: Sending a message to a group of subscribers. It can be one-to-many or many-to-many. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is CSMA/CD? Is it used in modern ethernet networks?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;CSMA/CD stands for Carrier Sense Multiple Access / Collision Detection. Its primary focus is to manage access to a shared medium/bus where only one host can transmit at a given point in time.&lt;/p&gt; &lt;p&gt;CSMA/CD algorithm:&lt;/p&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;Before sending a frame, it checks whether another host is already transmitting a frame.&lt;/li&gt; &lt;li&gt;If no one is transmitting, it starts transmitting the frame.&lt;/li&gt; &lt;li&gt;If two hosts transmit at the same time, we have a collision.&lt;/li&gt; &lt;li&gt;Both hosts stop sending the frame and they send everyone a &#39;jam signal&#39; notifying everyone that a collision occurred&lt;/li&gt; &lt;li&gt;They are waiting for a random time before sending it again&lt;/li&gt; &lt;li&gt;Once each host waited for a random time, they try to send the frame again and so the cycle starts again &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Describe the following network devices and the difference between them: 
  &lt;ul&gt; 
   &lt;li&gt;router&lt;/li&gt; 
   &lt;li&gt;switch&lt;/li&gt; 
   &lt;li&gt;hub&lt;/li&gt;
  &lt;/ul&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt;  &lt;p&gt;A router, switch, and hub are all network devices used to connect devices in a local area network (LAN). However, each device operates differently and has its specific use cases. Here is a brief description of each device and the differences between them:&lt;/p&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;Router: a network device that connects multiple network segments together. It operates at the&amp;nbsp;network layer (Layer 3)&amp;nbsp;of the OSI model and uses routing protocols to direct data between networks. Routers use IP addresses to identify devices and route data packets to the correct destination.&lt;/li&gt; &lt;li&gt;Switch: a network device that connects multiple devices on a LAN. It operates at the&amp;nbsp;data link layer (Layer 2)&amp;nbsp;of the OSI model and uses MAC addresses to identify devices and direct data packets to the correct destination. Switches allow devices on the same network to communicate with each other more efficiently and can prevent data collisions that can occur when multiple devices send data simultaneously.&lt;/li&gt; &lt;li&gt;Hub: a network device that connects multiple devices through a single cable and is used to connect multiple devices without segmenting a network. However, unlike a switch, it operates at the&amp;nbsp;physical layer (Layer 1)&amp;nbsp;of the OSI model and simply broadcasts data packets to all devices connected to it, regardless of whether the device is the intended recipient or not. This means that data collisions can occur, and the network&#39;s efficiency can suffer as a result. Hubs are generally not used in modern network setups, as switches are more efficient and provide better network performance. &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is a &quot;Collision Domain&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; A collision domain is a network segment in which devices can potentially interfere with each other by attempting to transmit data at the same time. When two devices transmit data at the same time, it can cause a collision, resulting in lost or corrupted data. In a collision domain, all devices share the same bandwidth, and any device can potentially interfere with the transmission of data by other devices. &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a &quot;Broadcast Domain&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; A broadcast domain is a network segment in which all devices can communicate with each other by sending broadcast messages. A broadcast message is a message that is sent to all devices in a network rather than a specific device. In a broadcast domain, all devices can receive and process broadcast messages, regardless of whether the message was intended for them or not. &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;three computers connected to a switch. How many collision domains are there? How many broadcast domains?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Three collision domains and one broadcast domain &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How does a router work?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A router is a physical or virtual appliance that passes information between two or more packet-switched computer networks. A router inspects a given data packet&#39;s destination Internet Protocol address (IP address), calculates the best way for it to reach its destination, and then forwards it accordingly.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is NAT?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Network Address Translation (NAT) is a process in which one or more local IP addresses are translated into one or more Global IP address and vice versa in order to provide Internet access to the local hosts.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a proxy? How does it work? What do we need it for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A proxy server acts as a gateway between you and the internet. It’s an intermediary server separating end users from the websites they browse.&lt;/p&gt; &lt;p&gt;If you’re using a proxy server, internet traffic flows through the proxy server on its way to the address you requested. The request then comes back through that same proxy server (there are exceptions to this rule), and then the proxy server forwards the data received from the website to you.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Proxy servers provide varying levels of functionality, security, and privacy depending on your use case, needs, or company policy. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is TCP? How does it work? What is the 3-way handshake?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;TCP 3-way handshake or three-way handshake is a process that is used in a TCP/IP network to make a connection between server and client.&lt;/p&gt; &lt;p&gt;A three-way handshake is primarily used to create a TCP socket connection. It works when:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;A client node sends an SYN data packet over an IP network to a server on the same or an external network. The objective of this packet is to ask/infer if the server is open for new connections.&lt;/li&gt; &lt;li&gt;The target server must have open ports that can accept and initiate new connections. When the server receives the SYN packet from the client node, it responds and returns a confirmation receipt – the ACK packet or SYN/ACK packet.&lt;/li&gt; &lt;li&gt;The client node receives the SYN/ACK from the server and responds with an ACK packet. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is round-trip delay or round-trip time?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;From &lt;a href=&quot;https://en.wikipedia.org/wiki/Round-trip_delay&quot;&gt;wikipedia&lt;/a&gt;: &quot;the length of time it takes for a signal to be sent plus the length of time it takes for an acknowledgment of that signal to be received&quot;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Bonus question: what is the RTT of LAN? &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How does an SSL handshake work?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; SSL handshake is a process that establishes a secure connection between a client and a server. &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;The client sends a Client Hello message to the server, which includes the client&#39;s version of the SSL/TLS protocol, a list of the cryptographic algorithms supported by the client, and a random value.&lt;/li&gt; &lt;li&gt;The server responds with a Server Hello message, which includes the server&#39;s version of the SSL/TLS protocol, a random value, and a session ID.&lt;/li&gt; &lt;li&gt;The server sends a Certificate message, which contains the server&#39;s certificate.&lt;/li&gt; &lt;li&gt;The server sends a Server Hello Done message, which indicates that the server is done sending messages for the Server Hello phase.&lt;/li&gt; &lt;li&gt;The client sends a Client Key Exchange message, which contains the client&#39;s public key.&lt;/li&gt; &lt;li&gt;The client sends a Change Cipher Spec message, which notifies the server that the client is about to send a message encrypted with the new cipher spec.&lt;/li&gt; &lt;li&gt;The client sends an Encrypted Handshake Message, which contains the pre-master secret encrypted with the server&#39;s public key.&lt;/li&gt; &lt;li&gt;The server sends a Change Cipher Spec message, which notifies the client that the server is about to send a message encrypted with the new cipher spec.&lt;/li&gt; &lt;li&gt;The server sends an Encrypted Handshake Message, which contains the pre-master secret encrypted with the client&#39;s public key.&lt;/li&gt; &lt;li&gt;The client and server can now exchange application data. &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is the difference between TCP and UDP?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;TCP establishes a connection between the client and the server to guarantee the order of the packages, on the other hand, UDP does not establish a connection between the client and server and doesn&#39;t handle package orders. This makes UDP more lightweight than TCP and a perfect candidate for services like streaming.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;http://www.penguintutor.com/linux/basic-network-reference&quot;&gt;Penguintutor.com&lt;/a&gt; provides a good explanation. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What TCP/IP protocols are you familiar with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain the &quot;default gateway&quot;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A default gateway serves as an access point or IP router that a networked computer uses to send information to a computer in another network or the internet. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is ARP? How does it work?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;ARP stands for Address Resolution Protocol. When you try to ping an IP address on your local network, say 192.168.1.1, your system has to turn the IP address 192.168.1.1 into a MAC address. This involves using ARP to resolve the address, hence its name.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Systems keep an ARP look-up table where they store information about what IP addresses are associated with what MAC addresses. When trying to send a packet to an IP address, the system will first consult this table to see if it already knows the MAC address. If there is a value cached, ARP is not used. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is TTL? What does it help to prevent?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;TTL (Time to Live) is a value in an IP (Internet Protocol) packet that determines how many hops or routers a packet can travel before it is discarded. Each time a packet is forwarded by a router, the TTL value is decreased by one. When the TTL value reaches zero, the packet is dropped, and an ICMP (Internet Control Message Protocol) message is sent back to the sender indicating that the packet has expired.&lt;/li&gt; &lt;li&gt;TTL is used to prevent packets from circulating indefinitely in the network, which can cause congestion and degrade network performance.&lt;/li&gt; &lt;li&gt;It also helps to prevent packets from being trapped in routing loops, where packets continuously travel between the same set of routers without ever reaching their destination.&lt;/li&gt; &lt;li&gt;In addition, TTL can be used to help detect and prevent IP spoofing attacks, where an attacker attempts to impersonate another device on the network by using a false or fake IP address. By limiting the number of hops that a packet can travel, TTL can help prevent packets from being routed to destinations that are not legitimate. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is DHCP? How does it work?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;It stands for Dynamic Host Configuration Protocol and allocates IP addresses, subnet masks, and gateways to hosts. This is how it works:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;A host upon entering a network broadcasts a message in search of a DHCP server (DHCP DISCOVER)&lt;/li&gt; 
   &lt;li&gt;An offer message is sent back by the DHCP server as a packet containing lease time, subnet mask, IP addresses, etc (DHCP OFFER)&lt;/li&gt; 
   &lt;li&gt;Depending on which offer is accepted, the client sends back a reply broadcast letting all DHCP servers know (DHCP REQUEST)&lt;/li&gt; 
   &lt;li&gt;The server sends an acknowledgment (DHCP ACK)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Read more &lt;a href=&quot;https://linuxjourney.com/lesson/dhcp-overview&quot;&gt;here&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you have two DHCP servers on the same network? How does it work?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;It is possible to have two DHCP servers on the same network, however, it is not recommended, and it is important to configure them carefully to prevent conflicts and configuration problems.&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;When two DHCP servers are configured on the same network, there is a&amp;nbsp;risk that both servers will assign IP addresses and other network configuration settings to the same device, which can cause conflicts and connectivity issues. Additionally, if the DHCP servers are configured with different network settings or options, devices on the network may receive conflicting or inconsistent configuration settings.&lt;/li&gt; &lt;li&gt;However, in some cases, it may be necessary to have two DHCP servers on the same network, such as in large networks where one DHCP server may not be able to handle all the requests. In such cases, DHCP servers can be configured to serve different IP address ranges or different subnets, so they do not interfere with each other. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is SSL tunneling? How does it work?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; 
  &lt;ul&gt; 
   &lt;li&gt;SSL (Secure Sockets Layer) tunneling is a technique used to establish a secure, encrypted connection between two endpoints over an insecure network, such as the Internet. The SSL tunnel is created by encapsulating the traffic within an SSL connection, which provides confidentiality, integrity, and authentication.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Here&#39;s how SSL tunneling works:&lt;/p&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;A client initiates an SSL connection to a server, which involves a handshake process to establish the SSL session.&lt;/li&gt; &lt;li&gt;Once the SSL session is established, the client and server negotiate encryption parameters, such as the encryption algorithm and key length, then exchange digital certificates to authenticate each other.&lt;/li&gt; &lt;li&gt;The client then sends traffic through the SSL tunnel to the server, which decrypts the traffic and forwards it to its destination.&lt;/li&gt; &lt;li&gt;The server sends traffic back through the SSL tunnel to the client, which decrypts the traffic and forwards it to the application. &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is a socket? Where can you see the list of sockets in your system?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;A socket is a software endpoint that enables two-way communication between processes over a network. Sockets provide a standardized interface for network communication, allowing applications to send and receive data across a network. To view the list of open sockets on a Linux system:&amp;nbsp; &lt;em&gt;&lt;strong&gt;netstat -an&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt; &lt;li&gt;This command displays a list of all open sockets, along with their protocol, local address, foreign address, and state. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is IPv6? Why should we consider using it if we have IPv4?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; 
  &lt;ul&gt; 
   &lt;li&gt;IPv6 (Internet Protocol version 6) is the latest version of the Internet Protocol (IP), which is used to identify and communicate with devices on a network. IPv6 addresses are 128-bit addresses and are expressed in hexadecimal notation, such as 2001:0db8:85a3:0000:0000:8a2e:0370:7334.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;There are several reasons why we should consider using IPv6 over IPv4:&lt;/p&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;Address space: IPv4 has a limited address space, which has been exhausted in many parts of the world. IPv6 provides a much larger address space, allowing for trillions of unique IP addresses.&lt;/li&gt; &lt;li&gt;Security: IPv6 includes built-in support for IPsec, which provides end-to-end encryption and authentication for network traffic.&lt;/li&gt; &lt;li&gt;Performance: IPv6 includes features that can help to improve network performance, such as multicast routing, which allows a single packet to be sent to multiple destinations simultaneously.&lt;/li&gt; &lt;li&gt;Simplified network configuration: IPv6 includes features that can simplify network configuration, such as stateless autoconfiguration, which allows devices to automatically configure their own IPv6 addresses without the need for a DHCP server.&lt;/li&gt; &lt;li&gt;Better mobility support: IPv6 includes features that can improve mobility support, such as Mobile IPv6, which allows devices to maintain their IPv6 addresses as they move between different networks. &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is VLAN?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;A VLAN (Virtual Local Area Network) is a logical network that groups together a set of devices on a physical network, regardless of their physical location. VLANs are created by configuring network switches to assign a specific VLAN ID to frames sent by devices connected to a specific port or group of ports on the switch. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is MTU?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;MTU stands for Maximum Transmission Unit. It&#39;s the size of the largest PDU (protocol Data Unit) that can be sent in a single transaction. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What happens if you send a packet that is bigger than the MTU?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;With the IPv4 protocol, the router can fragment the PDU and then send all the fragmented PDU through the transaction.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;With IPv6 protocol, it issues an error to the user&#39;s computer. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;True or False? Ping is using UDP because it doesn&#39;t care about reliable connection&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;False. Ping is actually using ICMP (Internet Control Message Protocol) which is a network protocol used to send diagnostic messages and control messages related to network communication. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is SDN?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;SDN stands for Software-Defined Networking. It is an approach to network management that emphasizes the centralization of network control, enabling administrators to manage network behavior through a software abstraction.&lt;/li&gt; &lt;li&gt;In a traditional network, network devices such as routers, switches, and firewalls are configured and managed individually, using specialized software or command-line interfaces. In contrast, SDN separates the network control plane from the data plane, allowing administrators to manage network behavior through a centralized software controller. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is ICMP? What is it used for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ICMP stands for Internet Control Message Protocol. It is a protocol used for diagnostic and control purposes in IP networks. It is a part of the Internet Protocol suite, operating at the network layer.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;ICMP messages are used for a variety of purposes, including:&lt;/p&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;Error reporting: ICMP messages are used to report errors that occur in the network, such as a packet that could not be delivered to its destination.&lt;/li&gt; &lt;li&gt;Ping: ICMP is used to send ping messages, which are used to test whether a host or network is reachable and to measure the round-trip time for packets.&lt;/li&gt; &lt;li&gt;Path MTU discovery: ICMP is used to discover the Maximum Transmission Unit (MTU) of a path, which is the largest packet size that can be transmitted without fragmentation.&lt;/li&gt; &lt;li&gt;Traceroute: ICMP is used by the traceroute utility to trace the path that packets take through the network.&lt;/li&gt; &lt;li&gt;Router discovery: ICMP is used to discover the routers in a network. &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is NAT? How does it work?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;NAT stands for Network Address Translation. It’s a way to map multiple local private addresses to a public one before transferring the information. Organizations that want multiple devices to employ a single IP address use NAT, as do most home routers. For example, your computer&#39;s private IP could be 192.168.1.100, but your router maps the traffic to its public IP (e.g. 1.1.1.1). Any device on the internet would see the traffic coming from your public IP (1.1.1.1) instead of your private IP (192.168.1.100). &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Which port number is used in each of the following protocols?: 
  &lt;ul&gt; 
   &lt;li&gt;SSH&lt;/li&gt; 
   &lt;li&gt;SMTP&lt;/li&gt; 
   &lt;li&gt;HTTP&lt;/li&gt; 
   &lt;li&gt;DNS&lt;/li&gt; 
   &lt;li&gt;HTTPS&lt;/li&gt; 
   &lt;li&gt;FTP&lt;/li&gt; 
   &lt;li&gt;SFTP&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;SSH - 22&lt;/li&gt; &lt;li&gt;SMTP - 25&lt;/li&gt; &lt;li&gt;HTTP - 80&lt;/li&gt; &lt;li&gt;DNS - 53&lt;/li&gt; &lt;li&gt;HTTPS - 443&lt;/li&gt; &lt;li&gt;FTP - 21&lt;/li&gt; &lt;li&gt;SFTP - 22 &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Which factors affect network performance?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Several factors can affect network performance, including:&lt;/p&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;Bandwidth: The available bandwidth of a network connection can significantly impact its performance. Networks with limited bandwidth can experience slow data transfer rates, high latency, and poor responsiveness.&lt;/li&gt; &lt;li&gt;Latency: Latency refers to the delay that occurs when data is transmitted from one point in a network to another. High latency can result in slow network performance, especially for real-time applications like video conferencing and online gaming.&lt;/li&gt; &lt;li&gt;Network congestion: When too many devices are using a network at the same time, network congestion can occur, leading to slow data transfer rates and poor network performance.&lt;/li&gt; &lt;li&gt;Packet loss: Packet loss occurs when packets of data are dropped during transmission. This can result in slower network speeds and lower overall network performance.&lt;/li&gt; &lt;li&gt;Network topology: The physical layout of a network, including the placement of switches, routers, and other network devices, can impact network performance.&lt;/li&gt; &lt;li&gt;Network protocol: Different network protocols have different performance characteristics, which can impact network performance. For example, TCP is a reliable protocol that can guarantee the delivery of data, but it can also result in slower performance due to the overhead required for error checking and retransmission.&lt;/li&gt; &lt;li&gt;Network security: Security measures such as firewalls and encryption can impact network performance, especially if they require significant processing power or introduce additional latency.&lt;/li&gt; &lt;li&gt;Distance: The physical distance between devices on a network can impact network performance, especially for wireless networks where signal strength and interference can affect connectivity and data transfer rates. &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is APIPA?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;APIPA is a set of IP addresses that devices are allocated when the main DHCP server is not reachable&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What IP range does APIPA use?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;APIPA uses the IP range: 169.254.0.1 - 169.254.255.254.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Control Plane and Data Plane&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;What does &quot;control plane&quot; refer to?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The control plane is a part of the network that decides how to route and forward packets to a different location. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What does &quot;data plane&quot; refer to?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The data plane is a part of the network that actually forwards the data/packets. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What does &quot;management plane&quot; refer to?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;It refers to monitoring and management functions. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;To which plane (data, control, ...) does creating routing tables belong to?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Control Plane. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Spanning Tree Protocol (STP).&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is link aggregation? Why is it used?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Asymmetric Routing? How to deal with it?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What overlay (tunnel) protocols are you familiar with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is GRE? How does it work?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is VXLAN? How does it work?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is SNAT?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain OSPF.&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;OSPF (Open Shortest Path First) is a routing protocol that can be implemented on various types of routers. In general, OSPF is supported on most modern routers, including those from vendors such as Cisco, Juniper, and Huawei. The protocol is designed to work with IP-based networks, including both IPv4 and IPv6. Also, it uses a hierarchical network design, where routers are grouped into areas, with each area having its own topology map and routing table. This design helps to reduce the amount of routing information that needs to be exchanged between routers and improve network scalability.&lt;/p&gt; &lt;p&gt;The OSPF 4 Types of routers are:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Internal Router&lt;/li&gt; 
   &lt;li&gt;Area Border Routers&lt;/li&gt; 
   &lt;li&gt;Autonomous Systems Boundary Routers&lt;/li&gt; 
   &lt;li&gt;Backbone Routers&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Learn more about OSPF router types: &lt;a href=&quot;https://www.educba.com/ospf-router-types/&quot;&gt;https://www.educba.com/ospf-router-types/&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is latency?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Latency is the time taken for information to reach its destination from the source. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is bandwidth?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Bandwidth is the capacity of a communication channel to measure how much data the latter can handle over a specific time period. More bandwidth would imply more traffic handling and thus more data transfer. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is throughput?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Throughput refers to the measurement of the real amount of data transferred over a certain period of time across any transmission channel. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;When performing a search query, what is more important, latency or throughput? And how to ensure that we manage global infrastructure? &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Latency. To have good latency, a search query should be forwarded to the closest data center. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;When uploading a video, what is more important, latency or throughput? And how to assure that?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Throughput. To have good throughput, the upload stream should be routed to an underutilized link. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What other considerations (except latency and throughput) are there when forwarding requests?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Keep caches updated (which means the request could be forwarded not to the closest data center) &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain Spine &amp;amp; Leaf&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &quot;Spine &amp;amp; Leaf&quot; is a networking topology commonly used in data center environments to connect multiple switches and manage network traffic efficiently. It is also known as &quot;spine-leaf&quot; architecture or &quot;leaf-spine&quot; topology. This design provides high bandwidth, low latency, and scalability, making it ideal for modern data centers handling large volumes of data and traffic. &lt;p&gt;Within a Spine &amp;amp; Leaf network there are two main tipology of switches:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Spine Switches: Spine switches are high-performance switches arranged in a spine layer. These switches act as the core of the network and are typically interconnected with each leaf switch. Each spine switch is connected to all the leaf switches in the data center.&lt;/li&gt; 
   &lt;li&gt;Leaf Switches: Leaf switches are connected to end devices like servers, storage arrays, and other networking equipment. Each leaf switch is connected to every spine switch in the data center. This creates a non-blocking, full-mesh connectivity between leaf and spine switches, ensuring any leaf switch can communicate with any other leaf switch with maximum throughput.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The Spine &amp;amp; Leaf architecture has become increasingly popular in data centers due to its ability to handle the demands of modern cloud computing, virtualization, and big data applications, providing a scalable, high-performance, and reliable network infrastructure &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Network Congestion? What can cause it?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Network congestion occurs when there is too much data to transmit on a network and it doesn&#39;t have enough capacity to handle the demand. &lt;br&gt; This can lead to increased latency and packet loss. The causes can be multiple, such as high network usage, large file transfers, malware, hardware issues, or network design problems. &lt;br&gt; To prevent network congestion, it&#39;s important to monitor your network usage and implement strategies to limit or manage the demand. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What can you tell me about the UDP packet format? What about the TCP packet format? How is it different?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the exponential backoff algorithm? Where is it used?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Using Hamming code, what would be the code word for the following data word 100111010001101?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;00110011110100011101 &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Give examples of protocols found in the application layer&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Hypertext Transfer Protocol (HTTP) - used for the webpages on the internet&lt;/li&gt; &lt;li&gt;Simple Mail Transfer Protocol (SMTP) - email transmission&lt;/li&gt; &lt;li&gt;Telecommunications Network - (TELNET) - terminal emulation to allow a client access to a telnet server&lt;/li&gt; &lt;li&gt;File Transfer Protocol (FTP) - facilitates the transfer of files between any two machines&lt;/li&gt; &lt;li&gt;Domain Name System (DNS) - domain name translation&lt;/li&gt; &lt;li&gt;Dynamic Host Configuration Protocol (DHCP) - allocates IP addresses, subnet masks, and gateways to hosts&lt;/li&gt; &lt;li&gt;Simple Network Management Protocol (SNMP) - gathers data on devices on the network &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Give examples of protocols found in the Network Layer&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Internet Protocol (IP) - assists in routing packets from one machine to another&lt;/li&gt; &lt;li&gt;Internet Control Message Protocol (ICMP) - lets one know what is going such as error messages and debugging information &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is HSTS?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; HTTP Strict Transport Security is a web server directive that informs user agents and web browsers how to handle its connection through a response header sent at the very beginning and back to the browser. This forces connections over HTTPS encryption, disregarding any script&#39;s call to load any resource in that domain over HTTP. &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Read more [here](&lt;a href=&quot;https://www.globalsign.com/en/blog/what-is-hsts-and-how-do-i-use-it#:~:text=HTTP%20Strict%20Transport%20Security%20(HSTS,and%20back%20to%20the%20browser.)&quot;&gt;https://www.globalsign.com/en/blog/what-is-hsts-and-how-do-i-use-it#:~:text=HTTP%20Strict%20Transport%20Security%20(HSTS,and%20back%20to%20the%20browser.)&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Network - Misc&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the Internet? Is it the same as the World Wide Web?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The internet refers to a network of networks, transferring huge amounts of data around the globe.&lt;br&gt; The World Wide Web is an application running on millions of servers, on top of the internet, accessed through what is known as the web browser &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the ISP?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;ISP (Internet Service Provider) is the local internet company provider. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Operating System&lt;/h2&gt; 
&lt;h3&gt;Operating System Exercises&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Topic&lt;/th&gt; 
   &lt;th&gt;Objective &amp;amp; Instructions&lt;/th&gt; 
   &lt;th&gt;Solution&lt;/th&gt; 
   &lt;th&gt;Comments&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fork 101&lt;/td&gt; 
   &lt;td&gt;Fork&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/os/fork_101.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/os/solutions/fork_101_solution.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fork 102&lt;/td&gt; 
   &lt;td&gt;Fork&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/os/fork_102.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/os/solutions/fork_102_solution.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Operating System - Self Assessment&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an operating system?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;From the book &quot;Operating Systems: Three Easy Pieces&quot;:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;responsible for making it easy to run programs (even allowing you to seemingly run many at the same time), allowing programs to share memory, enabling programs to interact with devices, and other fun stuff like that&quot;. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Operating System - Process&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you explain what is a process?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A process is a running program. A program is one or more instructions and the program (or process) is executed by the operating system. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;If you had to design an API for processes in an operating system, what would this API look like?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;It would support the following:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Create - allow to create new processes&lt;/li&gt; &lt;li&gt;Delete - allow to remove/destroy processes&lt;/li&gt; &lt;li&gt;State - allow to check the state of the process, whether it&#39;s running, stopped, waiting, etc.&lt;/li&gt; &lt;li&gt;Stop - allow to stop a running process &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;How a process is created?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;The OS is reading program&#39;s code and any additional relevant data&lt;/li&gt; &lt;li&gt;Program&#39;s code is loaded into the memory or more specifically, into the address space of the process.&lt;/li&gt; &lt;li&gt;Memory is allocated for program&#39;s stack (aka run-time stack). The stack also initialized by the OS with data like argv, argc and parameters to main()&lt;/li&gt; &lt;li&gt;Memory is allocated for program&#39;s heap which is required for dynamically allocated data like the data structures linked lists and hash tables&lt;/li&gt; &lt;li&gt;I/O initialization tasks are performed, like in Unix/Linux based systems, where each process has 3 file descriptors (input, output and error)&lt;/li&gt; &lt;li&gt;OS is running the program, starting from main() &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;True or False? The loading of the program into the memory is done eagerly (all at once)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;False. It was true in the past but today&#39;s operating systems perform lazy loading, which means only the relevant pieces required for the process to run are loaded first. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are different states of a process?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Running - it&#39;s executing instructions&lt;/li&gt; &lt;li&gt;Ready - it&#39;s ready to run, but for different reasons it&#39;s on hold&lt;/li&gt; &lt;li&gt;Blocked - it&#39;s waiting for some operation to complete, for example I/O disk request &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are some reasons for a process to become blocked?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;I/O operations (e.g. Reading from a disk)&lt;/li&gt; &lt;li&gt;Waiting for a packet from a network &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is Inter Process Communication (IPC)?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Inter-process communication (IPC) refers to the mechanisms provided by an operating system that allow processes to manage shared data. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is &quot;time sharing&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Even when using a system with one physical CPU, it&#39;s possible to allow multiple users to work on it and run programs. This is possible with time sharing, where computing resources are shared in a way it seems to the user, the system has multiple CPUs, but in fact it&#39;s simply one CPU shared by applying multiprogramming and multi-tasking. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is &quot;space sharing&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Somewhat the opposite of time sharing. While in time sharing a resource is used for a while by one entity and then the same resource can be used by another resource, in space sharing the space is shared by multiple entities but in a way where it&#39;s not being transferred between them.&lt;br&gt; It&#39;s used by one entity, until this entity decides to get rid of it. Take for example storage. In storage, a file is yours, until you decide to delete it. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What component determines which process runs at a given moment in time?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;CPU scheduler &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Operating System - Memory&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is &quot;virtual memory&quot; and what purpose does serve?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Virtual memory combines your computer&#39;s RAM with temporary space on your hard disk. When RAM runs low, virtual memory helps to move data from RAM to a space called a paging file. Moving data to paging file can free up the RAM, so your computer can complete its work. In general, the more RAM your computer has, the faster the programs run. &lt;a href=&quot;https://www.minitool.com/lib/virtual-memory.html&quot;&gt;https://www.minitool.com/lib/virtual-memory.html&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is demand paging?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Demand paging is a memory management technique where pages are loaded into physical memory only when accessed by a process. It optimizes memory usage by loading pages on demand, reducing startup latency and space overhead. However, it introduces some latency when accessing pages for the first time. Overall, it’s a cost-effective approach for managing memory resources in operating systems. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is copy-on-write?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; Copy-on-write (COW) is a resource management concept, with the goal to reduce unnecessary copying of information. It is a concept, which is implemented for instance within the POSIX fork syscall, which creates a duplicate process of the calling process. &lt;p&gt;The idea:&lt;/p&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;If resources are shared between 2 or more entities (for example shared memory segments between 2 processes), the resources don&#39;t need to be copied for every entity, but rather every entity has a READ operation access permission on the shared resource. (the shared segments are marked as read-only) (Think of every entity having a pointer to the location of the shared resource, which can be dereferenced to read its value)&lt;/li&gt; &lt;li&gt;If one entity would perform a WRITE operation on a shared resource, a problem would arise, since the resource also would be permanently changed for ALL other entities sharing it. (Think of a process modifying some variables on the stack, or allocatingy some data dynamically on the heap, these changes to the shared resource would also apply for ALL other processes, this is definitely an undesirable behaviour)&lt;/li&gt; &lt;li&gt;As a solution only, if a WRITE operation is about to be performed on a shared resource, this resource gets COPIED first and then the changes are applied. &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is a kernel, and what does it do?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;The kernel is part of the operating system and is responsible for tasks like:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Allocating memory&lt;/li&gt; &lt;li&gt;Schedule processes&lt;/li&gt; &lt;li&gt;Control CPU &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;True or False? Some pieces of the code in the kernel are loaded into protected areas of the memory so applications can&#39;t overwrite them.&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;True &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is POSIX?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;POSIX (Portable Operating System Interface) is a set of standards that define the interface between a Unix-like operating system and application programs. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is Semaphore and what its role in operating systems.&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A semaphore is a synchronization primitive used in operating systems and concurrent programming to control access to shared resources. It&#39;s a variable or abstract data type that acts as a counter or a signaling mechanism for managing access to resources by multiple processes or threads. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is cache? What is buffer?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Cache: Cache is usually used when processes are reading and writing to the disk to make the process faster, by making similar data used by different programs easily accessible. Buffer: Reserved place in RAM, which is used to hold data for temporary purposes. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Virtualization&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Virtualization?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Virtualization uses software to create an abstraction layer over computer hardware, that allows the hardware elements of a single computer - processors, memory, storage and more - to be divided into multiple virtual computers, commonly called virtual machines (VMs). &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a hypervisor?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Red Hat: &quot;A hypervisor is software that creates and runs virtual machines (VMs). A hypervisor, sometimes called a virtual machine monitor (VMM), isolates the hypervisor operating system and resources from the virtual machines and enables the creation and management of those VMs.&quot;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Read more &lt;a href=&quot;https://www.redhat.com/en/topics/virtualization/what-is-a-hypervisor&quot;&gt;here&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What types of hypervisors are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Hosted hypervisors and bare-metal hypervisors. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are the advantages and disadvantges of bare-metal hypervisor over a hosted hypervisor?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Due to having its own drivers and a direct access to hardware components, a baremetal hypervisor will often have better performances along with stability and scalability.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;On the other hand, there will probably be some limitation regarding loading (any) drivers so a hosted hypervisor will usually benefit from having a better hardware compatibility. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What types of virtualization are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Operating system virtualization Network functions virtualization Desktop virtualization &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Is containerization is a type of Virtualization?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Yes, it&#39;s a operating-system-level virtualization, where the kernel is shared and allows to use multiple isolated user-spaces instances. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How the introduction of virtual machines changed the industry and the way applications were deployed?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The introduction of virtual machines allowed companies to deploy multiple business applications on the same hardware, while each application is separated from each other in secured way, where each is running on its own separate operating system. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Virtual Machines&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Do we need virtual machines in the age of containers? Are they still relevant?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Yes, virtual machines are still relevant even in the age of containers. While containers provide a lightweight and portable alternative to virtual machines, they do have certain limitations. Virtual machines still matter because they offer isolation and security, can run different operating systems, and are good for legacy apps. Containers limitations for example are sharing the host kernel. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Prometheus&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Prometheus? What are some of Prometheus&#39;s main features?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Prometheus is a popular open-source systems monitoring and alerting toolkit, originally developed at SoundCloud. It is designed to collect and store time-series data, and to allow for querying and analysis of that data using a powerful query language called PromQL. Prometheus is frequently used to monitor cloud-native applications, microservices, and other modern infrastructure.&lt;/p&gt; &lt;p&gt;Some of the main features of Prometheus include:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1. Data model: Prometheus uses a flexible data model that allows users to organize and label their time-series data in a way that makes sense for their particular use case. Labels are used to identify different dimensions of the data, such as the source of the data or the environment in which it was collected.

2. Pull-based architecture: Prometheus uses a pull-based model to collect data from targets, meaning that the Prometheus server actively queries its targets for metrics data at regular intervals. This architecture is more scalable and reliable than a push-based model, which would require every target to push data to the server.

3. Time-series database: Prometheus stores all of its data in a time-series database, which allows users to perform queries over time ranges and to aggregate and analyze their data in various ways. The database is optimized for write-heavy workloads, and can handle a high volume of data with low latency.

4. Alerting: Prometheus includes a powerful alerting system that allows users to define rules based on their metrics data and to send alerts when certain conditions are met. Alerts can be sent via email, chat, or other channels, and can be customized to include specific details about the problem.

5. Visualization: Prometheus has a built-in graphing and visualization tool, called PromDash, which allows users to create custom dashboards to monitor their systems and applications. PromDash supports a variety of graph types and visualization options, and can be customized using CSS and JavaScript.
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Overall, Prometheus is a powerful and flexible tool for monitoring and analyzing systems and applications, and is widely used in the industry for cloud-native monitoring and observability.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;In what scenarios it might be better to NOT use Prometheus?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;From Prometheus documentation: &quot;if you need 100% accuracy, such as for per-request billing&quot;. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Describe Prometheus architecture and components&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;The Prometheus architecture consists of four major components:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;1. Prometheus Server: The Prometheus server is responsible for collecting and storing metrics data. It has a simple built-in storage layer that allows it to store time-series data in a time-ordered database.

2. Client Libraries: Prometheus provides a range of client libraries that enable applications to expose their metrics data in a format that can be ingested by the Prometheus server. These libraries are available for a range of programming languages, including Java, Python, and Go.

3. Exporters: Exporters are software components that expose existing metrics from third-party systems and make them available for ingestion by the Prometheus server. Prometheus provides exporters for a range of popular technologies, including MySQL, PostgreSQL, and Apache.

4. Alertmanager: The Alertmanager component is responsible for processing alerts generated by the Prometheus server. It can handle alerts from multiple sources and provides a range of features for deduplicating, grouping, and routing alerts to appropriate channels.
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Overall, the Prometheus architecture is designed to be highly scalable and resilient. The server and client libraries can be deployed in a distributed fashion to support monitoring across large-scale, highly dynamic environments &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you compare Prometheus to other solutions like InfluxDB for example?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Compared to other monitoring solutions, such as InfluxDB, Prometheus is known for its high performance and scalability. It can handle large volumes of data and can easily be integrated with other tools in the monitoring ecosystem. InfluxDB, on the other hand, is known for its ease of use and simplicity. It has a user-friendly interface and provides easy-to-use APIs for collecting and querying data.&lt;/p&gt; &lt;p&gt;Another popular solution, Nagios, is a more traditional monitoring system that relies on a push-based model for collecting data. Nagios has been around for a long time and is known for its stability and reliability. However, compared to Prometheus, Nagios lacks some of the more advanced features, such as multi-dimensional data model and powerful query language.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Overall, the choice of a monitoring solution depends on the specific needs and requirements of the organization. While Prometheus is a great choice for large-scale monitoring and alerting, InfluxDB may be a better fit for smaller environments that require ease of use and simplicity. Nagios remains a solid choice for organizations that prioritize stability and reliability over advanced features. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an Alert?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; In Prometheus, an alert is a notification triggered when a specific condition or threshold is met. Alerts can be configured to trigger when certain metrics cross a certain threshold or when specific events occur. Once an alert is triggered, it can be routed to various channels, such as email, pager, or chat, to notify relevant teams or individuals to take appropriate action. Alerts are a critical component of any monitoring system, as they allow teams to proactively detect and respond to issues before they impact users or cause system downtime. &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an Instance? What is a Job?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;In Prometheus, an instance refers to a single target that is being monitored. For example, a single server or service. A job is a set of instances that perform the same function, such as a set of web servers serving the same application. Jobs allow you to define and manage a group of targets together.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;In essence, an instance is an individual target that Prometheus collects metrics from, while a job is a collection of similar instances that can be managed as a group. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What core metrics types Prometheus supports?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; Prometheus supports several types of metrics, including: &lt;pre&gt;&lt;code&gt;1. Counter: A monotonically increasing value used for tracking counts of events or samples. Examples include the number of requests processed or the total number of errors encountered.

2. Gauge: A value that can go up or down, such as CPU usage or memory usage. Unlike counters, gauge values can be arbitrary, meaning they can go up and down based on changes in the system being monitored.

3. Histogram: A set of observations or events that are divided into buckets based on their value. Histograms help in analyzing the distribution of a metric, such as request latencies or response sizes.

4. Summary: A summary is similar to a histogram, but instead of buckets, it provides a set of quantiles for the observed values. Summaries are useful for monitoring the distribution of request latencies or response sizes over time.
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Prometheus also supports various functions and operators for aggregating and manipulating metrics, such as sum, max, min, and rate. These features make it a powerful tool for monitoring and alerting on system metrics. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an exporter? What is it used for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; The exporter serves as a bridge between the third-party system or application and Prometheus, making it possible for Prometheus to monitor and collect data from that system or application. &lt;p&gt;The exporter acts as a server, listening on a specific network port for requests from Prometheus to scrape metrics. It collects metrics from the third-party system or application and transforms them into a format that can be understood by Prometheus. The exporter then exposes these metrics to Prometheus via an HTTP endpoint, making them available for collection and analysis.&lt;/p&gt; &lt;p&gt;Exporters are commonly used to monitor various types of infrastructure components such as databases, web servers, and storage systems. For example, there are exporters available for monitoring popular databases such as MySQL and PostgreSQL, as well as web servers like Apache and Nginx.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Overall, exporters are a critical component of the Prometheus ecosystem, allowing for the monitoring of a wide range of systems and applications, and providing a high degree of flexibility and extensibility to the platform. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Which Prometheus best practices?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; Here are three of them: &lt;pre&gt;&lt;code&gt;1. Label carefully: Careful and consistent labeling of metrics is crucial for effective querying and alerting. Labels should be clear, concise, and include all relevant information about the metric.

2. Keep metrics simple: The metrics exposed by exporters should be simple and focus on a single aspect of the system being monitored. This helps avoid confusion and ensures that the metrics are easily understandable by all members of the team.

3. Use alerting sparingly: While alerting is a powerful feature of Prometheus, it should be used sparingly and only for the most critical issues. Setting up too many alerts can lead to alert fatigue and result in important alerts being ignored. It is recommended to set up only the most important alerts and adjust the thresholds over time based on the actual frequency of alerts.
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How to get total requests in a given period of time?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; To get the total requests in a given period of time using Prometheus, you can use the *sum* function along with the *rate* function. Here is an example query that will give you the total number of requests in the last hour: &lt;pre&gt;&lt;code&gt;sum(rate(http_requests_total[1h]))
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this query, &lt;em&gt;http_requests_total&lt;/em&gt; is the name of the metric that tracks the total number of HTTP requests, and the &lt;em&gt;rate&lt;/em&gt; function calculates the per-second rate of requests over the last hour. The &lt;em&gt;sum&lt;/em&gt; function then adds up all of the requests to give you the total number of requests in the last hour.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;You can adjust the time range by changing the duration in the &lt;em&gt;rate&lt;/em&gt; function. For example, if you wanted to get the total number of requests in the last day, you could change the function to &lt;em&gt;rate(http_requests_total[1d])&lt;/em&gt;. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What HA in Prometheus means?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;HA stands for High Availability. This means that the system is designed to be highly reliable and always available, even in the face of failures or other issues. In practice, this typically involves setting up multiple instances of Prometheus and ensuring that they are all synchronized and able to work together seamlessly. This can be achieved through a variety of techniques, such as load balancing, replication, and failover mechanisms. By implementing HA in Prometheus, users can ensure that their monitoring data is always available and up-to-date, even in the face of hardware or software failures, network issues, or other problems that might otherwise cause downtime or data loss. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do you join two metrics?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; In Prometheus, joining two metrics can be achieved using the *join()* function. The *join()* function combines two or more time series based on their label values. It takes two mandatory arguments: *on* and *table*. The on argument specifies the labels to join *on* and the *table* argument specifies the time series to join. &lt;p&gt;Here&#39;s an example of how to join two metrics using the &lt;em&gt;join()&lt;/em&gt; function:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sum_series(
  join(
    on(service, instance) request_count_total,
    on(service, instance) error_count_total,
  )
)
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;In this example, the &lt;em&gt;join()&lt;/em&gt; function combines the &lt;em&gt;request_count_total&lt;/em&gt; and &lt;em&gt;error_count_total&lt;/em&gt; time series based on their &lt;em&gt;service&lt;/em&gt; and &lt;em&gt;instance&lt;/em&gt; label values. The &lt;em&gt;sum_series()&lt;/em&gt; function then calculates the sum of the resulting time series &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How to write a query that returns the value of a label?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; To write a query that returns the value of a label in Prometheus, you can use the *label_values* function. The *label_values* function takes two arguments: the name of the label and the name of the metric. &lt;p&gt;For example, if you have a metric called &lt;em&gt;http_requests_total&lt;/em&gt; with a label called &lt;em&gt;method&lt;/em&gt;, and you want to return all the values of the &lt;em&gt;method&lt;/em&gt; label, you can use the following query:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;label_values(http_requests_total, method)
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;This will return a list of all the values for the &lt;em&gt;method&lt;/em&gt; label in the &lt;em&gt;http_requests_total&lt;/em&gt; metric. You can then use this list in further queries or to filter your data. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do you convert cpu_user_seconds to cpu usage in percentage?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; To convert *cpu_user_seconds* to CPU usage in percentage, you need to divide it by the total elapsed time and the number of CPU cores, and then multiply by 100. The formula is as follows: &lt;pre&gt;&lt;code&gt;100 * sum(rate(process_cpu_user_seconds_total{job=&quot;&amp;lt;job-name&amp;gt;&quot;}[&amp;lt;time-period&amp;gt;])) by (instance) / (&amp;lt;time-period&amp;gt; * &amp;lt;num-cpu-cores&amp;gt;)
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here, &lt;em&gt;
    &lt;job-name&gt;&lt;/job-name&gt;&lt;/em&gt; is the name of the job you want to query, &lt;em&gt;
    &lt;time-period&gt;&lt;/time-period&gt;&lt;/em&gt; is the time range you want to query (e.g. &lt;em&gt;5m&lt;/em&gt;, &lt;em&gt;1h&lt;/em&gt;), and &lt;em&gt;
    &lt;num-cpu-cores&gt;&lt;/num-cpu-cores&gt;&lt;/em&gt; is the number of CPU cores on the machine you are querying.&lt;/p&gt; &lt;p&gt;For example, to get the CPU usage in percentage for the last 5 minutes for a job named &lt;em&gt;my-job&lt;/em&gt; running on a machine with 4 CPU cores, you can use the following query:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;100 * sum(rate(process_cpu_user_seconds_total{job=&quot;my-job&quot;}[5m])) by (instance) / (5m * 4)
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Go&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are some characteristics of the Go programming language?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Strong and static typing - the type of the variables can&#39;t be changed over time and they have to be defined at compile time&lt;/li&gt; 
   &lt;li&gt;Simplicity&lt;/li&gt; 
   &lt;li&gt;Fast compile times&lt;/li&gt; 
   &lt;li&gt;Built-in concurrency&lt;/li&gt; 
   &lt;li&gt;Garbage collected&lt;/li&gt; 
   &lt;li&gt;Platform independent&lt;/li&gt; 
   &lt;li&gt;Compile to standalone binary - anything you need to run your app will be compiled into one binary. Very useful for version management in run-time.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Go also has good community. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the difference between &lt;code&gt;var x int = 2&lt;/code&gt; and &lt;code&gt;x := 2&lt;/code&gt;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;The result is the same, a variable with the value 2.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;With &lt;code&gt;var x int = 2&lt;/code&gt; we are setting the variable type to integer while with &lt;code&gt;x := 2&lt;/code&gt; we are letting Go figure out by itself the type. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;True or False? In Go we can redeclare variables and once declared we must use it.&lt;/summary&gt; 
 &lt;p&gt;False. We can&#39;t redeclare variables but yes, we must used declared variables. &lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What libraries of Go have you used?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;This should be answered based on your usage but some examples are:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;fmt - formatted I/O &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is the problem with the following block of code? How to fix it? &lt;pre&gt;&lt;code&gt;func main() {
    var x float32 = 13.5
    var y int
    y = x
}
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;The following block of code tries to convert the integer 101 to a string but instead we get &quot;e&quot;. Why is that? How to fix it? &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;package main

import &quot;fmt&quot;

func main() {
    var x int = 101
    var y string
    y = string(x)
    fmt.Println(y)
}
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;It looks what unicode value is set at 101 and uses it for converting the integer to a string. If you want to get &quot;101&quot; you should use the package &quot;strconv&quot; and replace &lt;code&gt;y = string(x)&lt;/code&gt; with &lt;code&gt;y = strconv.Itoa(x)&lt;/code&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is wrong with the following code?: &lt;pre&gt;&lt;code&gt;package main

func main() {
    var x = 2
    var y = 3
    const someConst = x + y
}
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Constants in Go can only be declared using constant expressions. But &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt; and their sum is variable. &lt;br&gt; &lt;code&gt;const initializer x + y is not a constant&lt;/code&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What will be the output of the following block of code?: &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;package main

import &quot;fmt&quot;

const (
	x = iota
	y = iota
)
const z = iota

func main() {
	fmt.Printf(&quot;%v\n&quot;, x)
	fmt.Printf(&quot;%v\n&quot;, y)
	fmt.Printf(&quot;%v\n&quot;, z)
}
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Go&#39;s iota identifier is used in const declarations to simplify definitions of incrementing numbers. Because it can be used in expressions, it provides a generality beyond that of simple enumerations. &lt;br&gt; &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; in the first iota group, &lt;code&gt;z&lt;/code&gt; in the second. &lt;br&gt; &lt;a href=&quot;https://github.com/golang/go/wiki/Iota&quot;&gt;Iota page in Go Wiki&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What _ is used for in Go?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;It avoids having to declare all the variables for the returns values. It is called the &lt;a href=&quot;https://golang.org/doc/effective_go.html#blank&quot;&gt;blank identifier&lt;/a&gt;. &lt;br&gt; &lt;a href=&quot;https://stackoverflow.com/questions/27764421/what-is-underscore-comma-in-a-go-declaration#answer-27764432&quot;&gt;answer in SO&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What will be the output of the following block of code?: &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;package main

import &quot;fmt&quot;

const (
	_ = iota + 3
	x
)

func main() {
	fmt.Printf(&quot;%v\n&quot;, x)
}
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Since the first iota is declared with the value &lt;code&gt;3&lt;/code&gt; (&lt;code&gt; + 3&lt;/code&gt;), the next one has the value &lt;code&gt;4&lt;/code&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What will be the output of the following block of code?: &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;package main

import (
	&quot;fmt&quot;
	&quot;sync&quot;
	&quot;time&quot;
)

func main() {
	var wg sync.WaitGroup

	wg.Add(1)
	go func() {
		time.Sleep(time.Second * 2)
		fmt.Println(&quot;1&quot;)
		wg.Done()
	}()

	go func() {
		fmt.Println(&quot;2&quot;)
	}()

	wg.Wait()
	fmt.Println(&quot;3&quot;)
}
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Output: 2 1 3&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://tutorialedge.net/golang/go-waitgroup-tutorial/&quot;&gt;Aritcle about sync/waitgroup&lt;/a&gt;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://golang.org/pkg/sync/&quot;&gt;Golang package sync&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What will be the output of the following block of code?: &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;package main

import (
	&quot;fmt&quot;
)

func mod1(a []int) {
	for i := range a {
		a[i] = 5
	}

	fmt.Println(&quot;1:&quot;, a)
}

func mod2(a []int) {
	a = append(a, 125) // !

	for i := range a {
		a[i] = 5
	}

	fmt.Println(&quot;2:&quot;, a)
}

func main() {
	s1 := []int{1, 2, 3, 4}
	mod1(s1)
	fmt.Println(&quot;1:&quot;, s1)

	s2 := []int{1, 2, 3, 4}
	mod2(s2)
	fmt.Println(&quot;2:&quot;, s2)
}
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Output: &lt;code&gt;&lt;br&gt; 1 [5 5 5 5]&lt;br&gt; 1 [5 5 5 5]&lt;br&gt; 2 [5 5 5 5 5]&lt;br&gt; 2 [1 2 3 4]&lt;br&gt; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;In &lt;code&gt;mod1&lt;/code&gt; a is link, and when we&#39;re using &lt;code&gt;a[i]&lt;/code&gt;, we&#39;re changing &lt;code&gt;s1&lt;/code&gt; value to. But in &lt;code&gt;mod2&lt;/code&gt;, &lt;code&gt;append&lt;/code&gt; creates new slice, and we&#39;re changing only &lt;code&gt;a&lt;/code&gt; value, not &lt;code&gt;s2&lt;/code&gt;.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://golangbot.com/arrays-and-slices/&quot;&gt;Aritcle about arrays&lt;/a&gt;, &lt;a href=&quot;https://blog.golang.org/slices&quot;&gt;Blog post about &lt;code&gt;append&lt;/code&gt;&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What will be the output of the following block of code?: &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;package main

import (
	&quot;container/heap&quot;
	&quot;fmt&quot;
)

// An IntHeap is a min-heap of ints.
type IntHeap []int

func (h IntHeap) Len() int           { return len(h) }
func (h IntHeap) Less(i, j int) bool { return h[i] &amp;lt; h[j] }
func (h IntHeap) Swap(i, j int)      { h[i], h[j] = h[j], h[i] }

func (h *IntHeap) Push(x interface{}) {
	// Push and Pop use pointer receivers because they modify the slice&#39;s length,
	// not just its contents.
	*h = append(*h, x.(int))
}

func (h *IntHeap) Pop() interface{} {
	old := *h
	n := len(old)
	x := old[n-1]
	*h = old[0 : n-1]
	return x
}

func main() {
	h := &amp;amp;IntHeap{4, 8, 3, 6}
	heap.Init(h)
	heap.Push(h, 7)

  fmt.Println((*h)[0])
}
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Output: 3&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://golang.org/pkg/container/heap/&quot;&gt;Golang container/heap package&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Mongo&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are the advantages of MongoDB? Or in other words, why choosing MongoDB and not other implementation of NoSQL?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;MongoDB advantages are as following:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Schemaless&lt;/li&gt; 
   &lt;li&gt;Easy to scale-out&lt;/li&gt; 
   &lt;li&gt;No complex joins&lt;/li&gt; 
   &lt;li&gt;Structure of a single object is clear&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the difference between SQL and NoSQL?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The main difference is that SQL databases are structured (data is stored in the form of tables with rows and columns - like an excel spreadsheet table) while NoSQL is unstructured, and the data storage can vary depending on how the NoSQL DB is set up, such as key-value pair, document-oriented, etc. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;In what scenarios would you prefer to use NoSQL/Mongo over SQL?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Heterogeneous data which changes often&lt;/li&gt; &lt;li&gt;Data consistency and integrity is not top priority&lt;/li&gt; &lt;li&gt;Best if the database needs to scale rapidly &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is a document? What is a collection?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;A document is a record in MongoDB, which is stored in BSON (Binary JSON) format and is the basic unit of data in MongoDB.&lt;/li&gt; &lt;li&gt;A collection is a group of related documents stored in a single database in MongoDB. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is an aggregator?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;An aggregator is a framework in MongoDB that performs operations on a set of data to return a single computed result. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is better? Embedded documents or referenced?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;There is no definitive answer to which is better, it depends on the specific use case and requirements. Some explanations : Embedded documents provide atomic updates, while referenced documents allow for better normalization. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Have you performed data retrieval optimizations in Mongo? If not, can you think about ways to optimize a slow data retrieval?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Some ways to optimize data retrieval in MongoDB are: indexing, proper schema design, query optimization and database load balancing. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h5&gt;Queries&lt;/h5&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain this query: &lt;code&gt;db.books.find({&quot;name&quot;: /abc/})&lt;/code&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain this query: &lt;code&gt;db.books.find().sort({x:1})&lt;/code&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the difference between find() and find_one()?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;&lt;code&gt;find()&lt;/code&gt; returns all documents that match the query conditions.&lt;/li&gt; &lt;li&gt;find_one() returns only one document that matches the query conditions (or null if no match is found). &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;How can you export data from Mongo DB?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;mongoexport&lt;/li&gt; &lt;li&gt;programming languages &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h2&gt;SQL&lt;/h2&gt; 
&lt;h3&gt;SQL Exercises&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Topic&lt;/th&gt; 
   &lt;th&gt;Objective &amp;amp; Instructions&lt;/th&gt; 
   &lt;th&gt;Solution&lt;/th&gt; 
   &lt;th&gt;Comments&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Functions vs. Comparisons&lt;/td&gt; 
   &lt;td&gt;Query Improvements&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/sql/improve_query.md&quot;&gt;Exercise&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/sql/solutions/improve_query.md&quot;&gt;Solution&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;SQL Self Assessment&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is SQL?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;SQL (Structured Query Language) is a standard language for relational databases (like MySQL, MariaDB, ...).&lt;br&gt; It&#39;s used for reading, updating, removing and creating data in a relational database. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How is SQL Different from NoSQL&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The main difference is that SQL databases are structured (data is stored in the form of tables with rows and columns - like an excel spreadsheet table) while NoSQL is unstructured, and the data storage can vary depending on how the NoSQL DB is set up, such as key-value pair, document-oriented, etc. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;When is it best to use SQL? NoSQL?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;SQL - Best used when data integrity is crucial. SQL is typically implemented with many businesses and areas within the finance field due to it&#39;s ACID compliance.&lt;/p&gt; &lt;p&gt;NoSQL - Great if you need to scale things quickly. NoSQL was designed with web applications in mind, so it works great if you need to quickly spread the same information around to multiple servers&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Additionally, since NoSQL does not adhere to the strict table with columns and rows structure that Relational Databases require, you can store different data types together. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h5&gt;Practical SQL - Basics&lt;/h5&gt; 
&lt;p&gt;For these questions, we will be using the Customers and Orders tables shown below:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Customers&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Customer_ID&lt;/th&gt; 
   &lt;th&gt;Customer_Name&lt;/th&gt; 
   &lt;th&gt;Items_in_cart&lt;/th&gt; 
   &lt;th&gt;Cash_spent_to_Date&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100204&lt;/td&gt; 
   &lt;td&gt;John Smith&lt;/td&gt; 
   &lt;td&gt;0&lt;/td&gt; 
   &lt;td&gt;20.00&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100205&lt;/td&gt; 
   &lt;td&gt;Jane Smith&lt;/td&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;40.00&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100206&lt;/td&gt; 
   &lt;td&gt;Bobby Frank&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;100.20&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;ORDERS&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Customer_ID&lt;/th&gt; 
   &lt;th&gt;Order_ID&lt;/th&gt; 
   &lt;th&gt;Item&lt;/th&gt; 
   &lt;th&gt;Price&lt;/th&gt; 
   &lt;th&gt;Date_sold&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100206&lt;/td&gt; 
   &lt;td&gt;A123&lt;/td&gt; 
   &lt;td&gt;Rubber Ducky&lt;/td&gt; 
   &lt;td&gt;2.20&lt;/td&gt; 
   &lt;td&gt;2019-09-18&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100206&lt;/td&gt; 
   &lt;td&gt;A123&lt;/td&gt; 
   &lt;td&gt;Bubble Bath&lt;/td&gt; 
   &lt;td&gt;8.00&lt;/td&gt; 
   &lt;td&gt;2019-09-18&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100206&lt;/td&gt; 
   &lt;td&gt;Q987&lt;/td&gt; 
   &lt;td&gt;80-Pack TP&lt;/td&gt; 
   &lt;td&gt;90.00&lt;/td&gt; 
   &lt;td&gt;2019-09-20&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100205&lt;/td&gt; 
   &lt;td&gt;Z001&lt;/td&gt; 
   &lt;td&gt;Cat Food - Tuna Fish&lt;/td&gt; 
   &lt;td&gt;10.00&lt;/td&gt; 
   &lt;td&gt;2019-08-05&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100205&lt;/td&gt; 
   &lt;td&gt;Z001&lt;/td&gt; 
   &lt;td&gt;Cat Food - Chicken&lt;/td&gt; 
   &lt;td&gt;10.00&lt;/td&gt; 
   &lt;td&gt;2019-08-05&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100205&lt;/td&gt; 
   &lt;td&gt;Z001&lt;/td&gt; 
   &lt;td&gt;Cat Food - Beef&lt;/td&gt; 
   &lt;td&gt;10.00&lt;/td&gt; 
   &lt;td&gt;2019-08-05&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100205&lt;/td&gt; 
   &lt;td&gt;Z001&lt;/td&gt; 
   &lt;td&gt;Cat Food - Kitty quesadilla&lt;/td&gt; 
   &lt;td&gt;10.00&lt;/td&gt; 
   &lt;td&gt;2019-08-05&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;100204&lt;/td&gt; 
   &lt;td&gt;X202&lt;/td&gt; 
   &lt;td&gt;Coffee&lt;/td&gt; 
   &lt;td&gt;20.00&lt;/td&gt; 
   &lt;td&gt;2019-04-29&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;How would I select all fields from this table?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Select * &lt;br&gt; From Customers; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How many items are in John&#39;s cart?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Select Items_in_cart &lt;br&gt; From Customers &lt;br&gt; Where Customer_Name = &quot;John Smith&quot;; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the sum of all the cash spent across all customers?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Select SUM(Cash_spent_to_Date) as SUM_CASH &lt;br&gt; From Customers; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How many people have items in their cart?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Select count(1) as Number_of_People_w_items &lt;br&gt; From Customers &lt;br&gt; where Items_in_cart &amp;gt; 0; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How would you join the customer table to the order table?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;You would join them on the unique key. In this case, the unique key is Customer_ID in both the Customers table and Orders table &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How would you show which customer ordered which items?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Select c.Customer_Name, o.Item &lt;br&gt; From Customers c &lt;br&gt; Left Join Orders o &lt;br&gt; On c.Customer_ID = o.Customer_ID;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Using a with statement, how would you show who ordered cat food, and the total amount of money spent?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;with cat_food as ( &lt;br&gt; Select Customer_ID, SUM(Price) as TOTAL_PRICE &lt;br&gt; From Orders &lt;br&gt; Where Item like &quot;%Cat Food%&quot; &lt;br&gt; Group by Customer_ID &lt;br&gt; ) &lt;br&gt; Select Customer_name, TOTAL_PRICE &lt;br&gt; From Customers c &lt;br&gt; Inner JOIN cat_food f &lt;br&gt; ON c.Customer_ID = f.Customer_ID &lt;br&gt; where c.Customer_ID in (Select Customer_ID from cat_food);&lt;/p&gt; &lt;p&gt;Although this was a simple statement, the &quot;with&quot; clause really shines when a complex query needs to be run on a table before joining to another. With statements are nice, because you create a pseudo temp when running your query, instead of creating a whole new table.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The Sum of all the purchases of cat food weren&#39;t readily available, so we used a with statement to create the pseudo table to retrieve the sum of the prices spent by each customer, then join the table normally. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Which of the following queries would you use? &lt;pre&gt;&lt;code&gt;SELECT count(*)                             SELECT count(*)
FROM shawarma_purchases                     FROM shawarma_purchases
WHERE                               vs.     WHERE
  YEAR(purchased_at) == &#39;2017&#39;              purchased_at &amp;gt;= &#39;2017-01-01&#39; AND
                                            purchased_at &amp;lt;= &#39;2017-31-12&#39;
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;pre&gt;&lt;code&gt;SELECT count(*)
FROM shawarma_purchases
WHERE
  purchased_at &amp;gt;= &#39;2017-01-01&#39; AND
  purchased_at &amp;lt;= &#39;2017-31-12&#39;
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;When you use a function (&lt;code&gt;YEAR(purchased_at)&lt;/code&gt;) it has to scan the whole database as opposed to using indexes and basically the column as it is, in its natural state. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;OpenStack&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What components/projects of OpenStack are you familiar with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; I’m most familiar with several core OpenStack components: 
  &lt;ul&gt; 
   &lt;li&gt;Nova for compute resource provisioning, including VM lifecycle management.&lt;/li&gt; 
   &lt;li&gt;Neutron for networking, focusing on creating and managing networks, subnets, and routers.&lt;/li&gt; 
   &lt;li&gt;Cinder for block storage, used to attach and manage storage volumes.&lt;/li&gt; 
   &lt;li&gt;Keystone for identity services, handling authentication and authorization.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;I’ve implemented these in past projects, configuring them for scalability and security to support multi-tenant environments.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you tell me what each of the following services/projects is responsible for?: 
  &lt;ul&gt; 
   &lt;li&gt;Nova&lt;/li&gt; 
   &lt;li&gt;Neutron&lt;/li&gt; 
   &lt;li&gt;Cinder&lt;/li&gt; 
   &lt;li&gt;Glance&lt;/li&gt; 
   &lt;li&gt;Keystone&lt;/li&gt;
  &lt;/ul&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt;  &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Nova - Manage virtual instances&lt;/li&gt; &lt;li&gt;Neutron - Manage networking by providing Network as a service (NaaS)&lt;/li&gt; &lt;li&gt;Cinder - Block Storage&lt;/li&gt; &lt;li&gt;Glance - Manage images for virtual machines and containers (search, get and register)&lt;/li&gt; &lt;li&gt;Keystone - Authentication service across the cloud &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Identify the service/project used for each of the following: 
  &lt;ul&gt; 
   &lt;li&gt;Copy or snapshot instances&lt;/li&gt; 
   &lt;li&gt;GUI for viewing and modifying resources&lt;/li&gt; 
   &lt;li&gt;Block Storage&lt;/li&gt; 
   &lt;li&gt;Manage virtual instances&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Glance - Images Service. Also used for copying or snapshot instances&lt;/li&gt; &lt;li&gt;Horizon - GUI for viewing and modifying resources&lt;/li&gt; &lt;li&gt;Cinder - Block Storage&lt;/li&gt; &lt;li&gt;Nova - Manage virtual instances &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is a tenant/project?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Determine true or false: 
  &lt;ul&gt; 
   &lt;li&gt;OpenStack is free to use&lt;/li&gt; 
   &lt;li&gt;The service responsible for networking is Glance&lt;/li&gt; 
   &lt;li&gt;The purpose of tenant/project is to share resources between different projects and users of OpenStack&lt;/li&gt;
  &lt;/ul&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Describe in detail how you bring up an instance with a floating IP&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;You get a call from a customer saying: &quot;I can ping my instance but can&#39;t connect (ssh) it&quot;. What might be the problem?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What types of networks OpenStack supports?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do you debug OpenStack storage issues? (tools, logs, ...)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do you debug OpenStack compute issues? (tools, logs, ...)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;OpenStack Deployment &amp;amp; TripleO&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Have you deployed OpenStack in the past? If yes, can you describe how you did it?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Are you familiar with TripleO? How is it different from Devstack or Packstack?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;You can read about TripleO right &lt;a href=&quot;https://docs.openstack.org/tripleo-docs/latest&quot;&gt;here&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;OpenStack Compute&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you describe Nova in detail?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Used to provision and manage virtual instances&lt;/li&gt; &lt;li&gt;It supports Multi-Tenancy in different levels - logging, end-user control, auditing, etc.&lt;/li&gt; &lt;li&gt;Highly scalable&lt;/li&gt; &lt;li&gt;Authentication can be done using internal system or LDAP&lt;/li&gt; &lt;li&gt;Supports multiple types of block storage&lt;/li&gt; &lt;li&gt;Tries to be hardware and hypervisor agnostice &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What do you know about Nova architecture and components?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;nova-api - the server which serves metadata and compute APIs&lt;/li&gt; &lt;li&gt;the different Nova components communicate by using a queue (Rabbitmq usually) and a database&lt;/li&gt; &lt;li&gt;a request for creating an instance is inspected by nova-scheduler which determines where the instance will be created and running&lt;/li&gt; &lt;li&gt;nova-compute is the component responsible for communicating with the hypervisor for creating the instance and manage its lifecycle &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h4&gt;OpenStack Networking (Neutron)&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Neutron in detail&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;One of the core component of OpenStack and a standalone project&lt;/li&gt; &lt;li&gt;Neutron focused on delivering networking as a service&lt;/li&gt; &lt;li&gt;With Neutron, users can set up networks in the cloud and configure and manage a variety of network services&lt;/li&gt; &lt;li&gt;Neutron interacts with: 
    &lt;ul&gt; 
     &lt;li&gt;Keystone - authorize API calls&lt;/li&gt; 
     &lt;li&gt;Nova - nova communicates with neutron to plug NICs into a network&lt;/li&gt; 
     &lt;li&gt;Horizon - supports networking entities in the dashboard and also provides topology view which includes networking details &lt;/li&gt;
    &lt;/ul&gt;&lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;    
&lt;details&gt; 
 &lt;summary&gt;Explain each of the following components: 
  &lt;ul&gt; 
   &lt;li&gt;neutron-dhcp-agent&lt;/li&gt; 
   &lt;li&gt;neutron-l3-agent&lt;/li&gt; 
   &lt;li&gt;neutron-metering-agent&lt;/li&gt; 
   &lt;li&gt;neutron-*-agtent&lt;/li&gt; 
   &lt;li&gt;neutron-server&lt;/li&gt;
  &lt;/ul&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt;  &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;neutron-l3-agent - L3/NAT forwarding (provides external network access for VMs for example)&lt;/li&gt; &lt;li&gt;neutron-dhcp-agent - DHCP services&lt;/li&gt; &lt;li&gt;neutron-metering-agent - L3 traffic metering&lt;/li&gt; &lt;li&gt;neutron-*-agtent - manages local vSwitch configuration on each compute (based on chosen plugin)&lt;/li&gt; &lt;li&gt;neutron-server - exposes networking API and passes requests to other plugins if required &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain these network types: 
  &lt;ul&gt; 
   &lt;li&gt;Management Network&lt;/li&gt; 
   &lt;li&gt;Guest Network&lt;/li&gt; 
   &lt;li&gt;API Network&lt;/li&gt; 
   &lt;li&gt;External Network&lt;/li&gt;
  &lt;/ul&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt;  &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Management Network - used for internal communication between OpenStack components. Any IP address in this network is accessible only within the datacetner&lt;/li&gt; &lt;li&gt;Guest Network - used for communication between instances/VMs&lt;/li&gt; &lt;li&gt;API Network - used for services API communication. Any IP address in this network is publicly accessible&lt;/li&gt; &lt;li&gt;External Network - used for public communication. Any IP address in this network is accessible by anyone on the internet &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;In which order should you remove the following entities: 
  &lt;ul&gt; 
   &lt;li&gt;Network&lt;/li&gt; 
   &lt;li&gt;Port&lt;/li&gt; 
   &lt;li&gt;Router&lt;/li&gt; 
   &lt;li&gt;Subnet&lt;/li&gt;
  &lt;/ul&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt;  
  &lt;ul&gt; 
   &lt;li&gt;Port&lt;/li&gt; 
   &lt;li&gt;Subnet&lt;/li&gt; 
   &lt;li&gt;Router&lt;/li&gt; 
   &lt;li&gt;Network&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;There are many reasons for that. One for example: you can&#39;t remove router if there are active ports assigned to it. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a provider network?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What components and services exist for L2 and L3?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the ML2 plug-in? Explain its architecture&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the L2 agent? How does it works and what is it responsible for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the L3 agent? How does it works and what is it responsible for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what the Metadata agent is responsible for&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What networking entities Neutron supports?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do you debug OpenStack networking issues? (tools, logs, ...)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;OpenStack - Glance&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Glance in detail&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Glance is the OpenStack image service&lt;/li&gt; &lt;li&gt;It handles requests related to instances disks and images&lt;/li&gt; &lt;li&gt;Glance also used for creating snapshots for quick instances backups&lt;/li&gt; &lt;li&gt;Users can use Glance to create new images or upload existing ones &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Describe Glance architecture&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;glance-api - responsible for handling image API calls such as retrieval and storage. It consists of two APIs: 1. registry-api - responsible for internal requests 2. user API - can be accessed publicly&lt;/li&gt; &lt;li&gt;glance-registry - responsible for handling image metadata requests (e.g. size, type, etc). This component is private which means it&#39;s not available publicly&lt;/li&gt; &lt;li&gt;metadata definition service - API for custom metadata&lt;/li&gt; &lt;li&gt;database - for storing images metadata&lt;/li&gt; &lt;li&gt;image repository - for storing images. This can be a filesystem, swift object storage, HTTP, etc. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h4&gt;OpenStack - Swift&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Swift in detail&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Swift is Object Store service and is an highly available, distributed and consistent store designed for storing a lot of data&lt;/li&gt; &lt;li&gt;Swift is distributing data across multiple servers while writing it to multiple disks&lt;/li&gt; &lt;li&gt;One can choose to add additional servers to scale the cluster. All while swift maintaining integrity of the information and data replications. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Can users store by default an object of 100GB in size?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Not by default. Object Storage API limits the maximum to 5GB per object but it can be adjusted. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain the following in regards to Swift: 
  &lt;ul&gt; 
   &lt;li&gt;Container&lt;/li&gt; 
   &lt;li&gt;Account&lt;/li&gt; 
   &lt;li&gt;Object&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Container - Defines a namespace for objects.&lt;/li&gt; &lt;li&gt;Account - Defines a namespace for containers&lt;/li&gt; &lt;li&gt;Object - Data content (e.g. image, document, ...) &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;True or False? there can be two objects with the same name in the same container but not in two different containers&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;False. Two objects can have the same name if they are in different containers. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;OpenStack - Cinder&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Cinder in detail&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Cinder is OpenStack Block Storage service&lt;/li&gt; &lt;li&gt;It basically provides used with storage resources they can consume with other services such as Nova&lt;/li&gt; &lt;li&gt;One of the most used implementations of storage supported by Cinder is LVM&lt;/li&gt; &lt;li&gt;From user perspective this is transparent which means the user doesn&#39;t know where, behind the scenes, the storage is located or what type of storage is used &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Describe Cinder&#39;s components&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;cinder-api - receives API requests&lt;/li&gt; &lt;li&gt;cinder-volume - manages attached block devices&lt;/li&gt; &lt;li&gt;cinder-scheduler - responsible for storing volumes &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h4&gt;OpenStack - Keystone&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you describe the following concepts in regards to Keystone? 
  &lt;ul&gt; 
   &lt;li&gt;Role&lt;/li&gt; 
   &lt;li&gt;Tenant/Project&lt;/li&gt; 
   &lt;li&gt;Service&lt;/li&gt; 
   &lt;li&gt;Endpoint&lt;/li&gt; 
   &lt;li&gt;Token&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Role - A list of rights and privileges determining what a user or a project can perform&lt;/li&gt; &lt;li&gt;Tenant/Project - Logical representation of a group of resources isolated from other groups of resources. It can be an account, organization, ...&lt;/li&gt; &lt;li&gt;Service - An endpoint which the user can use for accessing different resources&lt;/li&gt; &lt;li&gt;Endpoint - a network address which can be used to access a certain OpenStack service&lt;/li&gt; &lt;li&gt;Token - Used for access resources while describing which resources can be accessed by using a scope &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are the properties of a service? In other words, how a service is identified?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Using:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Name&lt;/li&gt; &lt;li&gt;ID number&lt;/li&gt; &lt;li&gt;Type&lt;/li&gt; &lt;li&gt;Description &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain the following: - PublicURL - InternalURL - AdminURL&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;PublicURL - Publicly accessible through public internet&lt;/li&gt; &lt;li&gt;InternalURL - Used for communication between services&lt;/li&gt; &lt;li&gt;AdminURL - Used for administrative management &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is a service catalog?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A list of services and their endpoints &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;OpenStack Advanced - Services&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Describe each of the following services 
  &lt;ul&gt; 
   &lt;li&gt;Swift&lt;/li&gt; 
   &lt;li&gt;Sahara&lt;/li&gt; 
   &lt;li&gt;Ironic&lt;/li&gt; 
   &lt;li&gt;Trove&lt;/li&gt; 
   &lt;li&gt;Aodh&lt;/li&gt; 
   &lt;li&gt;Ceilometer&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Swift - highly available, distributed, eventually consistent object/blob store&lt;/li&gt; &lt;li&gt;Sahara - Manage Hadoop Clusters&lt;/li&gt; &lt;li&gt;Ironic - Bare Metal Provisioning&lt;/li&gt; &lt;li&gt;Trove - Database as a service that runs on OpenStack&lt;/li&gt; &lt;li&gt;Aodh - Alarms Service&lt;/li&gt; &lt;li&gt;Ceilometer - Track and monitor usage &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Identify the service/project used for each of the following: 
  &lt;ul&gt; 
   &lt;li&gt;Database as a service which runs on OpenStack&lt;/li&gt; 
   &lt;li&gt;Bare Metal Provisioning&lt;/li&gt; 
   &lt;li&gt;Track and monitor usage&lt;/li&gt; 
   &lt;li&gt;Alarms Service&lt;/li&gt; 
   &lt;li&gt;Manage Hadoop Clusters&lt;/li&gt; 
   &lt;li&gt;highly available, distributed, eventually consistent object/blob store&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Database as a service which runs on OpenStack - Trove&lt;/li&gt; &lt;li&gt;Bare Metal Provisioning - Ironic&lt;/li&gt; &lt;li&gt;Track and monitor usage - Ceilometer&lt;/li&gt; &lt;li&gt;Alarms Service - Aodh&lt;/li&gt; &lt;li&gt;Manage Hadoop Clusters&lt;/li&gt; &lt;li&gt;Manage Hadoop Clusters - Sahara&lt;/li&gt; &lt;li&gt;highly available, distributed, eventually consistent object/blob store - Swift &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h4&gt;OpenStack Advanced - Keystone&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you describe Keystone service in detail?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;You can&#39;t have OpenStack deployed without Keystone&lt;/li&gt; &lt;li&gt;It Provides identity, policy and token services 
    &lt;ul&gt; 
     &lt;li&gt;The authentication provided is for both users and services&lt;/li&gt; 
     &lt;li&gt;The authorization supported is token-based and user-based.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;There is a policy defined based on RBAC stored in a JSON file and each line in that file defines the level of access to apply &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Describe Keystone architecture&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;There is a service API and admin API through which Keystone gets requests&lt;/li&gt; &lt;li&gt;Keystone has four backends: 
    &lt;ul&gt; 
     &lt;li&gt;Token Backend - Temporary Tokens for users and services&lt;/li&gt; 
     &lt;li&gt;Policy Backend - Rules management and authorization&lt;/li&gt; 
     &lt;li&gt;Identity Backend - users and groups (either standalone DB, LDAP, ...)&lt;/li&gt; 
     &lt;li&gt;Catalog Backend - Endpoints&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;It has pluggable environment where you can integrate with: 
    &lt;ul&gt; 
     &lt;li&gt;LDAP&lt;/li&gt; 
     &lt;li&gt;KVS (Key Value Store)&lt;/li&gt; 
     &lt;li&gt;SQL&lt;/li&gt; 
     &lt;li&gt;PAM&lt;/li&gt; 
     &lt;li&gt;Memcached &lt;/li&gt;
    &lt;/ul&gt;&lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;    
&lt;details&gt; 
 &lt;summary&gt;Describe the Keystone authentication process&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Keystone gets a call/request and checks whether it&#39;s from an authorized user, using username, password and authURL&lt;/li&gt; &lt;li&gt;Once confirmed, Keystone provides a token.&lt;/li&gt; &lt;li&gt;A token contains a list of user&#39;s projects so there is no to authenticate every time and a token can submitted instead &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h4&gt;OpenStack Advanced - Compute (Nova)&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;What each of the following does?: 
  &lt;ul&gt; 
   &lt;li&gt;nova-api&lt;/li&gt; 
   &lt;li&gt;nova-compuate&lt;/li&gt; 
   &lt;li&gt;nova-conductor&lt;/li&gt; 
   &lt;li&gt;nova-cert&lt;/li&gt; 
   &lt;li&gt;nova-consoleauth&lt;/li&gt; 
   &lt;li&gt;nova-scheduler&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;nova-api - responsible for managing requests/calls&lt;/li&gt; &lt;li&gt;nova-compute - responsible for managing instance lifecycle&lt;/li&gt; &lt;li&gt;nova-conductor - Mediates between nova-compute and the database so nova-compute doesn&#39;t access it directly &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What types of Nova proxies are you familiar with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Nova-novncproxy - Access through VNC connections&lt;/li&gt; &lt;li&gt;Nova-spicehtml5proxy - Access through SPICE&lt;/li&gt; &lt;li&gt;Nova-xvpvncproxy - Access through a VNC connection &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h4&gt;OpenStack Advanced - Networking (Neutron)&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain BGP dynamic routing&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the role of network namespaces in OpenStack?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;OpenStack Advanced - Horizon&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you describe Horizon in detail?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Django-based project focusing on providing an OpenStack dashboard and the ability to create additional customized dashboards&lt;/li&gt; &lt;li&gt;You can use it to access the different OpenStack services resources - instances, images, networks, ... 
    &lt;ul&gt; 
     &lt;li&gt;By accessing the dashboard, users can use it to list, create, remove and modify the different resources&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;It&#39;s also highly customizable and you can modify or add to it based on your needs &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What can you tell about Horizon architecture?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;API is backward compatible&lt;/li&gt; &lt;li&gt;There are three type of dashboards: user, system and settings&lt;/li&gt; &lt;li&gt;It provides core support for all OpenStack core projects such as Neutron, Nova, etc. (out of the box, no need to install extra packages or plugins)&lt;/li&gt; &lt;li&gt;Anyone can extend the dashboards and add new components&lt;/li&gt; &lt;li&gt;Horizon provides templates and core classes from which one can build its own dashboard &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h2&gt;Puppet&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Puppet? How does it works?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Puppet is a configuration management tool ensuring that all systems are configured to a desired and predictable state. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain Puppet architecture&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Puppet has a primary-secondary node architecture. The clients are distributed across the network and communicate with the primary-secondary environment where Puppet modules are present. The client agent sends a certificate with its ID to the server; the server then signs that certificate and sends it back to the client. This authentication allows for secure and verifiable communication between the client and the master. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Can you compare Puppet to other configuration management tools? Why did you chose to use Puppet?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Puppet is often compared to other configuration management tools like Chef, Ansible, SaltStack, and cfengine. The choice to use Puppet often depends on an organization&#39;s needs, such as ease of use, scalability, and community support. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain the following: 
  &lt;ul&gt; 
   &lt;li&gt;Module&lt;/li&gt; 
   &lt;li&gt;Manifest&lt;/li&gt; 
   &lt;li&gt;Node&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Modules - are a collection of manifests, templates, and files&lt;/li&gt; &lt;li&gt;Manifests - are the actual codes for configuring the clients&lt;/li&gt; &lt;li&gt;Node - allows you to assign specific configurations to specific nodes &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain Facter&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Facter is a standalone tool in Puppet that collects information about a system and its configuration, such as the operating system, IP addresses, memory, and network interfaces. This information can be used in Puppet manifests to make decisions about how resources should be managed, and to customize the behavior of Puppet based on the characteristics of the system. Facter is integrated into Puppet, and its facts can be used within Puppet manifests to make decisions about resource management. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is MCollective?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;MCollective is a middleware system that integrates with Puppet to provide orchestration, remote execution, and parallel job execution capabilities. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Do you have experience with writing modules? Which module have you created and for what?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is Hiera&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Hiera is a hierarchical data store in Puppet that is used to separate data from code, allowing data to be more easily separated, managed, and reused. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h2&gt;Elastic&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the Elastic Stack?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;The Elastic Stack consists of:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Elasticsearch&lt;/li&gt; 
   &lt;li&gt;Kibana&lt;/li&gt; 
   &lt;li&gt;Logstash&lt;/li&gt; 
   &lt;li&gt;Beats&lt;/li&gt; 
   &lt;li&gt;Elastic Hadoop&lt;/li&gt; 
   &lt;li&gt;APM Server&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Elasticsearch, Logstash and Kibana are also known as the ELK stack. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is Elasticsearch&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;From the official &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/current/documents-indices.html&quot;&gt;docs&lt;/a&gt;:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;Elasticsearch is a distributed document store. Instead of storing information as rows of columnar data, Elasticsearch stores complex data structures that have been serialized as JSON documents&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Logstash?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;From the &lt;a href=&quot;https://logit.io/blog/post/the-top-50-elk-stack-and-elasticsearch-interview-questions&quot;&gt;blog&lt;/a&gt;:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;Logstash is a powerful, flexible pipeline that collects, enriches and transports data. It works as an extract, transform &amp;amp; load (ETL) tool for collecting log messages.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what beats are&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Beats are lightweight data shippers. These data shippers installed on the client where the data resides. Examples of beats: Filebeat, Metricbeat, Auditbeat. There are much more.&lt;br&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Kibana?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;From the official docs:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;Kibana is an open source analytics and visualization platform designed to work with Elasticsearch. You use Kibana to search, view, and interact with data stored in Elasticsearch indices. You can easily perform advanced data analysis and visualize your data in a variety of charts, tables, and maps.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Describe what happens from the moment an app logged some information until it&#39;s displayed to the user in a dashboard when the Elastic stack is used&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;The process may vary based on the chosen architecture and the processing you may want to apply to the logs. One possible workflow is:&lt;/p&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;The data logged by the application is picked by filebeat and sent to logstash&lt;/li&gt; &lt;li&gt;Logstash process the log based on the defined filters. Once done, the output is sent to Elasticsearch&lt;/li&gt; &lt;li&gt;Elasticsearch stores the document it got and the document is indexed for quick future access&lt;/li&gt; &lt;li&gt;The user creates visualizations in Kibana which based on the indexed data&lt;/li&gt; &lt;li&gt;The user creates a dashboard which composed out of the visualization created in the previous step &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;h5&gt;Elasticsearch&lt;/h5&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a data node?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;This is where data is stored and also where different processing takes place (e.g. when you search for a data). &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a master node?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Part of a master node responsibilities:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Track the status of all the nodes in the cluster&lt;/li&gt; 
   &lt;li&gt;Verify replicas are working and the data is available from every data node.&lt;/li&gt; 
   &lt;li&gt;No hot nodes (no data node that works much harder than other nodes)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;While there can be multiple master nodes in reality only of them is the elected master node. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an ingest node?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A node which responsible for processing the data according to ingest pipeline. In case you don&#39;t need to use logstash then this node can receive data from beats and process it, similarly to how it can be processed in Logstash. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Coordinating only node?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;From the official docs:&lt;/p&gt; &lt;p&gt;Coordinating only nodes can benefit large clusters by offloading the coordinating node role from data and master-eligible nodes. They join the cluster and receive the full cluster state, like every other node, and they use the cluster state to route requests directly to the appropriate place(s).&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How data is stored in Elasticsearch?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Data is stored in an index&lt;/li&gt; &lt;li&gt;The index is spread across the cluster using shards &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is an Index?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Index in Elasticsearch is in most cases compared to a whole database from the SQL/NoSQL world.&lt;br&gt; You can choose to have one index to hold all the data of your app or have multiple indices where each index holds different type of your app (e.g. index for each service your app is running).&lt;/p&gt; &lt;p&gt;The official docs also offer a great explanation (in general, it&#39;s really good documentation, as every project should have):&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;An index can be thought of as an optimized collection of documents and each document is a collection of fields, which are the key-value pairs that contain your data&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Shards&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;An index is split into shards and documents are hashed to a particular shard. Each shard may be on a different node in a cluster and each one of the shards is a self contained index.&lt;br&gt; This allows Elasticsearch to scale to an entire cluster of servers. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an Inverted Index?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;From the official docs:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;An inverted index lists every unique word that appears in any document and identifies all of the documents each word occurs in.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a Document?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Continuing with the comparison to SQL/NoSQL a Document in Elasticsearch is a row in table in the case of SQL or a document in a collection in the case of NoSQL. As in NoSQL a document is a JSON object which holds data on a unit in your app. What is this unit depends on the your app. If your app related to book then each document describes a book. If you are app is about shirts then each document is a shirt. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;You check the health of your elasticsearch cluster and it&#39;s red. What does it mean? What can cause the status to be yellow instead of green?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Red means some data is unavailable in your cluster. Some shards of your indices are unassigned. There are some other states for the cluster. Yellow means that you have unassigned shards in the cluster. You can be in this state if you have single node and your indices have replicas. Green means that all shards in the cluster are assigned to nodes and your cluster is healthy. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;True or False? Elasticsearch indexes all data in every field and each indexed field has the same data structure for unified and quick query ability&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;False. From the official docs:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;Each indexed field has a dedicated, optimized data structure. For example, text fields are stored in inverted indices, and numeric and geo fields are stored in BKD trees.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What reserved fields a document has?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;_index&lt;/li&gt; &lt;li&gt;_id&lt;/li&gt; &lt;li&gt;_type &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain Mapping&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are the advantages of defining your own mapping? (or: when would you use your own mapping?)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;You can optimize fields for partial matching&lt;/li&gt; &lt;li&gt;You can define custom formats of known fields (e.g. date)&lt;/li&gt; &lt;li&gt;You can perform language-specific analysis &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain Replicas&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;In a network/cloud environment where failures can be expected any time, it is very useful and highly recommended to have a failover mechanism in case a shard/node somehow goes offline or disappears for whatever reason. To this end, Elasticsearch allows you to make one or more copies of your index’s shards into what are called replica shards, or replicas for short. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you explain Term Frequency &amp;amp; Document Frequency?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Term Frequency is how often a term appears in a given document and Document Frequency is how often a term appears in all documents. They both are used for determining the relevance of a term by calculating Term Frequency / Document Frequency. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;You check &quot;Current Phase&quot; under &quot;Index lifecycle management&quot; and you see it&#39;s set to &quot;hot&quot;. What does it mean?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;The index is actively being written to&quot;. More about the phases &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/7.6/ilm-policy-definition.html&quot;&gt;here&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What this command does? &lt;code&gt;curl -X PUT &quot;localhost:9200/customer/_doc/1?pretty&quot; -H &#39;Content-Type: application/json&#39; -d&#39;{ &quot;name&quot;: &quot;John Doe&quot; }&#39;&lt;/code&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;It creates customer index if it doesn&#39;t exists and adds a new document with the field name which is set to &quot;John Dow&quot;. Also, if it&#39;s the first document it will get the ID 1. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What will happen if you run the previous command twice? What about running it 100 times?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;If name value was different then it would update &quot;name&quot; to the new value&lt;/li&gt; &lt;li&gt;In any case, it bumps version field by one &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is the Bulk API? What would you use it for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Bulk API is used when you need to index multiple documents. For high number of documents it would be significantly faster to use rather than individual requests since there are less network roundtrips. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h5&gt;Query DSL&lt;/h5&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Elasticsearch query syntax (Booleans, Fields, Ranges)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is Relevance Score&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Query Context and Filter Context&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;From the official docs:&lt;/p&gt; &lt;p&gt;&quot;In the query context, a query clause answers the question “How well does this document match this query clause?” Besides deciding whether or not the document matches, the query clause also calculates a relevance score in the _score meta-field.&quot;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;In a filter context, a query clause answers the question “Does this document match this query clause?” The answer is a simple Yes or No — no scores are calculated. Filter context is mostly used for filtering structured data&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Describe how would an architecture of production environment with large amounts of data would be different from a small-scale environment&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;There are several possible answers for this question. One of them is as follows:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A small-scale architecture of elastic will consist of the elastic stack as it is. This means we will have beats, logstash, elastcsearch and kibana.&lt;br&gt; A production environment with large amounts of data can include some kind of buffering component (e.g. Reddis or RabbitMQ) and also security component such as Nginx. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h5&gt;Logstash&lt;/h5&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are Logstash plugins? What plugins types are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Input Plugins - how to collect data from different sources&lt;/li&gt; &lt;li&gt;Filter Plugins - processing data&lt;/li&gt; &lt;li&gt;Output Plugins - push data to different outputs/services/platforms &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is grok?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A logstash plugin which modifies information in one format and immerse it in another. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How grok works?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What grok patterns are you familiar with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is `_grokparsefailure?`&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do you test or debug grok patterns?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are Logstash Codecs? What codecs are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h5&gt;Kibana&lt;/h5&gt; 
&lt;details&gt; 
 &lt;summary&gt;What can you find under &quot;Discover&quot; in Kibana?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The raw data as it is stored in the index. You can search and filter it. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;You see in Kibana, after clicking on Discover, &quot;561 hits&quot;. What does it mean?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Total number of documents matching the search results. If not query used then simply the total number of documents. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What can you find under &quot;Visualize&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;Visualize&quot; is where you can create visual representations for your data (pie charts, graphs, ...) &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What visualization types are supported/included in Kibana?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What visualization type would you use for statistical outliers&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Describe in detail how do you create a dashboard in Kibana&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;Filebeat&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Filebeat?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Filebeat is used to monitor the logging directories inside of VMs or mounted as a sidecar if exporting logs from containers, and then forward these logs onward for further processing, usually to logstash. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;If one is using ELK, is it a must to also use filebeat? In what scenarios it&#39;s useful to use filebeat?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Filebeat is a typical component of the ELK stack, since it was developed by Elastic to work with the other products (Logstash and Kibana). It&#39;s possible to send logs directly to logstash, though this often requires coding changes for the application. Particularly for legacy applications with little test coverage, it might be a better option to use filebeat, since you don&#39;t need to make any changes to the application code. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a harvester?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Read &lt;a href=&quot;https://www.elastic.co/guide/en/beats/filebeat/current/how-filebeat-works.html#harvester&quot;&gt;here&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;True or False? a single harvester harvest multiple files, according to the limits set in filebeat.yml&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;False. One harvester harvests one file. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are filebeat modules?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;These are pre-configured modules for specific types of logging locations (eg, Traefik, Fargate, HAProxy) to make it easy to configure forwarding logs using filebeat. They have different configurations based on where you&#39;re collecting logs from. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Elastic Stack&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do you secure an Elastic Stack?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;You can generate certificates with the provided elastic utils and change configuration to enable security using certificates model. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Distributed&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Distributed Computing (or Distributed System)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;According to Martin Kleppmann:&lt;/p&gt; &lt;p&gt;&quot;Many processes running on many machines...only message-passing via an unreliable network with variable delays, and the system may suffer from partial failures, unreliable clocks, and process pauses.&quot;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Another definition: &quot;Systems that are physically separated, but logically connected&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What can cause a system to fail?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Network&lt;/li&gt; &lt;li&gt;CPU&lt;/li&gt; &lt;li&gt;Memory&lt;/li&gt; &lt;li&gt;Disk &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Do you know what is &quot;CAP theorem&quot;? (aka as Brewer&#39;s theorem)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;According to the CAP theorem, it&#39;s not possible for a distributed data store to provide more than two of the following at the same time:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Availability: Every request receives a response (it doesn&#39;t has to be the most recent data)&lt;/li&gt; &lt;li&gt;Consistency: Every request receives a response with the latest/most recent data&lt;/li&gt; &lt;li&gt;Partition tolerance: Even if some the data is lost/dropped, the system keeps running &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are the problems with the following design? How to improve it?&lt;br&gt; &lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/distributed/distributed_design_standby.png&quot; width=&quot;500x;&quot; height=&quot;350px;&quot;&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; 1. The transition can take time. In other words, noticeable downtime. 2. Standby server is a waste of resources - if first application server is running then the standby does nothing &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are the problems with the following design? How to improve it?&lt;br&gt; &lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/distributed/distributed_design_lb.png&quot; width=&quot;700x;&quot; height=&quot;350px;&quot;&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; Issues: If load balancer dies , we lose the ability to communicate with the application. &lt;p&gt;Ways to improve:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Add another load balancer&lt;/li&gt; &lt;li&gt;Use DNS A record for both load balancers&lt;/li&gt; &lt;li&gt;Use message queue &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is &quot;Shared-Nothing&quot; architecture?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;It&#39;s an architecture in which data is and retrieved from a single, non-shared, source usually exclusively connected to one node as opposed to architectures where the request can get to one of many nodes and the data will be retrieved from one shared location (storage, memory, ...). &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain the Sidecar Pattern (Or sidecar proxy)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h2&gt;Misc&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Topic&lt;/th&gt; 
   &lt;th&gt;Objective &amp;amp; Instructions&lt;/th&gt; 
   &lt;th&gt;Solution&lt;/th&gt; 
   &lt;th&gt;Comments&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Highly Available &quot;Hello World&quot;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/devops/ha_hello_world.md&quot;&gt;Exercise&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/devops/solutions/ha_hello_world.md&quot;&gt;Solution&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;What happens when you type in a URL in an address bar in a browser?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; 
  &lt;ol&gt; 
   &lt;li&gt;The browser searches for the record of the domain name IP address in the DNS in the following order:&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Browser cache&lt;/li&gt; 
   &lt;li&gt;Operating system cache&lt;/li&gt; 
   &lt;li&gt;The DNS server configured on the user&#39;s system (can be ISP DNS, public DNS, ...)&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;ol start=&quot;2&quot;&gt; 
   &lt;li&gt;If it couldn&#39;t find a DNS record locally, a full DNS resolution is started.&lt;/li&gt; 
   &lt;li&gt;It connects to the server using the TCP protocol&lt;/li&gt; 
   &lt;li&gt;The browser sends an HTTP request to the server&lt;/li&gt; 
   &lt;li&gt;The server sends an HTTP response back to the browser&lt;/li&gt; 
   &lt;li&gt;The browser renders the response (e.g. HTML)&lt;/li&gt; 
   &lt;li&gt;The browser then sends subsequent requests as needed to the server to get the embedded links, javascript, images in the HTML and then steps 3 to 5 are repeated.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;TODO: add more details! &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;API&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is an API&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;I like this definition from &lt;a href=&quot;https://blog.christianposta.com/microservices/api-gateways-are-going-through-an-identity-crisis&quot;&gt;blog.christianposta.com&lt;/a&gt;:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;An explicitly and purposefully defined interface designed to be invoked over a network that enables software developers to get programmatic access to data and functionality within an organization in a controlled and comfortable way.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an API specification?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;From &lt;a href=&quot;https://swagger.io/resources/articles/difference-between-api-documentation-specification&quot;&gt;swagger.io&lt;/a&gt;:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;An API specification provides a broad understanding of how an API behaves and how the API links with other APIs. It explains how the API functions and the results to expect when using the API&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;True or False? API Definition is the same as API Specification&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;False. From &lt;a href=&quot;https://swagger.io/resources/articles/difference-between-api-documentation-specification&quot;&gt;swagger.io&lt;/a&gt;:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;An API definition is similar to an API specification in that it provides an understanding of how an API is organized and how the API functions. But the API definition is aimed at machine consumption instead of human consumption of APIs.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an API gateway?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;An API gateway is like the gatekeeper that controls how different parts talk to each other and how information is exchanged between them.&lt;/p&gt; &lt;p&gt;The API gateway provides a single point of entry for all clients, and it can perform several tasks, including routing requests to the appropriate backend service, load balancing, security and authentication, rate limiting, caching, and monitoring.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;By using an API gateway, organizations can simplify the management of their APIs, ensure consistent security and governance, and improve the performance and scalability of their backend services. They are also commonly used in microservices architectures, where there are many small, independent services that need to be accessed by different clients. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are the advantages of using/implementing an API gateway?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Advantages:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Simplifies API management: Provides a single entry point for all requests, which simplifies the management and monitoring of multiple APIs.&lt;/li&gt; 
   &lt;li&gt;Improves security: Able to implement security features like authentication, authorization, and encryption to protect the backend services from unauthorized access.&lt;/li&gt; 
   &lt;li&gt;Enhances scalability: Can handle traffic spikes and distribute requests to backend services in a way that maximizes resource utilization and improves overall system performance.&lt;/li&gt; 
   &lt;li&gt;Enables service composition: Can combine different backend services into a single API, providing more granular control over the services that clients can access.&lt;/li&gt; 
   &lt;li&gt;Facilitates integration with external systems: Can be used to expose internal services to external partners or customers, making it easier to integrate with external systems and enabling new business models.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a Payload in API?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Automation? How it&#39;s related or different from Orchestration?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Automation is the act of automating tasks to reduce human intervention or interaction in regards to IT technology and systems.&lt;br&gt; While automation focuses on a task level, Orchestration is the process of automating processes and/or workflows which consists of multiple tasks that usually across multiple systems. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Tell me about interesting bugs you&#39;ve found and also fixed&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a Debugger and how it works?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What services an application might have?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Authorization&lt;/li&gt; &lt;li&gt;Logging&lt;/li&gt; &lt;li&gt;Authentication&lt;/li&gt; &lt;li&gt;Ordering&lt;/li&gt; &lt;li&gt;Front-end&lt;/li&gt; &lt;li&gt;Back-end ... &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is Metadata?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Data about data. Basically, it describes the type of information that an underlying data will hold. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;You can use one of the following formats: JSON, YAML, XML. Which one would you use? Why?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;I can&#39;t answer this for you :) &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What&#39;s KPI?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What&#39;s OKR?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What&#39;s DSL (Domain Specific Language)?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Domain Specific Language (DSLs) are used to create a customised language that represents the domain such that domain experts can easily interpret it. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What&#39;s the difference between KPI and OKR?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;YAML&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is YAML?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Data serialization language used by many technologies today like Kubernetes, Ansible, etc. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;True or False? Any valid JSON file is also a valid YAML file&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;True. Because YAML is superset of JSON. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the format of the following data? &lt;pre&gt;&lt;code&gt;{
    applications: [
        {
            name: &quot;my_app&quot;,
            language: &quot;python&quot;,
            version: 20.17
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; JSON &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the format of the following data? &lt;pre&gt;&lt;code&gt;applications:
  - app: &quot;my_app&quot;
    language: &quot;python&quot;
    version: 20.17
&lt;/code&gt;&lt;/pre&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; YAML &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How to write a multi-line string with YAML? What use cases is it good for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;pre&gt;&lt;code&gt;someMultiLineString: |
  look mama
  I can write a multi-line string
  I love YAML
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;It&#39;s good for use cases like writing a shell script where each line of the script is a different command. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the difference between &lt;code&gt;someMultiLineString: |&lt;/code&gt; to &lt;code&gt;someMultiLineString: &amp;gt;&lt;/code&gt;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;using &lt;code&gt;&amp;gt;&lt;/code&gt; will make the multi-line string to fold into a single line&lt;/p&gt; &lt;pre&gt;&lt;code&gt;someMultiLineString: &amp;gt;
  This is actually
  a single line
  do not let appearances fool you
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are placeholders in YAML?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;They allow you reference values instead of directly writing them and it is used like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;username: {{ my.user_name }}
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How can you define multiple YAML components in one file?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Using this: &lt;code&gt;---&lt;/code&gt; For Examples:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;document_number: 1
---
document_number: 2
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Firmware&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is a firmware&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Firmware&quot;&gt;Wikipedia&lt;/a&gt;: &quot;In computing, firmware is a specific class of computer software that provides the low-level control for a device&#39;s specific hardware. Firmware, such as the BIOS of a personal computer, may contain basic functions of a device, and may provide hardware abstraction services to higher-level software such as operating systems.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Cassandra&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;When running a cassandra cluster, how often do you need to run nodetool repair in order to keep the cluster consistent? 
  &lt;ul&gt; 
   &lt;li&gt;Within the columnFamily GC-grace Once a week&lt;/li&gt; 
   &lt;li&gt;Less than the compacted partition minimum bytes&lt;/li&gt; 
   &lt;li&gt;Depended on the compaction strategy&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h2&gt;HTTP&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is HTTP?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://avinetworks.com/glossary/layer-7/&quot;&gt;Avinetworks&lt;/a&gt;: HTTP stands for Hypertext Transfer Protocol. HTTP uses TCP port 80 to enable internet communication. It is part of the Application Layer (L7) in OSI Model. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Describe HTTP request lifecycle&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Resolve host by request to DNS resolver&lt;/li&gt; &lt;li&gt;Client SYN&lt;/li&gt; &lt;li&gt;Server SYN+ACK&lt;/li&gt; &lt;li&gt;Client SYN&lt;/li&gt; &lt;li&gt;HTTP request&lt;/li&gt; &lt;li&gt;HTTP response &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;True or False? HTTP is stateful&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;False. It doesn&#39;t maintain state for incoming request. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;How HTTP request looks like?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;It consists of:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Request line - request type&lt;/li&gt; &lt;li&gt;Headers - content info like length, encoding, etc.&lt;/li&gt; &lt;li&gt;Body (not always included) &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What HTTP method types are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;GET&lt;/li&gt; &lt;li&gt;POST&lt;/li&gt; &lt;li&gt;HEAD&lt;/li&gt; &lt;li&gt;PUT&lt;/li&gt; &lt;li&gt;DELETE&lt;/li&gt; &lt;li&gt;CONNECT&lt;/li&gt; &lt;li&gt;OPTIONS&lt;/li&gt; &lt;li&gt;TRACE &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What HTTP response codes are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;1xx - informational&lt;/li&gt; &lt;li&gt;2xx - Success&lt;/li&gt; &lt;li&gt;3xx - Redirect&lt;/li&gt; &lt;li&gt;4xx - Error, client fault&lt;/li&gt; &lt;li&gt;5xx - Error, server fault &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is HTTPS?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;HTTPS is a secure version of the HTTP protocol used to transfer data between a web browser and a web server. It encrypts the communication using SSL/TLS encryption to ensure that the data is private and secure.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Learn more: &lt;a href=&quot;https://www.cloudflare.com/learning/ssl/why-is-http-not-secure/&quot;&gt;https://www.cloudflare.com/learning/ssl/why-is-http-not-secure/&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain HTTP Cookies&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;HTTP is stateless. To share state, we can use Cookies.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;TODO: explain what is actually a Cookie &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is HTTP Pipelining?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;You get &quot;504 Gateway Timeout&quot; error from an HTTP server. What does it mean?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The server didn&#39;t receive a response from another server it communicates with in a timely manner. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a proxy?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A proxy is a server that acts as a middleman between a client device and a destination server. It can help improve privacy, security, and performance by hiding the client&#39;s IP address, filtering content, and caching frequently accessed data.&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Proxies can be used for load balancing, distributing traffic across multiple servers to help prevent server overload and improve website or application performance. They can also be used for data analysis, as they can log requests and traffic, providing useful insights into user behavior and preferences. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is a reverse proxy?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A reverse proxy is a type of proxy server that sits between a client and a server, but it is used to manage traffic going in the opposite direction of a traditional forward proxy. In a forward proxy, the client sends requests to the proxy server, which then forwards them to the destination server. However, in a reverse proxy, the client sends requests to the destination server, but the requests are intercepted by the reverse proxy before they reach the server.&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;They&#39;re commonly used to improve web server performance, provide high availability and fault tolerance, and enhance security by preventing direct access to the back-end server. They are often used in large-scale web applications and high-traffic websites to manage and distribute requests to multiple servers, resulting in improved scalability and reliability. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;When you publish a project, you usually publish it with a license. What types of licenses are you familiar with and which one do you prefer to use?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is &quot;X-Forwarded-For&quot;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/X-Forwarded-For&quot;&gt;Wikipedia&lt;/a&gt;: &quot;The X-Forwarded-For (XFF) HTTP header field is a common method for identifying the originating IP address of a client connecting to a web server through an HTTP proxy or load balancer.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Load Balancers&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a load balancer?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A load balancer accepts (or denies) incoming network traffic from a client, and based on some criteria (application related, network, etc.) it distributes those communications out to servers (at least one). &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Why to used a load balancer?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Scalability - using a load balancer, you can possibly add more servers in the backend to handle more requests/traffic from the clients, as opposed to using one server.&lt;/li&gt; &lt;li&gt;Redundancy - if one server in the backend dies, the load balancer will keep forwarding the traffic/requests to the second server so users won&#39;t even notice one of the servers in the backend is down. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What load balancer techniques/algorithms are you familiar with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Round Robin&lt;/li&gt; &lt;li&gt;Weighted Round Robin&lt;/li&gt; &lt;li&gt;Least Connection&lt;/li&gt; &lt;li&gt;Weighted Least Connection&lt;/li&gt; &lt;li&gt;Resource Based&lt;/li&gt; &lt;li&gt;Fixed Weighting&lt;/li&gt; &lt;li&gt;Weighted Response Time&lt;/li&gt; &lt;li&gt;Source IP Hash&lt;/li&gt; &lt;li&gt;URL Hash &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are the drawbacks of round robin algorithm in load balancing?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;A simple round robin algorithm knows nothing about the load and the spec of each server it forwards the requests to. It is possible, that multiple heavy workloads requests will get to the same server while other servers will got only lightweight requests which will result in one server doing most of the work, maybe even crashing at some point because it unable to handle all the heavy workloads requests by its own.&lt;/li&gt; &lt;li&gt;Each request from the client creates a whole new session. This might be a problem for certain scenarios where you would like to perform multiple operations where the server has to know about the result of operation so basically, being sort of aware of the history it has with the client. In round robin, first request might hit server X, while second request might hit server Y and ask to continue processing the data that was processed on server X already. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is an Application Load Balancer?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;In which scenarios would you use ALB?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;At what layers a load balancer can operate?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;L4 and L7 &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you perform load balancing without using a dedicated load balancer instance?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Yes, you can use DNS for performing load balancing. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is DNS load balancing? What its advantages? When would you use it?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;Load Balancers - Sticky Sessions&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are sticky sessions? What are their pros and cons?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Recommended read:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://access.redhat.com/solutions/900933&quot;&gt;Red Hat Article&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Cons:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Can cause uneven load on instance (since requests routed to the same instances) Pros:&lt;/li&gt; &lt;li&gt;Ensures in-proc sessions are not lost when a new request is created &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Name one use case for using sticky sessions&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;You would like to make sure the user doesn&#39;t lose the current session data. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What sticky sessions use for enabling the &quot;stickiness&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Cookies. There are application based cookies and duration based cookies. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain application-based cookies&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Generated by the application and/or the load balancer&lt;/li&gt; &lt;li&gt;Usually allows to include custom data &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain duration-based cookies&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Generated by the load balancer&lt;/li&gt; &lt;li&gt;Session is not sticky anymore once the duration elapsed &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h4&gt;Load Balancers - Load Balancing Algorithms&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain each of the following load balancing techniques 
  &lt;ul&gt; 
   &lt;li&gt;Round Robin&lt;/li&gt; 
   &lt;li&gt;Weighted Round Robin&lt;/li&gt; 
   &lt;li&gt;Least Connection&lt;/li&gt; 
   &lt;li&gt;Weighted Least Connection&lt;/li&gt; 
   &lt;li&gt;Resource Based&lt;/li&gt; 
   &lt;li&gt;Fixed Weighting&lt;/li&gt; 
   &lt;li&gt;Weighted Response Time&lt;/li&gt; 
   &lt;li&gt;Source IP Hash&lt;/li&gt; 
   &lt;li&gt;URL Hash&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain use case for connection draining?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; To ensure that a Classic Load Balancer stops sending requests to instances that are de-registering or unhealthy, while keeping the existing connections open, use connection draining. This enables the load balancer to complete in-flight requests made to instances that are de-registering or unhealthy. &lt;p&gt;The maximum timeout value can be set between 1 and 3,600 seconds on both GCP and AWS.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Licenses&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Are you familiar with &quot;Creative Commons&quot;? What do you know about it?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;The Creative Commons license is a set of copyright licenses that allow creators to share their work with the public while retaining some control over how it can be used. The license was developed as a response to the restrictive standards of traditional copyright laws, which limited access of creative works. Its creators to choose the terms under which their works can be shared, distributed, and used by others. They&#39;re six main types of Creative Commons licenses, each with different levels of restrictions and permissions, the six licenses are:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Attribution (CC BY): Allows others to distribute, remix, and build upon the work, even commercially, as long as they credit the original creator.&lt;/li&gt; 
   &lt;li&gt;Attribution-ShareAlike (CC BY-SA): Allows others to remix and build upon the work, even commercially, as long as they credit the original creator and release any new creations under the same license.&lt;/li&gt; 
   &lt;li&gt;Attribution-NoDerivs (CC BY-ND): Allows others to distribute the work, even commercially, but they cannot remix or change it in any way and must credit the original creator.&lt;/li&gt; 
   &lt;li&gt;Attribution-NonCommercial (CC BY-NC): Allows others to remix and build upon the work, but they cannot use it commercially and must credit the original creator.&lt;/li&gt; 
   &lt;li&gt;Attribution-NonCommercial-ShareAlike (CC BY-NC-SA): Allows others to remix and build upon the work, but they cannot use it commercially, must credit the original creator, and must release any new creations under the same license.&lt;/li&gt; 
   &lt;li&gt;Attribution-NonCommercial-NoDerivs (CC BY-NC-ND): Allows others to download and share the work, but they cannot use it commercially, remix or change it in any way, and must credit the original creator.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Simply stated, the Creative Commons licenses are a way for creators to share their work with the public while retaining some control over how it can be used. The licenses promote creativity, innovation, and collaboration, while also respecting the rights of creators while still encouraging the responsible use of creative works.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;More information: &lt;a href=&quot;https://creativecommons.org/licenses/&quot;&gt;https://creativecommons.org/licenses/&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain the differences between copyleft and permissive licenses&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;In Copyleft, any derivative work must use the same licensing while in permissive licensing there are no such condition. GPL-3 is an example of copyleft license while BSD is an example of permissive license. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Random&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;How a search engine works?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How auto completion works?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is faster than RAM?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;CPU cache. &lt;a href=&quot;https://www.enterprisestorageforum.com/hardware/cache-memory/&quot;&gt;Source&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a memory leak?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A memory leak is a programming error that occurs when a program fails to release memory that is no longer needed, causing the program to consume increasing amounts of memory over time.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The leaks can lead to a variety of problems, including system crashes, performance degradation, and instability. Usually occurring after failed maintenance on older systems and compatibility with new components over time. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is your favorite protocol?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;SSH HTTP DHCP DNS ... &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Cache API?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the C10K problem? Is it relevant today?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://idiallo.com/blog/c10k-2016&quot;&gt;https://idiallo.com/blog/c10k-2016&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Storage&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What types of storage are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;File&lt;/li&gt; &lt;li&gt;Block&lt;/li&gt; &lt;li&gt;Object &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain Object Storage&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Data is divided to self-contained objects&lt;/li&gt; &lt;li&gt;Objects can contain metadata &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are the pros and cons of object storage?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Pros:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Usually with object storage, you pay for what you use as opposed to other storage types where you pay for the storage space you allocate&lt;/li&gt; &lt;li&gt;Scalable storage: Object storage mostly based on a model where what you use, is what you get and you can add storage as need Cons:&lt;/li&gt; &lt;li&gt;Usually performs slower than other types of storage&lt;/li&gt; &lt;li&gt;No granular modification: to change an object, you have re-create it &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are some use cases for using object storage?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain File Storage&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;File Storage used for storing data in files, in a hierarchical structure&lt;/li&gt; &lt;li&gt;Some of the devices for file storage: hard drive, flash drive, cloud-based file storage&lt;/li&gt; &lt;li&gt;Files usually organized in directories &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are the pros and cons of File Storage?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Pros:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Users have full control of their own files and can run variety of operations on the files: delete, read, write and move.&lt;/li&gt; &lt;li&gt;Security mechanism allows for users to have a better control at things such as file locking &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are some examples of file storage?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Local filesystem Dropbox Google Drive &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What types of storage devices are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain IOPS&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain storage throughput&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a filesystem?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A file system is a way for computers and other electronic devices to organize and store data files. It provides a structure that helps to organize data into files and directories, making it easier to find and manage information. A file system is crucial for providing a way to store and manage data in an organized manner.&lt;/p&gt; &lt;p&gt;Commonly used filed systems: Windows:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;NTFS&lt;/li&gt; 
   &lt;li&gt;exFAT&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Mac OS:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;HFS+ *APFS&lt;/li&gt; 
  &lt;/ul&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Dark Data&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain MBR&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;p&gt;&lt;a name=&quot;questions-you-ask&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Questions you CAN ask&lt;/h2&gt; 
&lt;p&gt;A list of questions you as a candidate can ask the interviewer during or after the interview. These are only a suggestion, use them carefully. Not every interviewer will be able to answer these (or happy to) which should be perhaps a red flag warning for your regarding working in such place but that&#39;s really up to you.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What do you like about working here?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How does the company promote personal growth?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the current level of technical debt you are dealing with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Be careful when asking this question - all companies, regardless of size, have some level of tech debt. Phrase the question in the light that all companies have the deal with this, but you want to see the current pain points they are dealing with &lt;br&gt;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;This is a great way to figure how managers deal with unplanned work, and how good they are at setting expectations with projects. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Why I should NOT join you? (or &#39;what you don&#39;t like about working here?&#39;)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What was your favorite project you&#39;ve worked on?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;This can give you insights in some of the cool projects a company is working on, and if you would enjoy working on projects like these. This is also a good way to see if the managers are allowing employees to learn and grow with projects outside of the normal work you&#39;d do. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;If you could change one thing about your day to day, what would it be?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Similar to the tech debt question, this helps you identify any pain points with the company. Additionally, it can be a great way to show how you&#39;d be an asset to the team.&lt;br&gt;&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;For Example, if they mention they have problem X, and you&#39;ve solved that in the past, you can show how you&#39;d be able to mitigate that problem. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Let&#39;s say that we agree and you hire me to this position, after X months, what do you expect that I have achieved?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Not only this will tell you what is expected from you, it will also provide big hint on the type of work you are going to do in the first months of your job. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Testing&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain white-box testing&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain black-box testing&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are unit tests?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Unit test are a software testing technique that involves systimatically breaking down a system and testing each individual part of the assembly. These tests are automated and can be run repeatedly to allow developers to catch edge case scenarios or bugs quickly while developing.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The main objective of unit tests are to verify each function is producing proper outputs given a set of inputs. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What types of tests would you run to test a web application?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain test harness?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is A/B testing?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is network simulation and how do you perform it?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What types of performances tests are you familiar with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain the following types of tests: 
  &lt;ul&gt; 
   &lt;li&gt;Load Testing&lt;/li&gt; 
   &lt;li&gt;Stress Testing&lt;/li&gt; 
   &lt;li&gt;Capacity Testing&lt;/li&gt; 
   &lt;li&gt;Volume Testing&lt;/li&gt; 
   &lt;li&gt;Endurance Testing&lt;/li&gt; 
  &lt;/ul&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h2&gt;Regex&lt;/h2&gt; 
&lt;p&gt;Given a text file, perform the following exercises&lt;/p&gt; 
&lt;h4&gt;Extract&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Extract all the numbers&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;&quot;\d+&quot; &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Extract the first word of each line&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt; &lt;p&gt;&quot;^\w+&quot; Bonus: extract the last word of each line&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&quot;\w+(?=\W*$)&quot; (in most cases, depends on line formatting) &lt;/p&gt;&lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt;   
&lt;details&gt; 
 &lt;summary&gt;Extract all the IP addresses&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;&quot;\b(?:\d{1,3}\ .){3}\d{1,3}\b&quot; IPV4:(This format looks for 1 to 3 digit sequence 3 times) &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Extract dates in the format of yyyy-mm-dd or yyyy-dd-mm&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Extract email addresses&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;&quot;\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\ .[A-Za-z]{2,}\b&quot; &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h4&gt;Replace&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Replace tabs with four spaces&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Replace &#39;red&#39; with &#39;green&#39;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h2&gt;System Design&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what a &quot;single point of failure&quot; is. &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; A &quot;single point of failure&quot;, in a system or organization, if it were to fail would cause the entire system to fail or significantly disrupt it&#39;s operation. In other words, it is a vulnerability where there is no backup in place to compensate for the failure. &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is CDN?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;CDN (Content Delivery Network) responsible for distributing content geographically. Part of it, is what is known as edge locations, aka cache proxies, that allows users to get their content quickly due to cache features and geographical distribution. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Multi-CDN&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;In single CDN, the whole content is originated from content delivery network.&lt;br&gt; In multi-CDN, content is distributed across multiple different CDNs, each might be on a completely different provider/cloud. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are the benefits of Multi-CDN over a single CDN?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Resiliency: Relying on one CDN means no redundancy. With multiple CDNs you don&#39;t need to worry about your CDN being down&lt;/li&gt; &lt;li&gt;Flexibility in Costs: Using one CDN enforces you to specific rates of that CDN. With multiple CDNs you can take into consideration using less expensive CDNs to deliver the content.&lt;/li&gt; &lt;li&gt;Performance: With Multi-CDN there is bigger potential in choosing better locations which more close to the client asking the content&lt;/li&gt; &lt;li&gt;Scale: With multiple CDNs, you can scale services to support more extreme conditions &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Explain &quot;3-Tier Architecture&quot; (including pros and cons)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; A &quot;3-Tier Architecture&quot; is a pattern used in software development for designing and structuring applications. It divides the application into 3 interconnected layers: Presentation, Business logic and Data storage. PROS: * Scalability * Security * Reusability CONS: * Complexity * Performance overhead * Cost and development time &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Mono-repo vs. Multi-repo.What are the cons and pros of each approach?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; In a Mono-repo, all the code for an organization is stored in a single,centralized repository. PROS (Mono-repo): * Unified tooling * Code Sharing CONS (Mono-repo): * Increased complexity * Slower cloning &lt;p&gt;In a Multi-repo setup, each component is stored in it&#39;s own separate repository. Each repository has it&#39;s own version control history. PROS (Multi-repo):&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Simpler to manage&lt;/li&gt; &lt;li&gt;Different teams and developers can work on different parts of the project independently, making parallel development easier. CONS (Multi-repo):&lt;/li&gt; &lt;li&gt;Code duplication&lt;/li&gt; &lt;li&gt;Integration challenges &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are the drawbacks of monolithic architecture?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Not suitable for frequent code changes and the ability to deploy new features&lt;/li&gt; &lt;li&gt;Not designed for today&#39;s infrastructure (like public clouds)&lt;/li&gt; &lt;li&gt;Scaling a team to work monolithic architecture is more challenging&lt;/li&gt; &lt;li&gt;If a single component in this architecture fails, then the entire application fails. &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What are the advantages of microservices architecture over a monolithic architecture?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Each of the services individually fail without escalating into an application-wide outage.&lt;/li&gt; &lt;li&gt;Each service can be developed and maintained by a separate team and this team can choose its own tools and coding language &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What&#39;s a service mesh?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; It is a layer that facilitates communication management and control between microservices in a containerized application. It handles tasks such as load balancing, encryption, and monitoring. &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain &quot;Loose Coupling&quot;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; In &quot;Loose Coupling&quot;, components of a system communicate with each other with a little understanding of each other&#39;s internal workings. This improves scalability and ease of modification in complex systems. &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a message queue? When is it used?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; It is a communication mechanism used in distributed systems to enable asynchronous communication between different components. It is generally used when the systems use a microservices approach. &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;Scalability&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Scalability&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The ability easily grow in size and capacity based on demand and usage. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Elasticity&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;The ability to grow but also to reduce based on what is required &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Disaster Recovery&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Disaster recovery is the process of restoring critical business systems and data after a disruptive event. The goal is to minimize the impact and resume normal business activities quickly. This involves creating a plan, testing it, backing up critical data, and storing it in safe locations. In case of a disaster, the plan is then executed, backups are restored, and systems are hopefully brought back online. The recovery process may take hours or days depending on the damages of infrastructure. This makes business planning important, as a well-designed and tested disaster recovery plan can minimize the impact of a disaster and keep operations going. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Fault Tolerance and High Availability&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;Fault Tolerance - The ability to self-heal and return to normal capacity. Also the ability to withstand a failure and remain functional.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;High Availability - Being able to access a resource (in some use cases, using different platforms) &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the difference between high availability and Disaster Recovery?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://www.wintellect.com/high-availability-vs-disaster-recovery&quot;&gt;wintellect.com&lt;/a&gt;: &quot;High availability, simply put, is eliminating single points of failure and disaster recovery is the process of getting a system back to an operational state when a system is rendered inoperative. In essence, disaster recovery picks up when high availability fails, so HA first.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Vertical Scaling&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Vertical Scaling is the process of adding resources to increase power of existing servers. For example, adding more CPUs, adding more RAM, etc. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are the disadvantages of Vertical Scaling?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;With vertical scaling alone, the component still remains a single point of failure. In addition, it has hardware limit where if you don&#39;t have more resources, you might not be able to scale vertically. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Which type of cloud services usually support vertical scaling?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Databases, cache. It&#39;s common mostly for non-distributed systems. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Horizontal Scaling&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Horizontal Scaling is the process of adding more resources that will be able handle requests as one unit &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the disadvantage of Horizontal Scaling? What is often required in order to perform Horizontal Scaling?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A load balancer. You can add more resources, but if you would like them to be part of the process, you have to serve them the requests/responses. Also, data inconsistency is a concern with horizontal scaling. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain in which use cases will you use vertical scaling and in which use cases you will use horizontal scaling&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Resiliency and what ways are there to make a system more resilient&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain &quot;Consistent Hashing&quot;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How would you update each of the services in the following drawing without having app (foo.com) downtime?&lt;br&gt; &lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/design/cdn-no-downtime.png&quot; width=&quot;300x;&quot; height=&quot;400px;&quot;&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the problem with the following architecture and how would you fix it?&lt;br&gt; &lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/design/producers_consumers_issue.png&quot; width=&quot;400x;&quot; height=&quot;300px;&quot;&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;The load on the producers or consumers may be high which will then cause them to hang or crash.&lt;br&gt; Instead of working in &quot;push mode&quot;, the consumers can pull tasks only when they are ready to handle them. It can be fixed by using a streaming platform like Kafka, Kinesis, etc. This platform will make sure to handle the high load/traffic and pass tasks/messages to consumers only when the ready to get them.&lt;/p&gt; &lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/design/producers_consumers_fix.png&quot; width=&quot;300x;&quot; height=&quot;200px;&quot;&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Users report that there is huge spike in process time when adding little bit more data to process as an input. What might be the problem?&lt;br&gt; &lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/design/input-process-output.png&quot; width=&quot;300x;&quot; height=&quot;200px;&quot;&gt; &lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How would you scale the architecture from the previous question to hundreds of users?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;Cache&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is &quot;cache&quot;? In which cases would you use it?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is &quot;distributed cache&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a &quot;cache replacement policy&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Take a look &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_replacement_policies&quot;&gt;here&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Which cache replacement policies are you familiar with?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;You can find a list &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_replacement_policies&quot;&gt;here&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain the following cache policies: 
  &lt;ul&gt; 
   &lt;li&gt;FIFO&lt;/li&gt; 
   &lt;li&gt;LIFO&lt;/li&gt; 
   &lt;li&gt;LRU&lt;/li&gt;
  &lt;/ul&gt;&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt;  &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Read about it &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_replacement_policies&quot;&gt;here&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Why not writing everything to cache instead of a database/datastore?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; Caching and databases serve different purposes and are optimized for different use cases. &lt;p&gt;Caching is used to speed up read operations by storing frequently accessed data in memory or on a fast storage medium. By keeping data close to the application, caching reduces the latency and overhead of accessing data from a slower, more distant storage system such as a database or disk.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;On the other hand, databases are optimized for storing and managing persistent data. Databases are designed to handle concurrent read and write operations, enforce consistency and integrity constraints, and provide features such as indexing and querying. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Migrations&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;How you prepare for a migration? (or plan a migration)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;You can mention:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;roll-back &amp;amp; roll-forward cut over dress rehearsals DNS redirection &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain &quot;Branch by Abstraction&quot; technique&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;Design a system&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you design a video streaming website?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you design a photo upload website?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How would you build a URL shortener?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;More System Design Questions&lt;/h4&gt; 
&lt;p&gt;Additional exercises can be found in &lt;a href=&quot;https://github.com/bregman-arie/system-design-notebook&quot;&gt;system-design-notebook repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bregman-arie/system-design-notebook&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/system_design_notebook.png&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Hardware&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a CPU?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A central processing unit (CPU) performs basic arithmetic, logic, controlling, and input/output (I/O) operations specified by the instructions in the program. This contrasts with external components such as main memory and I/O circuitry, and specialized processors such as graphics processing units (GPUs). &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is RAM?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;RAM (Random Access Memory) is the hardware in a computing device where the operating system (OS), application programs and data in current use are kept so they can be quickly reached by the device&#39;s processor. RAM is the main memory in a computer. It is much faster to read from and write to than other kinds of storage, such as a hard disk drive (HDD), solid-state drive (SSD) or optical drive. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a GPU?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; A GPU, or Graphics Processing Unit, is a specialized electronic circuit designed to expedite image and video processing for display on a computer screen. &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is an embedded system?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;An embedded system is a computer system - a combination of a computer processor, computer memory, and input/output peripheral devices—that has a dedicated function within a larger mechanical or electronic system. It is embedded as part of a complete device often including electrical or electronic hardware and mechanical parts. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you give an example of an embedded system?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A common example of an embedded system is a microwave oven&#39;s digital control panel, which is managed by a microcontroller.&lt;/p&gt; &lt;p&gt;When committed to a certain goal, Raspberry Pi can serve as an embedded system.&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What types of storage are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;There are several types of storage, including hard disk drives (HDDs), solid-state drives (SSDs), and optical drives (CD/DVD/Blu-ray). Other types of storage include USB flash drives, memory cards, and network-attached storage (NAS). &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are some considerations DevOps teams should keep in mind when selecting hardware for their job?&lt;/summary&gt;
 &lt;br&gt; 
 &lt;p&gt;Choosing the right DevOps hardware is essential for ensuring streamlined CI/CD pipelines, timely feedback loops, and consistent service availability. Here&#39;s a distilled guide on what DevOps teams should consider:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Understanding Workloads&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: Consider the need for multi-core or high-frequency CPUs based on your tasks.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: Enough memory is vital for activities like large-scale coding or intensive automation.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Evaluate storage speed and capacity. SSDs might be preferable for swift operations.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expandability&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Horizontal Growth&lt;/strong&gt;: Check if you can boost capacity by adding more devices.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Vertical Growth&lt;/strong&gt;: Determine if upgrades (like RAM, CPU) to individual machines are feasible.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Connectivity Considerations&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Data Transfer&lt;/strong&gt;: Ensure high-speed network connections for activities like code retrieval and data transfers.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: Aim for low-latency networks, particularly important for distributed tasks.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Backup Routes&lt;/strong&gt;: Think about having backup network routes to avoid downtimes.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Consistent Uptime&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Plan for hardware backups like RAID configurations, backup power sources, or alternate network connections to ensure continuous service.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;System Compatibility&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Make sure your hardware aligns with your software, operating system, and intended platforms.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Power Efficiency&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Hardware that uses energy efficiently can reduce costs in long-term, especially in large setups.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Safety Measures&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Explore hardware-level security features, such as TPM, to enhance protection.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Overseeing &amp;amp; Control&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Tools like ILOM can be beneficial for remote handling.&lt;/li&gt; 
    &lt;li&gt;Make sure the hardware can be seamlessly monitored for health and performance.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Budgeting&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Consider both initial expenses and long-term costs when budgeting.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Support &amp;amp; Community&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Choose hardware from reputable vendors known for reliable support.&lt;/li&gt; 
    &lt;li&gt;Check for available drivers, updates, and community discussions around the hardware.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Planning Ahead&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Opt for hardware that can cater to both present and upcoming requirements.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Operational Environment&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Temperature Control&lt;/strong&gt;: Ensure cooling systems to manage heat from high-performance units.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Space Management&lt;/strong&gt;: Assess hardware size considering available rack space.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Reliable Power&lt;/strong&gt;: Factor in consistent and backup power sources.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cloud Coordination&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;If you&#39;re leaning towards a hybrid cloud setup, focus on how local hardware will mesh with cloud resources.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Life Span of Hardware&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Be aware of the hardware&#39;s expected duration and when you might need replacements or upgrades.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Optimized for Virtualization&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;If utilizing virtual machines or containers, ensure the hardware is compatible and optimized for such workloads.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Adaptability&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Modular hardware allows individual component replacements, offering more flexibility.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Avoiding Single Vendor Dependency&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Try to prevent reliance on a single vendor unless there are clear advantages.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Eco-Friendly Choices&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Prioritize sustainably produced hardware that&#39;s energy-efficient and environmentally responsible.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;In essence, DevOps teams should choose hardware that is compatible with their tasks, versatile, gives good performance, and stays within their budget. Furthermore, long-term considerations such as maintenance, potential upgrades, and compatibility with impending technological shifts must be prioritized.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the role of hardware in disaster recovery planning and implementation?&lt;/summary&gt;
 &lt;br&gt; 
 &lt;p&gt;Hardware is critical in disaster recovery (DR) solutions. While the broader scope of DR includes things like standard procedures, norms, and human roles, it&#39;s the hardware that keeps business processes running smoothly. Here&#39;s an outline of how hardware works with DR:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Storing Data and Ensuring Its Duplication&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Backup Equipment&lt;/strong&gt;: Devices like tape storage, backup servers, and external HDDs keep essential data stored safely at a different location.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Disk Arrays&lt;/strong&gt;: Systems such as RAID offer a safety net. If one disk crashes, the others compensate.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alternate Systems for Recovery&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Backup Servers&lt;/strong&gt;: These step in when the main servers falter, maintaining service flow.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Traffic Distributors&lt;/strong&gt;: Devices like load balancers share traffic across servers. If a server crashes, they reroute users to operational ones.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alternate Operation Hubs&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Ready-to-use Centers&lt;/strong&gt;: Locations equipped and primed to take charge immediately when the main center fails.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Basic Facilities&lt;/strong&gt;: Locations with necessary equipment but lacking recent data, taking longer to activate.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Semi-prepped Facilities&lt;/strong&gt;: Locations somewhat prepared with select systems and data, taking a moderate duration to activate.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Power Backup Mechanisms&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Instant Power Backup&lt;/strong&gt;: Devices like UPS offer power during brief outages, ensuring no abrupt shutdowns.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Long-term Power Solutions&lt;/strong&gt;: Generators keep vital systems operational during extended power losses.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Networking Equipment&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Backup Internet Connections&lt;/strong&gt;: Having alternatives ensures connectivity even if one provider faces issues.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Secure Connection Tools&lt;/strong&gt;: Devices ensuring safe remote access, especially crucial during DR situations.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;On-site Physical Setup&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Organized Housing&lt;/strong&gt;: Structures like racks to neatly store and manage hardware.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Emergency Temperature Control&lt;/strong&gt;: Backup cooling mechanisms to counter server overheating in HVAC malfunctions.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alternate Communication Channels&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Orbit-based Phones&lt;/strong&gt;: Handy when regular communication methods falter.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Direct Communication Devices&lt;/strong&gt;: Devices like radios useful when primary systems are down.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Protection Mechanisms&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Electronic Barriers &amp;amp; Alert Systems&lt;/strong&gt;: Devices like firewalls and intrusion detection keep DR systems safeguarded.&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Physical Entry Control&lt;/strong&gt;: Systems controlling entry and monitoring, ensuring only cleared personnel have access.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Uniformity and Compatibility in Hardware&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;It&#39;s simpler to manage and replace equipment in emergencies if hardware configurations are consistent and compatible.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Equipment for Trials and Upkeep&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;DR drills might use specific equipment to ensure the primary systems remain unaffected. This verifies the equipment&#39;s readiness and capacity to manage real crises.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;In summary, while software and human interventions are important in disaster recovery operations, it is the hardware that provides the underlying support. It is critical for efficient disaster recovery plans to keep this hardware resilient, duplicated, and routinely assessed.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a RAID?&lt;/summary&gt;
 &lt;br&gt; 
 &lt;b&gt; RAID is an acronym that stands for &quot;Redundant Array of Independent Disks.&quot; It is a technique that combines numerous hard drives into a single device known as an array in order to improve performance, expand storage capacity, and/or offer redundancy to prevent data loss. RAID levels (for example, RAID 0, RAID 1, and RAID 5) provide varied benefits in terms of performance, redundancy, and storage efficiency. &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a microcontroller?&lt;/summary&gt;
 &lt;br&gt; 
 &lt;b&gt; A microcontroller is a small integrated circuit that controls certain tasks in an embedded system. It typically includes a CPU, memory, and input/output peripherals. &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a Network Interface Controller or NIC?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; A Network Interface Controller (NIC) is a piece of hardware that connects a computer to a network and allows it to communicate with other devices. &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a DMA?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Direct memory access (DMA) is a feature of computer systems that allows certain hardware subsystems to access main system memory independently of the central processing unit (CPU).DMA enables devices to share and receive data from the main memory in a computer. It does this while still allowing the CPU to perform other tasks. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a Real-Time Operating Systems?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A real-time operating system (RTOS) is an operating system (OS) for real-time computing applications that processes data and events that have critically defined time constraints. An RTOS is distinct from a time-sharing operating system, such as Unix, which manages the sharing of system resources with a scheduler, data buffers, or fixed task prioritization in a multitasking or multiprogramming environment. Processing time requirements need to be fully understood and bound rather than just kept as a minimum. All processing must occur within the defined constraints. Real-time operating systems are event-driven and preemptive, meaning the OS can monitor the relevant priority of competing tasks, and make changes to the task priority. Event-driven systems switch between tasks based on their priorities, while time-sharing systems switch the task based on clock interrupts. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;List of interrupt types&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;There are six classes of interrupts possible:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;External&lt;/li&gt; &lt;li&gt;Machine check&lt;/li&gt; &lt;li&gt;I/O&lt;/li&gt; &lt;li&gt;Program&lt;/li&gt; &lt;li&gt;Restart&lt;/li&gt; &lt;li&gt;Supervisor call (SVC) &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h2&gt;Big Data&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is exactly Big Data&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;As defined by Doug Laney:&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Volume: Extremely large volumes of data&lt;/li&gt; &lt;li&gt;Velocity: Real time, batch, streams of data&lt;/li&gt; &lt;li&gt;Variety: Various forms of data, structured, semi-structured and unstructured&lt;/li&gt; &lt;li&gt;Veracity or Variability: Inconsistent, sometimes inaccurate, varying data &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is DataOps? How is it related to DevOps?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;DataOps seeks to reduce the end-to-end cycle time of data analytics, from the origin of ideas to the literal creation of charts, graphs and models that create value. DataOps combines Agile development, DevOps and statistical process controls and applies them to data analytics. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Data Architecture?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;An answer from &lt;a href=&quot;https://www.talend.com/resources/what-is-data-architecture&quot;&gt;talend.com&lt;/a&gt;:&lt;/p&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&quot;Data architecture is the process of standardizing how organizations collect, store, transform, distribute, and use data. The goal is to deliver relevant data to people who need it, when they need it, and help them make sense of it.&quot; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain the different formats of data&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Structured - data that has defined format and length (e.g. numbers, words)&lt;/li&gt; &lt;li&gt;Semi-structured - Doesn&#39;t conform to a specific format but is self-describing (e.g. XML, SWIFT)&lt;/li&gt; &lt;li&gt;Unstructured - does not follow a specific format (e.g. images, test messages) &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is a Data Warehouse?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Data_warehouse&quot;&gt;Wikipedia&#39;s explanation on Data Warehouse&lt;/a&gt; &lt;a href=&quot;https://aws.amazon.com/data-warehouse&quot;&gt;Amazon&#39;s explanation on Data Warehouse&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Data Lake?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Data_lake&quot;&gt;Data Lake - Wikipedia&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can you explain the difference between a data lake and a data warehouse?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is &quot;Data Versioning&quot;? What models of &quot;Data Versioning&quot; are there?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is ETL?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h4&gt;Apache Hadoop&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is Hadoop&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Apache_Hadoop&quot;&gt;Apache Hadoop - Wikipedia&lt;/a&gt; &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Hadoop YARN&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;Responsible for managing the compute resources in clusters and scheduling users&#39; applications &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Hadoop MapReduce&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;A programming model for large-scale data processing &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Hadoop Distributed File Systems (HDFS)&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Distributed file system providing high aggregate bandwidth across the cluster.&lt;/li&gt; &lt;li&gt;For a user it looks like a regular file system structure but behind the scenes it&#39;s distributed across multiple machines in a cluster&lt;/li&gt; &lt;li&gt;Typical file size is TB and it can scale and supports millions of files&lt;/li&gt; &lt;li&gt;It&#39;s fault tolerant which means it provides automatic recovery from faults&lt;/li&gt; &lt;li&gt;It&#39;s best suited for running long batch operations rather than live analysis &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What do you know about HDFS architecture?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;&lt;a href=&quot;http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html&quot;&gt;HDFS Architecture&lt;/a&gt;&lt;/p&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Master-slave architecture&lt;/li&gt; &lt;li&gt;Namenode - master, Datanodes - slaves&lt;/li&gt; &lt;li&gt;Files split into blocks&lt;/li&gt; &lt;li&gt;Blocks stored on datanodes&lt;/li&gt; &lt;li&gt;Namenode controls all metadata &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;h2&gt;Ceph&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain what is Ceph&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; Ceph is an Open-Source Distributed Storage System designed to provide excellent performance, reliability, and scalability. It&#39;s often used in cloud computing environments and Data Centers. &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;True or False? Ceph favor consistency and correctness over performances&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; True &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Which services or types of storage Ceph supports?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Object (RGW)&lt;/li&gt; &lt;li&gt;Block (RBD)&lt;/li&gt; &lt;li&gt;File (CephFS) &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;What is RADOS?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Reliable Autonomic Distributed Object Storage&lt;/li&gt; &lt;li&gt;Provides low-level data object storage service&lt;/li&gt; &lt;li&gt;Strong Consistency&lt;/li&gt; &lt;li&gt;Simplifies design and implementation of higher layers (block, file, object) &lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;  
&lt;details&gt; 
 &lt;summary&gt;Describe RADOS software components&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;ul&gt;
  &lt;b&gt; &lt;li&gt;Monitor 
    &lt;ul&gt; 
     &lt;li&gt;Central authority for authentication, data placement, policy&lt;/li&gt; 
     &lt;li&gt;Coordination point for all other cluster components&lt;/li&gt; 
     &lt;li&gt;Protect critical cluster state with Paxos&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Manager 
    &lt;ul&gt; 
     &lt;li&gt;Aggregates real-time metrics (throughput, disk usage, etc.)&lt;/li&gt; 
     &lt;li&gt;Host for pluggable management functions&lt;/li&gt; 
     &lt;li&gt;1 active, 1+ standby per cluster&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;OSD (Object Storage Daemon) 
    &lt;ul&gt; 
     &lt;li&gt;Stores data on an HDD or SSD&lt;/li&gt; 
     &lt;li&gt;Services client IO requests &lt;/li&gt;
    &lt;/ul&gt;&lt;/li&gt;&lt;/b&gt;
 &lt;/ul&gt;
&lt;/details&gt;    
&lt;details&gt; 
 &lt;summary&gt;What is the workflow of retrieving data from Ceph?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; The work flow is as follows: 
  &lt;ol&gt; 
   &lt;li&gt;The client sends a request to the ceph cluster to retrieve data:&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;strong&gt;Client could be any of the following&lt;/strong&gt;&lt;/p&gt; 
   &lt;blockquote&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Ceph Block Device&lt;/li&gt; 
     &lt;li&gt;Ceph Object Gateway&lt;/li&gt; 
     &lt;li&gt;Any third party ceph client&lt;/li&gt; 
    &lt;/ul&gt; 
   &lt;/blockquote&gt; 
  &lt;/blockquote&gt; 
  &lt;ol start=&quot;2&quot;&gt; 
   &lt;li&gt;The client retrieves the latest cluster map from the Ceph Monitor&lt;/li&gt; 
   &lt;li&gt;The client uses the CRUSH algorithm to map the object to a placement group. The placement group is then assigned to a OSD.&lt;/li&gt; 
   &lt;li&gt;Once the placement group and the OSD Daemon are determined, the client can retrieve the data from the appropriate OSD&lt;/li&gt; 
  &lt;/ol&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is the workflow of writing data to Ceph?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; The work flow is as follows: 
  &lt;ol&gt; 
   &lt;li&gt;The client sends a request to the ceph cluster to retrieve data&lt;/li&gt; 
   &lt;li&gt;The client retrieves the latest cluster map from the Ceph Monitor&lt;/li&gt; 
   &lt;li&gt;The client uses the CRUSH algorithm to map the object to a placement group. The placement group is then assigned to a Ceph OSD Daemon dynamically.&lt;/li&gt; 
   &lt;li&gt;The client sends the data to the primary OSD of the determined placement group. If the data is stored in an erasure-coded pool, the primary OSD is responsible for encoding the object into data chunks and coding chunks, and distributing them to the other OSDs.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;What are &quot;Placement Groups&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Describe in the detail the following: Objects -&amp;gt; Pool -&amp;gt; Placement Groups -&amp;gt; OSDs&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is OMAP?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is a metadata server? How it works?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
&lt;/details&gt; 
&lt;h2&gt;Packer&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;What is Packer? What is it used for?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;In general, Packer automates machine images creation. It allows you to focus on configuration prior to deployment while making the images. This allows you start the instances much faster in most cases. &lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Packer follows a &quot;configuration-&amp;gt;deployment&quot; model or &quot;deployment-&amp;gt;configuration&quot;?&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;A configuration-&amp;gt;deployment which has some advantages like:&lt;/p&gt; &lt;/b&gt;
 &lt;ol&gt;
  &lt;b&gt; &lt;li&gt;Deployment Speed - you configure once prior to deployment instead of configuring every time you deploy. This allows you to start instances/services much quicker.&lt;/li&gt; &lt;li&gt;More immutable infrastructure - with configuration-&amp;gt;deployment it&#39;s not likely to have very different deployments since most of the configuration is done prior to the deployment. Issues like dependencies errors are handled/discovered prior to deployment in this model. &lt;/li&gt;&lt;/b&gt;
 &lt;/ol&gt;
&lt;/details&gt;  
&lt;h2&gt;Release&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Explain Semantic Versioning&lt;/summary&gt;
 &lt;br&gt;
 &lt;b&gt; &lt;p&gt;&lt;a href=&quot;https://semver.org/&quot;&gt;This&lt;/a&gt; page explains it perfectly:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Given a version number MAJOR.MINOR.PATCH, increment the:

MAJOR version when you make incompatible API changes
MINOR version when you add functionality in a backwards compatible manner
PATCH version when you make backwards compatible bug fixes
Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format.
&lt;/code&gt;&lt;/pre&gt; &lt;/b&gt;
 &lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;/details&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Certificates&lt;/h2&gt; 
&lt;p&gt;If you are looking for a way to prepare for a certain exam this is the section for you. Here you&#39;ll find a list of certificates, each references to a separate file with focused questions that will help you to prepare to the exam. Good luck :)&lt;/p&gt; 
&lt;h4&gt;AWS&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/certificates/aws-cloud-practitioner.md&quot;&gt;Cloud Practitioner&lt;/a&gt; (Latest update: 2020)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/certificates/aws-solutions-architect-associate.md&quot;&gt;Solutions Architect Associate&lt;/a&gt; (Latest update: 2021)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/certificates/aws-cloud-sysops-associate.md&quot;&gt;Cloud SysOps Administration Associate&lt;/a&gt; (Latest update: Oct 2022)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Azure&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/certificates/azure-fundamentals-az-900.md&quot;&gt;AZ-900&lt;/a&gt; (Latest update: 2021)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Kubernetes&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/topics/kubernetes/CKA.md&quot;&gt;Certified Kubernetes Administrator (CKA)&lt;/a&gt; (Latest update: 2022)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Additional DevOps and SRE Projects&lt;/h2&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bregman-arie/sre-checklist&quot;&gt;&lt;img width=&quot;500px&quot; src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/sre_checklist.png&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bregman-arie/howtheydevops&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/how_they_devops.png&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bregman-arie/devops-resources&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/devops_resources.png&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/bregman-arie/infraverse&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/images/infraverse.png&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;p&gt;Thanks to all of our amazing &lt;a href=&quot;https://github.com/bregman-arie/devops-exercises/graphs/contributors&quot;&gt;contributors&lt;/a&gt; who make it easy for everyone to learn new things :)&lt;/p&gt; 
&lt;p&gt;Logos credits can be found &lt;a href=&quot;https://raw.githubusercontent.com/bregman-arie/devops-exercises/master/credits.md&quot;&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://creativecommons.org/licenses/by-nc-nd/3.0/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-CC%20BY--NC--ND%203.0-lightgrey.svg?sanitize=true&quot; alt=&quot;License: CC BY-NC-ND 3.0&quot;&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>geekan/MetaGPT</title>
      <link>https://github.com/geekan/MetaGPT</link>
      <description>&lt;p&gt;🌟 The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MetaGPT: The Multi-Agent Framework&lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/resources/MetaGPT-new-log.png&quot; alt=&quot;MetaGPT logo: Enable GPT to work in a software company, collaborating to tackle more complex tasks.&quot; width=&quot;150px&quot;&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; [ &lt;b&gt;En&lt;/b&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/README_CN.md&quot;&gt;中&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/README_FR.md&quot;&gt;Fr&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/README_JA.md&quot;&gt;日&lt;/a&gt; ] &lt;b&gt;Assign different roles to GPTs to form a collaborative entity for complex tasks.&lt;/b&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true&quot; alt=&quot;License: MIT&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/DYn29wFk9z&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/DYn29wFk9z?style=flat&quot; alt=&quot;Discord Follow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/MetaGPT_&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/MetaGPT?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h4 align=&quot;center&quot;&gt; &lt;/h4&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;p&gt;🚀 Mar. 10, 2025: 🎉 &lt;a href=&quot;https://mgx.dev/&quot;&gt;mgx.dev&lt;/a&gt; is the #1 Product of the Week on @ProductHunt! 🏆&lt;/p&gt; 
&lt;p&gt;🚀 Mar. &amp;nbsp; 4, 2025: 🎉 &lt;a href=&quot;https://mgx.dev/&quot;&gt;mgx.dev&lt;/a&gt; is the #1 Product of the Day on @ProductHunt! 🏆&lt;/p&gt; 
&lt;p&gt;🚀 Feb. 19, 2025: Today we are officially launching our natural language programming product: &lt;a href=&quot;https://mgx.dev/&quot;&gt;MGX (MetaGPT X)&lt;/a&gt; - the world&#39;s first AI agent development team. More details on &lt;a href=&quot;https://x.com/MetaGPT_/status/1892199535130329356&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;🚀 Feb. 17, 2025: We introduced two papers: &lt;a href=&quot;https://arxiv.org/pdf/2502.06855&quot;&gt;SPO&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/pdf/2502.12018&quot;&gt;AOT&lt;/a&gt;, check the &lt;a href=&quot;https://raw.githubusercontent.com/geekan/MetaGPT/main/examples&quot;&gt;code&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;🚀 Jan. 22, 2025: Our paper &lt;a href=&quot;https://openreview.net/forum?id=z5uVAKwmjf&quot;&gt;AFlow: Automating Agentic Workflow Generation&lt;/a&gt; accepted for &lt;strong&gt;oral presentation (top 1.8%)&lt;/strong&gt; at ICLR 2025, &lt;strong&gt;ranking #2&lt;/strong&gt; in the LLM-based Agent category.&lt;/p&gt; 
&lt;p&gt;👉👉 &lt;a href=&quot;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/NEWS.md&quot;&gt;Earlier news&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Software Company as Multi-Agent System&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;MetaGPT takes a &lt;strong&gt;one line requirement&lt;/strong&gt; as input and outputs &lt;strong&gt;user stories / competitive analysis / requirements / data structures / APIs / documents, etc.&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Internally, MetaGPT includes &lt;strong&gt;product managers / architects / project managers / engineers.&lt;/strong&gt; It provides the entire process of a &lt;strong&gt;software company along with carefully orchestrated SOPs.&lt;/strong&gt; 
  &lt;ol&gt; 
   &lt;li&gt;&lt;code&gt;Code = SOP(Team)&lt;/code&gt; is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/resources/software_company_cd.jpeg&quot; alt=&quot;A software company consists of LLM-based roles&quot;&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;Software Company Multi-Agent Schematic (Gradually Implementing)&lt;/p&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Ensure that Python 3.9 or later, but less than 3.12, is installed on your system. You can check this by using: &lt;code&gt;python --version&lt;/code&gt;.&lt;br&gt; You can use conda like this: &lt;code&gt;conda create -n metagpt python=3.9 &amp;amp;&amp;amp; conda activate metagpt&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install --upgrade metagpt
# or `pip install --upgrade git+https://github.com/geekan/MetaGPT.git`
# or `git clone https://github.com/geekan/MetaGPT &amp;amp;&amp;amp; cd MetaGPT &amp;amp;&amp;amp; pip install --upgrade -e .`
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Install &lt;a href=&quot;https://nodejs.org/en/download&quot;&gt;node&lt;/a&gt; and &lt;a href=&quot;https://pnpm.io/installation#using-npm&quot;&gt;pnpm&lt;/a&gt; before actual use.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;For detailed installation guidance, please refer to &lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-stable-version&quot;&gt;cli_install&lt;/a&gt; or &lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-with-docker&quot;&gt;docker_install&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;p&gt;You can init the config of MetaGPT by running the following command, or manually create &lt;code&gt;~/.metagpt/config2.yaml&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Check https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html for more details
metagpt --init-config  # it will create ~/.metagpt/config2.yaml, just modify it to your needs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can configure &lt;code&gt;~/.metagpt/config2.yaml&lt;/code&gt; according to the &lt;a href=&quot;https://github.com/geekan/MetaGPT/raw/main/config/config2.example.yaml&quot;&gt;example&lt;/a&gt; and &lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html&quot;&gt;doc&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;llm:
  api_type: &quot;openai&quot;  # or azure / ollama / groq etc. Check LLMType for more options
  model: &quot;gpt-4-turbo&quot;  # or gpt-3.5-turbo
  base_url: &quot;https://api.openai.com/v1&quot;  # or forward url / other llm url
  api_key: &quot;YOUR_API_KEY&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage&lt;/h3&gt; 
&lt;p&gt;After installation, you can use MetaGPT at CLI&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;metagpt &quot;Create a 2048 game&quot;  # this will create a repo in ./workspace
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or use it as library&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from metagpt.software_company import generate_repo
from metagpt.utils.project_repo import ProjectRepo

repo: ProjectRepo = generate_repo(&quot;Create a 2048 game&quot;)  # or ProjectRepo(&quot;&amp;lt;path&amp;gt;&quot;)
print(repo)  # it will print the repo structure with files
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also use &lt;a href=&quot;https://github.com/geekan/MetaGPT/tree/main/examples/di&quot;&gt;Data Interpreter&lt;/a&gt; to write code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import asyncio
from metagpt.roles.di.data_interpreter import DataInterpreter

async def main():
    di = DataInterpreter()
    await di.run(&quot;Run data analysis on sklearn Iris dataset, include a plot&quot;)

asyncio.run(main())  # or await main() in a jupyter notebook setting
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;QuickStart &amp;amp; Demo Video&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try it on &lt;a href=&quot;https://huggingface.co/spaces/deepwisdom/MetaGPT-SoftwareCompany&quot;&gt;MetaGPT Huggingface Space&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://youtu.be/uT75J_KG_aY&quot;&gt;Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!!&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d&quot;&gt;Official Demo Video&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/888cb169-78c3-4a42-9d62-9d90ed3928c9&quot;&gt;https://github.com/user-attachments/assets/888cb169-78c3-4a42-9d62-9d90ed3928c9&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Tutorial&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;🗒 &lt;a href=&quot;https://docs.deepwisdom.ai/main/en/&quot;&gt;Online Document&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;💻 &lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/get_started/quickstart.html&quot;&gt;Usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🔎 &lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/get_started/introduction.html&quot;&gt;What can MetaGPT do?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🛠 How to build your own agents? 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/tutorials/agent_101.html&quot;&gt;MetaGPT Usage &amp;amp; Development Guide | Agent 101&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/tutorials/multi_agent_101.html&quot;&gt;MetaGPT Usage &amp;amp; Development Guide | MultiAgent 101&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;🧑‍💻 Contribution 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/ROADMAP.md&quot;&gt;Develop Roadmap&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;🔖 Use Cases 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/interpreter/intro.html&quot;&gt;Data Interpreter&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/use_cases/multi_agent/debate.html&quot;&gt;Debate&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/researcher.html&quot;&gt;Researcher&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/receipt_assistant.html&quot;&gt;Receipt Assistant&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;❓ &lt;a href=&quot;https://docs.deepwisdom.ai/main/en/guide/faq.html&quot;&gt;FAQs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;h3&gt;Discord Join US&lt;/h3&gt; 
&lt;p&gt;📢 Join Our &lt;a href=&quot;https://discord.gg/ZRHeExS6xv&quot;&gt;Discord Channel&lt;/a&gt;! Looking forward to seeing you there! 🎉&lt;/p&gt; 
&lt;h3&gt;Contributor form&lt;/h3&gt; 
&lt;p&gt;📝 &lt;a href=&quot;https://airtable.com/appInfdG0eJ9J4NNL/pagK3Fh1sGclBvVkV/form&quot;&gt;Fill out the form&lt;/a&gt; to become a contributor. We are looking forward to your participation!&lt;/p&gt; 
&lt;h3&gt;Contact Information&lt;/h3&gt; 
&lt;p&gt;If you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Email:&lt;/strong&gt; &lt;a href=&quot;mailto:alexanderwu@deepwisdom.ai&quot;&gt;alexanderwu@deepwisdom.ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Issues:&lt;/strong&gt; For more technical inquiries, you can also create a new issue in our &lt;a href=&quot;https://github.com/geekan/metagpt/issues&quot;&gt;GitHub repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We will respond to all questions within 2-3 business days.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;To stay updated with the latest research and development, follow &lt;a href=&quot;https://twitter.com/MetaGPT_&quot;&gt;@MetaGPT_&lt;/a&gt; on Twitter.&lt;/p&gt; 
&lt;p&gt;To cite &lt;a href=&quot;https://openreview.net/forum?id=VtmBAGCN7o&quot;&gt;MetaGPT&lt;/a&gt; in publications, please use the following BibTeX entries.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inproceedings{hong2024metagpt,
      title={Meta{GPT}: Meta Programming for A Multi-Agent Collaborative Framework},
      author={Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J{\&quot;u}rgen Schmidhuber},
      booktitle={The Twelfth International Conference on Learning Representations},
      year={2024},
      url={https://openreview.net/forum?id=VtmBAGCN7o}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more work, please refer to &lt;a href=&quot;https://raw.githubusercontent.com/geekan/MetaGPT/main/docs/ACADEMIC_WORK.md&quot;&gt;Academic Work&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hiyouga/LLaMA-Factory</title>
      <link>https://github.com/hiyouga/LLaMA-Factory</link>
      <description>&lt;p&gt;Unified Efficient Fine-Tuning of 100+ LLMs &amp; VLMs (ACL 2024)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/logo.png&quot; alt=&quot;# LLaMA Factory&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/hiyouga/LLaMA-Factory/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social&quot; alt=&quot;GitHub Repo stars&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/hiyouga/LLaMA-Factory/commits/main&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory&quot; alt=&quot;GitHub last commit&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/hiyouga/LLaMA-Factory/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange&quot; alt=&quot;GitHub contributors&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml&quot;&gt;&lt;img src=&quot;https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub workflow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/llamafactory/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/llamafactory&quot; alt=&quot;PyPI&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://scholar.google.com/scholar?cites=12620864006390196564&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/citation-349-green&quot; alt=&quot;Citation&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/hiyouga/LLaMA-Factory/pulls&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-blue&quot; alt=&quot;GitHub pull request&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://twitter.com/llamafactory_ai&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/llamafactory_ai&quot; alt=&quot;Twitter&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/rKfvV9r9FK&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&amp;amp;style=flat&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://gitcode.com/zhengyaowei/LLaMA-Factory&quot;&gt;&lt;img src=&quot;https://gitcode.com/zhengyaowei/LLaMA-Factory/star/badge.svg?sanitize=true&quot; alt=&quot;GitCode&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open in Colab&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory&quot;&gt;&lt;img src=&quot;https://gallery.pai-ml.com/assets/open-in-dsw.svg?sanitize=true&quot; alt=&quot;Open in DSW&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://huggingface.co/spaces/hiyouga/LLaMA-Board&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue&quot; alt=&quot;Spaces&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://modelscope.cn/studios/hiyouga/LLaMA-Board&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue&quot; alt=&quot;Studios&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/SageMaker-Open%20in%20AWS-blue&quot; alt=&quot;SageMaker&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3 align=&quot;center&quot;&gt; Easily fine-tune 100+ large language models with zero-code &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#quickstart&quot;&gt;CLI&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#fine-tuning-with-llama-board-gui-powered-by-gradio&quot;&gt;Web UI&lt;/a&gt; &lt;/h3&gt; 
&lt;p align=&quot;center&quot;&gt; 
 &lt;picture&gt; 
  &lt;img alt=&quot;Github trend&quot; src=&quot;https://trendshift.io/api/badge/repositories/4535&quot;&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;p&gt;👋 Join our &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/wechat.jpg&quot;&gt;WeChat&lt;/a&gt; or &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/wechat_npu.jpg&quot;&gt;NPU user group&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;[ English | &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/README_zh.md&quot;&gt;中文&lt;/a&gt; ]&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Fine-tuning a large language model can be easy as...&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e&quot;&gt;https://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Choose your path:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: &lt;a href=&quot;https://llamafactory.readthedocs.io/en/latest/&quot;&gt;https://llamafactory.readthedocs.io/en/latest/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Colab (free)&lt;/strong&gt;: &lt;a href=&quot;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&quot;&gt;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Local machine&lt;/strong&gt;: Please refer to &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#getting-started&quot;&gt;usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;PAI-DSW (free trial)&lt;/strong&gt;: &lt;a href=&quot;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory&quot;&gt;Llama3 Example&lt;/a&gt; | &lt;a href=&quot;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl&quot;&gt;Qwen2-VL Example&lt;/a&gt; | &lt;a href=&quot;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b&quot;&gt;DeepSeek-R1-Distill Example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Amazon SageMaker&lt;/strong&gt;: &lt;a href=&quot;https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/&quot;&gt;Blog&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#features&quot;&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#benchmark&quot;&gt;Benchmark&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#changelog&quot;&gt;Changelog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#supported-models&quot;&gt;Supported Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#supported-training-approaches&quot;&gt;Supported Training Approaches&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#provided-datasets&quot;&gt;Provided Datasets&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#requirement&quot;&gt;Requirement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#getting-started&quot;&gt;Getting Started&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#installation&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#data-preparation&quot;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#quickstart&quot;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#fine-tuning-with-llama-board-gui-powered-by-gradio&quot;&gt;Fine-Tuning with LLaMA Board GUI&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#build-docker&quot;&gt;Build Docker&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#deploy-with-openai-style-api-and-vllm&quot;&gt;Deploy with OpenAI-style API and vLLM&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelscope-hub&quot;&gt;Download from ModelScope Hub&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelers-hub&quot;&gt;Download from Modelers Hub&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-wb-logger&quot;&gt;Use W&amp;amp;B Logger&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-swanlab-logger&quot;&gt;Use SwanLab Logger&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#projects-using-llama-factory&quot;&gt;Projects using LLaMA Factory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#license&quot;&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#citation&quot;&gt;Citation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#acknowledgement&quot;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Various models&lt;/strong&gt;: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrated methods&lt;/strong&gt;: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable resources&lt;/strong&gt;: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced algorithms&lt;/strong&gt;: &lt;a href=&quot;https://github.com/jiaweizzhao/GaLore&quot;&gt;GaLore&lt;/a&gt;, &lt;a href=&quot;https://github.com/Ledzy/BAdam&quot;&gt;BAdam&lt;/a&gt;, &lt;a href=&quot;https://github.com/zhuhanqing/APOLLO&quot;&gt;APOLLO&lt;/a&gt;, &lt;a href=&quot;https://github.com/zyushun/Adam-mini&quot;&gt;Adam-mini&lt;/a&gt;, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Practical tricks&lt;/strong&gt;: &lt;a href=&quot;https://github.com/Dao-AILab/flash-attention&quot;&gt;FlashAttention-2&lt;/a&gt;, &lt;a href=&quot;https://github.com/unslothai/unsloth&quot;&gt;Unsloth&lt;/a&gt;, &lt;a href=&quot;https://github.com/linkedin/Liger-Kernel&quot;&gt;Liger Kernel&lt;/a&gt;, RoPE scaling, NEFTune and rsLoRA.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Wide tasks&lt;/strong&gt;: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Experiment monitors&lt;/strong&gt;: LlamaBoard, TensorBoard, Wandb, MLflow, &lt;a href=&quot;https://github.com/SwanHubX/SwanLab&quot;&gt;SwanLab&lt;/a&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Faster inference&lt;/strong&gt;: OpenAI-style API, Gradio UI and CLI with &lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;vLLM worker&lt;/a&gt; or &lt;a href=&quot;https://github.com/sgl-project/sglang&quot;&gt;SGLang worker&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Day-N Support for Fine-Tuning Cutting-Edge Models&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Support Date&lt;/th&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Day 0&lt;/td&gt; 
   &lt;td&gt;Qwen2.5 / Qwen2.5-VL / Gemma 3 / InternLM 3 / MiniCPM-o-2.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Day 1&lt;/td&gt; 
   &lt;td&gt;Llama 3 / GLM-4 / Mistral Small / PaliGemma2&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Benchmark&lt;/h2&gt; 
&lt;p&gt;Compared to ChatGLM&#39;s &lt;a href=&quot;https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning&quot;&gt;P-Tuning&lt;/a&gt;, LLaMA Factory&#39;s LoRA tuning offers up to &lt;strong&gt;3.7 times faster&lt;/strong&gt; training speed with a better Rouge score on the advertising text generation task. By leveraging 4-bit quantization technique, LLaMA Factory&#39;s QLoRA further improves the efficiency regarding the GPU memory.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/assets/benchmark.svg?sanitize=true&quot; alt=&quot;benchmark&quot;&gt;&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;Definitions&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Training Speed&lt;/strong&gt;: the number of training samples processed per second during the training. (bs=4, cutoff_len=1024)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Rouge Score&lt;/strong&gt;: Rouge-2 score on the development set of the &lt;a href=&quot;https://aclanthology.org/D19-1321.pdf&quot;&gt;advertising text generation&lt;/a&gt; task. (bs=4, cutoff_len=1024)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;GPU Memory&lt;/strong&gt;: Peak GPU memory usage in 4-bit quantized training. (bs=1, cutoff_len=1024)&lt;/li&gt; 
  &lt;li&gt;We adopt &lt;code&gt;pre_seq_len=128&lt;/code&gt; for ChatGLM&#39;s P-Tuning and &lt;code&gt;lora_rank=32&lt;/code&gt; for LLaMA Factory&#39;s LoRA tuning.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Changelog&lt;/h2&gt; 
&lt;p&gt;[25/03/15] We supported &lt;strong&gt;&lt;a href=&quot;https://github.com/sgl-project/sglang&quot;&gt;SGLang&lt;/a&gt;&lt;/strong&gt; as inference backend. Try &lt;code&gt;infer_backend: sglang&lt;/code&gt; to accelerate inference.&lt;/p&gt; 
&lt;p&gt;[25/03/12] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/blog/gemma3&quot;&gt;Gemma-3&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; 
&lt;p&gt;[25/02/24] Announcing &lt;strong&gt;&lt;a href=&quot;https://github.com/hiyouga/EasyR1&quot;&gt;EasyR1&lt;/a&gt;&lt;/strong&gt;, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.&lt;/p&gt; 
&lt;p&gt;[25/02/11] We supported saving the &lt;strong&gt;&lt;a href=&quot;https://github.com/ollama/ollama&quot;&gt;Ollama&lt;/a&gt;&lt;/strong&gt; modelfile when exporting the model checkpoints. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
&lt;p&gt;[25/02/05] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/Qwen/Qwen2-Audio-7B-Instruct&quot;&gt;Qwen2-Audio&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/openbmb/MiniCPM-o-2_6&quot;&gt;MiniCPM-o-2.6&lt;/a&gt;&lt;/strong&gt; on audio understanding tasks.&lt;/p&gt; 
&lt;p&gt;[25/01/31] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/deepseek-ai/DeepSeek-R1&quot;&gt;DeepSeek-R1&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct&quot;&gt;Qwen2.5-VL&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;Full Changelog&lt;/summary&gt; 
 &lt;p&gt;[25/01/15] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2412.05270&quot;&gt;APOLLO&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[25/01/14] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/openbmb/MiniCPM-o-2_6&quot;&gt;MiniCPM-o-2.6&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/openbmb/MiniCPM-V-2_6&quot;&gt;MiniCPM-V-2.6&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href=&quot;https://github.com/BUAADreamer&quot;&gt;@BUAADreamer&lt;/a&gt;&#39;s PR.&lt;/p&gt; 
 &lt;p&gt;[25/01/14] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/collections/internlm/&quot;&gt;InternLM 3&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href=&quot;https://github.com/hhaAndroid&quot;&gt;@hhaAndroid&lt;/a&gt;&#39;s PR.&lt;/p&gt; 
 &lt;p&gt;[25/01/10] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/microsoft/phi-4&quot;&gt;Phi-4&lt;/a&gt;&lt;/strong&gt; model.&lt;/p&gt; 
 &lt;p&gt;[24/12/21] We supported using &lt;strong&gt;&lt;a href=&quot;https://github.com/SwanHubX/SwanLab&quot;&gt;SwanLab&lt;/a&gt;&lt;/strong&gt; for experiment tracking and visualization. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#use-swanlab-logger&quot;&gt;this section&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[24/11/27] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B&quot;&gt;Skywork-o1&lt;/a&gt;&lt;/strong&gt; model and the &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT&quot;&gt;OpenO1&lt;/a&gt;&lt;/strong&gt; dataset.&lt;/p&gt; 
 &lt;p&gt;[24/10/09] We supported downloading pre-trained models and datasets from the &lt;strong&gt;&lt;a href=&quot;https://modelers.cn/models&quot;&gt;Modelers Hub&lt;/a&gt;&lt;/strong&gt;. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelers-hub&quot;&gt;this tutorial&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/09/19] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://qwenlm.github.io/blog/qwen2.5/&quot;&gt;Qwen2.5&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; 
 &lt;p&gt;[24/08/30] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://qwenlm.github.io/blog/qwen2-vl/&quot;&gt;Qwen2-VL&lt;/a&gt;&lt;/strong&gt; models. Thank &lt;a href=&quot;https://github.com/simonJJJ&quot;&gt;@simonJJJ&lt;/a&gt;&#39;s PR.&lt;/p&gt; 
 &lt;p&gt;[24/08/27] We supported &lt;strong&gt;&lt;a href=&quot;https://github.com/linkedin/Liger-Kernel&quot;&gt;Liger Kernel&lt;/a&gt;&lt;/strong&gt;. Try &lt;code&gt;enable_liger_kernel: true&lt;/code&gt; for efficient training.&lt;/p&gt; 
 &lt;p&gt;[24/08/09] We supported &lt;strong&gt;&lt;a href=&quot;https://github.com/zyushun/Adam-mini&quot;&gt;Adam-mini&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage. Thank &lt;a href=&quot;https://github.com/relic-yuexi&quot;&gt;@relic-yuexi&lt;/a&gt;&#39;s PR.&lt;/p&gt; 
 &lt;p&gt;[24/07/04] We supported &lt;a href=&quot;https://github.com/MeetKai/functionary/tree/main/functionary/train/packing&quot;&gt;contamination-free packed training&lt;/a&gt;. Use &lt;code&gt;neat_packing: true&lt;/code&gt; to activate it. Thank &lt;a href=&quot;https://github.com/chuan298&quot;&gt;@chuan298&lt;/a&gt;&#39;s PR.&lt;/p&gt; 
 &lt;p&gt;[24/06/16] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.02948&quot;&gt;PiSSA&lt;/a&gt;&lt;/strong&gt; algorithm. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/06/07] We supported fine-tuning the &lt;strong&gt;&lt;a href=&quot;https://qwenlm.github.io/blog/qwen2/&quot;&gt;Qwen2&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;https://github.com/THUDM/GLM-4&quot;&gt;GLM-4&lt;/a&gt;&lt;/strong&gt; models.&lt;/p&gt; 
 &lt;p&gt;[24/05/26] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14734&quot;&gt;SimPO&lt;/a&gt;&lt;/strong&gt; algorithm for preference learning. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/05/20] We supported fine-tuning the &lt;strong&gt;PaliGemma&lt;/strong&gt; series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with &lt;code&gt;paligemma&lt;/code&gt; template for chat completion.&lt;/p&gt; 
 &lt;p&gt;[24/05/18] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.01306&quot;&gt;KTO&lt;/a&gt;&lt;/strong&gt; algorithm for preference learning. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/05/14] We supported training and inference on the Ascend NPU devices. Check &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#installation&quot;&gt;installation&lt;/a&gt; section for details.&lt;/p&gt; 
 &lt;p&gt;[24/04/26] We supported fine-tuning the &lt;strong&gt;LLaVA-1.5&lt;/strong&gt; multimodal LLMs. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/04/22] We provided a &lt;strong&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&quot;&gt;Colab notebook&lt;/a&gt;&lt;/strong&gt; for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check &lt;a href=&quot;https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat&quot;&gt;Llama3-8B-Chinese-Chat&lt;/a&gt; and &lt;a href=&quot;https://huggingface.co/zhichen/Llama3-Chinese&quot;&gt;Llama3-Chinese&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[24/04/21] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.02258&quot;&gt;Mixture-of-Depths&lt;/a&gt;&lt;/strong&gt; according to &lt;a href=&quot;https://github.com/astramind-ai/Mixture-of-depths&quot;&gt;AstraMindAI&#39;s implementation&lt;/a&gt;. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/04/16] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.02827&quot;&gt;BAdam&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/04/16] We supported &lt;strong&gt;&lt;a href=&quot;https://github.com/unslothai/unsloth&quot;&gt;unsloth&lt;/a&gt;&lt;/strong&gt;&#39;s long-sequence training (Llama-2-7B-56k within 24GB). It achieves &lt;strong&gt;117%&lt;/strong&gt; speed and &lt;strong&gt;50%&lt;/strong&gt; memory compared with FlashAttention-2, more benchmarks can be found in &lt;a href=&quot;https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison&quot;&gt;this page&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;[24/03/31] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.07691&quot;&gt;ORPO&lt;/a&gt;&lt;/strong&gt;. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/21] Our paper &quot;&lt;a href=&quot;https://arxiv.org/abs/2403.13372&quot;&gt;LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models&lt;/a&gt;&quot; is available at arXiv!&lt;/p&gt; 
 &lt;p&gt;[24/03/20] We supported &lt;strong&gt;FSDP+QLoRA&lt;/strong&gt; that fine-tunes a 70B model on 2x24GB GPUs. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/13] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.12354&quot;&gt;LoRA+&lt;/a&gt;&lt;/strong&gt;. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/07] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.03507&quot;&gt;GaLore&lt;/a&gt;&lt;/strong&gt; optimizer. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/03/07] We integrated &lt;strong&gt;&lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;vLLM&lt;/a&gt;&lt;/strong&gt; for faster and concurrent inference. Try &lt;code&gt;infer_backend: vllm&lt;/code&gt; to enjoy &lt;strong&gt;270%&lt;/strong&gt; inference speed.&lt;/p&gt; 
 &lt;p&gt;[24/02/28] We supported weight-decomposed LoRA (&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.09353&quot;&gt;DoRA&lt;/a&gt;&lt;/strong&gt;). Try &lt;code&gt;use_dora: true&lt;/code&gt; to activate DoRA training.&lt;/p&gt; 
 &lt;p&gt;[24/02/15] We supported &lt;strong&gt;block expansion&lt;/strong&gt; proposed by &lt;a href=&quot;https://github.com/TencentARC/LLaMA-Pro&quot;&gt;LLaMA Pro&lt;/a&gt;. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this &lt;a href=&quot;https://qwenlm.github.io/blog/qwen1.5/&quot;&gt;blog post&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[24/01/18] We supported &lt;strong&gt;agent tuning&lt;/strong&gt; for most models, equipping model with tool using abilities by fine-tuning with &lt;code&gt;dataset: glaive_toolcall_en&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;[23/12/23] We supported &lt;strong&gt;&lt;a href=&quot;https://github.com/unslothai/unsloth&quot;&gt;unsloth&lt;/a&gt;&lt;/strong&gt;&#39;s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try &lt;code&gt;use_unsloth: true&lt;/code&gt; argument to activate unsloth patch. It achieves &lt;strong&gt;170%&lt;/strong&gt; speed in our benchmark, check &lt;a href=&quot;https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison&quot;&gt;this page&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[23/12/12] We supported fine-tuning the latest MoE model &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/mistralai/Mixtral-8x7B-v0.1&quot;&gt;Mixtral 8x7B&lt;/a&gt;&lt;/strong&gt; in our framework. See hardware requirement &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#hardware-requirement&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;[23/12/01] We supported downloading pre-trained models and datasets from the &lt;strong&gt;&lt;a href=&quot;https://modelscope.cn/models&quot;&gt;ModelScope Hub&lt;/a&gt;&lt;/strong&gt;. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/#download-from-modelscope-hub&quot;&gt;this tutorial&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[23/10/21] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.05914&quot;&gt;NEFTune&lt;/a&gt;&lt;/strong&gt; trick for fine-tuning. Try &lt;code&gt;neftune_noise_alpha: 5&lt;/code&gt; argument to activate NEFTune.&lt;/p&gt; 
 &lt;p&gt;[23/09/27] We supported &lt;strong&gt;$S^2$-Attn&lt;/strong&gt; proposed by &lt;a href=&quot;https://github.com/dvlab-research/LongLoRA&quot;&gt;LongLoRA&lt;/a&gt; for the LLaMA models. Try &lt;code&gt;shift_attn: true&lt;/code&gt; argument to enable shift short attention.&lt;/p&gt; 
 &lt;p&gt;[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[23/09/10] We supported &lt;strong&gt;&lt;a href=&quot;https://github.com/Dao-AILab/flash-attention&quot;&gt;FlashAttention-2&lt;/a&gt;&lt;/strong&gt;. Try &lt;code&gt;flash_attn: fa2&lt;/code&gt; argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.&lt;/p&gt; 
 &lt;p&gt;[23/08/12] We supported &lt;strong&gt;RoPE scaling&lt;/strong&gt; to extend the context length of the LLaMA models. Try &lt;code&gt;rope_scaling: linear&lt;/code&gt; argument in training and &lt;code&gt;rope_scaling: dynamic&lt;/code&gt; argument at inference to extrapolate the position embeddings.&lt;/p&gt; 
 &lt;p&gt;[23/08/11] We supported &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.18290&quot;&gt;DPO training&lt;/a&gt;&lt;/strong&gt; for instruction-tuned models. See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
 &lt;p&gt;[23/07/31] We supported &lt;strong&gt;dataset streaming&lt;/strong&gt;. Try &lt;code&gt;streaming: true&lt;/code&gt; and &lt;code&gt;max_steps: 10000&lt;/code&gt; arguments to load your dataset in streaming mode.&lt;/p&gt; 
 &lt;p&gt;[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos (&lt;a href=&quot;https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat&quot;&gt;LLaMA-2&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/hiyouga/Baichuan-13B-sft&quot;&gt;Baichuan&lt;/a&gt;) for details.&lt;/p&gt; 
 &lt;p&gt;[23/07/18] We developed an &lt;strong&gt;all-in-one Web UI&lt;/strong&gt; for training, evaluation and inference. Try &lt;code&gt;train_web.py&lt;/code&gt; to fine-tune models in your Web browser. Thank &lt;a href=&quot;https://github.com/KanadeSiina&quot;&gt;@KanadeSiina&lt;/a&gt; and &lt;a href=&quot;https://github.com/codemayq&quot;&gt;@codemayq&lt;/a&gt; for their efforts in the development.&lt;/p&gt; 
 &lt;p&gt;[23/07/09] We released &lt;strong&gt;&lt;a href=&quot;https://github.com/hiyouga/FastEdit&quot;&gt;FastEdit&lt;/a&gt;&lt;/strong&gt; ⚡🩹, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow &lt;a href=&quot;https://github.com/hiyouga/FastEdit&quot;&gt;FastEdit&lt;/a&gt; if you are interested.&lt;/p&gt; 
 &lt;p&gt;[23/06/29] We provided a &lt;strong&gt;reproducible example&lt;/strong&gt; of training a chat model using instruction-following datasets, see &lt;a href=&quot;https://huggingface.co/hiyouga/Baichuan-7B-sft&quot;&gt;Baichuan-7B-sft&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;[23/06/22] We aligned the &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/api_demo.py&quot;&gt;demo API&lt;/a&gt; with the &lt;a href=&quot;https://platform.openai.com/docs/api-reference/chat&quot;&gt;OpenAI&#39;s&lt;/a&gt; format where you can insert the fine-tuned model in &lt;strong&gt;arbitrary ChatGPT-based applications&lt;/strong&gt;.&lt;/p&gt; 
 &lt;p&gt;[23/06/03] We supported quantized training and inference (aka &lt;strong&gt;&lt;a href=&quot;https://github.com/artidoro/qlora&quot;&gt;QLoRA&lt;/a&gt;&lt;/strong&gt;). See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples&lt;/a&gt; for usage.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Model size&lt;/th&gt; 
   &lt;th&gt;Template&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/baichuan-inc&quot;&gt;Baichuan 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B&lt;/td&gt; 
   &lt;td&gt;baichuan2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/bigscience&quot;&gt;BLOOM/BLOOMZ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;560M/1.1B/1.7B/3B/7.1B/176B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/THUDM&quot;&gt;ChatGLM3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;6B&lt;/td&gt; 
   &lt;td&gt;chatglm3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/CohereForAI&quot;&gt;Command R&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;35B/104B&lt;/td&gt; 
   &lt;td&gt;cohere&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/deepseek-ai&quot;&gt;DeepSeek (Code/MoE)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/16B/67B/236B&lt;/td&gt; 
   &lt;td&gt;deepseek&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/deepseek-ai&quot;&gt;DeepSeek 2.5/3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;236B/671B&lt;/td&gt; 
   &lt;td&gt;deepseek3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/deepseek-ai&quot;&gt;DeepSeek R1 (Distill)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5B/7B/8B/14B/32B/70B/671B&lt;/td&gt; 
   &lt;td&gt;deepseek3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/tiiuae&quot;&gt;Falcon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/11B/40B/180B&lt;/td&gt; 
   &lt;td&gt;falcon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/google&quot;&gt;Gemma/Gemma 2/CodeGemma&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2B/7B/9B/27B&lt;/td&gt; 
   &lt;td&gt;gemma&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/google&quot;&gt;Gemma 3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/4B/12B/27B&lt;/td&gt; 
   &lt;td&gt;gemma3/gemma (1B)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/THUDM&quot;&gt;GLM-4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;9B&lt;/td&gt; 
   &lt;td&gt;glm4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/openai-community&quot;&gt;GPT-2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.1B/0.4B/0.8B/1.5B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/ibm-granite&quot;&gt;Granite 3.0-3.1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/2B/3B/8B&lt;/td&gt; 
   &lt;td&gt;granite3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/IndexTeam&quot;&gt;Index&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.9B&lt;/td&gt; 
   &lt;td&gt;index&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/tencent/&quot;&gt;Hunyuan&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;hunyuan&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/internlm&quot;&gt;InternLM 2-3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/8B/20B&lt;/td&gt; 
   &lt;td&gt;intern2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/facebookresearch/llama&quot;&gt;Llama&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B/33B/65B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/meta-llama&quot;&gt;Llama 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B/70B&lt;/td&gt; 
   &lt;td&gt;llama2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/meta-llama&quot;&gt;Llama 3-3.3&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/3B/8B/70B&lt;/td&gt; 
   &lt;td&gt;llama3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/meta-llama&quot;&gt;Llama 3.2 Vision&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11B/90B&lt;/td&gt; 
   &lt;td&gt;mllama&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/llava-hf&quot;&gt;LLaVA-1.5&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B&lt;/td&gt; 
   &lt;td&gt;llava&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/llava-hf&quot;&gt;LLaVA-NeXT&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/8B/13B/34B/72B/110B&lt;/td&gt; 
   &lt;td&gt;llava_next&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/llava-hf&quot;&gt;LLaVA-NeXT-Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/34B&lt;/td&gt; 
   &lt;td&gt;llava_next_video&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/openbmb&quot;&gt;MiniCPM&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/2B/4B&lt;/td&gt; 
   &lt;td&gt;cpm/cpm3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/openbmb&quot;&gt;MiniCPM-o-2.6/MiniCPM-V-2.6&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;minicpm_o/minicpm_v&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/mistralai&quot;&gt;Ministral/Mistral-Nemo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B/12B&lt;/td&gt; 
   &lt;td&gt;ministral&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/mistralai&quot;&gt;Mistral/Mixtral&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/8x7B/8x22B&lt;/td&gt; 
   &lt;td&gt;mistral&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/mistralai&quot;&gt;Mistral Small&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;24B&lt;/td&gt; 
   &lt;td&gt;mistral_small&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/allenai&quot;&gt;OLMo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1B/7B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/google&quot;&gt;PaliGemma/PaliGemma2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3B/10B/28B&lt;/td&gt; 
   &lt;td&gt;paligemma&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/microsoft&quot;&gt;Phi-1.5/Phi-2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.3B/2.7B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/microsoft&quot;&gt;Phi-3/Phi-3.5&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;4B/14B&lt;/td&gt; 
   &lt;td&gt;phi&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/microsoft&quot;&gt;Phi-3-small&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;phi_small&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/microsoft&quot;&gt;Phi-4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;14B&lt;/td&gt; 
   &lt;td&gt;phi4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/mistralai&quot;&gt;Pixtral&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12B&lt;/td&gt; 
   &lt;td&gt;pixtral&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/Qwen&quot;&gt;Qwen/QwQ (1-2.5) (Code/Math/MoE)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.5B/1.5B/3B/7B/14B/32B/72B/110B&lt;/td&gt; 
   &lt;td&gt;qwen&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/Qwen&quot;&gt;Qwen2-Audio&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;qwen2_audio&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/Qwen&quot;&gt;Qwen2-VL/Qwen2.5-VL/QVQ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2B/3B/7B/32B/72B&lt;/td&gt; 
   &lt;td&gt;qwen2_vl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/Skywork&quot;&gt;Skywork o1&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;skywork_o1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/bigcode&quot;&gt;StarCoder 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3B/7B/15B&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/Tele-AI&quot;&gt;TeleChat2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3B/7B/35B/115B&lt;/td&gt; 
   &lt;td&gt;telechat2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/xverse&quot;&gt;XVERSE&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;7B/13B/65B&lt;/td&gt; 
   &lt;td&gt;xverse&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/01-ai&quot;&gt;Yi/Yi-1.5 (Code)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;1.5B/6B/9B/34B&lt;/td&gt; 
   &lt;td&gt;yi&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/01-ai&quot;&gt;Yi-VL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;6B/34B&lt;/td&gt; 
   &lt;td&gt;yi_vl&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/IEITYuan&quot;&gt;Yuan 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2B/51B/102B&lt;/td&gt; 
   &lt;td&gt;yuan&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] For the &quot;base&quot; models, the &lt;code&gt;template&lt;/code&gt; argument can be chosen from &lt;code&gt;default&lt;/code&gt;, &lt;code&gt;alpaca&lt;/code&gt;, &lt;code&gt;vicuna&lt;/code&gt; etc. But make sure to use the &lt;strong&gt;corresponding template&lt;/strong&gt; for the &quot;instruct/chat&quot; models.&lt;/p&gt; 
 &lt;p&gt;Remember to use the &lt;strong&gt;SAME&lt;/strong&gt; template in training and inference.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Please refer to &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/llamafactory/extras/constants.py&quot;&gt;constants.py&lt;/a&gt; for a full list of models we supported.&lt;/p&gt; 
&lt;p&gt;You also can add a custom chat template to &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/src/llamafactory/data/template.py&quot;&gt;template.py&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Supported Training Approaches&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Approach&lt;/th&gt; 
   &lt;th&gt;Full-tuning&lt;/th&gt; 
   &lt;th&gt;Freeze-tuning&lt;/th&gt; 
   &lt;th&gt;LoRA&lt;/th&gt; 
   &lt;th&gt;QLoRA&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Pre-Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Supervised Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Reward Modeling&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KTO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ORPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SimPO Training&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] The implementation details of PPO can be found in &lt;a href=&quot;https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html&quot;&gt;this blog&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Provided Datasets&lt;/h2&gt; 
&lt;details&gt;
 &lt;summary&gt;Pre-training datasets&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/wiki_demo.txt&quot;&gt;Wiki Demo (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/tiiuae/falcon-refinedweb&quot;&gt;RefinedWeb (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2&quot;&gt;RedPajama V2 (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/olm/olm-wikipedia-20221220&quot;&gt;Wikipedia (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered&quot;&gt;Wikipedia (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/EleutherAI/pile&quot;&gt;Pile (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/Skywork/SkyPile-150B&quot;&gt;SkyPile (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/HuggingFaceFW/fineweb&quot;&gt;FineWeb (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu&quot;&gt;FineWeb-Edu (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/bigcode/the-stack&quot;&gt;The Stack (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/bigcode/starcoderdata&quot;&gt;StarCoder (en)&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Supervised fine-tuning datasets&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/identity.json&quot;&gt;Identity (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/tatsu-lab/stanford_alpaca&quot;&gt;Stanford Alpaca (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/ymcui/Chinese-LLaMA-Alpaca-3&quot;&gt;Stanford Alpaca (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&quot;&gt;Alpaca GPT4 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2&quot;&gt;Glaive Function Calling V2 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/GAIR/lima&quot;&gt;LIMA (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/JosephusCheung/GuanacoDataset&quot;&gt;Guanaco Dataset (multilingual)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/BelleGroup/train_2M_CN&quot;&gt;BELLE 2M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/BelleGroup/train_1M_CN&quot;&gt;BELLE 1M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/BelleGroup/train_0.5M_CN&quot;&gt;BELLE 0.5M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M&quot;&gt;BELLE Dialogue 0.4M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/BelleGroup/school_math_0.25M&quot;&gt;BELLE School Math 0.25M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M&quot;&gt;BELLE Multiturn Chat 0.8M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/thunlp/UltraChat&quot;&gt;UltraChat (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/garage-bAInd/Open-Platypus&quot;&gt;OpenPlatypus (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k&quot;&gt;CodeAlpaca 20k (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/QingyiSi/Alpaca-CoT&quot;&gt;Alpaca CoT (multilingual)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/Open-Orca/OpenOrca&quot;&gt;OpenOrca (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/Open-Orca/SlimOrca&quot;&gt;SlimOrca (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/TIGER-Lab/MathInstruct&quot;&gt;MathInstruct (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M&quot;&gt;Firefly 1.1M (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/wiki_qa&quot;&gt;Wiki QA (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/suolyer/webqa&quot;&gt;Web QA (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/zxbsmk/webnovel_cn&quot;&gt;WebNovel (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/berkeley-nest/Nectar&quot;&gt;Nectar (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data&quot;&gt;deepctrl (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/HasturOfficial/adgen&quot;&gt;Advertise Generating (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k&quot;&gt;ShareGPT Hyperfiltered (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/shibing624/sharegpt_gpt4&quot;&gt;ShareGPT4 (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k&quot;&gt;UltraChat 200k (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/THUDM/AgentInstruct&quot;&gt;AgentInstruct (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/lmsys/lmsys-chat-1m&quot;&gt;LMSYS Chat 1M (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k&quot;&gt;Evol Instruct V2 (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/HuggingFaceTB/cosmopedia&quot;&gt;Cosmopedia (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/hfl/stem_zh_instruction&quot;&gt;STEM (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo&quot;&gt;Ruozhiba (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/m-a-p/neo_sft_phase2&quot;&gt;Neo-sft (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered&quot;&gt;Magpie-Pro-300K-Filtered (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/argilla/magpie-ultra-v0.1&quot;&gt;Magpie-ultra-v0.1 (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/TIGER-Lab/WebInstructSub&quot;&gt;WebInstructSub (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT&quot;&gt;OpenO1-SFT (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k&quot;&gt;Open-Thoughts (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/open-r1/OpenR1-Math-220k&quot;&gt;Open-R1-Math (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT&quot;&gt;Chinese-DeepSeek-R1-Distill (zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k&quot;&gt;LLaVA mixed (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions&quot;&gt;Pokemon-gpt4o-captions (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/oasst_de&quot;&gt;Open Assistant (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de&quot;&gt;Dolly 15k (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de&quot;&gt;Alpaca GPT4 (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de&quot;&gt;OpenSchnabeltier (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de&quot;&gt;Evol Instruct (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/dolphin_de&quot;&gt;Dolphin (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/booksum_de&quot;&gt;Booksum (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de&quot;&gt;Airoboros (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de&quot;&gt;Ultrachat (de)&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Preference datasets&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k&quot;&gt;DPO mixed (en&amp;amp;zh)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized&quot;&gt;UltraFeedback (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/openbmb/RLHF-V-Dataset&quot;&gt;RLHF-V (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/Zhihui/VLFeedback&quot;&gt;VLFeedback (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/Intel/orca_dpo_pairs&quot;&gt;Orca DPO Pairs (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/Anthropic/hh-rlhf&quot;&gt;HH-RLHF (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/berkeley-nest/Nectar&quot;&gt;Nectar (en)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de&quot;&gt;Orca DPO (de)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/datasets/argilla/kto-mix-15k&quot;&gt;KTO mixed (en)&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;Some datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install --upgrade huggingface_hub
huggingface-cli login
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Requirement&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Mandatory&lt;/th&gt; 
   &lt;th&gt;Minimum&lt;/th&gt; 
   &lt;th&gt;Recommend&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;python&lt;/td&gt; 
   &lt;td&gt;3.9&lt;/td&gt; 
   &lt;td&gt;3.10&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;torch&lt;/td&gt; 
   &lt;td&gt;1.13.1&lt;/td&gt; 
   &lt;td&gt;2.6.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;transformers&lt;/td&gt; 
   &lt;td&gt;4.41.2&lt;/td&gt; 
   &lt;td&gt;4.50.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;datasets&lt;/td&gt; 
   &lt;td&gt;2.16.0&lt;/td&gt; 
   &lt;td&gt;3.2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;accelerate&lt;/td&gt; 
   &lt;td&gt;0.34.0&lt;/td&gt; 
   &lt;td&gt;1.2.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;peft&lt;/td&gt; 
   &lt;td&gt;0.14.0&lt;/td&gt; 
   &lt;td&gt;0.15.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;trl&lt;/td&gt; 
   &lt;td&gt;0.8.6&lt;/td&gt; 
   &lt;td&gt;0.9.6&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Optional&lt;/th&gt; 
   &lt;th&gt;Minimum&lt;/th&gt; 
   &lt;th&gt;Recommend&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CUDA&lt;/td&gt; 
   &lt;td&gt;11.6&lt;/td&gt; 
   &lt;td&gt;12.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;deepspeed&lt;/td&gt; 
   &lt;td&gt;0.10.0&lt;/td&gt; 
   &lt;td&gt;0.16.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;bitsandbytes&lt;/td&gt; 
   &lt;td&gt;0.39.0&lt;/td&gt; 
   &lt;td&gt;0.43.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;vllm&lt;/td&gt; 
   &lt;td&gt;0.4.3&lt;/td&gt; 
   &lt;td&gt;0.7.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;flash-attn&lt;/td&gt; 
   &lt;td&gt;2.3.0&lt;/td&gt; 
   &lt;td&gt;2.7.2&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Hardware Requirement&lt;/h3&gt; 
&lt;p&gt;* &lt;em&gt;estimated&lt;/em&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Bits&lt;/th&gt; 
   &lt;th&gt;7B&lt;/th&gt; 
   &lt;th&gt;14B&lt;/th&gt; 
   &lt;th&gt;30B&lt;/th&gt; 
   &lt;th&gt;70B&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;x&lt;/code&gt;B&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Full (&lt;code&gt;bf16&lt;/code&gt; or &lt;code&gt;fp16&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;32&lt;/td&gt; 
   &lt;td&gt;120GB&lt;/td&gt; 
   &lt;td&gt;240GB&lt;/td&gt; 
   &lt;td&gt;600GB&lt;/td&gt; 
   &lt;td&gt;1200GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;18x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Full (&lt;code&gt;pure_bf16&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;60GB&lt;/td&gt; 
   &lt;td&gt;120GB&lt;/td&gt; 
   &lt;td&gt;300GB&lt;/td&gt; 
   &lt;td&gt;600GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;8x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Freeze/LoRA/GaLore/APOLLO/BAdam&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;16GB&lt;/td&gt; 
   &lt;td&gt;32GB&lt;/td&gt; 
   &lt;td&gt;64GB&lt;/td&gt; 
   &lt;td&gt;160GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QLoRA&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;10GB&lt;/td&gt; 
   &lt;td&gt;20GB&lt;/td&gt; 
   &lt;td&gt;40GB&lt;/td&gt; 
   &lt;td&gt;80GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;x&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QLoRA&lt;/td&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;6GB&lt;/td&gt; 
   &lt;td&gt;12GB&lt;/td&gt; 
   &lt;td&gt;24GB&lt;/td&gt; 
   &lt;td&gt;48GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;x/2&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QLoRA&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;4GB&lt;/td&gt; 
   &lt;td&gt;8GB&lt;/td&gt; 
   &lt;td&gt;16GB&lt;/td&gt; 
   &lt;td&gt;24GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;x/4&lt;/code&gt;GB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Installation is mandatory.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e &quot;.[torch,metrics]&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Extra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, awq, aqlm, vllm, sglang, galore, apollo, badam, adam-mini, qwen, minicpm_v, modelscope, openmind, swanlab, quality&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Use &lt;code&gt;pip install --no-deps -e .&lt;/code&gt; to resolve package conflicts.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt;
 &lt;summary&gt;Setting up a virtual environment with &lt;b&gt;uv&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Create an isolated Python environment with &lt;a href=&quot;https://github.com/astral-sh/uv&quot;&gt;uv&lt;/a&gt;:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;uv sync --extra torch --extra metrics --prerelease=allow
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Run LLaMA-Factory in the isolated environment:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;uv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;For Windows users&lt;/summary&gt; 
 &lt;h4&gt;Install BitsAndBytes&lt;/h4&gt; 
 &lt;p&gt;If you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of &lt;code&gt;bitsandbytes&lt;/code&gt; library, which supports CUDA 11.1 to 12.2, please select the appropriate &lt;a href=&quot;https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels&quot;&gt;release version&lt;/a&gt; based on your CUDA version.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Install Flash Attention-2&lt;/h4&gt; 
 &lt;p&gt;To enable FlashAttention-2 on the Windows platform, please use the script from &lt;a href=&quot;https://huggingface.co/lldacing/flash-attention-windows-wheel&quot;&gt;flash-attention-windows-wheel&lt;/a&gt; to compile and install it by yourself.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;For Ascend NPU users&lt;/summary&gt; 
 &lt;p&gt;To install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: &lt;code&gt;pip install -e &quot;.[torch-npu,metrics]&quot;&lt;/code&gt;. Additionally, you need to install the &lt;strong&gt;&lt;a href=&quot;https://www.hiascend.com/developer/download/community/result?module=cann&quot;&gt;Ascend CANN Toolkit and Kernels&lt;/a&gt;&lt;/strong&gt;. Please follow the &lt;a href=&quot;https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html&quot;&gt;installation tutorial&lt;/a&gt; or use the following commands:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# replace the url according to your CANN version and devices
# install CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-&quot;$(uname -i)&quot;.run
bash Ascend-cann-toolkit_8.0.0.alpha002_linux-&quot;$(uname -i)&quot;.run --install

# install CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-&quot;$(uname -i)&quot;.run
bash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-&quot;$(uname -i)&quot;.run --install

# set env variables
source /usr/local/Ascend/ascend-toolkit/set_env.sh
&lt;/code&gt;&lt;/pre&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Requirement&lt;/th&gt; 
    &lt;th&gt;Minimum&lt;/th&gt; 
    &lt;th&gt;Recommend&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CANN&lt;/td&gt; 
    &lt;td&gt;8.0.RC1&lt;/td&gt; 
    &lt;td&gt;8.0.0.alpha002&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torch&lt;/td&gt; 
    &lt;td&gt;2.1.0&lt;/td&gt; 
    &lt;td&gt;2.4.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torch-npu&lt;/td&gt; 
    &lt;td&gt;2.1.0&lt;/td&gt; 
    &lt;td&gt;2.4.0.post2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;deepspeed&lt;/td&gt; 
    &lt;td&gt;0.13.2&lt;/td&gt; 
    &lt;td&gt;0.13.2&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;Remember to use &lt;code&gt;ASCEND_RT_VISIBLE_DEVICES&lt;/code&gt; instead of &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; to specify the device to use.&lt;/p&gt; 
 &lt;p&gt;If you cannot infer model on NPU devices, try setting &lt;code&gt;do_sample: false&lt;/code&gt; in the configurations.&lt;/p&gt; 
 &lt;p&gt;Download the pre-built Docker images: &lt;a href=&quot;http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html&quot;&gt;32GB&lt;/a&gt; | &lt;a href=&quot;http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html&quot;&gt;64GB&lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt;Install BitsAndBytes&lt;/h4&gt; 
 &lt;p&gt;To use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Manually compile bitsandbytes: Refer to &lt;a href=&quot;https://huggingface.co/docs/bitsandbytes/installation?backend=Ascend+NPU&amp;amp;platform=Ascend+NPU&quot;&gt;the installation documentation&lt;/a&gt; for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Install bitsandbytes from source
# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch
git clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git
cd bitsandbytes/

# Install dependencies
pip install -r requirements-dev.txt

# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference
apt-get install -y build-essential cmake

# Compile &amp;amp; install  
cmake -DCOMPUTE_BACKEND=npu -S .
make
pip install .
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start=&quot;2&quot;&gt; 
  &lt;li&gt;Install transformers from the main branch.&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone -b main https://github.com/huggingface/transformers.git
cd transformers
pip install .
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start=&quot;3&quot;&gt; 
  &lt;li&gt;Set &lt;code&gt;double_quantization: false&lt;/code&gt; in the configuration. You can refer to the &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/train_qlora/llama3_lora_sft_bnb_npu.yaml&quot;&gt;example&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h3&gt;Data Preparation&lt;/h3&gt; 
&lt;p&gt;Please refer to &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/README.md&quot;&gt;data/README.md&lt;/a&gt; for checking the details about the format of dataset files. You can either use datasets on HuggingFace / ModelScope / Modelers hub or load the dataset in local disk.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Please update &lt;code&gt;data/dataset_info.json&lt;/code&gt; to use your custom dataset.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Quickstart&lt;/h3&gt; 
&lt;p&gt;Use the following 3 commands to run LoRA &lt;strong&gt;fine-tuning&lt;/strong&gt;, &lt;strong&gt;inference&lt;/strong&gt; and &lt;strong&gt;merging&lt;/strong&gt; of the Llama3-8B-Instruct model, respectively.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/examples/README.md&quot;&gt;examples/README.md&lt;/a&gt; for advanced usage (including distributed training).&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Use &lt;code&gt;llamafactory-cli help&lt;/code&gt; to show help information.&lt;/p&gt; 
 &lt;p&gt;Read &lt;a href=&quot;https://github.com/hiyouga/LLaMA-Factory/issues/4614&quot;&gt;FAQs&lt;/a&gt; first if you encounter any problems.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Fine-Tuning with LLaMA Board GUI (powered by &lt;a href=&quot;https://github.com/gradio-app/gradio&quot;&gt;Gradio&lt;/a&gt;)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llamafactory-cli webui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Build Docker&lt;/h3&gt; 
&lt;p&gt;For CUDA users:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd docker/docker-cuda/
docker compose up -d
docker compose exec llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Ascend NPU users:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd docker/docker-npu/
docker compose up -d
docker compose exec llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For AMD ROCm users:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd docker/docker-rocm/
docker compose up -d
docker compose exec llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt;
 &lt;summary&gt;Build without Docker Compose&lt;/summary&gt; 
 &lt;p&gt;For CUDA users:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker build -f ./docker/docker-cuda/Dockerfile \
    --build-arg INSTALL_BNB=false \
    --build-arg INSTALL_VLLM=false \
    --build-arg INSTALL_DEEPSPEED=false \
    --build-arg INSTALL_FLASHATTN=false \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    -t llamafactory:latest .

docker run -dit --gpus=all \
    -v ./hf_cache:/root/.cache/huggingface \
    -v ./ms_cache:/root/.cache/modelscope \
    -v ./om_cache:/root/.cache/openmind \
    -v ./data:/app/data \
    -v ./output:/app/output \
    -p 7860:7860 \
    -p 8000:8000 \
    --shm-size 16G \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For Ascend NPU users:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Choose docker image upon your environment
docker build -f ./docker/docker-npu/Dockerfile \
    --build-arg INSTALL_DEEPSPEED=false \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    -t llamafactory:latest .

# Change `device` upon your resources
docker run -dit \
    -v ./hf_cache:/root/.cache/huggingface \
    -v ./ms_cache:/root/.cache/modelscope \
    -v ./om_cache:/root/.cache/openmind \
    -v ./data:/app/data \
    -v ./output:/app/output \
    -v /usr/local/dcmi:/usr/local/dcmi \
    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
    -v /etc/ascend_install.info:/etc/ascend_install.info \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/davinci0 \
    --device /dev/davinci_manager \
    --device /dev/devmm_svm \
    --device /dev/hisi_hdc \
    --shm-size 16G \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For AMD ROCm users:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker build -f ./docker/docker-rocm/Dockerfile \
    --build-arg INSTALL_BNB=false \
    --build-arg INSTALL_VLLM=false \
    --build-arg INSTALL_DEEPSPEED=false \
    --build-arg INSTALL_FLASHATTN=false \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    -t llamafactory:latest .

docker run -dit \
    -v ./hf_cache:/root/.cache/huggingface \
    -v ./ms_cache:/root/.cache/modelscope \
    -v ./om_cache:/root/.cache/openmind \
    -v ./data:/app/data \
    -v ./output:/app/output \
    -v ./saves:/app/saves \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/kfd \
    --device /dev/dri \
    --shm-size 16G \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Details about volume&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;hf_cache&lt;/code&gt;: Utilize Hugging Face cache on the host machine. Reassignable if a cache already exists in a different directory.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;ms_cache&lt;/code&gt;: Similar to Hugging Face cache but for ModelScope users.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;om_cache&lt;/code&gt;: Similar to Hugging Face cache but for Modelers users.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;data&lt;/code&gt;: Place datasets on this dir of the host machine so that they can be selected on LLaMA Board GUI.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;output&lt;/code&gt;: Set export dir to this location so that the merged result can be accessed directly on the host machine.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;Deploy with OpenAI-style API and vLLM&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;API_PORT=8000 llamafactory-cli api examples/inference/llama3_vllm.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Visit &lt;a href=&quot;https://platform.openai.com/docs/api-reference/chat/create&quot;&gt;this page&lt;/a&gt; for API document.&lt;/p&gt; 
 &lt;p&gt;Examples: &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/scripts/api_example/test_image.py&quot;&gt;Image understanding&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/scripts/api_example/test_toolcall.py&quot;&gt;Function calling&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Download from ModelScope Hub&lt;/h3&gt; 
&lt;p&gt;If you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Train the model by specifying a model ID of the ModelScope Hub as the &lt;code&gt;model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at &lt;a href=&quot;https://modelscope.cn/models&quot;&gt;ModelScope Hub&lt;/a&gt;, e.g., &lt;code&gt;LLM-Research/Meta-Llama-3-8B-Instruct&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Download from Modelers Hub&lt;/h3&gt; 
&lt;p&gt;You can also use Modelers Hub to download models and datasets.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;export USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Train the model by specifying a model ID of the Modelers Hub as the &lt;code&gt;model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at &lt;a href=&quot;https://modelers.cn/models&quot;&gt;Modelers Hub&lt;/a&gt;, e.g., &lt;code&gt;TeleAI/TeleChat-7B-pt&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Use W&amp;amp;B Logger&lt;/h3&gt; 
&lt;p&gt;To use &lt;a href=&quot;https://wandb.ai&quot;&gt;Weights &amp;amp; Biases&lt;/a&gt; for logging experimental results, you need to add the following arguments to yaml files.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;report_to: wandb
run_name: test_run # optional
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Set &lt;code&gt;WANDB_API_KEY&lt;/code&gt; to &lt;a href=&quot;https://wandb.ai/authorize&quot;&gt;your key&lt;/a&gt; when launching training tasks to log in with your W&amp;amp;B account.&lt;/p&gt; 
&lt;h3&gt;Use SwanLab Logger&lt;/h3&gt; 
&lt;p&gt;To use &lt;a href=&quot;https://github.com/SwanHubX/SwanLab&quot;&gt;SwanLab&lt;/a&gt; for logging experimental results, you need to add the following arguments to yaml files.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;use_swanlab: true
swanlab_run_name: test_run # optional
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When launching training tasks, you can log in to SwanLab in three ways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Add &lt;code&gt;swanlab_api_key=&amp;lt;your_api_key&amp;gt;&lt;/code&gt; to the yaml file, and set it to your &lt;a href=&quot;https://swanlab.cn/settings&quot;&gt;API key&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Set the environment variable &lt;code&gt;SWANLAB_API_KEY&lt;/code&gt; to your &lt;a href=&quot;https://swanlab.cn/settings&quot;&gt;API key&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Use the &lt;code&gt;swanlab login&lt;/code&gt; command to complete the login.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Projects using LLaMA Factory&lt;/h2&gt; 
&lt;p&gt;If you have a project that should be incorporated, please contact via email or create a pull request.&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;Click to show&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. &lt;a href=&quot;https://arxiv.org/abs/2308.02223&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. &lt;a href=&quot;https://arxiv.org/abs/2308.10092&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. &lt;a href=&quot;https://arxiv.org/abs/2308.10526&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. &lt;a href=&quot;https://arxiv.org/abs/2311.07816&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. &lt;a href=&quot;https://arxiv.org/abs/2312.15710&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. &lt;a href=&quot;https://arxiv.org/abs/2401.04319&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. &lt;a href=&quot;https://arxiv.org/abs/2401.07286&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.05904&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.07625&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.11176&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.11187&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.11746&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.11801&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. &lt;a href=&quot;https://arxiv.org/abs/2402.11809&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.11819&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.12204&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.14714&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. &lt;a href=&quot;https://arxiv.org/abs/2402.15043&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. &lt;a href=&quot;https://arxiv.org/abs/2403.02333&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. &lt;a href=&quot;https://arxiv.org/abs/2403.03419&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. &lt;a href=&quot;https://arxiv.org/abs/2403.08228&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. &lt;a href=&quot;https://arxiv.org/abs/2403.09073&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. EDT: Improving Large Language Models&#39; Generation by Entropy-based Dynamic Temperature Sampling. 2024. &lt;a href=&quot;https://arxiv.org/abs/2403.14541&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. &lt;a href=&quot;https://arxiv.org/abs/2403.15246&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. &lt;a href=&quot;https://arxiv.org/abs/2403.16008&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. &lt;a href=&quot;https://arxiv.org/abs/2403.16443&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.00604&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.02827&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.04167&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.04316&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.07084&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.09836&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.11581&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.14215&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.16621&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. &lt;a href=&quot;https://arxiv.org/abs/2404.17140&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. &lt;a href=&quot;https://arxiv.org/abs/2404.18585&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. &lt;a href=&quot;https://arxiv.org/abs/2405.04760&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Dammu et al. &quot;They are uncultured&quot;: Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. &lt;a href=&quot;https://arxiv.org/abs/2405.05378&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2405.09055&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. &lt;a href=&quot;https://arxiv.org/abs/2405.12739&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. &lt;a href=&quot;https://arxiv.org/abs/2405.13816&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2405.20215&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. &lt;a href=&quot;https://aclanthology.org/2024.lt4hala-1.30&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.00380&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.02106&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.03136&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.04496&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.05688&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.05955&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.06973&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.07115&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhu et al. Are Large Language Models Good Statisticians?. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.07815&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.10099&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.10173&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.12074&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.14408&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.14546&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.15695&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.17233&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.18069&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh&#39;s Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. &lt;a href=&quot;https://aclanthology.org/2024.americasnlp-1.25&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. &lt;a href=&quot;https://arxiv.org/abs/2406.19949&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yang et al. Financial Knowledge Large Language Model. 2024. &lt;a href=&quot;https://arxiv.org/abs/2407.00365&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. &lt;a href=&quot;https://arxiv.org/abs/2407.01470&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. &lt;a href=&quot;https://arxiv.org/abs/2407.06129&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. &lt;a href=&quot;https://arxiv.org/abs/2407.08044&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. &lt;a href=&quot;https://arxiv.org/abs/2407.09756&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. &lt;a href=&quot;https://scholarcommons.scu.edu/cseng_senior/272/&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. &lt;a href=&quot;https://arxiv.org/abs/2407.13561&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. &lt;a href=&quot;https://arxiv.org/abs/2407.16637&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. &lt;a href=&quot;https://arxiv.org/abs/2407.17535&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. &lt;a href=&quot;https://arxiv.org/abs/2407.19705&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. &lt;a href=&quot;https://arxiv.org/abs/2408.00137&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. &lt;a href=&quot;https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. &lt;a href=&quot;https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. &lt;a href=&quot;https://arxiv.org/abs/2408.04693&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. &lt;a href=&quot;https://arxiv.org/abs/2408.04168&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. &lt;a href=&quot;https://aclanthology.org/2024.finnlp-2.1/&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. &lt;a href=&quot;https://arxiv.org/abs/2408.08072&quot;&gt;[arxiv]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3627673.3679611&quot;&gt;[paper]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/Yu-Yang-Li/StarWhisper&quot;&gt;StarWhisper&lt;/a&gt;&lt;/strong&gt;: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/FudanDISC/DISC-LawLLM&quot;&gt;DISC-LawLLM&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/X-D-Lab/Sunsimiao&quot;&gt;Sunsimiao&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/WangRongsheng/CareGPT&quot;&gt;CareGPT&lt;/a&gt;&lt;/strong&gt;: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/PKU-YuanGroup/Machine-Mindset/&quot;&gt;MachineMindset&lt;/a&gt;&lt;/strong&gt;: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://huggingface.co/Nekochu/Luminia-13B-v3&quot;&gt;Luminia-13B-v3&lt;/a&gt;&lt;/strong&gt;: A large language model specialized in generate metadata for stable diffusion. &lt;a href=&quot;https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt&quot;&gt;[demo]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/BUAADreamer/Chinese-LLaVA-Med&quot;&gt;Chinese-LLaVA-Med&lt;/a&gt;&lt;/strong&gt;: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/THUDM/AutoRE&quot;&gt;AutoRE&lt;/a&gt;&lt;/strong&gt;: A document-level relation extraction system based on large language models.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NVIDIA/RTX-AI-Toolkit&quot;&gt;NVIDIA RTX AI Toolkit&lt;/a&gt;&lt;/strong&gt;: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/LazyAGI/LazyLLM&quot;&gt;LazyLLM&lt;/a&gt;&lt;/strong&gt;: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NLPJCL/RAG-Retrieval&quot;&gt;RAG-Retrieval&lt;/a&gt;&lt;/strong&gt;: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. &lt;a href=&quot;https://zhuanlan.zhihu.com/p/987727357&quot;&gt;[blog]&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/Qihoo360/360-LLaMA-Factory&quot;&gt;360-LLaMA-Factory&lt;/a&gt;&lt;/strong&gt;: A modified library that supports long sequence SFT &amp;amp; DPO using ring attention.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://novasky-ai.github.io/posts/sky-t1/&quot;&gt;Sky-T1&lt;/a&gt;&lt;/strong&gt;: An o1-like model fine-tuned by NovaSky AI with very small cost.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&quot;&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Please follow the model licenses to use the corresponding model weights: &lt;a href=&quot;https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf&quot;&gt;Baichuan 2&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/spaces/bigscience/license&quot;&gt;BLOOM&lt;/a&gt; / &lt;a href=&quot;https://github.com/THUDM/ChatGLM3/raw/main/MODEL_LICENSE&quot;&gt;ChatGLM3&lt;/a&gt; / &lt;a href=&quot;https://cohere.com/c4ai-cc-by-nc-license&quot;&gt;Command R&lt;/a&gt; / &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-LLM/raw/main/LICENSE-MODEL&quot;&gt;DeepSeek&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt&quot;&gt;Falcon&lt;/a&gt; / &lt;a href=&quot;https://ai.google.dev/gemma/terms&quot;&gt;Gemma&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE&quot;&gt;GLM-4&lt;/a&gt; / &lt;a href=&quot;https://github.com/openai/gpt-2/raw/master/LICENSE&quot;&gt;GPT-2&lt;/a&gt; / &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&quot;&gt;Granite&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE&quot;&gt;Index&lt;/a&gt; / &lt;a href=&quot;https://github.com/InternLM/InternLM#license&quot;&gt;InternLM&lt;/a&gt; / &lt;a href=&quot;https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md&quot;&gt;Llama&lt;/a&gt; / &lt;a href=&quot;https://ai.meta.com/llama/license/&quot;&gt;Llama 2 (LLaVA-1.5)&lt;/a&gt; / &lt;a href=&quot;https://llama.meta.com/llama3/license/&quot;&gt;Llama 3&lt;/a&gt; / &lt;a href=&quot;https://github.com/OpenBMB/MiniCPM/raw/main/MiniCPM%20Model%20License.md&quot;&gt;MiniCPM&lt;/a&gt; / &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&quot;&gt;Mistral/Mixtral/Pixtral&lt;/a&gt; / &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&quot;&gt;OLMo&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx&quot;&gt;Phi-1.5/Phi-2&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE&quot;&gt;Phi-3/Phi-4&lt;/a&gt; / &lt;a href=&quot;https://github.com/QwenLM/Qwen/raw/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT&quot;&gt;Qwen&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf&quot;&gt;Skywork&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement&quot;&gt;StarCoder 2&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf&quot;&gt;TeleChat2&lt;/a&gt; / &lt;a href=&quot;https://github.com/xverse-ai/XVERSE-13B/raw/main/MODEL_LICENSE.pdf&quot;&gt;XVERSE&lt;/a&gt; / &lt;a href=&quot;https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE&quot;&gt;Yi&lt;/a&gt; / &lt;a href=&quot;https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/LICENSE&quot;&gt;Yi-1.5&lt;/a&gt; / &lt;a href=&quot;https://github.com/IEIT-Yuan/Yuan-2.0/raw/main/LICENSE-Yuan&quot;&gt;Yuan 2&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If this work is helpful, please kindly cite as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inproceedings{zheng2024llamafactory,
  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},
  address={Bangkok, Thailand},
  publisher={Association for Computational Linguistics},
  year={2024},
  url={http://arxiv.org/abs/2403.13372}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;This repo benefits from &lt;a href=&quot;https://github.com/huggingface/peft&quot;&gt;PEFT&lt;/a&gt;, &lt;a href=&quot;https://github.com/huggingface/trl&quot;&gt;TRL&lt;/a&gt;, &lt;a href=&quot;https://github.com/artidoro/qlora&quot;&gt;QLoRA&lt;/a&gt; and &lt;a href=&quot;https://github.com/lm-sys/FastChat&quot;&gt;FastChat&lt;/a&gt;. Thanks for their wonderful works.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenBB-finance/OpenBB</title>
      <link>https://github.com/OpenBB-finance/OpenBB</link>
      <description>&lt;p&gt;Investment Research for Everyone, Everywhere.&lt;/p&gt;&lt;hr&gt;&lt;br&gt; 
&lt;img src=&quot;https://github.com/OpenBB-finance/OpenBB/raw/develop/images/platform-light.svg?raw=true#gh-light-mode-only&quot; alt=&quot;OpenBB Platform logo&quot; width=&quot;600&quot;&gt; 
&lt;img src=&quot;https://github.com/OpenBB-finance/OpenBB/raw/develop/images/platform-dark.svg?raw=true#gh-dark-mode-only&quot; alt=&quot;OpenBB Platform logo&quot; width=&quot;600&quot;&gt; 
&lt;br&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;a href=&quot;https://x.com/openbb_finance&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;amp;label=Follow%20%40openbb_finance&quot; alt=&quot;Twitter&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.com/invite/xPHTuHCmuV&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/831165782750789672&quot; alt=&quot;Discord Shield&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode&quot; alt=&quot;Open in Dev Containers&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://codespaces.new/OpenBB-finance/OpenBB&quot;&gt; &lt;img src=&quot;https://github.com/codespaces/badge.svg?sanitize=true&quot; height=&quot;20&quot;&gt; &lt;/a&gt; &lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb&quot;&gt; &lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/openbb/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/openbb?color=blue&amp;amp;label=PyPI%20Package&quot; alt=&quot;PyPI&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The first financial Platform that is free and fully open source.&lt;/p&gt; 
&lt;p&gt;The OpenBB Platform offers access to equity, options, crypto, forex, macro economy, fixed income, and more while also offering a broad range of extensions to enhance the user experience according to their needs.&lt;/p&gt; 
&lt;p&gt;Sign up to the &lt;a href=&quot;https://my.openbb.co/login&quot;&gt;OpenBB Hub&lt;/a&gt; to get the most out of the OpenBB ecosystem.&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;If you are looking for our &lt;strong&gt;FREE&lt;/strong&gt; AI-powered Research and Analytics Workspace, you can find it here: &lt;a href=&quot;https://pro.openbb.co&quot;&gt;pro.openbb.co&lt;/a&gt;.&lt;/p&gt; 
&lt;a href=&quot;https://pro.openbb.co&quot;&gt; 
 &lt;div align=&quot;center&quot;&gt; 
  &lt;img src=&quot;https://openbb.co/api/image?src=https%253A%252F%252Fopenbb-cms.directus.app%252Fassets%252Ff431ed60-5e46-439a-a9f7-4b06e72d0720.png&amp;amp;width=2400&amp;amp;height=1552&amp;amp;fit=cover&amp;amp;position=center&amp;amp;background%5B%5D=0&amp;amp;background%5B%5D=0&amp;amp;background%5B%5D=0&amp;amp;background%5B%5D=0&amp;amp;quality=100&amp;amp;compressionLevel=9&amp;amp;loop=0&amp;amp;delay=100&amp;amp;crop=null&quot; alt=&quot;Logo&quot; width=&quot;600&quot;&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;We also have an open source AI financial analyst agent that can access all of the data within OpenBB, and that repo can be found &lt;a href=&quot;https://github.com/OpenBB-finance/openbb-agents&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;hr&gt; 
&lt;!-- TABLE OF CONTENTS --&gt; 
&lt;details closed=&quot;closed&quot;&gt; 
 &lt;summary&gt;&lt;h2 style=&quot;display: inline-block&quot;&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#1-installation&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#2-contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#3-license&quot;&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#4-disclaimer&quot;&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#5-contacts&quot;&gt;Contacts&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#6-star-history&quot;&gt;Star History&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#7-contributors&quot;&gt;Contributors&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;1. Installation&lt;/h2&gt; 
&lt;p&gt;The OpenBB Platform can be installed as a &lt;a href=&quot;https://pypi.org/project/openbb/&quot;&gt;PyPI package&lt;/a&gt; by running &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process in the &lt;a href=&quot;https://docs.openbb.co/platform/installation&quot;&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;OpenBB Platform CLI installation&lt;/h3&gt; 
&lt;p&gt;The OpenBB Platform CLI is a command-line interface that allows you to access the OpenBB Platform directly from your command line.&lt;/p&gt; 
&lt;p&gt;It can be installed by running &lt;code&gt;pip install openbb-cli&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process in the &lt;a href=&quot;https://docs.openbb.co/cli/installation&quot;&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;2. Contributing&lt;/h2&gt; 
&lt;p&gt;There are three main ways of contributing to this project. (Hopefully you have starred the project by now ⭐️)&lt;/p&gt; 
&lt;h3&gt;Become a Contributor&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;More information on our &lt;a href=&quot;https://docs.openbb.co/platform/developer_guide/contributing&quot;&gt;Contributing Documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Create a GitHub ticket&lt;/h3&gt; 
&lt;p&gt;Before creating a ticket make sure the one you are creating doesn&#39;t exist already &lt;a href=&quot;https://github.com/OpenBB-finance/OpenBB/issues&quot;&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md&amp;amp;title=%5BBug%5D&quot;&gt;Report bug&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=enhancement.md&amp;amp;title=%5BIMPROVE%5D&quot;&gt;Suggest improvement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=new+feature&amp;amp;template=feature_request.md&amp;amp;title=%5BFR%5D&quot;&gt;Request a feature&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Provide feedback&lt;/h3&gt; 
&lt;p&gt;We are most active on &lt;a href=&quot;https://openbb.co/discord&quot;&gt;our Discord&lt;/a&gt;, but feel free to reach out to us in any of &lt;a href=&quot;https://openbb.co/links&quot;&gt;our social media&lt;/a&gt; for feedback.&lt;/p&gt; 
&lt;h2&gt;3. License&lt;/h2&gt; 
&lt;p&gt;Distributed under the AGPLv3 License. See &lt;a href=&quot;https://github.com/OpenBB-finance/OpenBB/raw/main/LICENSE&quot;&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;4. Disclaimer&lt;/h2&gt; 
&lt;p&gt;Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment amount, and may not be suitable for all investors.&lt;/p&gt; 
&lt;p&gt;Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.&lt;/p&gt; 
&lt;p&gt;The data contained in the OpenBB Platform is not necessarily accurate.&lt;/p&gt; 
&lt;p&gt;OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.&lt;/p&gt; 
&lt;p&gt;All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.&lt;/p&gt; 
&lt;p&gt;Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.&lt;/p&gt; 
&lt;h2&gt;5. Contacts&lt;/h2&gt; 
&lt;p&gt;If you have any questions about the platform or anything OpenBB, feel free to email us at &lt;code&gt;support@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you want to say hi, or are interested in partnering with us, feel free to reach us at &lt;code&gt;hello@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Any of our social media platforms: &lt;a href=&quot;https://openbb.co/links&quot;&gt;openbb.co/links&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. Star History&lt;/h2&gt; 
&lt;p&gt;This is a proxy of our growth and that we are just getting started.&lt;/p&gt; 
&lt;p&gt;But for more metrics important to us check &lt;a href=&quot;https://openbb.co/open&quot;&gt;openbb.co/open&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;7. Contributors&lt;/h2&gt; 
&lt;p&gt;OpenBB wouldn&#39;t be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.&lt;/p&gt; 
&lt;a href=&quot;https://github.com/OpenBB-finance/OpenBB/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB&quot; width=&quot;800&quot;&gt; &lt;/a&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>browser-use/browser-use</title>
      <link>https://github.com/browser-use/browser-use</link>
      <description>&lt;p&gt;Make websites accessible for AI agents&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;./static/browser-use-dark.png&quot;&gt; 
 &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;./static/browser-use.png&quot;&gt; 
 &lt;img alt=&quot;Shows a black Browser Use Logo in light color mode and a white one in dark color mode.&quot; src=&quot;https://raw.githubusercontent.com/browser-use/browser-use/main/static/browser-use.png&quot; width=&quot;full&quot;&gt; 
&lt;/picture&gt; 
&lt;h1 align=&quot;center&quot;&gt;Enable AI to control your browser 🤖&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/gregpr07/browser-use/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/gregpr07/browser-use?style=social&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://link.browser-use.com/discord&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/1303749220842340412?color=7289DA&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=white&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://cloud.browser-use.com&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Cloud-%E2%98%81%EF%B8%8F-blue&quot; alt=&quot;Cloud&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://docs.browser-use.com&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-%F0%9F%93%95-blue&quot; alt=&quot;Documentation&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://x.com/gregpr07&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/Gregor?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://x.com/mamagnus00&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/Magnus?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://app.workweave.ai/reports/repository/org_T5Pvn3UBswTHIsN1dWS3voPg/881458615&quot;&gt;&lt;img src=&quot;https://img.shields.io/endpoint?url=https%3A%2F%2Fapp.workweave.ai%2Fapi%2Frepository%2Fbadge%2Forg_T5Pvn3UBswTHIsN1dWS3voPg%2F881458615&amp;amp;labelColor=#EC6341&quot; alt=&quot;Weave Badge&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;🌐 Browser-use is the easiest way to connect your AI agents with the browser.&lt;/p&gt; 
&lt;p&gt;💡 See what others are building and share your projects in our &lt;a href=&quot;https://link.browser-use.com/discord&quot;&gt;Discord&lt;/a&gt;! Want Swag? Check out our &lt;a href=&quot;https://browsermerch.com&quot;&gt;Merch store&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;🌤️ Skip the setup - try our &lt;b&gt;hosted version&lt;/b&gt; for instant browser automation! &lt;b&gt;&lt;a href=&quot;https://cloud.browser-use.com&quot;&gt;Try the cloud ☁︎&lt;/a&gt;&lt;/b&gt;.&lt;/p&gt; 
&lt;h1&gt;Quick start&lt;/h1&gt; 
&lt;p&gt;With pip (Python&amp;gt;=3.11):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install browser-use
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install Playwright:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;playwright install chromium
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Spin up your agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from langchain_openai import ChatOpenAI
from browser_use import Agent
import asyncio
from dotenv import load_dotenv
load_dotenv()

async def main():
    agent = Agent(
        task=&quot;Compare the price of gpt-4o and DeepSeek-V3&quot;,
        llm=ChatOpenAI(model=&quot;gpt-4o&quot;),
    )
    await agent.run()

asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Add your API keys for the provider you want to use to your &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;OPENAI_API_KEY=
ANTHROPIC_API_KEY=
AZURE_ENDPOINT=
AZURE_OPENAI_API_KEY=
GEMINI_API_KEY=
DEEPSEEK_API_KEY=
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For other settings, models, and more, check out the &lt;a href=&quot;https://docs.browser-use.com&quot;&gt;documentation 📕&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Test with UI&lt;/h3&gt; 
&lt;p&gt;You can test &lt;a href=&quot;https://github.com/browser-use/web-ui&quot;&gt;browser-use with a UI repository&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Or simply run the gradio example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;uv pip install gradio
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python examples/ui/gradio_demo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Demos&lt;/h1&gt; 
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/browser-use/browser-use/raw/main/examples/use-cases/shopping.py&quot;&gt;Task&lt;/a&gt;: Add grocery items to cart, and checkout.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=L2Ya9PYNns8&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/d9359085-bde6-41d4-aa4e-6520d0221872&quot; alt=&quot;AI Did My Groceries&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;p&gt;Prompt: Add my latest LinkedIn follower to my leads in Salesforce.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/1440affc-a552-442e-b702-d0d3b277b0ae&quot; alt=&quot;LinkedIn to Salesforce&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/browser-use/browser-use/raw/main/examples/use-cases/find_and_apply_to_jobs.py&quot;&gt;Prompt&lt;/a&gt;: Read my CV &amp;amp; find ML jobs, save them to a file, and then start applying for them in new tabs, if you need help, ask me.&#39;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/171fb4d6-0355-46f2-863e-edb04a828d04&quot;&gt;https://github.com/user-attachments/assets/171fb4d6-0355-46f2-863e-edb04a828d04&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/browser-use/browser-use/raw/main/examples/browser/real_browser.py&quot;&gt;Prompt&lt;/a&gt;: Write a letter in Google Docs to my Papa, thanking him for everything, and save the document as a PDF.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/242ade3e-15bc-41c2-988f-cbc5415a66aa&quot; alt=&quot;Letter to Papa&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/browser-use/browser-use/raw/main/examples/custom-functions/save_to_file_hugging_face.py&quot;&gt;Prompt&lt;/a&gt;: Look up models with a license of cc-by-sa-4.0 and sort by most likes on Hugging face, save top 5 to file.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/de73ee39-432c-4b97-b4e8-939fd7f323b3&quot;&gt;https://github.com/user-attachments/assets/de73ee39-432c-4b97-b4e8-939fd7f323b3&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;h2&gt;More examples&lt;/h2&gt; 
&lt;p&gt;For more examples see the &lt;a href=&quot;https://raw.githubusercontent.com/browser-use/browser-use/main/examples&quot;&gt;examples&lt;/a&gt; folder or join the &lt;a href=&quot;https://link.browser-use.com/discord&quot;&gt;Discord&lt;/a&gt; and show off your project.&lt;/p&gt; 
&lt;h1&gt;Vision&lt;/h1&gt; 
&lt;p&gt;Tell your computer what to do, and it gets it done.&lt;/p&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;h3&gt;Agent&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Improve agent memory (summarize, compress, RAG, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Enhance planning capabilities (load website specific context)&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Reduce token consumption (system prompt, DOM state)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;DOM Extraction&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Improve extraction for datepickers, dropdowns, special elements&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Improve state representation for UI elements&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Rerunning tasks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; LLM as fallback&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Make it easy to define workfow templates where LLM fills in the details&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Return playwright script from the agent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Datasets&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Create datasets for complex tasks&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Benchmark various models against each other&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Fine-tuning models for specific tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;User Experience&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Human-in-the-loop execution&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Improve the generated GIF quality&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled&gt; Create various demos for tutorial execution, job application, QA testing, social media, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We love contributions! Feel free to open issues for bugs or feature requests. To contribute to the docs, check out the &lt;code&gt;/docs&lt;/code&gt; folder.&lt;/p&gt; 
&lt;h2&gt;Local Setup&lt;/h2&gt; 
&lt;p&gt;To learn more about the library, check out the &lt;a href=&quot;https://docs.browser-use.com/development/local-setup&quot;&gt;local setup 📕&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Cooperations&lt;/h2&gt; 
&lt;p&gt;We are forming a commission to define best practices for UI/UX design for browser agents. Together, we&#39;re exploring how software redesign improves the performance of AI agents and gives these companies a competitive advantage by designing their existing software to be at the forefront of the agent age.&lt;/p&gt; 
&lt;p&gt;Email &lt;a href=&quot;mailto:tbiddle@loop11.com?subject=I%20want%20to%20join%20the%20UI/UX%20commission%20for%20AI%20agents&amp;amp;body=Hi%20Toby%2C%0A%0AI%20found%20you%20in%20the%20browser-use%20GitHub%20README.%0A%0A&quot;&gt;Toby&lt;/a&gt; to apply for a seat on the committee.&lt;/p&gt; 
&lt;h2&gt;Swag&lt;/h2&gt; 
&lt;p&gt;Want to show off your Browser-use swag? Check out our &lt;a href=&quot;https://browsermerch.com&quot;&gt;Merch store&lt;/a&gt;. Good contributors will receive swag for free 👀.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use Browser Use in your research or project, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@software{browser_use2024,
  author = {Müller, Magnus and Žunič, Gregor},
  title = {Browser Use: Enable AI to control your browser},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/browser-use/browser-use}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://github.com/user-attachments/assets/06fa3078-8461-4560-b434-445510c1766f&quot; width=&quot;400&quot;&gt; 
 &lt;p&gt;&lt;a href=&quot;https://x.com/gregpr07&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/Gregor?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://x.com/mamagnus00&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/Magnus?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt;
  Made with ❤️ in Zurich and San Francisco 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>allenai/olmocr</title>
      <link>https://github.com/allenai/olmocr</link>
      <description>&lt;p&gt;Toolkit for linearizing PDFs for LLM datasets/training&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;!-- &lt;img src=&quot;https://github.com/allenai/OLMo/assets/8812459/774ac485-a535-4768-8f7c-db7be20f5cc3&quot; width=&quot;300&quot;/&gt; --&gt; 
 &lt;img src=&quot;https://github.com/user-attachments/assets/d70c8644-3e64-4230-98c3-c52fddaeccb6&quot; alt=&quot;olmOCR Logo&quot; width=&quot;300&quot;&gt; 
 &lt;br&gt; 
 &lt;br&gt; 
 &lt;h1&gt;olmOCR&lt;/h1&gt; 
&lt;/div&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://github.com/allenai/OLMo/raw/main/LICENSE&quot;&gt; &lt;img alt=&quot;GitHub License&quot; src=&quot;https://img.shields.io/github/license/allenai/OLMo&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://github.com/allenai/olmocr/releases&quot;&gt; &lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/release/allenai/olmocr.svg?sanitize=true&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://olmocr.allenai.org/papers/olmocr.pdf&quot;&gt; &lt;img alt=&quot;Tech Report&quot; src=&quot;https://img.shields.io/badge/Paper-olmOCR-blue&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://olmocr.allenai.org&quot;&gt; &lt;img alt=&quot;Demo&quot; src=&quot;https://img.shields.io/badge/Ai2-Demo-F0529C&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://discord.gg/sZq3jTNVNG&quot;&gt; &lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/badge/Discord%20-%20blue?style=flat&amp;amp;logo=discord&amp;amp;label=Ai2&amp;amp;color=%235B65E9&quot;&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;A toolkit for training language models to work with PDF documents in the wild.&lt;/p&gt; 
&lt;p&gt;Try the online demo: &lt;a href=&quot;https://olmocr.allenai.org/&quot;&gt;https://olmocr.allenai.org/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;What is included:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A prompting strategy to get really good natural text parsing using ChatGPT 4o - &lt;a href=&quot;https://github.com/allenai/olmocr/raw/main/olmocr/data/buildsilver.py&quot;&gt;buildsilver.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;An side-by-side eval toolkit for comparing different pipeline versions - &lt;a href=&quot;https://github.com/allenai/olmocr/raw/main/olmocr/eval/runeval.py&quot;&gt;runeval.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Basic filtering by language and SEO spam removal - &lt;a href=&quot;https://github.com/allenai/olmocr/raw/main/olmocr/filter/filter.py&quot;&gt;filter.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Finetuning code for Qwen2-VL and Molmo-O - &lt;a href=&quot;https://github.com/allenai/olmocr/raw/main/olmocr/train/train.py&quot;&gt;train.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Processing millions of PDFs through a finetuned model using Sglang - &lt;a href=&quot;https://github.com/allenai/olmocr/raw/main/olmocr/pipeline.py&quot;&gt;pipeline.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Viewing &lt;a href=&quot;https://github.com/allenai/dolma&quot;&gt;Dolma docs&lt;/a&gt; created from PDFs - &lt;a href=&quot;https://github.com/allenai/olmocr/raw/main/olmocr/viewer/dolmaviewer.py&quot;&gt;dolmaviewer.py&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Recent NVIDIA GPU (tested on RTX 4090, L40S, A100, H100) with at least 20 GB of GPU RAM&lt;/li&gt; 
 &lt;li&gt;30GB of free disk space&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You will need to install poppler-utils and additional fonts for rendering PDF images.&lt;/p&gt; 
&lt;p&gt;Install dependencies (Ubuntu/Debian)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt-get update
sudo apt-get install poppler-utils ttf-mscorefonts-installer msttcorefonts fonts-crosextra-caladea fonts-crosextra-carlito gsfonts lcdf-typetools
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Set up a conda environment and install olmocr&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;conda create -n olmocr python=3.11
conda activate olmocr

git clone https://github.com/allenai/olmocr.git
cd olmocr

pip install -e .[gpu] --find-links https://flashinfer.ai/whl/cu124/torch2.4/flashinfer/
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Local Usage Example&lt;/h3&gt; 
&lt;p&gt;For quick testing, try the &lt;a href=&quot;https://olmocr.allen.ai/&quot;&gt;web demo&lt;/a&gt;. To run locally, a GPU is required, as inference is powered by &lt;a href=&quot;https://github.com/sgl-project/sglang&quot;&gt;sglang&lt;/a&gt; under the hood. Convert a Single PDF:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python -m olmocr.pipeline ./localworkspace --pdfs tests/gnarly_pdfs/horribleocr.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Convert Multiple PDFs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python -m olmocr.pipeline ./localworkspace --pdfs tests/gnarly_pdfs/*.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Results will be stored as JSON in &lt;code&gt;./localworkspace&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Viewing Results&lt;/h4&gt; 
&lt;p&gt;Extracted text is stored as &lt;a href=&quot;https://github.com/allenai/dolma&quot;&gt;Dolma&lt;/a&gt;-style JSONL inside of the &lt;code&gt;./localworkspace/results&lt;/code&gt; directory.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cat localworkspace/results/output_*.jsonl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;View results side-by-side with the original PDFs (uses &lt;code&gt;dolmaviewer&lt;/code&gt; command):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python -m olmocr.viewer.dolmaviewer localworkspace/results/output_*.jsonl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now open &lt;code&gt;./dolma_previews/tests_gnarly_pdfs_horribleocr_pdf.html&lt;/code&gt; in your favorite browser.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/128922d1-63e6-4d34-84f2-d7901237da1f&quot; alt=&quot;image&quot;&gt;&lt;/p&gt; 
&lt;h3&gt;Multi-node / Cluster Usage&lt;/h3&gt; 
&lt;p&gt;If you want to convert millions of PDFs, using multiple nodes running in parallel, then olmOCR supports reading your PDFs from AWS S3, and coordinating work using an AWS S3 output bucket.&lt;/p&gt; 
&lt;p&gt;For example, you can start this command on your first worker node, and it will set up a simple work queue in your AWS bucket and start converting PDFs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now on any subsequent nodes, just run this and they will start grabbing items from the same workspace queue.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are at Ai2 and want to linearize millions of PDFs efficiently using &lt;a href=&quot;https://www.beaker.org&quot;&gt;beaker&lt;/a&gt;, just add the &lt;code&gt;--beaker&lt;/code&gt; flag. This will prepare the workspace on your local machine, and then launch N GPU workers in the cluster to start converting PDFs.&lt;/p&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf --beaker --beaker_gpus 4
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Full documentation for the pipeline&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python -m olmocr.pipeline --help
usage: pipeline.py [-h] [--pdfs PDFS] [--workspace_profile WORKSPACE_PROFILE] [--pdf_profile PDF_PROFILE] [--pages_per_group PAGES_PER_GROUP]
                   [--max_page_retries MAX_PAGE_RETRIES] [--max_page_error_rate MAX_PAGE_ERROR_RATE] [--workers WORKERS] [--apply_filter] [--stats] [--model MODEL]
                   [--model_max_context MODEL_MAX_CONTEXT] [--model_chat_template MODEL_CHAT_TEMPLATE] [--target_longest_image_dim TARGET_LONGEST_IMAGE_DIM]
                   [--target_anchor_text_len TARGET_ANCHOR_TEXT_LEN] [--beaker] [--beaker_workspace BEAKER_WORKSPACE] [--beaker_cluster BEAKER_CLUSTER]
                   [--beaker_gpus BEAKER_GPUS] [--beaker_priority BEAKER_PRIORITY]
                   workspace

Manager for running millions of PDFs through a batch inference pipeline

positional arguments:
  workspace             The filesystem path where work will be stored, can be a local folder, or an s3 path if coordinating work with many workers, s3://bucket/prefix/

options:
  -h, --help            show this help message and exit
  --pdfs PDFS           Path to add pdfs stored in s3 to the workspace, can be a glob path s3://bucket/prefix/*.pdf or path to file containing list of pdf paths
  --workspace_profile WORKSPACE_PROFILE
                        S3 configuration profile for accessing the workspace
  --pdf_profile PDF_PROFILE
                        S3 configuration profile for accessing the raw pdf documents
  --pages_per_group PAGES_PER_GROUP
                        Aiming for this many pdf pages per work item group
  --max_page_retries MAX_PAGE_RETRIES
                        Max number of times we will retry rendering a page
  --max_page_error_rate MAX_PAGE_ERROR_RATE
                        Rate of allowable failed pages in a document, 1/250 by default
  --workers WORKERS     Number of workers to run at a time
  --apply_filter        Apply basic filtering to English pdfs which are not forms, and not likely seo spam
  --stats               Instead of running any job, reports some statistics about the current workspace
  --model MODEL         List of paths where you can find the model to convert this pdf. You can specify several different paths here, and the script will try to use the
                        one which is fastest to access
  --model_max_context MODEL_MAX_CONTEXT
                        Maximum context length that the model was fine tuned under
  --model_chat_template MODEL_CHAT_TEMPLATE
                        Chat template to pass to sglang server
  --target_longest_image_dim TARGET_LONGEST_IMAGE_DIM
                        Dimension on longest side to use for rendering the pdf pages
  --target_anchor_text_len TARGET_ANCHOR_TEXT_LEN
                        Maximum amount of anchor text to use (characters)
  --beaker              Submit this job to beaker instead of running locally
  --beaker_workspace BEAKER_WORKSPACE
                        Beaker workspace to submit to
  --beaker_cluster BEAKER_CLUSTER
                        Beaker clusters you want to run on
  --beaker_gpus BEAKER_GPUS
                        Number of gpu replicas to run
  --beaker_priority BEAKER_PRIORITY
                        Beaker priority level for the job
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Team&lt;/h2&gt; 
&lt;!-- start team --&gt; 
&lt;p&gt;&lt;strong&gt;olmOCR&lt;/strong&gt; is developed and maintained by the AllenNLP team, backed by &lt;a href=&quot;https://allenai.org/&quot;&gt;the Allen Institute for Artificial Intelligence (AI2)&lt;/a&gt;. AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. To learn more about who specifically contributed to this codebase, see &lt;a href=&quot;https://github.com/allenai/olmocr/graphs/contributors&quot;&gt;our contributors&lt;/a&gt; page.&lt;/p&gt; 
&lt;!-- end team --&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;!-- start license --&gt; 
&lt;p&gt;&lt;strong&gt;olmOCR&lt;/strong&gt; is licensed under &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;&gt;Apache 2.0&lt;/a&gt;. A full copy of the license can be found &lt;a href=&quot;https://github.com/allenai/olmocr/raw/main/LICENSE&quot;&gt;on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- end license --&gt; 
&lt;h2&gt;Citing&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@misc{olmocr,
      title={{olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models}},
      author={Jake Poznanski and Jon Borchardt and Jason Dunkelberger and Regan Huff and Daniel Lin and Aman Rangapur and Christopher Wilhelm and Kyle Lo and Luca Soldaini},
      year={2025},
      eprint={2502.18443},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.18443},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>huggingface/lerobot</title>
      <link>https://github.com/huggingface/lerobot</link>
      <description>&lt;p&gt;🤗 LeRobot: Making AI for Robotics more accessible with end-to-end learning&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; 
 &lt;picture&gt; 
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;media/lerobot-logo-thumbnail.png&quot;&gt; 
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;media/lerobot-logo-thumbnail.png&quot;&gt; 
  &lt;img alt=&quot;LeRobot, Hugging Face Robotics Library&quot; src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/lerobot-logo-thumbnail.png&quot; style=&quot;max-width: 100%;&quot;&gt; 
 &lt;/picture&gt; &lt;br&gt; &lt;br&gt; &lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;a href=&quot;https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml?query=branch%3Amain&quot;&gt;&lt;img src=&quot;https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml/badge.svg?branch=main&quot; alt=&quot;Tests&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://codecov.io/gh/huggingface/lerobot&quot;&gt;&lt;img src=&quot;https://codecov.io/gh/huggingface/lerobot/branch/main/graph/badge.svg?token=TODO&quot; alt=&quot;Coverage&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.python.org/downloads/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/pyversions/lerobot&quot; alt=&quot;Python versions&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/huggingface/lerobot/raw/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&quot; alt=&quot;License&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/lerobot/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/status/lerobot&quot; alt=&quot;Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/lerobot/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/lerobot&quot; alt=&quot;Version&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/huggingface/lerobot/tree/main/examples&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Examples-green.svg?sanitize=true&quot; alt=&quot;Examples&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/huggingface/lerobot/raw/main/CODE_OF_CONDUCT.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Contributor%20Covenant-v2.1%20adopted-ff69b4.svg?sanitize=true&quot; alt=&quot;Contributor Covenant&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/s3KuuzsPFb&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/C5P34WJ68S?style=flat&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2 align=&quot;center&quot;&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/huggingface/lerobot/raw/main/examples/10_use_so100.md&quot;&gt; Build Your Own SO-100 Robot!&lt;/a&gt;&lt;/p&gt; &lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/so100/leader_follower.webp?raw=true&quot; alt=&quot;SO-100 leader and follower arms&quot; title=&quot;SO-100 leader and follower arms&quot; width=&quot;50%&quot;&gt; 
 &lt;p&gt;&lt;strong&gt;Meet the SO-100 – Just $110 per arm!&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Train it in minutes with a few simple moves on your laptop.&lt;/p&gt; 
 &lt;p&gt;Then sit back and watch your creation act autonomously! 🤯&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://github.com/huggingface/lerobot/raw/main/examples/10_use_so100.md&quot;&gt; Get the full SO-100 tutorial here.&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;Want to take it to the next level? Make your SO-100 mobile by building LeKiwi!&lt;/p&gt; 
 &lt;p&gt;Check out the &lt;a href=&quot;https://github.com/huggingface/lerobot/raw/main/examples/11_use_lekiwi.md&quot;&gt;LeKiwi tutorial&lt;/a&gt; and bring your robot to life on wheels.&lt;/p&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/lekiwi/kiwi.webp?raw=true&quot; alt=&quot;LeKiwi mobile robot&quot; title=&quot;LeKiwi mobile robot&quot; width=&quot;50%&quot;&gt; 
&lt;/div&gt; 
&lt;br&gt; 
&lt;h3 align=&quot;center&quot;&gt; &lt;p&gt;LeRobot: State-of-the-art AI for real-world robotics&lt;/p&gt; &lt;/h3&gt; 
&lt;hr&gt; 
&lt;p&gt;🤗 LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry to robotics so that everyone can contribute and benefit from sharing datasets and pretrained models.&lt;/p&gt; 
&lt;p&gt;🤗 LeRobot contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.&lt;/p&gt; 
&lt;p&gt;🤗 LeRobot already provides a set of pretrained models, datasets with human collected demonstrations, and simulation environments to get started without assembling a robot. In the coming weeks, the plan is to add more and more support for real-world robotics on the most affordable and capable robots out there.&lt;/p&gt; 
&lt;p&gt;🤗 LeRobot hosts pretrained models and datasets on this Hugging Face community page: &lt;a href=&quot;https://huggingface.co/lerobot&quot;&gt;huggingface.co/lerobot&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Examples of pretrained models on simulation environments&lt;/h4&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/aloha_act.gif&quot; width=&quot;100%&quot; alt=&quot;ACT policy on ALOHA env&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/simxarm_tdmpc.gif&quot; width=&quot;100%&quot; alt=&quot;TDMPC policy on SimXArm env&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/gym/pusht_diffusion.gif&quot; width=&quot;100%&quot; alt=&quot;Diffusion policy on PushT env&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;ACT policy on ALOHA env&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;TDMPC policy on SimXArm env&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Diffusion policy on PushT env&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Acknowledgment&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Thanks to Tony Zhao, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from &lt;a href=&quot;https://tonyzhaozh.github.io/aloha&quot;&gt;ALOHA&lt;/a&gt; and &lt;a href=&quot;https://mobile-aloha.github.io&quot;&gt;Mobile ALOHA&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from &lt;a href=&quot;https://diffusion-policy.cs.columbia.edu&quot;&gt;Diffusion Policy&lt;/a&gt; and &lt;a href=&quot;https://umi-gripper.github.io&quot;&gt;UMI Gripper&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from &lt;a href=&quot;https://github.com/nicklashansen/tdmpc&quot;&gt;TDMPC&lt;/a&gt; and &lt;a href=&quot;https://www.yunhaifeng.com/FOWM&quot;&gt;FOWM&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Thanks to Antonio Loquercio and Ashish Kumar for their early support.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href=&quot;https://sjlee.cc/&quot;&gt;Seungjae (Jay) Lee&lt;/a&gt;, &lt;a href=&quot;https://mahis.life/&quot;&gt;Mahi Shafiullah&lt;/a&gt; and colleagues for open sourcing &lt;a href=&quot;https://sjlee.cc/vq-bet/&quot;&gt;VQ-BeT&lt;/a&gt; policy and helping us adapt the codebase to our repository. The policy is adapted from &lt;a href=&quot;https://github.com/jayLEE0301/vq_bet_official&quot;&gt;VQ-BeT repo&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Download our source code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/huggingface/lerobot.git
cd lerobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Create a virtual environment with Python 3.10 and activate it, e.g. with &lt;a href=&quot;https://docs.anaconda.com/free/miniconda/index.html&quot;&gt;&lt;code&gt;miniconda&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;conda create -y -n lerobot python=3.10
conda activate lerobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When using &lt;code&gt;miniconda&lt;/code&gt;, if you don&#39;t have &lt;code&gt;ffmpeg&lt;/code&gt; in your environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;conda install ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install 🤗 LeRobot:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install --no-binary=av -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; If you encounter build errors, you may need to install additional dependencies (&lt;code&gt;cmake&lt;/code&gt;, &lt;code&gt;build-essential&lt;/code&gt;, and &lt;code&gt;ffmpeg libs&lt;/code&gt;). On Linux, run: &lt;code&gt;sudo apt-get install cmake build-essential python-dev pkg-config libavformat-dev libavcodec-dev libavdevice-dev libavutil-dev libswscale-dev libswresample-dev libavfilter-dev pkg-config&lt;/code&gt;. For other systems, see: &lt;a href=&quot;https://pyav.org/docs/develop/overview/installation.html#bring-your-own-ffmpeg&quot;&gt;Compiling PyAV&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For simulations, 🤗 LeRobot comes with gymnasium environments that can be installed as extras:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/huggingface/gym-aloha&quot;&gt;aloha&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/huggingface/gym-xarm&quot;&gt;xarm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/huggingface/gym-pusht&quot;&gt;pusht&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For instance, to install 🤗 LeRobot with aloha and pusht, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install --no-binary=av -e &quot;.[aloha, pusht]&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use &lt;a href=&quot;https://docs.wandb.ai/quickstart&quot;&gt;Weights and Biases&lt;/a&gt; for experiment tracking, log in with&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;wandb login
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(note: you will also need to enable WandB in the configuration. See below.)&lt;/p&gt; 
&lt;h2&gt;Walkthrough&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;.
├── examples             # contains demonstration examples, start here to learn about LeRobot
|   └── advanced         # contains even more examples for those who have mastered the basics
├── lerobot
|   ├── configs          # contains config classes with all options that you can override in the command line
|   ├── common           # contains classes and utilities
|   |   ├── datasets       # various datasets of human demonstrations: aloha, pusht, xarm
|   |   ├── envs           # various sim environments: aloha, pusht, xarm
|   |   ├── policies       # various policies: act, diffusion, tdmpc
|   |   ├── robot_devices  # various real devices: dynamixel motors, opencv cameras, koch robots
|   |   └── utils          # various utilities
|   └── scripts          # contains functions to execute via command line
|       ├── eval.py                 # load policy and evaluate it on an environment
|       ├── train.py                # train a policy via imitation learning and/or reinforcement learning
|       ├── control_robot.py        # teleoperate a real robot, record data, run a policy
|       ├── push_dataset_to_hub.py  # convert your dataset into LeRobot dataset format and upload it to the Hugging Face hub
|       └── visualize_dataset.py    # load a dataset and render its demonstrations
├── outputs               # contains results of scripts execution: logs, videos, model checkpoints
└── tests                 # contains pytest utilities for continuous integration
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Visualize datasets&lt;/h3&gt; 
&lt;p&gt;Check out &lt;a href=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/1_load_lerobot_dataset.py&quot;&gt;example 1&lt;/a&gt; that illustrates how to use our dataset class which automatically downloads data from the Hugging Face hub.&lt;/p&gt; 
&lt;p&gt;You can also locally visualize episodes from a dataset on the hub by executing our script from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python lerobot/scripts/visualize_dataset.py \
    --repo-id lerobot/pusht \
    --episode-index 0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or from a dataset in a local folder with the &lt;code&gt;root&lt;/code&gt; option and the &lt;code&gt;--local-files-only&lt;/code&gt; (in the following case the dataset will be searched for in &lt;code&gt;./my_local_data_dir/lerobot/pusht&lt;/code&gt;)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python lerobot/scripts/visualize_dataset.py \
    --repo-id lerobot/pusht \
    --root ./my_local_data_dir \
    --local-files-only 1 \
    --episode-index 0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It will open &lt;code&gt;rerun.io&lt;/code&gt; and display the camera streams, robot states and actions, like this:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144&quot;&gt;https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Our script can also visualize datasets stored on a distant server. See &lt;code&gt;python lerobot/scripts/visualize_dataset.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; 
&lt;h3&gt;The &lt;code&gt;LeRobotDataset&lt;/code&gt; format&lt;/h3&gt; 
&lt;p&gt;A dataset in &lt;code&gt;LeRobotDataset&lt;/code&gt; format is very simple to use. It can be loaded from a repository on the Hugging Face hub or a local folder simply with e.g. &lt;code&gt;dataset = LeRobotDataset(&quot;lerobot/aloha_static_coffee&quot;)&lt;/code&gt; and can be indexed into like any Hugging Face and PyTorch dataset. For instance &lt;code&gt;dataset[0]&lt;/code&gt; will retrieve a single temporal frame from the dataset containing observation(s) and an action as PyTorch tensors ready to be fed to a model.&lt;/p&gt; 
&lt;p&gt;A specificity of &lt;code&gt;LeRobotDataset&lt;/code&gt; is that, rather than retrieving a single frame by its index, we can retrieve several frames based on their temporal relationship with the indexed frame, by setting &lt;code&gt;delta_timestamps&lt;/code&gt; to a list of relative times with respect to the indexed frame. For example, with &lt;code&gt;delta_timestamps = {&quot;observation.image&quot;: [-1, -0.5, -0.2, 0]}&lt;/code&gt; one can retrieve, for a given index, 4 frames: 3 &quot;previous&quot; frames 1 second, 0.5 seconds, and 0.2 seconds before the indexed frame, and the indexed frame itself (corresponding to the 0 entry). See example &lt;a href=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/1_load_lerobot_dataset.py&quot;&gt;1_load_lerobot_dataset.py&lt;/a&gt; for more details on &lt;code&gt;delta_timestamps&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Under the hood, the &lt;code&gt;LeRobotDataset&lt;/code&gt; format makes use of several ways to serialize data which can be useful to understand if you plan to work more closely with this format. We tried to make a flexible yet simple dataset format that would cover most type of features and specificities present in reinforcement learning and robotics, in simulation and in real-world, with a focus on cameras and robot states but easily extended to other types of sensory inputs as long as they can be represented by a tensor.&lt;/p&gt; 
&lt;p&gt;Here are the important details and internal structure organization of a typical &lt;code&gt;LeRobotDataset&lt;/code&gt; instantiated with &lt;code&gt;dataset = LeRobotDataset(&quot;lerobot/aloha_static_coffee&quot;)&lt;/code&gt;. The exact features will change from dataset to dataset but not the main aspects:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;dataset attributes:
  ├ hf_dataset: a Hugging Face dataset (backed by Arrow/parquet). Typical features example:
  │  ├ observation.images.cam_high (VideoFrame):
  │  │   VideoFrame = {&#39;path&#39;: path to a mp4 video, &#39;timestamp&#39; (float32): timestamp in the video}
  │  ├ observation.state (list of float32): position of an arm joints (for instance)
  │  ... (more observations)
  │  ├ action (list of float32): goal position of an arm joints (for instance)
  │  ├ episode_index (int64): index of the episode for this sample
  │  ├ frame_index (int64): index of the frame for this sample in the episode ; starts at 0 for each episode
  │  ├ timestamp (float32): timestamp in the episode
  │  ├ next.done (bool): indicates the end of en episode ; True for the last frame in each episode
  │  └ index (int64): general index in the whole dataset
  ├ episode_data_index: contains 2 tensors with the start and end indices of each episode
  │  ├ from (1D int64 tensor): first frame index for each episode — shape (num episodes,) starts with 0
  │  └ to: (1D int64 tensor): last frame index for each episode — shape (num episodes,)
  ├ stats: a dictionary of statistics (max, mean, min, std) for each feature in the dataset, for instance
  │  ├ observation.images.cam_high: {&#39;max&#39;: tensor with same number of dimensions (e.g. `(c, 1, 1)` for images, `(c,)` for states), etc.}
  │  ...
  ├ info: a dictionary of metadata on the dataset
  │  ├ codebase_version (str): this is to keep track of the codebase version the dataset was created with
  │  ├ fps (float): frame per second the dataset is recorded/synchronized to
  │  ├ video (bool): indicates if frames are encoded in mp4 video files to save space or stored as png files
  │  └ encoding (dict): if video, this documents the main options that were used with ffmpeg to encode the videos
  ├ videos_dir (Path): where the mp4 videos or png images are stored/accessed
  └ camera_keys (list of string): the keys to access camera features in the item returned by the dataset (e.g. `[&quot;observation.images.cam_high&quot;, ...]`)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;A &lt;code&gt;LeRobotDataset&lt;/code&gt; is serialised using several widespread file formats for each of its parts, namely:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;hf_dataset stored using Hugging Face datasets library serialization to parquet&lt;/li&gt; 
 &lt;li&gt;videos are stored in mp4 format to save space&lt;/li&gt; 
 &lt;li&gt;metadata are stored in plain json/jsonl files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Dataset can be uploaded/downloaded from the HuggingFace hub seamlessly. To work on a local dataset, you can specify its location with the &lt;code&gt;root&lt;/code&gt; argument if it&#39;s not in the default &lt;code&gt;~/.cache/huggingface/lerobot&lt;/code&gt; location.&lt;/p&gt; 
&lt;h3&gt;Evaluate a pretrained policy&lt;/h3&gt; 
&lt;p&gt;Check out &lt;a href=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/2_evaluate_pretrained_policy.py&quot;&gt;example 2&lt;/a&gt; that illustrates how to download a pretrained policy from Hugging Face hub, and run an evaluation on its corresponding environment.&lt;/p&gt; 
&lt;p&gt;We also provide a more capable script to parallelize the evaluation over multiple environments during the same rollout. Here is an example with a pretrained model hosted on &lt;a href=&quot;https://huggingface.co/lerobot/diffusion_pusht&quot;&gt;lerobot/diffusion_pusht&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python lerobot/scripts/eval.py \
    --policy.path=lerobot/diffusion_pusht \
    --env.type=pusht \
    --eval.batch_size=10 \
    --eval.n_episodes=10 \
    --policy.use_amp=false \
    --policy.device=cuda
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: After training your own policy, you can re-evaluate the checkpoints with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python lerobot/scripts/eval.py --policy.path={OUTPUT_DIR}/checkpoints/last/pretrained_model
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;code&gt;python lerobot/scripts/eval.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; 
&lt;h3&gt;Train your own policy&lt;/h3&gt; 
&lt;p&gt;Check out &lt;a href=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/3_train_policy.py&quot;&gt;example 3&lt;/a&gt; that illustrate how to train a model using our core library in python, and &lt;a href=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/4_train_policy_with_script.md&quot;&gt;example 4&lt;/a&gt; that shows how to use our training script from command line.&lt;/p&gt; 
&lt;p&gt;To use wandb for logging training and evaluation curves, make sure you&#39;ve run &lt;code&gt;wandb login&lt;/code&gt; as a one-time setup step. Then, when running the training command above, enable WandB in the configuration by adding &lt;code&gt;--wandb.enable=true&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;A link to the wandb logs for the run will also show up in yellow in your terminal. Here is an example of what they look like in your browser. Please also check &lt;a href=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/4_train_policy_with_script.md#typical-logs-and-metrics&quot;&gt;here&lt;/a&gt; for the explanation of some commonly used metrics in logs.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/huggingface/lerobot/main/media/wandb.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt; 
&lt;p&gt;Note: For efficiency, during training every checkpoint is evaluated on a low number of episodes. You may use &lt;code&gt;--eval.n_episodes=500&lt;/code&gt; to evaluate on more episodes than the default. Or, after training, you may want to re-evaluate your best checkpoints on more episodes or change the evaluation settings. See &lt;code&gt;python lerobot/scripts/eval.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; 
&lt;h4&gt;Reproduce state-of-the-art (SOTA)&lt;/h4&gt; 
&lt;p&gt;We provide some pretrained policies on our &lt;a href=&quot;https://huggingface.co/lerobot&quot;&gt;hub page&lt;/a&gt; that can achieve state-of-the-art performances. You can reproduce their training by loading the config from their run. Simply running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python lerobot/scripts/train.py --config_path=lerobot/diffusion_pusht
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;reproduces SOTA results for Diffusion Policy on the PushT task.&lt;/p&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;If you would like to contribute to 🤗 LeRobot, please check out our &lt;a href=&quot;https://github.com/huggingface/lerobot/raw/main/CONTRIBUTING.md&quot;&gt;contribution guide&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- ### Add a new dataset

To add a dataset to the hub, you need to login using a write-access token, which can be generated from the [Hugging Face settings](https://huggingface.co/settings/tokens):
```bash
huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
```

Then point to your raw dataset folder (e.g. `data/aloha_static_pingpong_test_raw`), and push your dataset to the hub with:
```bash
python lerobot/scripts/push_dataset_to_hub.py \
--raw-dir data/aloha_static_pingpong_test_raw \
--out-dir data \
--repo-id lerobot/aloha_static_pingpong_test \
--raw-format aloha_hdf5
```

See `python lerobot/scripts/push_dataset_to_hub.py --help` for more instructions.

If your dataset format is not supported, implement your own in `lerobot/common/datasets/push_dataset_to_hub/${raw_format}_format.py` by copying examples like [pusht_zarr](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/pusht_zarr_format.py), [umi_zarr](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/umi_zarr_format.py), [aloha_hdf5](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/aloha_hdf5_format.py), or [xarm_pkl](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/xarm_pkl_format.py). --&gt; 
&lt;h3&gt;Add a pretrained policy&lt;/h3&gt; 
&lt;p&gt;Once you have trained a policy you may upload it to the Hugging Face hub using a hub id that looks like &lt;code&gt;${hf_user}/${repo_name}&lt;/code&gt; (e.g. &lt;a href=&quot;https://huggingface.co/lerobot/diffusion_pusht&quot;&gt;lerobot/diffusion_pusht&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;You first need to find the checkpoint folder located inside your experiment directory (e.g. &lt;code&gt;outputs/train/2024-05-05/20-21-12_aloha_act_default/checkpoints/002500&lt;/code&gt;). Within that there is a &lt;code&gt;pretrained_model&lt;/code&gt; directory which should contain:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;config.json&lt;/code&gt;: A serialized version of the policy configuration (following the policy&#39;s dataclass config).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;model.safetensors&lt;/code&gt;: A set of &lt;code&gt;torch.nn.Module&lt;/code&gt; parameters, saved in &lt;a href=&quot;https://huggingface.co/docs/safetensors/index&quot;&gt;Hugging Face Safetensors&lt;/a&gt; format.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;train_config.json&lt;/code&gt;: A consolidated configuration containing all parameter userd for training. The policy configuration should match &lt;code&gt;config.json&lt;/code&gt; exactly. Thisis useful for anyone who wants to evaluate your policy or for reproducibility.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To upload these to the hub, run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;huggingface-cli upload ${hf_user}/${repo_name} path/to/pretrained_model
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href=&quot;https://github.com/huggingface/lerobot/raw/main/lerobot/scripts/eval.py&quot;&gt;eval.py&lt;/a&gt; for an example of how other people may use your policy.&lt;/p&gt; 
&lt;h3&gt;Improve your code with profiling&lt;/h3&gt; 
&lt;p&gt;An example of a code snippet to profile the evaluation of a policy:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from torch.profiler import profile, record_function, ProfilerActivity

def trace_handler(prof):
    prof.export_chrome_trace(f&quot;tmp/trace_schedule_{prof.step_num}.json&quot;)

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    schedule=torch.profiler.schedule(
        wait=2,
        warmup=2,
        active=3,
    ),
    on_trace_ready=trace_handler
) as prof:
    with record_function(&quot;eval_policy&quot;):
        for i in range(num_episodes):
            prof.step()
            # insert code to profile, potentially whole body of eval_policy function
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you want, you can cite this work with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@misc{cadene2024lerobot,
    author = {Cadene, Remi and Alibert, Simon and Soare, Alexander and Gallouedec, Quentin and Zouitine, Adil and Wolf, Thomas},
    title = {LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch},
    howpublished = &quot;\url{https://github.com/huggingface/lerobot}&quot;,
    year = {2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Additionally, if you are using any of the particular policy architecture, pretrained models, or datasets, it is recommended to cite the original authors of the work as they appear below:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://diffusion-policy.cs.columbia.edu&quot;&gt;Diffusion Policy&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@article{chi2024diffusionpolicy,
	author = {Cheng Chi and Zhenjia Xu and Siyuan Feng and Eric Cousineau and Yilun Du and Benjamin Burchfiel and Russ Tedrake and Shuran Song},
	title ={Diffusion Policy: Visuomotor Policy Learning via Action Diffusion},
	journal = {The International Journal of Robotics Research},
	year = {2024},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://tonyzhaozh.github.io/aloha&quot;&gt;ACT or ALOHA&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@article{zhao2023learning,
  title={Learning fine-grained bimanual manipulation with low-cost hardware},
  author={Zhao, Tony Z and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:2304.13705},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.nicklashansen.com/td-mpc/&quot;&gt;TDMPC&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inproceedings{Hansen2022tdmpc,
	title={Temporal Difference Learning for Model Predictive Control},
	author={Nicklas Hansen and Xiaolong Wang and Hao Su},
	booktitle={ICML},
	year={2022}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://sjlee.cc/vq-bet/&quot;&gt;VQ-BeT&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@article{lee2024behavior,
  title={Behavior generation with latent actions},
  author={Lee, Seungjae and Wang, Yibin and Etukuru, Haritheja and Kim, H Jin and Shafiullah, Nur Muhammad Mahi and Pinto, Lerrel},
  journal={arXiv preprint arXiv:2403.03181},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://star-history.com/#huggingface/lerobot&amp;amp;Timeline&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=huggingface/lerobot&amp;amp;type=Timeline&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/qlib</title>
      <link>https://github.com/microsoft/qlib</link>
      <description>&lt;p&gt;Qlib is an AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms. including supervised learning, market dynamics modeling, and RL.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://pypi.org/project/pyqlib/#files&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&amp;amp;logoColor=white&quot; alt=&quot;Python Versions&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/pyqlib/#files&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey&quot; alt=&quot;Platform&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/pyqlib/#history&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/pyqlib&quot; alt=&quot;PypI Versions&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/pyqlib/&quot;&gt;&lt;img src=&quot;https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg?sanitize=true&quot; alt=&quot;Upload Python Package&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/actions&quot;&gt;&lt;img src=&quot;https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main&quot; alt=&quot;Github Actions Test Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://qlib.readthedocs.io/en/latest/?badge=latest&quot;&gt;&lt;img src=&quot;https://readthedocs.org/projects/qlib/badge/?version=latest&quot; alt=&quot;Documentation Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/l/pyqlib&quot; alt=&quot;License&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://gitter.im/Microsoft/qlib?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&quot;&gt;&lt;img src=&quot;https://badges.gitter.im/Microsoft/qlib.svg?sanitize=true&quot; alt=&quot;Join the chat at https://gitter.im/Microsoft/qlib&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;📰&lt;/span&gt; &lt;strong&gt;What&#39;s NEW!&lt;/strong&gt; &amp;nbsp; &lt;span&gt;💖&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;Recent released features&lt;/p&gt; 
&lt;h3&gt;Introducing &lt;a href=&quot;https://github.com/microsoft/RD-Agent&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/rdagent_logo.png&quot; alt=&quot;RD_Agent&quot; style=&quot;height: 2em&quot;&gt;&lt;/a&gt;: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&amp;amp;D&lt;/h3&gt; 
&lt;p&gt;We are excited to announce the release of &lt;strong&gt;RD-Agent&lt;/strong&gt;📢, a powerful tool that supports automated factor mining and model optimization in quant investment R&amp;amp;D.&lt;/p&gt; 
&lt;p&gt;RD-Agent is now available on &lt;a href=&quot;https://github.com/microsoft/RD-Agent&quot;&gt;GitHub&lt;/a&gt;, and we welcome your star🌟!&lt;/p&gt; 
&lt;p&gt;To learn more, please visit our &lt;a href=&quot;https://rdagent.azurewebsites.net/&quot;&gt;♾️Demo page&lt;/a&gt;. Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.&lt;/p&gt; 
&lt;p&gt;We have prepared several demo videos for you:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Scenario&lt;/th&gt; 
   &lt;th&gt;Demo video (English)&lt;/th&gt; 
   &lt;th&gt;Demo video (中文)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Quant Factor Mining&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://rdagent.azurewebsites.net/factor_loop?lang=en&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://rdagent.azurewebsites.net/factor_loop?lang=zh&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Quant Factor Mining from reports&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://rdagent.azurewebsites.net/report_factor?lang=en&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://rdagent.azurewebsites.net/report_factor?lang=zh&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Quant Model Optimization&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://rdagent.azurewebsites.net/model_loop?lang=en&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://rdagent.azurewebsites.net/model_loop?lang=zh&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BPQP for End-to-end learning&lt;/td&gt; 
   &lt;td&gt;📈Coming soon!(&lt;a href=&quot;https://github.com/microsoft/qlib/pull/1863&quot;&gt;Under review&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;🔥LLM-driven Auto Quant Factory🔥&lt;/td&gt; 
   &lt;td&gt;🚀 Released in &lt;a href=&quot;https://github.com/microsoft/RD-Agent&quot;&gt;♾️RD-Agent&lt;/a&gt; on Aug 8, 2024&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KRNN and Sandwich models&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/1414/&quot;&gt;Released&lt;/a&gt; on May 26, 2023&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Release Qlib v0.9.0&lt;/td&gt; 
   &lt;td&gt;&lt;img alt=&quot;octocat&quot; src=&quot;https://github.githubassets.com/images/icons/emoji/octocat.png?v8&quot;&gt;) &lt;a href=&quot;https://github.com/microsoft/qlib/releases/tag/v0.9.0&quot;&gt;Released&lt;/a&gt; on Dec 9, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RL Learning Framework&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;span&gt;📈&lt;/span&gt; Released on Nov 10, 2022. &lt;a href=&quot;https://github.com/microsoft/qlib/pull/1332&quot;&gt;#1332&lt;/a&gt;, &lt;a href=&quot;https://github.com/microsoft/qlib/pull/1322&quot;&gt;#1322&lt;/a&gt;, &lt;a href=&quot;https://github.com/microsoft/qlib/pull/1316&quot;&gt;#1316&lt;/a&gt;,&lt;a href=&quot;https://github.com/microsoft/qlib/pull/1299&quot;&gt;#1299&lt;/a&gt;,&lt;a href=&quot;https://github.com/microsoft/qlib/pull/1263&quot;&gt;#1263&lt;/a&gt;, &lt;a href=&quot;https://github.com/microsoft/qlib/pull/1244&quot;&gt;#1244&lt;/a&gt;, &lt;a href=&quot;https://github.com/microsoft/qlib/pull/1169&quot;&gt;#1169&lt;/a&gt;, &lt;a href=&quot;https://github.com/microsoft/qlib/pull/1125&quot;&gt;#1125&lt;/a&gt;, &lt;a href=&quot;https://github.com/microsoft/qlib/pull/1076&quot;&gt;#1076&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;HIST and IGMTF models&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/1040&quot;&gt;Released&lt;/a&gt; on Apr 10, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qlib &lt;a href=&quot;https://github.com/microsoft/qlib/tree/main/examples/tutorial&quot;&gt;notebook tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;📖 &lt;a href=&quot;https://github.com/microsoft/qlib/pull/1037&quot;&gt;Released&lt;/a&gt; on Apr 7, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ibovespa index data&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;🍚&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/990&quot;&gt;Released&lt;/a&gt; on Apr 6, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Point-in-Time database&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/343&quot;&gt;Released&lt;/a&gt; on Mar 10, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Arctic Provider Backend &amp;amp; Orderbook data example&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/744&quot;&gt;Released&lt;/a&gt; on Jan 17, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Meta-Learning-based framework &amp;amp; DDG-DA&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;span&gt;🔨&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/743&quot;&gt;Released&lt;/a&gt; on Jan 10, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Planning-based portfolio optimization&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/754&quot;&gt;Released&lt;/a&gt; on Dec 28, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Release Qlib v0.8.0&lt;/td&gt; 
   &lt;td&gt;&lt;img alt=&quot;octocat&quot; src=&quot;https://github.githubassets.com/images/icons/emoji/octocat.png?v8&quot;&gt;) &lt;a href=&quot;https://github.com/microsoft/qlib/releases/tag/v0.8.0&quot;&gt;Released&lt;/a&gt; on Dec 8, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ADD model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/704&quot;&gt;Released&lt;/a&gt; on Nov 22, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ADARNN model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/689&quot;&gt;Released&lt;/a&gt; on Nov 14, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TCN model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/668&quot;&gt;Released&lt;/a&gt; on Nov 4, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Nested Decision Framework&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/438&quot;&gt;Released&lt;/a&gt; on Oct 1, 2021. &lt;a href=&quot;https://github.com/microsoft/qlib/raw/main/examples/nested_decision_execution/workflow.py&quot;&gt;Example&lt;/a&gt; and &lt;a href=&quot;https://qlib.readthedocs.io/en/latest/component/highfreq.html&quot;&gt;Doc&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Temporal Routing Adaptor (TRA)&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/531&quot;&gt;Released&lt;/a&gt; on July 30, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Transformer &amp;amp; Localformer&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/508&quot;&gt;Released&lt;/a&gt; on July 22, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Release Qlib v0.7.0&lt;/td&gt; 
   &lt;td&gt;&lt;img alt=&quot;octocat&quot; src=&quot;https://github.githubassets.com/images/icons/emoji/octocat.png?v8&quot;&gt;) &lt;a href=&quot;https://github.com/microsoft/qlib/releases/tag/v0.7.0&quot;&gt;Released&lt;/a&gt; on July 12, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TCTS Model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/491&quot;&gt;Released&lt;/a&gt; on July 1, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Online serving and automatic model rolling&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/290&quot;&gt;Released&lt;/a&gt; on May 17, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DoubleEnsemble Model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/286&quot;&gt;Released&lt;/a&gt; on Mar 2, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;High-frequency data processing example&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;🔨&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/257&quot;&gt;Released&lt;/a&gt; on Feb 5, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;High-frequency trading example&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/227&quot;&gt;Part of code released&lt;/a&gt; on Jan 28, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;High-frequency data(1min)&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;🍚&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/221&quot;&gt;Released&lt;/a&gt; on Jan 27, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tabnet Model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;📈&lt;/span&gt; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/205&quot;&gt;Released&lt;/a&gt; on Jan 22, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Features released before 2021 are not listed here.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/logo/1.png&quot;&gt; &lt;/p&gt; 
&lt;p&gt;Qlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.&lt;/p&gt; 
&lt;p&gt;An increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market&#39;s complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.&lt;/p&gt; 
&lt;p&gt;It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. For more details, please refer to our paper &lt;a href=&quot;https://arxiv.org/abs/2009.11189&quot;&gt;&quot;Qlib: An AI-oriented Quantitative Investment Platform&quot;&lt;/a&gt;.&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Frameworks, Tutorial, Data &amp;amp; DevOps&lt;/th&gt; 
   &lt;th&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#plans&quot;&gt;&lt;strong&gt;Plans&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#framework-of-qlib&quot;&gt;Framework of Qlib&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#quick-start&quot;&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
    &lt;ul dir=&quot;auto&quot;&gt; 
     &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#installation&quot;&gt;Installation&lt;/a&gt; &lt;/li&gt; 
     &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#data-preparation&quot;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; 
     &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#auto-quant-research-workflow&quot;&gt;Auto Quant Research Workflow&lt;/a&gt;&lt;/li&gt; 
     &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#building-customized-quant-research-workflow-by-code&quot;&gt;Building Customized Quant Research Workflow by Code&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt; &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#quant-dataset-zoo&quot;&gt;&lt;strong&gt;Quant Dataset Zoo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#learning-framework&quot;&gt;Learning Framework&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#more-about-qlib&quot;&gt;More About Qlib&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#offline-mode-and-online-mode&quot;&gt;Offline Mode and Online Mode&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#performance-of-qlib-data-server&quot;&gt;Performance of Qlib Data Server&lt;/a&gt;&lt;/li&gt;
     &lt;/ul&gt; &lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#related-reports&quot;&gt;Related Reports&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#contact-us&quot;&gt;Contact Us&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &lt;/td&gt; 
   &lt;td valign=&quot;baseline&quot;&gt; &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#main-challenges--solutions-in-quant-research&quot;&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#forecasting-finding-valuable-signalspatterns&quot;&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/a&gt; 
       &lt;ul&gt; 
        &lt;li type=&quot;disc&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#quant-model-paper-zoo&quot;&gt;&lt;strong&gt;Quant Model (Paper) Zoo&lt;/strong&gt;&lt;/a&gt; 
         &lt;ul&gt; 
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#run-a-single-model&quot;&gt;Run a Single Model&lt;/a&gt;&lt;/li&gt; 
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#run-multiple-models&quot;&gt;Run Multiple Models&lt;/a&gt;&lt;/li&gt; 
         &lt;/ul&gt; &lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
      &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#adapting-to-market-dynamics&quot;&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/li&gt; 
      &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#reinforcement-learning-modeling-continuous-decisions&quot;&gt;Reinforcement Learning: modeling continuous decisions&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Plans&lt;/h1&gt; 
&lt;p&gt;New features under development(order by estimated release time). Your feedbacks about the features are very important.&lt;/p&gt; 
&lt;!-- | Feature                        | Status      | --&gt; 
&lt;!-- | --                      | ------    | --&gt; 
&lt;h1&gt;Framework of Qlib&lt;/h1&gt; 
&lt;div style=&quot;align: center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/framework-abstract.jpg&quot;&gt; 
&lt;/div&gt; 
&lt;p&gt;The high-level framework of Qlib can be found above(users can find the &lt;a href=&quot;https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework&quot;&gt;detailed framework&lt;/a&gt; of Qlib&#39;s design when getting into nitty gritty). The components are designed as loose-coupled modules, and each component could be used stand-alone.&lt;/p&gt; 
&lt;p&gt;Qlib provides a strong infrastructure to support Quant research. &lt;a href=&quot;https://qlib.readthedocs.io/en/latest/component/data.html&quot;&gt;Data&lt;/a&gt; is always an important part. A strong learning framework is designed to support diverse learning paradigms (e.g. &lt;a href=&quot;https://qlib.readthedocs.io/en/latest/component/rl.html&quot;&gt;reinforcement learning&lt;/a&gt;, &lt;a href=&quot;https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section&quot;&gt;supervised learning&lt;/a&gt;) and patterns at different levels(e.g. &lt;a href=&quot;https://qlib.readthedocs.io/en/latest/component/meta.html&quot;&gt;market dynamic modeling&lt;/a&gt;). By modeling the market, &lt;a href=&quot;https://qlib.readthedocs.io/en/latest/component/strategy.html&quot;&gt;trading strategies&lt;/a&gt; will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be &lt;a href=&quot;https://qlib.readthedocs.io/en/latest/component/highfreq.html&quot;&gt;nested to be optimized and run together&lt;/a&gt;. At last, a comprehensive &lt;a href=&quot;https://qlib.readthedocs.io/en/latest/component/report.html&quot;&gt;analysis&lt;/a&gt; will be provided and the model can be &lt;a href=&quot;https://qlib.readthedocs.io/en/latest/component/online.html&quot;&gt;served online&lt;/a&gt; in a low cost.&lt;/p&gt; 
&lt;h1&gt;Quick Start&lt;/h1&gt; 
&lt;p&gt;This quick start guide tries to demonstrate&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;It&#39;s very easy to build a complete Quant research workflow and try your ideas with &lt;em&gt;Qlib&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;Though with &lt;em&gt;public data&lt;/em&gt; and &lt;em&gt;simple models&lt;/em&gt;, machine learning technologies &lt;strong&gt;work very well&lt;/strong&gt; in practical Quant investment.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Here is a quick &lt;strong&gt;&lt;a href=&quot;https://terminalizer.com/view/3f24561a4470&quot;&gt;demo&lt;/a&gt;&lt;/strong&gt; shows how to install &lt;code&gt;Qlib&lt;/code&gt;, and run LightGBM with &lt;code&gt;qrun&lt;/code&gt;. &lt;strong&gt;But&lt;/strong&gt;, please make sure you have already prepared the data following the &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/#data-preparation&quot;&gt;instruction&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;This table demonstrates the supported Python version of &lt;code&gt;Qlib&lt;/code&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;install with pip&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;install from source&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;plot&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.8&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.9&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.10&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.11&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.12&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Conda&lt;/strong&gt; is suggested for managing your Python environment. In some cases, using Python outside of a &lt;code&gt;conda&lt;/code&gt; environment may result in missing header files, causing the installation failure of certain packages.&lt;/li&gt; 
 &lt;li&gt;Please pay attention that installing cython in Python 3.6 will raise some error when installing &lt;code&gt;Qlib&lt;/code&gt; from source. If users use Python 3.6 on their machines, it is recommended to &lt;em&gt;upgrade&lt;/em&gt; Python to version 3.8 or higher, or use &lt;code&gt;conda&lt;/code&gt;&#39;s Python to install &lt;code&gt;Qlib&lt;/code&gt; from source.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Install with pip&lt;/h3&gt; 
&lt;p&gt;Users can easily install &lt;code&gt;Qlib&lt;/code&gt; by pip according to the following command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;  pip install pyqlib
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.&lt;/p&gt; 
&lt;h3&gt;Install from source&lt;/h3&gt; 
&lt;p&gt;Also, users can install the latest dev version &lt;code&gt;Qlib&lt;/code&gt; by the source code according to the following steps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Before installing &lt;code&gt;Qlib&lt;/code&gt; from source, users need to install some dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install numpy
pip install --upgrade cython
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repository and install &lt;code&gt;Qlib&lt;/code&gt; as follows.&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/microsoft/qlib.git &amp;amp;&amp;amp; cd qlib
pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Tips&lt;/strong&gt;: If you fail to install &lt;code&gt;Qlib&lt;/code&gt; or run the examples in your environment, comparing your steps and the &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/.github/workflows/test_qlib_from_source.yml&quot;&gt;CI workflow&lt;/a&gt; may help you find the problem.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Tips for Mac&lt;/strong&gt;: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with &lt;code&gt;brew install libomp&lt;/code&gt; and then run &lt;code&gt;pip install .&lt;/code&gt; to build it successfully.&lt;/p&gt; 
&lt;h2&gt;Data Preparation&lt;/h2&gt; 
&lt;p&gt;❗ Due to more restrict data security policy. The offical dataset is disabled temporarily. You can try &lt;a href=&quot;https://github.com/chenditc/investment_data/releases&quot;&gt;this data source&lt;/a&gt; contributed by the community. Here is an example to download the latest data.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;wget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz
mkdir -p ~/.qlib/qlib_data/cn_data
tar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2
rm -f qlib_bin.tar.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The official dataset below will resume in short future.&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;Load and prepare data by running the following code:&lt;/p&gt; 
&lt;h3&gt;Get with module&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# get 1d data
python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

# get 1min data
python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Get from source&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# get 1d data
python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

# get 1min data
python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This dataset is created by public data collected by &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/scripts/data_collector/&quot;&gt;crawler scripts&lt;/a&gt;, which have been released in the same repository. Users could create the same dataset with it. &lt;a href=&quot;https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset&quot;&gt;Description of dataset&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Please pay &lt;strong&gt;ATTENTION&lt;/strong&gt; that the data is collected from &lt;a href=&quot;https://finance.yahoo.com/lookup&quot;&gt;Yahoo Finance&lt;/a&gt;, and the data might not be perfect. We recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the &lt;a href=&quot;https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format&quot;&gt;related document&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt; 
&lt;h3&gt;Automatic update of daily frequency data (from yahoo finance)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This step is &lt;em&gt;Optional&lt;/em&gt; if users only want to try their models and strategies on history data.&lt;/p&gt; 
 &lt;p&gt;It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Users can&#39;t incrementally update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use &lt;a href=&quot;https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance&quot;&gt;yahoo collector&lt;/a&gt; to download Yahoo data from scratch and then incrementally update it.&lt;/p&gt; 
 &lt;p&gt;For more information, please refer to: &lt;a href=&quot;https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance&quot;&gt;yahoo collector&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Automatic update of data to the &quot;qlib&quot; directory each trading day(Linux)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;use &lt;em&gt;crontab&lt;/em&gt;: &lt;code&gt;crontab -e&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;set up timed tasks:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;* * * * 1-5 python &amp;lt;script path&amp;gt; update_data_to_bin --qlib_data_1d_dir &amp;lt;user data dir&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;script path&lt;/strong&gt;: &lt;em&gt;scripts/data_collector/yahoo/collector.py&lt;/em&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Manual update of data&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir &amp;lt;user data dir&amp;gt; --trading_date &amp;lt;start date&amp;gt; --end_date &amp;lt;end date&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;em&gt;trading_date&lt;/em&gt;: start of trading day&lt;/li&gt; 
   &lt;li&gt;&lt;em&gt;end_date&lt;/em&gt;: end of trading day(not included)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Checking the health of the data&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not. &lt;pre&gt;&lt;code&gt;python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Of course, you can also add some parameters to adjust the test results, such as this. &lt;pre&gt;&lt;code&gt;python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data --missing_data_num 30055 --large_step_threshold_volume 94485 --large_step_threshold_price 20
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;If you want more information about &lt;code&gt;check_data_health&lt;/code&gt;, please refer to the &lt;a href=&quot;https://qlib.readthedocs.io/en/latest/component/data.html#checking-the-health-of-the-data&quot;&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- 
- Run the initialization code and get stock data:

  ```python
  import qlib
  from qlib.data import D
  from qlib.constant import REG_CN

  # Initialization
  mount_path = &quot;~/.qlib/qlib_data/cn_data&quot;  # target_dir
  qlib.init(mount_path=mount_path, region=REG_CN)

  # Get stock data by Qlib
  # Load trading calendar with the given time range and frequency
  print(D.calendar(start_time=&#39;2010-01-01&#39;, end_time=&#39;2017-12-31&#39;, freq=&#39;day&#39;)[:2])

  # Parse a given market name into a stockpool config
  instruments = D.instruments(&#39;csi500&#39;)
  print(D.list_instruments(instruments=instruments, start_time=&#39;2010-01-01&#39;, end_time=&#39;2017-12-31&#39;, as_list=True)[:6])

  # Load features of certain instruments in given time range
  instruments = [&#39;SH600000&#39;]
  fields = [&#39;$close&#39;, &#39;$volume&#39;, &#39;Ref($close, 1)&#39;, &#39;Mean($close, 3)&#39;, &#39;$high-$low&#39;]
  print(D.features(instruments, fields, start_time=&#39;2010-01-01&#39;, end_time=&#39;2017-12-31&#39;, freq=&#39;day&#39;).head())
  ```
 --&gt; 
&lt;h2&gt;Docker images&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Pulling a docker image from a docker hub repository &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker pull pyqlib/qlib_image_stable:stable
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Start a new Docker container &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run -it --name &amp;lt;container name&amp;gt; -v &amp;lt;Mounted local directory&amp;gt;:/app qlib_image_stable
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;At this point you are in the docker environment and can run the qlib scripts. An example: &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;&amp;gt;&amp;gt;&amp;gt; python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn
&amp;gt;&amp;gt;&amp;gt; python qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Exit the container &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;&amp;gt;&amp;gt;&amp;gt; exit
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Restart the container &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker start -i -a &amp;lt;container name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Stop the container &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker stop &amp;lt;container name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Delete the container &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker rm &amp;lt;container name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;If you want to know more information, please refer to the &lt;a href=&quot;https://qlib.readthedocs.io/en/latest/developer/how_to_build_image.html&quot;&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Auto Quant Research Workflow&lt;/h2&gt; 
&lt;p&gt;Qlib provides a tool named &lt;code&gt;qrun&lt;/code&gt; to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Quant Research Workflow: Run &lt;code&gt;qrun&lt;/code&gt; with lightgbm workflow config (&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml&quot;&gt;workflow_config_lightgbm_Alpha158.yaml&lt;/a&gt; as following.&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;  cd examples  # Avoid running program under the directory contains `qlib`
  qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If users want to use &lt;code&gt;qrun&lt;/code&gt; under debug mode, please use the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The result of &lt;code&gt;qrun&lt;/code&gt; is as follows, please refer to &lt;a href=&quot;https://qlib.readthedocs.io/en/latest/component/strategy.html#result&quot;&gt;docs&lt;/a&gt; for more explanations about the result.&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;
&#39;The following are analysis results of the excess return without cost.&#39;
                       risk
mean               0.000708
std                0.005626
annualized_return  0.178316
information_ratio  1.996555
max_drawdown      -0.081806
&#39;The following are analysis results of the excess return with cost.&#39;
                       risk
mean               0.000512
std                0.005626
annualized_return  0.128982
information_ratio  1.444287
max_drawdown      -0.091078
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here are detailed documents for &lt;code&gt;qrun&lt;/code&gt; and &lt;a href=&quot;https://qlib.readthedocs.io/en/latest/component/workflow.html&quot;&gt;workflow&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Graphical Reports Analysis: First, run &lt;code&gt;python -m pip install .[analysis]&lt;/code&gt; to install the required dependencies. Then run &lt;code&gt;examples/workflow_by_code.ipynb&lt;/code&gt; with &lt;code&gt;jupyter notebook&lt;/code&gt; to get graphical reports.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Forecasting signal (model prediction) analysis&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Cumulative Return of groups &lt;img src=&quot;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_cumulative_return.png&quot; alt=&quot;Cumulative Return&quot;&gt;&lt;/li&gt; 
     &lt;li&gt;Return distribution &lt;img src=&quot;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_long_short.png&quot; alt=&quot;long_short&quot;&gt;&lt;/li&gt; 
     &lt;li&gt;Information Coefficient (IC) &lt;img src=&quot;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_IC.png&quot; alt=&quot;Information Coefficient&quot;&gt; &lt;img src=&quot;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_monthly_IC.png&quot; alt=&quot;Monthly IC&quot;&gt; &lt;img src=&quot;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_NDQ.png&quot; alt=&quot;IC&quot;&gt;&lt;/li&gt; 
     &lt;li&gt;Auto Correlation of forecasting signal (model prediction) &lt;img src=&quot;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_auto_correlation.png&quot; alt=&quot;Auto Correlation&quot;&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Portfolio analysis&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Backtest return &lt;img src=&quot;https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/report.png&quot; alt=&quot;Report&quot;&gt;&lt;/li&gt; 
    &lt;/ul&gt; 
    &lt;!-- 
- Score IC
![Score IC](docs/_static/img/score_ic.png)
- Cumulative Return
![Cumulative Return](docs/_static/img/cumulative_return.png)
- Risk Analysis
![Risk Analysis](docs/_static/img/risk_analysis.png)
- Rank Label
![Rank Label](docs/_static/img/rank_label.png)
--&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://qlib.readthedocs.io/en/latest/component/report.html&quot;&gt;Explanation&lt;/a&gt; of above results&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Building Customized Quant Research Workflow by Code&lt;/h2&gt; 
&lt;p&gt;The automatic workflow may not suit the research workflow of all Quant researchers. To support a flexible Quant research workflow, Qlib also provides a modularized interface to allow researchers to build their own workflow by code. &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/workflow_by_code.ipynb&quot;&gt;Here&lt;/a&gt; is a demo for customized Quant research workflow by code.&lt;/p&gt; 
&lt;h1&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/h1&gt; 
&lt;p&gt;Quant investment is a very unique scenario with lots of key challenges to be solved. Currently, Qlib provides some solutions for several of them.&lt;/p&gt; 
&lt;h2&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/h2&gt; 
&lt;p&gt;Accurate forecasting of the stock price trend is a very important part to construct profitable portfolios. However, huge amount of data with various formats in the financial market which make it challenging to build forecasting models.&lt;/p&gt; 
&lt;p&gt;An increasing number of SOTA Quant research works/papers, which focus on building forecasting models to mine valuable signals/patterns in complex financial data, are released in &lt;code&gt;Qlib&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks&quot;&gt;Quant Model (Paper) Zoo&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Here is a list of models built on &lt;code&gt;Qlib&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/XGBoost/&quot;&gt;GBDT based on XGBoost (Tianqi Chen, et al. KDD 2016)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LightGBM/&quot;&gt;GBDT based on LightGBM (Guolin Ke, et al. NIPS 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/CatBoost/&quot;&gt;GBDT based on Catboost (Liudmila Prokhorenkova, et al. NIPS 2018)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/MLP/&quot;&gt;MLP based on pytorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LSTM/&quot;&gt;LSTM based on pytorch (Sepp Hochreiter, et al. Neural computation 1997)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/GRU/&quot;&gt;GRU based on pytorch (Kyunghyun Cho, et al. 2014)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ALSTM&quot;&gt;ALSTM based on pytorch (Yao Qin, et al. IJCAI 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/GATs/&quot;&gt;GATs based on pytorch (Petar Velickovic, et al. 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/SFM/&quot;&gt;SFM based on pytorch (Liheng Zhang, et al. KDD 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TFT/&quot;&gt;TFT based on tensorflow (Bryan Lim, et al. International Journal of Forecasting 2019)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TabNet/&quot;&gt;TabNet based on pytorch (Sercan O. Arik, et al. AAAI 2019)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/DoubleEnsemble/&quot;&gt;DoubleEnsemble based on LightGBM (Chuheng Zhang, et al. ICDM 2020)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TCTS/&quot;&gt;TCTS based on pytorch (Xueqing Wu, et al. ICML 2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Transformer/&quot;&gt;Transformer based on pytorch (Ashish Vaswani, et al. NeurIPS 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Localformer/&quot;&gt;Localformer based on pytorch (Juyong Jiang, et al.)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TRA/&quot;&gt;TRA based on pytorch (Hengxu, Dong, et al. KDD 2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TCN/&quot;&gt;TCN based on pytorch (Shaojie Bai, et al. 2018)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ADARNN/&quot;&gt;ADARNN based on pytorch (YunTao Du, et al. 2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ADD/&quot;&gt;ADD based on pytorch (Hongshun Tang, et al.2020)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/IGMTF/&quot;&gt;IGMTF based on pytorch (Wentao Xu, et al.2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/HIST/&quot;&gt;HIST based on pytorch (Wentao Xu, et al.2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/KRNN/&quot;&gt;KRNN based on pytorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Sandwich/&quot;&gt;Sandwich based on pytorch&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Your PR of new Quant models is highly welcomed.&lt;/p&gt; 
&lt;p&gt;The performance of each model on the &lt;code&gt;Alpha158&lt;/code&gt; and &lt;code&gt;Alpha360&lt;/code&gt; datasets can be found &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/README.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Run a single model&lt;/h3&gt; 
&lt;p&gt;All the models listed above are runnable with &lt;code&gt;Qlib&lt;/code&gt;. Users can find the config files we provide and some details about the model through the &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks&quot;&gt;benchmarks&lt;/a&gt; folder. More information can be retrieved at the model files listed above.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Qlib&lt;/code&gt; provides three different ways to run a single model, users can pick the one that fits their cases best:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Users can use the tool &lt;code&gt;qrun&lt;/code&gt; mentioned above to run a model&#39;s workflow based from a config file.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Users can create a &lt;code&gt;workflow_by_code&lt;/code&gt; python script based on the &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/workflow_by_code.py&quot;&gt;one&lt;/a&gt; listed in the &lt;code&gt;examples&lt;/code&gt; folder.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Users can use the script &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py&quot;&gt;&lt;code&gt;run_all_model.py&lt;/code&gt;&lt;/a&gt; listed in the &lt;code&gt;examples&lt;/code&gt; folder to run a model. Here is an example of the specific shell command to be used: &lt;code&gt;python run_all_model.py run --models=lightgbm&lt;/code&gt;, where the &lt;code&gt;--models&lt;/code&gt; arguments can take any number of models listed above(the available models can be found in &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/&quot;&gt;benchmarks&lt;/a&gt;). For more use cases, please refer to the file&#39;s &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py&quot;&gt;docstrings&lt;/a&gt;.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of &lt;code&gt;tensorflow==1.15.0&lt;/code&gt;)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Run multiple models&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Qlib&lt;/code&gt; also provides a script &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py&quot;&gt;&lt;code&gt;run_all_model.py&lt;/code&gt;&lt;/a&gt; which can run multiple models for several iterations. (&lt;strong&gt;Note&lt;/strong&gt;: the script only support &lt;em&gt;Linux&lt;/em&gt; for now. Other OS will be supported in the future. Besides, it doesn&#39;t support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)&lt;/p&gt; 
&lt;p&gt;The script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as &lt;code&gt;IC&lt;/code&gt; and &lt;code&gt;backtest&lt;/code&gt; results will be generated and stored.&lt;/p&gt; 
&lt;p&gt;Here is an example of running all the models for 10 iterations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;python run_all_model.py run 10
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It also provides the API to run specific models at once. For more use cases, please refer to the file&#39;s &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py&quot;&gt;docstrings&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic&quot;&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Due to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data. So adapting the forecasting models/strategies to market dynamics is very important to the model/strategies&#39; performance.&lt;/p&gt; 
&lt;p&gt;Here is a list of solutions built on &lt;code&gt;Qlib&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic/baseline/&quot;&gt;Rolling Retraining&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic/DDG-DA/&quot;&gt;DDG-DA on pytorch (Wendi, et al. AAAI 2022)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Reinforcement Learning: modeling continuous decisions&lt;/h2&gt; 
&lt;p&gt;Qlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.&lt;/p&gt; 
&lt;p&gt;Here is a list of solutions built on &lt;code&gt;Qlib&lt;/code&gt; categorized by scenarios.&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution&quot;&gt;RL for order execution&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution&quot;&gt;Here&lt;/a&gt; is the introduction of this scenario. All the methods below are compared &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_twap.yml&quot;&gt;TWAP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_ppo.yml&quot;&gt;PPO: &quot;An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization&quot;, IJCAL 2020&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_opds.yml&quot;&gt;OPDS: &quot;Universal Trading for Order Execution with Oracle Policy Distillation&quot;, AAAI 2021&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Quant Dataset Zoo&lt;/h1&gt; 
&lt;p&gt;Dataset plays a very important role in Quant. Here is a list of the datasets built on &lt;code&gt;Qlib&lt;/code&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dataset&lt;/th&gt; 
   &lt;th&gt;US Market&lt;/th&gt; 
   &lt;th&gt;China Market&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/qlib/contrib/data/handler.py&quot;&gt;Alpha360&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;√&lt;/td&gt; 
   &lt;td&gt;√&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/qlib/contrib/data/handler.py&quot;&gt;Alpha158&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;√&lt;/td&gt; 
   &lt;td&gt;√&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href=&quot;https://qlib.readthedocs.io/en/latest/advanced/alpha.html&quot;&gt;Here&lt;/a&gt; is a tutorial to build dataset with &lt;code&gt;Qlib&lt;/code&gt;. Your PR to build new Quant dataset is highly welcomed.&lt;/p&gt; 
&lt;h1&gt;Learning Framework&lt;/h1&gt; 
&lt;p&gt;Qlib is high customizable and a lot of its components are learnable. The learnable components are instances of &lt;code&gt;Forecast Model&lt;/code&gt; and &lt;code&gt;Trading Agent&lt;/code&gt;. They are learned based on the &lt;code&gt;Learning Framework&lt;/code&gt; layer and then applied to multiple scenarios in &lt;code&gt;Workflow&lt;/code&gt; layer. The learning framework leverages the &lt;code&gt;Workflow&lt;/code&gt; layer as well(e.g. sharing &lt;code&gt;Information Extractor&lt;/code&gt;, creating environments based on &lt;code&gt;Execution Env&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;Based on learning paradigms, they can be categorized into reinforcement learning and supervised learning.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For supervised learning, the detailed docs can be found &lt;a href=&quot;https://qlib.readthedocs.io/en/latest/component/model.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For reinforcement learning, the detailed docs can be found &lt;a href=&quot;https://qlib.readthedocs.io/en/latest/component/rl.html&quot;&gt;here&lt;/a&gt;. Qlib&#39;s RL learning framework leverages &lt;code&gt;Execution Env&lt;/code&gt; in &lt;code&gt;Workflow&lt;/code&gt; layer to create environments. It&#39;s worth noting that &lt;code&gt;NestedExecutor&lt;/code&gt; is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;More About Qlib&lt;/h1&gt; 
&lt;p&gt;If you want to have a quick glance at the most frequently used components of qlib, you can try notebooks &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/examples/tutorial/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The detailed documents are organized in &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/docs/&quot;&gt;docs&lt;/a&gt;. &lt;a href=&quot;http://www.sphinx-doc.org&quot;&gt;Sphinx&lt;/a&gt; and the readthedocs theme is required to build the documentation in html formats.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd docs/
conda install sphinx sphinx_rtd_theme -y
# Otherwise, you can install them with pip
# pip install sphinx sphinx_rtd_theme
make html
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also view the &lt;a href=&quot;http://qlib.readthedocs.io/&quot;&gt;latest document&lt;/a&gt; online directly.&lt;/p&gt; 
&lt;p&gt;Qlib is in active and continuing development. Our plan is in the roadmap, which is managed as a &lt;a href=&quot;https://github.com/microsoft/qlib/projects/1&quot;&gt;github project&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Offline Mode and Online Mode&lt;/h1&gt; 
&lt;p&gt;The data server of Qlib can either deployed as &lt;code&gt;Offline&lt;/code&gt; mode or &lt;code&gt;Online&lt;/code&gt; mode. The default mode is offline mode.&lt;/p&gt; 
&lt;p&gt;Under &lt;code&gt;Offline&lt;/code&gt; mode, the data will be deployed locally.&lt;/p&gt; 
&lt;p&gt;Under &lt;code&gt;Online&lt;/code&gt; mode, the data will be deployed as a shared data service. The data and their cache will be shared by all the clients. The data retrieval performance is expected to be improved due to a higher rate of cache hits. It will consume less disk space, too. The documents of the online mode can be found in &lt;a href=&quot;https://qlib-server.readthedocs.io/&quot;&gt;Qlib-Server&lt;/a&gt;. The online mode can be deployed automatically with &lt;a href=&quot;https://qlib-server.readthedocs.io/en/latest/build.html#one-click-deployment-in-azure&quot;&gt;Azure CLI based scripts&lt;/a&gt;. The source code of online data server can be found in &lt;a href=&quot;https://github.com/microsoft/qlib-server&quot;&gt;Qlib-Server repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Performance of Qlib Data Server&lt;/h2&gt; 
&lt;p&gt;The performance of data processing is important to data-driven methods like AI technologies. As an AI-oriented platform, Qlib provides a solution for data storage and data processing. To demonstrate the performance of Qlib data server, we compare it with several other data storage solutions.&lt;/p&gt; 
&lt;p&gt;We evaluate the performance of several storage solutions by finishing the same task, which creates a dataset (14 features/factors) from the basic OHLCV daily data of a stock market (800 stocks each day from 2007 to 2020). The task involves data queries and processing.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;HDF5&lt;/th&gt; 
   &lt;th&gt;MySQL&lt;/th&gt; 
   &lt;th&gt;MongoDB&lt;/th&gt; 
   &lt;th&gt;InfluxDB&lt;/th&gt; 
   &lt;th&gt;Qlib -E -D&lt;/th&gt; 
   &lt;th&gt;Qlib +E -D&lt;/th&gt; 
   &lt;th&gt;Qlib +E +D&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Total (1CPU) (seconds)&lt;/td&gt; 
   &lt;td&gt;184.4±3.7&lt;/td&gt; 
   &lt;td&gt;365.3±7.5&lt;/td&gt; 
   &lt;td&gt;253.6±6.7&lt;/td&gt; 
   &lt;td&gt;368.2±3.6&lt;/td&gt; 
   &lt;td&gt;147.0±8.8&lt;/td&gt; 
   &lt;td&gt;47.6±1.0&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;7.4±0.3&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Total (64CPU) (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;8.8±0.6&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;4.2±0.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;+(-)E&lt;/code&gt; indicates with (out) &lt;code&gt;ExpressionCache&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;+(-)D&lt;/code&gt; indicates with (out) &lt;code&gt;DatasetCache&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Most general-purpose databases take too much time to load data. After looking into the underlying implementation, we find that data go through too many layers of interfaces and unnecessary format transformations in general-purpose database solutions. Such overheads greatly slow down the data loading process. Qlib data are stored in a compact format, which is efficient to be combined into arrays for scientific computation.&lt;/p&gt; 
&lt;h1&gt;Related Reports&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://analyticsindiamag.com/qlib/&quot;&gt;Guide To Qlib: Microsoft’s AI Investment Platform&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/47bP5YwxfTp2uTHjUBzJQQ&quot;&gt;微软也搞AI量化平台？还是开源的！&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/vsJv7lsgjEi-ALYUz4CvtQ&quot;&gt;微矿Qlib：业内首个AI量化投资开源平台&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contact Us&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you have any issues, please create issue &lt;a href=&quot;https://github.com/microsoft/qlib/issues/new/choose&quot;&gt;here&lt;/a&gt; or send messages in &lt;a href=&quot;https://gitter.im/Microsoft/qlib&quot;&gt;gitter&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;If you want to make contributions to &lt;code&gt;Qlib&lt;/code&gt;, please &lt;a href=&quot;https://github.com/microsoft/qlib/compare&quot;&gt;create pull requests&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For other reasons, you are welcome to contact us by email(&lt;a href=&quot;mailto:qlib@microsoft.com&quot;&gt;qlib@microsoft.com&lt;/a&gt;). 
  &lt;ul&gt; 
   &lt;li&gt;We are recruiting new members(both FTEs and interns), your resumes are welcome!&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Join IM discussion groups:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href=&quot;https://gitter.im/Microsoft/qlib&quot;&gt;Gitter&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://github.com/microsoft/qlib/raw/main/docs/_static/img/qrcode/gitter_qr.png&quot; alt=&quot;image&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;We appreciate all contributions and thank all the contributors! &lt;a href=&quot;https://github.com/microsoft/qlib/graphs/contributors&quot;&gt;&lt;img src=&quot;https://contrib.rocks/image?repo=microsoft/qlib&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Before we released Qlib as an open-source project on Github in Sep 2020, Qlib is an internal project in our group. Unfortunately, the internal commit history is not kept. A lot of members in our group have also contributed a lot to Qlib, which includes Ruihua Wang, Yinda Zhang, Haisu Yu, Shuyu Wang, Bochen Pang, and &lt;a href=&quot;https://github.com/evanzd/evanzd&quot;&gt;Dong Zhou&lt;/a&gt;. Especially thanks to &lt;a href=&quot;https://github.com/evanzd/evanzd&quot;&gt;Dong Zhou&lt;/a&gt; due to his initial version of Qlib.&lt;/p&gt; 
&lt;h2&gt;Guidance&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions.&lt;br&gt; &lt;strong&gt;Here are some &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/qlib/main/docs/developer/code_standard_and_dev_guide.rst&quot;&gt;code standards and development guidance&lt;/a&gt; for submiting a pull request.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Making contributions is not a hard thing. Solving an issue(maybe just answering a question raised in &lt;a href=&quot;https://github.com/microsoft/qlib/issues&quot;&gt;issues list&lt;/a&gt; or &lt;a href=&quot;https://gitter.im/Microsoft/qlib&quot;&gt;gitter&lt;/a&gt;), fixing/issuing a bug, improving the documents and even fixing a typo are important contributions to Qlib.&lt;/p&gt; 
&lt;p&gt;For example, if you want to contribute to Qlib&#39;s document/code, you can follow the steps in the figure below.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://github.com/demon143/qlib/raw/main/docs/_static/img/change%20doc.gif&quot;&gt; &lt;/p&gt; 
&lt;p&gt;If you don&#39;t know how to start to contribute, you can refer to the following examples.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Examples&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Solving issues&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/microsoft/qlib/issues/749&quot;&gt;Answer a question&lt;/a&gt;; &lt;a href=&quot;https://github.com/microsoft/qlib/issues/765&quot;&gt;issuing&lt;/a&gt; or &lt;a href=&quot;https://github.com/microsoft/qlib/pull/792&quot;&gt;fixing&lt;/a&gt; a bug&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Docs&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/microsoft/qlib/pull/797/files&quot;&gt;Improve docs quality&lt;/a&gt; ; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/774&quot;&gt;Fix a typo&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Feature&lt;/td&gt; 
   &lt;td&gt;Implement a &lt;a href=&quot;https://github.com/microsoft/qlib/projects&quot;&gt;requested feature&lt;/a&gt; like &lt;a href=&quot;https://github.com/microsoft/qlib/pull/754&quot;&gt;this&lt;/a&gt;; &lt;a href=&quot;https://github.com/microsoft/qlib/pull/539/files&quot;&gt;Refactor interfaces&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dataset&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/microsoft/qlib/pull/733&quot;&gt;Add a dataset&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Models&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/microsoft/qlib/pull/689&quot;&gt;Implement a new model&lt;/a&gt;, &lt;a href=&quot;https://github.com/microsoft/qlib/tree/main/examples/benchmarks#contributing&quot;&gt;some instructions to contribute models&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/microsoft/qlib/labels/good%20first%20issue&quot;&gt;Good first issues&lt;/a&gt; are labelled to indicate that they are easy to start your contributions.&lt;/p&gt; 
&lt;p&gt;You can find some impefect implementation in Qlib by &lt;code&gt;rg &#39;TODO|FIXME&#39; qlib&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you would like to become one of Qlib&#39;s maintainers to contribute more (e.g. help merge PR, triage issues), please contact us by email(&lt;a href=&quot;mailto:qlib@microsoft.com&quot;&gt;qlib@microsoft.com&lt;/a&gt;). We are glad to help to upgrade your permission.&lt;/p&gt; 
&lt;h2&gt;Licence&lt;/h2&gt; 
&lt;p&gt;Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the right to use your contribution. For details, visit &lt;a href=&quot;https://cla.opensource.microsoft.com&quot;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href=&quot;https://opensource.microsoft.com/codeofconduct/&quot;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&quot;https://opensource.microsoft.com/codeofconduct/faq/&quot;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&quot;mailto:opencode@microsoft.com&quot;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mem0ai/mem0</title>
      <link>https://github.com/mem0ai/mem0</link>
      <description>&lt;p&gt;The Memory layer for AI Agents&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://github.com/mem0ai/mem0&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/mem0ai/mem0/main/docs/images/banner-sm.png&quot; width=&quot;800px&quot; alt=&quot;Mem0 - The Memory Layer for Personalized AI&quot;&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;display: flex; justify-content: center; gap: 20px; align-items: center;&quot;&gt; &lt;a href=&quot;https://trendshift.io/repositories/11194&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https://trendshift.io/api/badge/repositories/11194&quot; alt=&quot;mem0ai%2Fmem0 | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://www.ycombinator.com/launches/LpA-mem0-open-source-memory-layer-for-ai-apps&quot; target=&quot;_blank&quot;&gt; &lt;img alt=&quot;Launch YC: Mem0 - Open Source Memory Layer for AI Apps&quot; src=&quot;https://www.ycombinator.com/launches/LpA-mem0-open-source-memory-layer-for-ai-apps/upvote_embed.svg?sanitize=true&quot;&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://mem0.ai&quot;&gt;Learn more&lt;/a&gt; · &lt;a href=&quot;https://mem0.dev/DiG&quot;&gt;Join Discord&lt;/a&gt; · &lt;a href=&quot;https://mem0.dev/demo&quot;&gt;Demo&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://mem0.dev/DiG&quot;&gt; &lt;img src=&quot;https://dcbadge.vercel.app/api/server/6PzXDgEjG5?style=flat&quot; alt=&quot;Mem0 Discord&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://pepy.tech/project/mem0ai&quot;&gt; &lt;img src=&quot;https://img.shields.io/pypi/dm/mem0ai&quot; alt=&quot;Mem0 PyPI - Downloads&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://github.com/mem0ai/mem0&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/commit-activity/m/mem0ai/mem0?style=flat-square&quot; alt=&quot;GitHub commit activity&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/mem0ai&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https://img.shields.io/pypi/v/mem0ai?color=%2334D058&amp;amp;label=pypi%20package&quot; alt=&quot;Package version&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://www.npmjs.com/package/mem0ai&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https://img.shields.io/npm/v/mem0ai&quot; alt=&quot;Npm package&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://www.ycombinator.com/companies/mem0&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Y%20Combinator-S24-orange?style=flat-square&quot; alt=&quot;Y Combinator S24&quot;&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://mem0.ai&quot;&gt;Mem0&lt;/a&gt; (pronounced as &quot;mem-zero&quot;) enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. Mem0 remembers user preferences, adapts to individual needs, and continuously improves over time, making it ideal for customer support chatbots, AI assistants, and autonomous systems.&lt;/p&gt; 
&lt;h3&gt;Features &amp;amp; Use Cases&lt;/h3&gt; 
&lt;p&gt;Core Capabilities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Level Memory&lt;/strong&gt;: User, Session, and AI Agent memory retention with adaptive personalization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Developer-Friendly&lt;/strong&gt;: Simple API integration, cross-platform consistency, and hassle-free managed service&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Applications:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AI Assistants&lt;/strong&gt;: Seamless conversations with context and personalization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Learning &amp;amp; Support&lt;/strong&gt;: Tailored content recommendations and context-aware customer assistance&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Healthcare &amp;amp; Companions&lt;/strong&gt;: Patient history tracking and deeper relationship building&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Productivity &amp;amp; Gaming&lt;/strong&gt;: Streamlined workflows and adaptive environments based on user behavior&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;p&gt;Get started quickly with &lt;a href=&quot;https://app.mem0.ai&quot;&gt;Mem0 Platform&lt;/a&gt; - our fully managed solution that provides automatic updates, advanced analytics, enterprise security, and dedicated support. &lt;a href=&quot;https://app.mem0.ai&quot;&gt;Create a free account&lt;/a&gt; to begin.&lt;/p&gt; 
&lt;p&gt;For complete control, you can self-host Mem0 using our open-source package. See the &lt;a href=&quot;https://raw.githubusercontent.com/mem0ai/mem0/main/#quickstart&quot;&gt;Quickstart guide&lt;/a&gt; below to set up your own instance.&lt;/p&gt; 
&lt;h2&gt;Quickstart Guide &lt;a name=&quot;quickstart&quot;&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Install the Mem0 package via pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install mem0ai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install the Mem0 package via npm:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;npm install mem0ai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;p&gt;Mem0 requires an LLM to function, with &lt;code&gt;gpt-4o-mini&lt;/code&gt; from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our &lt;a href=&quot;https://docs.mem0.ai/llms&quot;&gt;Supported LLMs documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;First step is to instantiate the memory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from openai import OpenAI
from mem0 import Memory

openai_client = OpenAI()
memory = Memory()

def chat_with_memories(message: str, user_id: str = &quot;default_user&quot;) -&amp;gt; str:
    # Retrieve relevant memories
    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)
    memories_str = &quot;\n&quot;.join(f&quot;- {entry[&#39;memory&#39;]}&quot; for entry in relevant_memories[&quot;results&quot;])
    
    # Generate Assistant response
    system_prompt = f&quot;You are a helpful AI. Answer the question based on query and memories.\nUser Memories:\n{memories_str}&quot;
    messages = [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_prompt}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message}]
    response = openai_client.chat.completions.create(model=&quot;gpt-4o-mini&quot;, messages=messages)
    assistant_response = response.choices[0].message.content

    # Create new memories from the conversation
    messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: assistant_response})
    memory.add(messages, user_id=user_id)

    return assistant_response

def main():
    print(&quot;Chat with AI (type &#39;exit&#39; to quit)&quot;)
    while True:
        user_input = input(&quot;You: &quot;).strip()
        if user_input.lower() == &#39;exit&#39;:
            print(&quot;Goodbye!&quot;)
            break
        print(f&quot;AI: {chat_with_memories(user_input)}&quot;)

if __name__ == &quot;__main__&quot;:
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the example for &lt;a href=&quot;https://docs.mem0.ai/examples/ai_companion_js&quot;&gt;Node.js&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more advanced usage and API documentation, visit our &lt;a href=&quot;https://docs.mem0.ai&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] For a hassle-free experience, try our &lt;a href=&quot;https://app.mem0.ai&quot;&gt;hosted platform&lt;/a&gt; with automatic updates and enterprise features.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Mem0 - ChatGPT with Memory: A personalized AI chat app powered by Mem0 that remembers your preferences, facts, and memories.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/cebc4f8e-bdb9-4837-868d-13c5ab7bb433&quot;&gt;Mem0 - ChatGPT with Memory&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Try live &lt;a href=&quot;https://mem0.dev/demo/&quot;&gt;demo&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;AI Companion: Experience personalized conversations with an AI that remembers your preferences and past interactions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/3fc72023-a72c-4593-8be0-3cee3ba744da&quot;&gt;AI Companion Demo&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Enhance your AI interactions by storing memories across ChatGPT, Perplexity, and Claude using our browser extension. Get &lt;a href=&quot;https://chromewebstore.google.com/detail/mem0/onihkkbipkfeijkadecaafbgagkhglop?hl=en&quot;&gt;chrome extension&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/ca92e40b-c453-4ff6-b25e-739fb18a8650&quot;&gt;Chrome Extension Demo&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Customer support bot using &lt;strong&gt;Langgraph and Mem0&lt;/strong&gt;. Get the complete code from &lt;a href=&quot;https://docs.mem0.ai/integrations/langgraph&quot;&gt;here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/ca6b482e-7f46-42c8-aa08-f88d1d93a5f4&quot;&gt;Langgraph: Customer Bot&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use Mem0 with CrewAI to get personalized results. Full example &lt;a href=&quot;https://docs.mem0.ai/integrations/crewai&quot;&gt;here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/user-attachments/assets/69172a79-ccb9-4340-91f1-caa7d2dd4213&quot;&gt;CrewAI Demo&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;For detailed usage instructions and API reference, visit our &lt;a href=&quot;https://docs.mem0.ai&quot;&gt;documentation&lt;/a&gt;. You&#39;ll find:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Complete API reference&lt;/li&gt; 
 &lt;li&gt;Integration guides&lt;/li&gt; 
 &lt;li&gt;Advanced configuration options&lt;/li&gt; 
 &lt;li&gt;Best practices and examples&lt;/li&gt; 
 &lt;li&gt;More details about: 
  &lt;ul&gt; 
   &lt;li&gt;Open-source version&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://app.mem0.ai&quot;&gt;Hosted Mem0 Platform&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;Join our community for support and discussions. If you have any questions, feel free to reach out to us using one of the following methods:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://mem0.dev/DiG&quot;&gt;Join our Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://x.com/mem0ai&quot;&gt;Follow us on Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;mailto:founders@mem0.ai&quot;&gt;Email founders&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache 2.0 License - see the &lt;a href=&quot;https://raw.githubusercontent.com/mem0ai/mem0/main/LICENSE&quot;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hacksider/Deep-Live-Cam</title>
      <link>https://github.com/hacksider/Deep-Live-Cam</link>
      <description>&lt;p&gt;real time face swap and one-click video deepfake with only a single image&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&quot;center&quot;&gt;Deep-Live-Cam&lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt; Real-time face swap and video deepfake with a single click and only a single image. &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://trendshift.io/repositories/11395&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11395&quot; alt=&quot;hacksider%2FDeep-Live-Cam | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/demo.gif&quot; alt=&quot;Demo GIF&quot; width=&quot;800&quot;&gt; &lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.&lt;/p&gt; 
&lt;p&gt;We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Ethical Use: Users are expected to use this software responsibly and legally. If using a real person&#39;s face, obtain their consent and clearly label any output as a deepfake when sharing online.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.&lt;/p&gt; 
&lt;p&gt;Users are expected to use this software responsibly and legally. If using a real person&#39;s face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.&lt;/p&gt; 
&lt;h2&gt;Quick Start - Pre-built (Windows / Nvidia)&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://hacksider.gumroad.com/l/vccdmm&quot;&gt; &lt;img src=&quot;https://github.com/user-attachments/assets/7d993b32-e3e8-4cd3-bbfb-a549152ebdd5&quot; width=&quot;285&quot; height=&quot;77&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a href=&quot;https://hacksider.gumroad.com/l/vccdmm&quot;&gt; &lt;h5&gt;This is the fastest build you can get if you have a discrete NVIDIA GPU.&lt;/h5&gt; &lt;h6&gt;These Pre-builts are perfect for non-technical users or those who don&#39;t have time to, or can&#39;t manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually. This will be 60 days ahead on the open source version.&lt;/h6&gt; &lt;h2&gt;TLDR; Live Deepfake in just 3 Clicks&lt;/h2&gt; &lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6&quot; alt=&quot;easysteps&quot;&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Select a face&lt;/li&gt; 
  &lt;li&gt;Select which camera to use&lt;/li&gt; 
  &lt;li&gt;Press live!&lt;/li&gt; 
 &lt;/ol&gt; &lt;h2&gt;Features &amp;amp; Uses - Everything is in real-time&lt;/h2&gt; &lt;h3&gt;Mouth Mask&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Retain your original mouth for accurate movement using Mouth Mask&lt;/strong&gt;&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/ludwig.gif&quot; alt=&quot;resizable-gif&quot;&gt; &lt;/p&gt; &lt;h3&gt;Face Mapping&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Use different faces on multiple subjects simultaneously&lt;/strong&gt;&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/streamers.gif&quot; alt=&quot;face_mapping_source&quot;&gt; &lt;/p&gt; &lt;h3&gt;Your Movie, Your Face&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Watch movies with any face in real-time&lt;/strong&gt;&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/movie.gif&quot; alt=&quot;movie&quot;&gt; &lt;/p&gt; &lt;h3&gt;Live Show&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Run Live shows and performances&lt;/strong&gt;&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/live_show.gif&quot; alt=&quot;show&quot;&gt; &lt;/p&gt; &lt;h3&gt;Memes&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Create Your Most Viral Meme Yet&lt;/strong&gt;&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/meme.gif&quot; alt=&quot;show&quot; width=&quot;450&quot;&gt; &lt;br&gt; &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt; &lt;/p&gt; &lt;h3&gt;Omegle&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Surprise people on Omegle&lt;/strong&gt;&lt;/p&gt; &lt;p align=&quot;center&quot;&gt; 
  &lt;video src=&quot;https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0&quot; width=&quot;450&quot; controls&gt;&lt;/video&gt; &lt;/p&gt; &lt;h2&gt;Installation (Manual)&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the prebuilt version.&lt;/strong&gt;&lt;/p&gt; &lt;/a&gt;
&lt;details&gt;
 &lt;a href=&quot;https://hacksider.gumroad.com/l/vccdmm&quot;&gt; &lt;summary&gt;Click to see the process&lt;/summary&gt; &lt;h3&gt;Installation&lt;/h3&gt; &lt;p&gt;This is more likely to work on your computer but will be slower as it utilizes the CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Set up Your Platform&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Python (3.10 recommended)&lt;/li&gt; 
   &lt;li&gt;pip&lt;/li&gt; 
   &lt;li&gt;git&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=OlNWCpFdVMA&quot;&gt;ffmpeg&lt;/a&gt; - &lt;code&gt;iex (irm ffmpeg.tc.ht)&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://visualstudio.microsoft.com/visual-cpp-build-tools/&quot;&gt;Visual Studio 2022 Runtimes (Windows)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt;&lt;/a&gt; 
 &lt;p&gt;&lt;strong&gt;2. Clone the Repository&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;3. Download the Models&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth&quot;&gt;GFPGANv1.4&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx&quot;&gt;inswapper_128_fp16.onnx&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;Place these files in the &quot;&lt;strong&gt;models&lt;/strong&gt;&quot; folder.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;4. Install Dependencies&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;We highly recommend using a &lt;code&gt;venv&lt;/code&gt; to avoid issues.&lt;/p&gt; 
 &lt;p&gt;For Windows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;For macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Apple Silicon (M1/M2/M3) requires specific setup:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Install Python 3.10 (specific version is important)
brew install python@3.10

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.10
python3.10 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;** In case something goes wrong and you need to reinstall the virtual environment **&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Run:&lt;/strong&gt; If you don&#39;t have a GPU, you can run Deep-Live-Cam using &lt;code&gt;python run.py&lt;/code&gt;. Note that initial execution will download models (~300MB).&lt;/p&gt; 
 &lt;h3&gt;GPU Acceleration&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;CUDA Execution Provider (Nvidia)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install &lt;a href=&quot;https://developer.nvidia.com/cuda-11-8-0-download-archive&quot;&gt;CUDA Toolkit 11.8.0&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.16.3
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start=&quot;3&quot;&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python run.py --execution-provider cuda
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;CoreML Execution Provider (Apple Silicon)&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Apple Silicon (M1/M2/M3) specific installation:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Make sure you&#39;ve completed the macOS setup above using Python 3.10.&lt;/li&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start=&quot;3&quot;&gt; 
  &lt;li&gt;Usage (important: specify Python 3.10):&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python3.10 run.py --execution-provider coreml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Important Notes for macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;You &lt;strong&gt;must&lt;/strong&gt; use Python 3.10, not newer versions like 3.11 or 3.13&lt;/li&gt; 
  &lt;li&gt;Always run with &lt;code&gt;python3.10&lt;/code&gt; command not just &lt;code&gt;python&lt;/code&gt; if you have multiple Python versions installed&lt;/li&gt; 
  &lt;li&gt;If you get error about &lt;code&gt;_tkinter&lt;/code&gt; missing, reinstall the tkinter package: &lt;code&gt;brew reinstall python-tk@3.10&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;If you get model loading errors, check that your models are in the correct folder&lt;/li&gt; 
  &lt;li&gt;If you encounter conflicts with other Python versions, consider uninstalling them: &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# List all installed Python versions
brew list | grep python

# Uninstall conflicting versions if needed
brew uninstall --ignore-dependencies python@3.11 python@3.13

# Keep only Python 3.10
brew cleanup
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;CoreML Execution Provider (Apple Legacy)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.13.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start=&quot;2&quot;&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python run.py --execution-provider coreml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;DirectML Execution Provider (Windows)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.15.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start=&quot;2&quot;&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python run.py --execution-provider directml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;OpenVINO™ Execution Provider (Intel)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.15.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start=&quot;2&quot;&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python run.py --execution-provider openvino
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1. Image/Video Mode&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Execute &lt;code&gt;python run.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Choose a source face image and a target image/video.&lt;/li&gt; 
 &lt;li&gt;Click &quot;Start&quot;.&lt;/li&gt; 
 &lt;li&gt;The output will be saved in a directory named after the target video.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2. Webcam Mode&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Execute &lt;code&gt;python run.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Select a source face image.&lt;/li&gt; 
 &lt;li&gt;Click &quot;Live&quot;.&lt;/li&gt; 
 &lt;li&gt;Wait for the preview to appear (10-30 seconds).&lt;/li&gt; 
 &lt;li&gt;Use a screen capture tool like OBS to stream.&lt;/li&gt; 
 &lt;li&gt;To change the face, select a new source image.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Tips and Tricks&lt;/h2&gt; 
&lt;p&gt;Check out these helpful guides to get the most out of Deep-Live-Cam:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://deeplivecam.net/index.php/blog/tips-and-tricks/unlocking-the-secrets-to-the-perfect-deepfake-image&quot;&gt;Unlocking the Secrets to the Perfect Deepfake Image&lt;/a&gt; - Learn how to create the best deepfake with full head coverage&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://deeplivecam.net/index.php/blog/tips-and-tricks/video-call-with-deeplivecam&quot;&gt;Video Call with DeepLiveCam&lt;/a&gt; - Make your meetings livelier by using DeepLiveCam with OBS and meeting software&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://deeplivecam.net/index.php/blog/tips-and-tricks/have-a-special-guest&quot;&gt;Have a Special Guest!&lt;/a&gt; - Tutorial on how to use face mapping to add special guests to your stream&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://deeplivecam.net/index.php/blog/tips-and-tricks/watch-deepfake-movies-in-realtime&quot;&gt;Watch Deepfake Movies in Realtime&lt;/a&gt; - See yourself star in any video without processing the video&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://deeplivecam.net/index.php/blog/tips-and-tricks/better-quality-without-sacrificing-speed&quot;&gt;Better Quality without Sacrificing Speed&lt;/a&gt; - Tips for achieving better results without impacting performance&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://deeplivecam.net/index.php/blog/tips-and-tricks/instant-vtuber&quot;&gt;Instant Vtuber!&lt;/a&gt; - Create a new persona/vtuber easily using Metahuman Creator&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Visit our &lt;a href=&quot;https://deeplivecam.net/index.php/blog/tips-and-tricks&quot;&gt;official blog&lt;/a&gt; for more tips and tutorials.&lt;/p&gt; 
&lt;h2&gt;Command Line Arguments (Unmaintained)&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program&#39;s version number and exit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.&lt;/p&gt; 
&lt;h2&gt;Press&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;We are always open to criticism and are ready to improve, that&#39;s why we didn&#39;t cherry-pick anything.&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/&quot;&gt;&lt;em&gt;&quot;Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger&quot;&lt;/em&gt;&lt;/a&gt; - Ars Technica&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/&quot;&gt;&lt;em&gt;&quot;Thanks Deep Live Cam, shapeshifters are among us now&quot;&lt;/em&gt;&lt;/a&gt; - Dataconomy&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story&quot;&gt;&lt;em&gt;&quot;This free AI tool lets you become anyone during video-calls&quot;&lt;/em&gt;&lt;/a&gt; - NewsBytes&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying&quot;&gt;&lt;em&gt;&quot;OK, this viral AI live stream software is truly terrifying&quot;&lt;/em&gt;&lt;/a&gt; - Creative Bloq&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/&quot;&gt;&lt;em&gt;&quot;Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo&quot;&lt;/em&gt;&lt;/a&gt; - PetaPixel&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.techeblog.com/deep-live-cam-ai-transform-face/&quot;&gt;&lt;em&gt;&quot;Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included&quot;&lt;/em&gt;&lt;/a&gt; - TechEBlog&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/&quot;&gt;&lt;em&gt;&quot;An AI tool that &quot;makes you look like anyone&quot; during a video call is going viral online&quot;&lt;/em&gt;&lt;/a&gt; - Telegrafi&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts&quot;&gt;&lt;em&gt;&quot;This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts&quot;&lt;/em&gt;&lt;/a&gt; - Emerge&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/&quot;&gt;&lt;em&gt;&quot;New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces&quot;&lt;/em&gt;&lt;/a&gt; - Digital Music News&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/&quot;&gt;&lt;em&gt;&quot;This real-time webcam deepfake tool raises alarms about the future of identity theft&quot;&lt;/em&gt;&lt;/a&gt; - DIYPhotography&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?time_continue=1074&amp;amp;v=py4Tc-Y8BcY&quot;&gt;&lt;em&gt;&quot;That&#39;s Crazy, Oh God. That&#39;s Fucking Freaky Dude... That&#39;s So Wild Dude&quot;&lt;/em&gt;&lt;/a&gt; - SomeOrdinaryGamers&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;amp;t=2686&quot;&gt;&lt;em&gt;&quot;Alright look look look, now look chat, we can do any face we want to look like chat&quot;&lt;/em&gt;&lt;/a&gt; - IShowSpeed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://ffmpeg.org/&quot;&gt;ffmpeg&lt;/a&gt;: for making video-related operations easy&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/deepinsight&quot;&gt;deepinsight&lt;/a&gt;: for their &lt;a href=&quot;https://github.com/deepinsight/insightface&quot;&gt;insightface&lt;/a&gt; project which provided a well-made library and models. Please be reminded that the &lt;a href=&quot;https://github.com/deepinsight/insightface?tab=readme-ov-file#license&quot;&gt;use of the model is for non-commercial research purposes only&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/havok2-htwo&quot;&gt;havok2-htwo&lt;/a&gt;: for sharing the code for webcam&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/GosuDRM&quot;&gt;GosuDRM&lt;/a&gt;: for the open version of roop&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pereiraroland26&quot;&gt;pereiraroland26&lt;/a&gt;: Multiple faces support&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/vic4key&quot;&gt;vic4key&lt;/a&gt;: For supporting/contributing to this project&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/kier007&quot;&gt;kier007&lt;/a&gt;: for improving the user experience&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/qitianai&quot;&gt;qitianai&lt;/a&gt;: for multi-lingual support&lt;/li&gt; 
 &lt;li&gt;and &lt;a href=&quot;https://github.com/hacksider/Deep-Live-Cam/graphs/contributors&quot;&gt;all developers&lt;/a&gt; behind libraries used in this project.&lt;/li&gt; 
 &lt;li&gt;Footnote: Please be informed that the base author of the code is &lt;a href=&quot;https://github.com/s0md3v/roop&quot;&gt;s0md3v&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;All the wonderful users who helped make this project go viral by starring the repo ❤️&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/hacksider/Deep-Live-Cam/stargazers&quot;&gt;&lt;img src=&quot;https://reporoster.com/stars/hacksider/Deep-Live-Cam&quot; alt=&quot;Stargazers&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg?sanitize=true&quot; alt=&quot;Alt&quot; title=&quot;Repobeats analytics image&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;Stars to the Moon 🚀&lt;/h2&gt; 
&lt;a href=&quot;https://star-history.com/#hacksider/deep-live-cam&amp;amp;Date&quot;&gt; 
 &lt;picture&gt; 
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date&amp;amp;theme=dark&quot;&gt; 
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date&quot;&gt; 
  &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date&quot;&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>ocrmypdf/OCRmyPDF</title>
      <link>https://github.com/ocrmypdf/OCRmyPDF</link>
      <description>&lt;p&gt;OCRmyPDF adds an OCR text layer to scanned PDF files, allowing them to be searched&lt;/p&gt;&lt;hr&gt;&lt;img src=&quot;https://raw.githubusercontent.com/ocrmypdf/OCRmyPDF/main/docs/images/logo.svg?sanitize=true&quot; width=&quot;240&quot; alt=&quot;OCRmyPDF&quot;&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/ocrmypdf/OCRmyPDF/actions/workflows/build.yml&quot;&gt;&lt;img src=&quot;https://github.com/ocrmypdf/OCRmyPDF/actions/workflows/build.yml/badge.svg?sanitize=true&quot; alt=&quot;Build Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/ocrmypdf/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/ocrmypdf.svg?sanitize=true&quot; alt=&quot;PyPI version&quot; title=&quot;PyPI version&quot;&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/homebrew/v/ocrmypdf.svg?sanitize=true&quot; alt=&quot;Homebrew version&quot; title=&quot;Homebrew version&quot;&gt; &lt;img src=&quot;https://readthedocs.org/projects/ocrmypdf/badge/?version=latest&quot; alt=&quot;ReadTheDocs&quot; title=&quot;RTD&quot;&gt; &lt;img src=&quot;https://img.shields.io/pypi/pyversions/ocrmypdf&quot; alt=&quot;Python versions&quot; title=&quot;Supported Python versions&quot;&gt;&lt;/p&gt; 
&lt;p&gt;OCRmyPDF adds an OCR text layer to scanned PDF files, allowing them to be searched or copy-pasted.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;ocrmypdf                      # it&#39;s a scriptable command line program
   -l eng+fra                 # it supports multiple languages
   --rotate-pages             # it can fix pages that are misrotated
   --deskew                   # it can deskew crooked PDFs!
   --title &quot;My PDF&quot;           # it can change output metadata
   --jobs 4                   # it uses multiple cores by default
   --output-type pdfa         # it produces PDF/A by default
   input_scanned.pdf          # takes PDF input (or images)
   output_searchable.pdf      # produces validated PDF output
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href=&quot;https://ocrmypdf.readthedocs.io/en/latest/release_notes.html&quot;&gt;See the release notes for details on the latest changes&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Main features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Generates a searchable &lt;a href=&quot;https://en.wikipedia.org/?title=PDF/A&quot;&gt;PDF/A&lt;/a&gt; file from a regular PDF&lt;/li&gt; 
 &lt;li&gt;Places OCR text accurately below the image to ease copy / paste&lt;/li&gt; 
 &lt;li&gt;Keeps the exact resolution of the original embedded images&lt;/li&gt; 
 &lt;li&gt;When possible, inserts OCR information as a &quot;lossless&quot; operation without disrupting any other content&lt;/li&gt; 
 &lt;li&gt;Optimizes PDF images, often producing files smaller than the input file&lt;/li&gt; 
 &lt;li&gt;If requested, deskews and/or cleans the image before performing OCR&lt;/li&gt; 
 &lt;li&gt;Validates input and output files&lt;/li&gt; 
 &lt;li&gt;Distributes work across all available CPU cores&lt;/li&gt; 
 &lt;li&gt;Uses &lt;a href=&quot;https://github.com/tesseract-ocr/tesseract&quot;&gt;Tesseract OCR&lt;/a&gt; engine to recognize more than &lt;a href=&quot;https://github.com/tesseract-ocr/tessdata&quot;&gt;100 languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Keeps your private data private.&lt;/li&gt; 
 &lt;li&gt;Scales properly to handle files with thousands of pages.&lt;/li&gt; 
 &lt;li&gt;Battle-tested on millions of PDFs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/ocrmypdf/OCRmyPDF/main/misc/screencast/demo.svg?sanitize=true&quot; alt=&quot;Demo of OCRmyPDF in a terminal session&quot;&gt; 
&lt;p&gt;For details: please consult the &lt;a href=&quot;https://ocrmypdf.readthedocs.io/en/latest/&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Motivation&lt;/h2&gt; 
&lt;p&gt;I searched the web for a free command line tool to OCR PDF files: I found many, but none of them were really satisfying:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Either they produced PDF files with misplaced text under the image (making copy/paste impossible)&lt;/li&gt; 
 &lt;li&gt;Or they did not handle accents and multilingual characters&lt;/li&gt; 
 &lt;li&gt;Or they changed the resolution of the embedded images&lt;/li&gt; 
 &lt;li&gt;Or they generated ridiculously large PDF files&lt;/li&gt; 
 &lt;li&gt;Or they crashed when trying to OCR&lt;/li&gt; 
 &lt;li&gt;Or they did not produce valid PDF files&lt;/li&gt; 
 &lt;li&gt;On top of that none of them produced PDF/A files (format dedicated for long time storage)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;...so I decided to develop my own tool.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Linux, Windows, macOS and FreeBSD are supported. Docker images are also available, for both x64 and ARM.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Operating system&lt;/th&gt; 
   &lt;th&gt;Install command&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Debian, Ubuntu&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;apt install ocrmypdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows Subsystem for Linux&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;apt install ocrmypdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fedora&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;dnf install ocrmypdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;macOS (Homebrew)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;brew install ocrmypdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;macOS (MacPorts)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;port install ocrmypdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;macOS (nix)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;nix-env -i ocrmypdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LinuxBrew&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;brew install ocrmypdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;FreeBSD&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pkg install py-ocrmypdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ubuntu Snap&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;snap install ocrmypdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For everyone else, &lt;a href=&quot;https://ocrmypdf.readthedocs.io/en/latest/installation.html&quot;&gt;see our documentation&lt;/a&gt; for installation steps.&lt;/p&gt; 
&lt;h2&gt;Languages&lt;/h2&gt; 
&lt;p&gt;OCRmyPDF uses Tesseract for OCR, and relies on its language packs. For Linux users, you can often find packages that provide language packs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Display a list of all Tesseract language packs
apt-cache search tesseract-ocr

# Debian/Ubuntu users
apt-get install tesseract-ocr-chi-sim  # Example: Install Chinese Simplified language pack

# Arch Linux users
pacman -S tesseract-data-eng tesseract-data-deu # Example: Install the English and German language packs

# brew macOS users
brew install tesseract-lang
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then pass the &lt;code&gt;-l LANG&lt;/code&gt; argument to OCRmyPDF to give a hint as to what languages it should search for. Multiple languages can be requested.&lt;/p&gt; 
&lt;p&gt;OCRmyPDF supports Tesseract 4.1.1+. It will automatically use whichever version it finds first on the &lt;code&gt;PATH&lt;/code&gt; environment variable. On Windows, if &lt;code&gt;PATH&lt;/code&gt; does not provide a Tesseract binary, we use the highest version number that is installed according to the Windows Registry.&lt;/p&gt; 
&lt;h2&gt;Documentation and support&lt;/h2&gt; 
&lt;p&gt;Once OCRmyPDF is installed, the built-in help which explains the command syntax and options can be accessed via:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;ocrmypdf --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Our &lt;a href=&quot;https://ocrmypdf.readthedocs.io/en/latest/index.html&quot;&gt;documentation is served on Read the Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Please report issues on our &lt;a href=&quot;https://github.com/ocrmypdf/OCRmyPDF/issues&quot;&gt;GitHub issues&lt;/a&gt; page, and follow the issue template for quick response.&lt;/p&gt; 
&lt;h2&gt;Feature demo&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Add an OCR layer and convert to PDF/A
ocrmypdf input.pdf output.pdf

# Convert an image to single page PDF
ocrmypdf input.jpg output.pdf

# Add OCR to a file in place (only modifies file on success)
ocrmypdf myfile.pdf myfile.pdf

# OCR with non-English languages (look up your language&#39;s ISO 639-3 code)
ocrmypdf -l fra LeParisien.pdf LeParisien.pdf

# OCR multilingual documents
ocrmypdf -l eng+fra Bilingual-English-French.pdf Bilingual-English-French.pdf

# Deskew (straighten crooked pages)
ocrmypdf --deskew input.pdf output.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more features, see the &lt;a href=&quot;https://ocrmypdf.readthedocs.io/en/latest/index.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;In addition to the required Python version, OCRmyPDF requires external program installations of Ghostscript and Tesseract OCR. OCRmyPDF is pure Python, and runs on pretty much everything: Linux, macOS, Windows and FreeBSD.&lt;/p&gt; 
&lt;h2&gt;Press &amp;amp; Media&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://medium.com/@ikirichenko/going-paperless-with-ocrmypdf-e2f36143f46a&quot;&gt;Going paperless with OCRmyPDF&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://medium.com/@treyharris/converting-a-scanned-document-into-a-compressed-searchable-pdf-with-redactions-63f61c34fe4c&quot;&gt;Converting a scanned document into a compressed searchable PDF with redactions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://heise.de/-2279695&quot;&gt;c&#39;t 1-2014, page 59&lt;/a&gt;: Detailed presentation of OCRmyPDF v1.0 in the leading German IT magazine c&#39;t&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://heise.de/-2356670&quot;&gt;heise Open Source, 09/2014: Texterkennung mit OCRmyPDF&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.heise.de/ratgeber/Durchsuchbare-PDF-Dokumente-mit-OCRmyPDF-erstellen-4607592.html&quot;&gt;heise Durchsuchbare PDF-Dokumente mit OCRmyPDF erstellen&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.linuxlinks.com/excellent-utilities-ocrmypdf-add-ocr-text-layer-scanned-pdfs/&quot;&gt;Excellent Utilities: OCRmyPDF&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.linux-community.de/ausgaben/linuxuser/2021/06/texterkennung-mit-ocrmypdf-und-scanbd-automatisieren/&quot;&gt;LinuxUser Texterkennung mit OCRmyPDF und Scanbd automatisieren&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=32028752&quot;&gt;Y Combinator discussion&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Business enquiries&lt;/h2&gt; 
&lt;p&gt;OCRmyPDF would not be the software that it is today without companies and users choosing to provide support for feature development and consulting enquiries. We are happy to discuss all enquiries, whether for extending the existing feature set, or integrating OCRmyPDF into a larger system.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The OCRmyPDF software is licensed under the Mozilla Public License 2.0 (MPL-2.0). This license permits integration of OCRmyPDF with other code, included commercial and closed source, but asks you to publish source-level modifications you make to OCRmyPDF.&lt;/p&gt; 
&lt;p&gt;Some components of OCRmyPDF have other licenses, as indicated by standard SPDX license identifiers or the DEP5 copyright and licensing information file. Generally speaking, non-core code is licensed under MIT, and the documentation and test files are licensed under Creative Commons ShareAlike 4.0 (CC-BY-SA 4.0).&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;The software is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>eosphoros-ai/DB-GPT</title>
      <link>https://github.com/eosphoros-ai/DB-GPT</link>
      <description>&lt;p&gt;AI Native Data App Development framework with AWEL(Agentic Workflow Expression Language) and Agents&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DB-GPT: AI Native Data App Development framework with AWEL(Agentic Workflow Expression Language) and Agents&lt;/h1&gt; 
&lt;p align=&quot;left&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/assets/LOGO.png&quot; width=&quot;100%&quot;&gt; &lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt; &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT&quot;&gt; &lt;img alt=&quot;stars&quot; src=&quot;https://img.shields.io/github/stars/eosphoros-ai/db-gpt?style=social&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT&quot;&gt; &lt;img alt=&quot;forks&quot; src=&quot;https://img.shields.io/github/forks/eosphoros-ai/db-gpt?style=social&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt; &lt;img alt=&quot;License: MIT&quot; src=&quot;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT/releases&quot;&gt; &lt;img alt=&quot;Release Notes&quot; src=&quot;https://img.shields.io/github/release/eosphoros-ai/DB-GPT&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT/issues&quot;&gt; &lt;img alt=&quot;Open Issues&quot; src=&quot;https://img.shields.io/github/issues-raw/eosphoros-ai/DB-GPT&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://discord.gg/7uQnPuveTY&quot;&gt; &lt;img alt=&quot;Discord&quot; src=&quot;https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&amp;amp;style=flat&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://join.slack.com/t/slack-inu2564/shared_invite/zt-29rcnyw2b-N~ubOD9kFc7b7MDOAM1otA&quot;&gt; &lt;img alt=&quot;Slack&quot; src=&quot;https://badgen.net/badge/Slack/Join%20DB-GPT/0abd59?icon=slack&quot;&gt; &lt;/a&gt; &lt;a href=&quot;https://codespaces.new/eosphoros-ai/DB-GPT&quot;&gt; &lt;img alt=&quot;Open in GitHub Codespaces&quot; src=&quot;https://github.com/codespaces/badge.svg?sanitize=true&quot;&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/README.zh.md&quot;&gt;&lt;strong&gt;简体中文&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/README.ja.md&quot;&gt;&lt;strong&gt;日本語&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://discord.gg/7uQnPuveTY&quot;&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://docs.dbgpt.site&quot;&gt;&lt;strong&gt;Documents&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT/raw/main/README.zh.md#%E8%81%94%E7%B3%BB%E6%88%91%E4%BB%AC&quot;&gt;&lt;strong&gt;微信&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://github.com/eosphoros-ai/community&quot;&gt;&lt;strong&gt;Community&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://arxiv.org/pdf/2312.17449.pdf&quot;&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;What is DB-GPT?&lt;/h2&gt; 
&lt;p&gt;🤖 &lt;strong&gt;DB-GPT is an open source AI native data app development framework with AWEL(Agentic Workflow Expression Language) and agents&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;The purpose is to build infrastructure in the field of large models, through the development of multiple technical capabilities such as multi-model management (SMMF), Text2SQL effect optimization, RAG framework and optimization, Multi-Agents framework collaboration, AWEL (agent workflow orchestration), etc. Which makes large model applications with data simpler and more convenient.&lt;/p&gt; 
&lt;p&gt;🚀 &lt;strong&gt;In the Data 3.0 era, based on models and databases, enterprises and developers can build their own bespoke applications with less code.&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;DISCKAIMER&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/DISCKAIMER.md&quot;&gt;disckaimer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;AI-Native Data App&lt;/h3&gt; 
&lt;hr&gt; 
&lt;ul&gt; 
 &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;http://docs.dbgpt.cn/blog/db-gpt-v070-release&quot;&gt;Released V0.7.0 | A set of significant upgrades&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT/pull/2497&quot;&gt;Support MCP Protocol&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-R1&quot;&gt;Support DeepSeek R1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://huggingface.co/Qwen/QwQ-32B&quot;&gt;Support QwQ-32B&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;&quot;&gt;Refactor the basic modules&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/packages/dbgpt-app&quot;&gt;dbgpt-app&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/packages/dbgpt-core&quot;&gt;dbgpt-core&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/packages/dbgpt-serve&quot;&gt;dbgpt-serve&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/packages/dbgpt-client&quot;&gt;dbgpt-client&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/packages/dbgpt-accelerator&quot;&gt;dbgpt-accelerator&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/packages/dbgpt-ext&quot;&gt;dbgpt-ext&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/a2f0a875-df8c-4f0d-89a3-eed321c02113&quot; alt=&quot;app_chat_v0 6&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/c8cc85bb-e3c2-4fab-8fb9-7b4b469d0611&quot; alt=&quot;app_manage_chat_data_v0 6&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/b15d6ebe-54c4-4527-a16d-02fbbaf20dc9&quot; alt=&quot;chat_dashboard_display_v0 6&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/40761507-a1e1-49d4-b49a-3dd9a5ea41cc&quot; alt=&quot;agent_prompt_awel_v0 6&quot;&gt;&lt;/p&gt; 
&lt;h2&gt;Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/#install&quot;&gt;Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/#features&quot;&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/#contribution&quot;&gt;Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/#contact-information&quot;&gt;Contact&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;The architecture of DB-GPT is shown in the following figure:&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/assets/dbgpt.png&quot; width=&quot;800&quot;&gt; &lt;/p&gt; 
&lt;p&gt;The core capabilities include the following parts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;RAG (Retrieval Augmented Generation)&lt;/strong&gt;: RAG is currently the most practically implemented and urgently needed domain. DB-GPT has already implemented a framework based on RAG, allowing users to build knowledge-based applications using the RAG capabilities of DB-GPT.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;GBI (Generative Business Intelligence)&lt;/strong&gt;: Generative BI is one of the core capabilities of the DB-GPT project, providing the foundational data intelligence technology to build enterprise report analysis and business insights.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fine-tuning Framework&lt;/strong&gt;: Model fine-tuning is an indispensable capability for any enterprise to implement in vertical and niche domains. DB-GPT provides a complete fine-tuning framework that integrates seamlessly with the DB-GPT project. In recent fine-tuning efforts, an accuracy rate based on the Spider dataset has been achieved at 82.5%.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data-Driven Multi-Agents Framework&lt;/strong&gt;: DB-GPT offers a data-driven self-evolving multi-agents framework, aiming to continuously make decisions and execute based on data.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data Factory&lt;/strong&gt;: The Data Factory is mainly about cleaning and processing trustworthy knowledge and data in the era of large models.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data Sources&lt;/strong&gt;: Integrating various data sources to seamlessly connect production business data to the core capabilities of DB-GPT.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;SubModule&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT-Hub&quot;&gt;DB-GPT-Hub&lt;/a&gt; Text-to-SQL workflow with high performance by applying Supervised Fine-Tuning (SFT) on Large Language Models (LLMs).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/eosphoros-ai/dbgpts&quot;&gt;dbgpts&lt;/a&gt; dbgpts is the official repository which contains some data apps、AWEL operators、AWEL workflow templates and agents which build upon DB-GPT.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Text2SQL Finetune&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;support llms 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; LLaMA&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; LLaMA-2&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; BLOOM&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; BLOOMZ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; Falcon&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; Baichuan&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; Baichuan2&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; InternLM&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; Qwen&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; XVERSE&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled&gt; ChatGLM2&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT-Hub&quot;&gt;More Information about Text2SQL finetune&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT-Plugins&quot;&gt;DB-GPT-Plugins&lt;/a&gt; DB-GPT Plugins that can run Auto-GPT plugin directly&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/eosphoros-ai/GPT-Vis&quot;&gt;GPT-Vis&lt;/a&gt; Visualization protocol&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&amp;amp;logo=docker&amp;amp;logoColor=white&quot; alt=&quot;Docker&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&amp;amp;logo=linux&amp;amp;logoColor=black&quot; alt=&quot;Linux&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&amp;amp;logo=macos&amp;amp;logoColor=F0F0F0&quot; alt=&quot;macOS&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&amp;amp;logo=windows&amp;amp;logoColor=white&quot; alt=&quot;Windows&quot;&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/overview&quot;&gt;&lt;strong&gt;Usage Tutorial&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/installation&quot;&gt;&lt;strong&gt;Install&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/installation/docker&quot;&gt;Docker&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/installation/sourcecode&quot;&gt;Source Code&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/quickstart&quot;&gt;&lt;strong&gt;Quickstart&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/operation_manual&quot;&gt;&lt;strong&gt;Application&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/cookbook/app/data_analysis_app_develop&quot;&gt;Development Guide&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/application/app_usage&quot;&gt;App Usage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/application/awel_flow_usage&quot;&gt;AWEL Flow Usage&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/operation_manual/advanced_tutorial/debugging&quot;&gt;&lt;strong&gt;Debugging&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/application/advanced_tutorial/cli&quot;&gt;&lt;strong&gt;Advanced Usage&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/application/advanced_tutorial/smmf&quot;&gt;SMMF&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/application/fine_tuning_manual/dbgpt_hub&quot;&gt;Finetune&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/awel/tutorial&quot;&gt;AWEL&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;At present, we have introduced several key features to showcase our current capabilities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Private Domain Q&amp;amp;A &amp;amp; Data Processing&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The DB-GPT project offers a range of functionalities designed to improve knowledge base construction and enable efficient storage and retrieval of both structured and unstructured data. These functionalities include built-in support for uploading multiple file formats, the ability to integrate custom data extraction plug-ins, and unified vector storage and retrieval capabilities for effectively managing large volumes of information.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-Data Source &amp;amp; GBI(Generative Business intelligence)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The DB-GPT project facilitates seamless natural language interaction with diverse data sources, including Excel, databases, and data warehouses. It simplifies the process of querying and retrieving information from these sources, empowering users to engage in intuitive conversations and gain insights. Moreover, DB-GPT supports the generation of analytical reports, providing users with valuable data summaries and interpretations.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-Agents&amp;amp;Plugins&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It offers support for custom plug-ins to perform various tasks and natively integrates the Auto-GPT plug-in model. The Agents protocol adheres to the Agent Protocol standard.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Automated Fine-tuning text2SQL&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We&#39;ve also developed an automated fine-tuning lightweight framework centred on large language models (LLMs), Text2SQL datasets, LoRA/QLoRA/Pturning, and other fine-tuning methods. This framework simplifies Text-to-SQL fine-tuning, making it as straightforward as an assembly line process. &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT-Hub&quot;&gt;DB-GPT-Hub&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;SMMF(Service-oriented Multi-model Management Framework)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We offer extensive model support, including dozens of large language models (LLMs) from both open-source and API agents, such as LLaMA/LLaMA2, Baichuan, ChatGLM, Wenxin, Tongyi, Zhipu, and many more.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;News 
    &lt;ul&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/QwQ-32B&quot;&gt;QwQ-32B&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/deepseek-ai/DeepSeek-R1&quot;&gt;DeepSeek-R1&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/deepseek-ai/DeepSeek-V3&quot;&gt;DeepSeek-V3&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B&quot;&gt;DeepSeek-R1-Distill-Llama-70B&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B&quot;&gt;DeepSeek-R1-Distill-Qwen-32B&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B&quot;&gt;DeepSeek-R1-Distill-Qwen-14B&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B&quot;&gt;DeepSeek-R1-Distill-Llama-8B&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B&quot;&gt;DeepSeek-R1-Distill-Qwen-7B&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B&quot;&gt;DeepSeek-R1-Distill-Qwen-1.5B&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct&quot;&gt;Qwen2.5-Coder-32B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct&quot;&gt;Qwen2.5-Coder-14B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-72B-Instruct&quot;&gt;Qwen2.5-72B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-32B-Instruct&quot;&gt;Qwen2.5-32B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-14B-Instruct&quot;&gt;Qwen2.5-14B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-7B-Instruct&quot;&gt;Qwen2.5-7B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-3B-Instruct&quot;&gt;Qwen2.5-3B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct&quot;&gt;Qwen2.5-1.5B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct&quot;&gt;Qwen2.5-0.5B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct&quot;&gt;Qwen2.5-Coder-7B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct&quot;&gt;Qwen2.5-Coder-1.5B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct&quot;&gt;Meta-Llama-3.1-405B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct&quot;&gt;Meta-Llama-3.1-70B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct&quot;&gt;Meta-Llama-3.1-8B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/google/gemma-2-27b-it&quot;&gt;gemma-2-27b-it&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/google/gemma-2-9b-it&quot;&gt;gemma-2-9b-it&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct&quot;&gt;DeepSeek-Coder-V2-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct&quot;&gt;DeepSeek-Coder-V2-Lite-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct&quot;&gt;Qwen2-57B-A14B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-72B-Instruct&quot;&gt;Qwen2-72B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-7B-Instruct&quot;&gt;Qwen2-7B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-1.5B-Instruct&quot;&gt;Qwen2-1.5B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-0.5B-Instruct&quot;&gt;Qwen2-0.5B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/THUDM/glm-4-9b-chat&quot;&gt;glm-4-9b-chat&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3&quot;&gt;Phi-3&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/01-ai/Yi-1.5-34B-Chat&quot;&gt;Yi-1.5-34B-Chat&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/01-ai/Yi-1.5-9B-Chat&quot;&gt;Yi-1.5-9B-Chat&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/01-ai/Yi-1.5-6B-Chat&quot;&gt;Yi-1.5-6B-Chat&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen1.5-110B-Chat&quot;&gt;Qwen1.5-110B-Chat&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B-Chat&quot;&gt;Qwen1.5-MoE-A2.7B-Chat&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct&quot;&gt;Meta-Llama-3-70B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct&quot;&gt;Meta-Llama-3-8B-Instruct&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat&quot;&gt;CodeQwen1.5-7B-Chat&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen1.5-32B-Chat&quot;&gt;Qwen1.5-32B-Chat&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Nexusflow/Starling-LM-7B-beta&quot;&gt;Starling-LM-7B-beta&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/google/gemma-7b-it&quot;&gt;gemma-7b-it&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/google/gemma-2b-it&quot;&gt;gemma-2b-it&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0&quot;&gt;SOLAR-10.7B&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1&quot;&gt;Mixtral-8x7B&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/Qwen/Qwen-72B-Chat&quot;&gt;Qwen-72B-Chat&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;🔥🔥🔥 &lt;a href=&quot;https://huggingface.co/01-ai/Yi-34B-Chat&quot;&gt;Yi-34B-Chat&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.site/docs/modules/smmf&quot;&gt;More Supported LLMs&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Privacy and Security&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We ensure the privacy and security of data through the implementation of various technologies, including privatized large models and proxy desensitization.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Support Datasources&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;http://docs.dbgpt.cn/docs/modules/connections&quot;&gt;Datasources&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Image&lt;/h2&gt; 
&lt;p&gt;🌐 &lt;a href=&quot;https://www.codewithgpu.com/i/eosphoros-ai/DB-GPT/dbgpt&quot;&gt;AutoDL Image&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Language Switching&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;In the .env configuration file, modify the LANGUAGE parameter to switch to different languages. The default is English (Chinese: zh, English: en, other languages to be added later).
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;To check detailed guidelines for new contributions, please refer &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT/raw/main/CONTRIBUTING.md&quot;&gt;how to contribute&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributors Wall&lt;/h3&gt; 
&lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=eosphoros-ai/DB-GPT&amp;amp;max=200&quot;&gt; &lt;/a&gt; 
&lt;h2&gt;Licence&lt;/h2&gt; 
&lt;p&gt;The MIT License (MIT)&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you want to understand the overall architecture of DB-GPT, please cite &lt;a href=&quot;https://arxiv.org/abs/2312.17449&quot; target=&quot;_blank&quot;&gt;paper&lt;/a&gt; and &lt;a href=&quot;https:// arxiv.org/abs/2404.10209&quot; target=&quot;_blank&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you want to learn about using DB-GPT for Agent development, please cite the &lt;a href=&quot;https://arxiv.org/abs/2412.13520&quot; target=&quot;_blank&quot;&gt;paper&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@article{xue2023dbgpt,
      title={DB-GPT: Empowering Database Interactions with Private Large Language Models}, 
      author={Siqiao Xue and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Danrui Qi and Hong Yi and Shaodong Liu and Faqiang Chen},
      year={2023},
      journal={arXiv preprint arXiv:2312.17449},
      url={https://arxiv.org/abs/2312.17449}
}
@misc{huang2024romasrolebasedmultiagentdatabase,
      title={ROMAS: A Role-Based Multi-Agent System for Database monitoring and Planning}, 
      author={Yi Huang and Fangyin Cheng and Fan Zhou and Jiahui Li and Jian Gong and Hongjun Yang and Zhidong Fan and Caigao Jiang and Siqiao Xue and Faqiang Chen},
      year={2024},
      eprint={2412.13520},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.13520}, 
}
@inproceedings{xue2024demonstration,
      title={Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models}, 
      author={Siqiao Xue and Danrui Qi and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Hong Yi and Shaodong Liu and Hongjun Yang and Faqiang Chen},
      year={2024},
      booktitle = &quot;Proceedings of the VLDB Endowment&quot;,
      url={https://arxiv.org/abs/2404.10209}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact Information&lt;/h2&gt; 
&lt;p&gt;We are working on building a community, if you have any ideas for building the community, feel free to contact us. &lt;a href=&quot;https://discord.gg/7uQnPuveTY&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&amp;amp;style=flat&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://star-history.com/#csunny/DB-GPT&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=csunny/DB-GPT&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot;&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
